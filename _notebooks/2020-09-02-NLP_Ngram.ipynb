{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.047023Z",
     "start_time": "2019-09-06T18:06:40.772432Z"
    }
   },
   "source": [
    "# NLP ngrams With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.', from [wikipedia](https://en.wikipedia.org/wiki/N-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../../processed_data/nf_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.052605Z",
     "start_time": "2019-09-06T18:06:41.048997Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Total Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.058253Z",
     "start_time": "2019-09-06T18:06:41.054381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 272025 words in the combination of all abstracts.\n"
     ]
    }
   ],
   "source": [
    "text = \" \".join(review for review in df.abstract)\n",
    "print (\"There are {} words in the combination of all abstracts.\".format(len(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.110142Z",
     "start_time": "2019-09-06T18:06:41.059783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate . ‚Äù and\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    # Remove newlines and quotes\n",
    "    text = text.replace('\\n', ' ');\n",
    "    text = text.replace('\"', '');\n",
    "\n",
    "    # Make sure punctuation marks are treated as their own \"words,\"\n",
    "    # so that they will be included in the Markov chain\n",
    "    punctuation = [',','.',';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol));\n",
    "\n",
    "    words = text.split(' ')\n",
    "    # Filter out empty words\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "                # Create a new dictionary for this word\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] += 1\n",
    "    return wordDict\n",
    "\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "#Generate a Markov chain of length 100\n",
    "length = 100\n",
    "chain = ['Vietnam']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "\n",
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.118280Z",
     "start_time": "2019-09-06T18:06:41.111721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFirstSentenceContaining(ngram, text):\n",
    "    #print(ngram)\n",
    "    sentences = text.upper().split(\". \")\n",
    "    for sentence in sentences: \n",
    "        if ngram in sentence:\n",
    "            return sentence+'\\n'\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print(getFirstSentenceContaining('I', text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.143499Z",
     "start_time": "2019-09-06T18:06:41.120150Z"
    }
   },
   "outputs": [],
   "source": [
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.307501Z",
     "start_time": "2019-09-06T18:06:41.147699Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "def cleanInput(content):\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "\n",
    "\n",
    "content = str(text)\n",
    "\n",
    "ngrams = getNgrams(content, 3)\n",
    "#print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.474129Z",
     "start_time": "2019-09-06T18:06:41.309231Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def isCommon(ngram):\n",
    "    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n",
    "    for word in ngram:\n",
    "        if word in commonWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        if not isCommon(content[i:i+n]):\n",
    "            output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "ngrams = getNgrams(content, 3)\n",
    "#print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.506030Z",
     "start_time": "2019-09-06T18:06:41.476051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD.\n",
      "   THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE ‚ÄòOTHER SIDE,‚Äô ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION\n",
      "\n",
      "COLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTION‚ÄôS INTELLECTUAL AND HISTORICAL FRAMEWORK OF ‚ÄúPREPAREDNESS‚Äù TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR\n",
      "\n",
      " I ARGUE THAT A NORM PROTECTING STATES‚Äô TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS\n",
      "\n",
      " IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS.\n",
      "         THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFirstSentenceContaining(ngram, content):\n",
    "    #print(ngram)\n",
    "    sentences = content.upper().split(\". \")\n",
    "    for sentence in sentences: \n",
    "        if ngram in sentence:\n",
    "            return sentence+'\\n'\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print(getFirstSentenceContaining('SINO-JAPANESE WAR 1894-1895', content))\n",
    "print(getFirstSentenceContaining('2ND VIETNAM WAR', content))\n",
    "print(getFirstSentenceContaining('COLD WAR ARMY', content))\n",
    "print(getFirstSentenceContaining('WORLD WAR II', content))\n",
    "print(getFirstSentenceContaining('ARMS CONTROL AGREEMENTS', content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.584864Z",
     "start_time": "2019-09-06T18:06:41.508439Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    # Remove newlines and quotes\n",
    "    text = text.replace('\\n', ' ');\n",
    "    text = text.replace('\"', '');\n",
    "\n",
    "    # Make sure punctuation marks are treated as their own \"words,\"\n",
    "    # so that they will be included in the Markov chain\n",
    "    punctuation = [',','.',';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol));\n",
    "\n",
    "    words = text.split(' ')\n",
    "    # Filter out empty words\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "                # Create a new dictionary for this word\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] += 1\n",
    "    return wordDict\n",
    "\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "#Generate a Markov chain of length 100\n",
    "length = 100\n",
    "chain = ['I']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "\n",
    "#print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.590948Z",
     "start_time": "2019-09-06T18:06:41.586488Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    content = content.split(' ')\n",
    "    content = [word for word in content if word != '']\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.599373Z",
     "start_time": "2019-09-06T18:06:41.592891Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.729865Z",
     "start_time": "2019-09-06T18:06:41.601365Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(getNgrams(content, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:06:41.851755Z",
     "start_time": "2019-09-06T18:06:41.731696Z"
    }
   },
   "outputs": [],
   "source": [
    "def isCommon(ngram):\n",
    "    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n",
    "    for word in ngram:\n",
    "        if word in commonWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNgramsFromSentence(text, n):\n",
    "    output = []\n",
    "    for i in range(len(text)-n+1):\n",
    "        if not isCommon(text[i:i+n]):\n",
    "            output.append(text[i:i+n])\n",
    "    return output\n",
    "\n",
    "ngrams = getNgrams(text, 3)\n",
    "#print(ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}