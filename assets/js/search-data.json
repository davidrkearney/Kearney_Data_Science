{
  
    
        "post0": {
            "title": "Title",
            "content": "from pyspark.sql.functions import col from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType from pyspark.sql.functions import * . # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.createOrReplaceTempView(&quot;fiscal_stats&quot;) sums = spark.sql(&quot;&quot;&quot; select year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr from fiscal_stats group by 1 order by year asc &quot;&quot;&quot;) sums.show() . +-+++ year|total_yearly_it|total_yearly_fr| +-+++ 1996| 19825341| 2.9579215E7| 1997| 21391321| 2.9110765E7| 1998| 25511453| 3.8154711E7| 1999| 31922107| 4.2128627E7| 2000| 38721293| 4.8288092E7| 2001| 50754944| 5.8910649E7| 2002| 62375881| 6.2071474E7| 2003| 69316709| 7.2479293E7| 2004| 88626786| null| 2005| 98263665| null| 2006| 119517822| 1.3349148E8| 2007| 153467611| 2.27385701E8| +-+++ df.describe().toPandas().transpose() . 0 1 2 3 4 . summary count | mean | stddev | min | max | . _c0 360 | 179.5 | 104.06728592598157 | 0 | 359 | . province 360 | None | None | Anhui | Zhejiang | . specific 356 | 583470.7303370787 | 654055.3290782663 | 8964.0 | 3937966.0 | . general 169 | 309127.53846153844 | 355423.5760674793 | 0.0 | 1737800.0 | . year 360 | 2001.5 | 3.4568570586927794 | 1996 | 2007 | . gdp 360 | 4428.653416666667 | 4484.668659976412 | 64.98 | 31777.01 | . fdi 360 | 196139.38333333333 | 303043.97011891654 | 2 | 1743140 | . rnr 294 | 0.0355944252244898 | 0.16061503029299648 | 0.0 | 1.214285714 | . rr 296 | 0.059688621057432424 | 0.15673351824073453 | 0.0 | 0.84 | . i 287 | 0.08376351662369343 | 0.1838933104683607 | 0.0 | 1.05 | . fr 295 | 2522449.0034013605 | 3491329.8613106664 | #REF! | 9898522 | . reg 360 | None | None | East China | Southwest China | . it 360 | 2165819.2583333333 | 1769294.2935487411 | 147897 | 10533312 | . df2 = df.withColumn(&quot;gdp&quot;,col(&quot;gdp&quot;).cast(IntegerType())) .withColumn(&quot;specific&quot;,col(&quot;specific&quot;).cast(IntegerType())) .withColumn(&quot;general&quot;,col(&quot;general&quot;).cast(IntegerType())) .withColumn(&quot;year&quot;,col(&quot;year&quot;).cast(IntegerType())) .withColumn(&quot;fdi&quot;,col(&quot;fdi&quot;).cast(IntegerType())) .withColumn(&quot;rnr&quot;,col(&quot;rnr&quot;).cast(IntegerType())) .withColumn(&quot;rr&quot;,col(&quot;rr&quot;).cast(IntegerType())) .withColumn(&quot;i&quot;,col(&quot;i&quot;).cast(IntegerType())) .withColumn(&quot;fr&quot;,col(&quot;fr&quot;).cast(IntegerType())) . df2.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import LinearRegression assembler = VectorAssembler(inputCols=[&#39;gdp&#39;, &#39;fdi&#39;], outputCol=&quot;features&quot;) train_df = assembler.transform(df2) . train_df.select(&quot;specific&quot;, &quot;year&quot;).show() . +--+-+ specific|year| +--+-+ 147002|1996| 151981|1997| 174930|1998| 285324|1999| 195580|2000| 250898|2001| 434149|2002| 619201|2003| 898441|2004| 898441|2005| 1457872|2006| 2213991|2007| 165957|1996| 165957|1997| 245198|1998| 388083|1999| 281769|2000| 441923|2001| 558569|2002| 642581|2003| +--+-+ only showing top 20 rows lr = LinearRegression(featuresCol = &#39;features&#39;, labelCol=&#39;it&#39;) lr_model = lr.fit(train_df) trainingSummary = lr_model.summary print(&quot;Coefficients: &quot; + str(lr_model.coefficients)) print(&quot;RMSE: %f&quot; % trainingSummary.rootMeanSquaredError) print(&quot;R2: %f&quot; % trainingSummary.r2) . Coefficients: [495.05888709337756,-4.968141828763066] RMSE: 1234228.673087 R2: 0.512023 lr_predictions = lr_model.transform(train_df) lr_predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show(5) from pyspark.ml.evaluation import RegressionEvaluator lr_evaluator = RegressionEvaluator(predictionCol=&quot;prediction&quot;, labelCol=&quot;it&quot;,metricName=&quot;r2&quot;) . ++-+-+ prediction| it| features| ++-+-+ 1732528.7382477913| 631930|[2093.0,50661.0]| 1894133.7432895212| 657860|[2347.0,43443.0]| 2069017.8229123235| 889463|[2542.0,27673.0]| 2160838.7084181504|1227364|[2712.0,26131.0]| 2226501.9982726825|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows print(&quot;R Squared (R2) on test data = %g&quot; % lr_evaluator.evaluate(lr_predictions)) . R Squared (R2) on test data = 0.512023 print(&quot;numIterations: %d&quot; % trainingSummary.totalIterations) print(&quot;objectiveHistory: %s&quot; % str(trainingSummary.objectiveHistory)) trainingSummary.residuals.show() . numIterations: 1 objectiveHistory: [0.0] +-+ residuals| +-+ -1100598.7382477913| -1236273.7432895212| -1179554.8229123235| -933474.7084181504| -727391.9982726825| -222546.39659531135| -94585.30175113119| 108072.63313654158| 389732.58121094666| 621021.2194867637| 1885768.997742407| 3938310.059555837| -554084.125169754| -615660.3899049093| -352195.3468934437| -348450.00565795833| -918476.5594253046| -710059.9133252408| -1148661.0062004486| -911572.322055324| +-+ only showing top 20 rows predictions = lr_model.transform(test_df) predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show() . ++-++ prediction| it| features| ++-++ 976371.9212205639| 306114| [64.0,679.0]| 990722.2032541803| 415547| [91.0,481.0]| 1016348.0830204486| 983251| [139.0,106.0]| 1036290.7062801318| 218361| [184.0,576.0]| 1034023.4471330958| 178668| [202.0,2826.0]| 1060130.0768520113| 274994| [245.0,1856.0]| 1023513.0851009073| 546541|[263.0,11020.0]| 1053250.6267921| 361358| [264.0,5134.0]| 1123768.8091592425| 866691| [377.0,2200.0]| 1128604.8330225947| 948521| [390.0,2522.0]| 810587.2575938476| 177748|[442.0,71715.0]| 1159703.254297337| 736165| [445.0,1743.0]| 1066975.770986663|1260633|[466.0,22500.0]| 1288507.6625716756|1423771| [725.0,3718.0]| 1320055.238474972| 573905| [793.0,4144.0]| 1188611.0570700848|2347862|[797.0,31000.0]| 1321857.482976733| 582711| [805.0,4977.0]| 1033849.5995896922| 746784|[819.0,64343.0]| 1445051.792853667|1216605|[1029.0,2501.0]| 1437887.1056682135|1258100|[1052.0,6235.0]| ++-++ only showing top 20 rows from pyspark.ml.regression import DecisionTreeRegressor dt = DecisionTreeRegressor(featuresCol =&#39;features&#39;, labelCol = &#39;it&#39;) dt_model = dt.fit(train_df) dt_predictions = dt_model.transform(train_df) dt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = dt_evaluator.evaluate(dt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . Root Mean Squared Error (RMSE) on test data = 1.01114e+06 from pyspark.ml.regression import GBTRegressor gbt = GBTRegressor(featuresCol = &#39;features&#39;, labelCol = &#39;it&#39;, maxIter=10) gbt_model = gbt.fit(train_df) gbt_predictions = gbt_model.transform(train_df) gbt_predictions.select(&#39;prediction&#39;, &#39;it&#39;, &#39;features&#39;).show(5) gbt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = gbt_evaluator.evaluate(gbt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . ++-+-+ prediction| it| features| ++-+-+ 1388898.308543053| 631930|[2093.0,50661.0]| 1388898.308543053| 657860|[2347.0,43443.0]| 1649083.6277172007| 889463|[2542.0,27673.0]| 1649083.6277172007|1227364|[2712.0,26131.0]| 1649083.6277172007|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows Root Mean Squared Error (RMSE) on test data = 778728",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "relUrl": "/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Science, Big Data and Healthcare Research",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. Here’s a footnote 1. . Footnotes . links &#8617; . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/markdown/2020/06/08/1st-markdown-post.html",
            "relUrl": "/markdown/2020/06/08/1st-markdown-post.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "# &quot;# Calculating time with machine learning functions&quot; &gt; &quot;Calculating time with machine learning functions&quot; - toc:true- branch: master- badges: true- comments: true - author: DRK - categories: [jupyter] . &quot;&quot;&quot;A timer for ML functions&quot;&quot;&quot; . &#39;A timer for ML functions&#39; . #collapse-hide from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper . . @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . from collections import Counter import math, random # # data splitting # def split_data(data, prob): &quot;&quot;&quot;split data into fractions [prob, 1 - prob]&quot;&quot;&quot; results = [], [] for row in data: results[0 if random.random() &lt; prob else 1].append(row) return results def train_test_split(x, y, test_pct): data = list(zip(x, y)) # pair corresponding values train, test = split_data(data, 1 - test_pct) # split the dataset of pairs x_train, y_train = list(zip(*train)) # magical un-zip trick x_test, y_test = list(zip(*test)) return x_train, x_test, y_train, y_test # # correctness # def accuracy(tp, fp, fn, tn): correct = tp + tn total = tp + fp + fn + tn return correct / total def precision(tp, fp, fn, tn): return tp / (tp + fp) def recall(tp, fp, fn, tn): return tp / (tp + fn) def f1_score(tp, fp, fn, tn): p = precision(tp, fp, fn, tn) r = recall(tp, fp, fn, tn) return 2 * p * r / (p + r) if __name__ == &quot;__main__&quot;: print(&quot;accuracy(70, 4930, 13930, 981070)&quot;, accuracy(70, 4930, 13930, 981070)) print(&quot;precision(70, 4930, 13930, 981070)&quot;, precision(70, 4930, 13930, 981070)) print(&quot;recall(70, 4930, 13930, 981070)&quot;, recall(70, 4930, 13930, 981070)) print(&quot;f1_score(70, 4930, 13930, 981070)&quot;, f1_score(70, 4930, 13930, 981070)) . favorite_number = 7 def add(a, b): return a + b def sub(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): return a / b def count_vowels(word): count = 0 for letter in word.lower(): count += letter in &#39;aeiou&#39; return count . import example_module as sm print(sm.favorite_number) # add two numbers together print(sm.add(3, 8)) # count the number of vowels in a string print(sm.count_vowels(&#39;Testing&#39;)) . import pandas as pd from alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler config_handler.set_global(theme=&#39;ascii&#39;, spinner=&#39;notes&#39;, bar=&#39;solid&#39;) with alive_bar(3) as bar: df = pd.read_csv(&#39;../../data/csvs/example.csv&#39;) bar(&#39;file read, printing file&#39;) print(df.head) bar(&#39;data printed ok, printing methods of data&#39;) print(dir(df)) bar(&#39;process complete&#39;) . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-3-24a2950a8375&gt; in &lt;module&gt; 4 5 with alive_bar(3) as bar: -&gt; 6 df = pd.read_csv(&#39;../../data/csvs/example.csv&#39;) 7 bar(&#39;file read, printing file&#39;) 8 print(df.head) ~/anaconda3/envs/insight/lib/python3.7/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision) 674 ) 675 --&gt; 676 return _read(filepath_or_buffer, kwds) 677 678 parser_f.__name__ = name ~/anaconda3/envs/insight/lib/python3.7/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds) 446 447 # Create the parser. --&gt; 448 parser = TextFileReader(fp_or_buf, **kwds) 449 450 if chunksize or iterator: ~/anaconda3/envs/insight/lib/python3.7/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds) 878 self.options[&#34;has_index_names&#34;] = kwds[&#34;has_index_names&#34;] 879 --&gt; 880 self._make_engine(self.engine) 881 882 def close(self): ~/anaconda3/envs/insight/lib/python3.7/site-packages/pandas/io/parsers.py in _make_engine(self, engine) 1112 def _make_engine(self, engine=&#34;c&#34;): 1113 if engine == &#34;c&#34;: -&gt; 1114 self._engine = CParserWrapper(self.f, **self.options) 1115 else: 1116 if engine == &#34;python&#34;: ~/anaconda3/envs/insight/lib/python3.7/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds) 1889 kwds[&#34;usecols&#34;] = self.usecols 1890 -&gt; 1891 self._reader = parsers.TextReader(src, **kwds) 1892 self.unnamed_cols = self._reader.unnamed_cols 1893 pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source() FileNotFoundError: [Errno 2] File ../../data/csvs/example.csv does not exist: &#39;../../data/csvs/example.csv&#39; . from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/06/07/kwargs-decorators.html",
            "relUrl": "/2020/06/07/kwargs-decorators.html",
            "date": " • Jun 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "ML Metrics",
            "content": "Front Matter . # &quot;# ML Metrics&quot; &gt; &quot;Metrics for ml models&quot; - toc:true- branch: master- badges: true- comments: true - author: - categories: [jupyter] . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 | The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 | First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 | I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 | Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 | Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/02/20/test.html",
            "relUrl": "/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}