{
  
    
        "post0": {
            "title": "Setting up pyspark locally, for development.",
            "content": "Welcome to ____ __ / __/__ ___ _____/ /__ _ / _ / _ `/ __/ &#39;_/ /__ / .__/ _,_/_/ /_/ _ version 2.4.4 /_/ Using Python version 3.7.3 (default, Mar 27 2019 22:11:17) SparkSession available as &#39;spark&#39;. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/big%20data/2020/11/11/pyspark-shell.html",
            "relUrl": "/big%20data/2020/11/11/pyspark-shell.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Dask API for analytics",
            "content": "Credit: code from https://github.com/coiled/data-science-at-scale . from dask.distributed import Client client = Client(n_workers=4) client . /opt/venv/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use. Perhaps you already have a cluster running? Hosting the HTTP server on port 45123 instead http_address[&#34;port&#34;], self.http_server.port . Client . Scheduler: tcp://127.0.0.1:43829 | Dashboard: http://127.0.0.1:45123/status | . | Cluster . Workers: 4 | Cores: 4 | Memory: 5.00 GB | . | . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df import dask.dataframe as dd import aiohttp ddf = dd.read_csv( url, blocksize=&quot;10 MiB&quot;, ).persist() . ddf . Dask DataFrame Structure: Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . npartitions=1 . int64 | object | float64 | float64 | int64 | float64 | int64 | float64 | float64 | float64 | object | object | int64 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: read-csv, 1 tasks # See that we actually have a collection of Pandas DataFrames ddf.map_partitions(type).compute() . 0 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; dtype: object . # View head of Dask DataFrame ddf.head() . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . gdp = ddf.groupby(&#39;province&#39;).gdp.mean() gdp.compute() . province Anhui 3905.870000 Beijing 4673.453333 Chongqing 2477.712500 Fujian 4864.023333 Gansu 1397.832500 Guangdong 15358.781667 Guangxi 2924.104167 Guizhou 1422.010833 Hainan 686.714167 Hebei 6936.825000 Heilongjiang 4041.241667 Henan 7208.966667 Hubei 4772.503333 Hunan 4765.891667 Jiangsu 10761.846667 Jiangxi 2460.782500 Jilin 2274.854167 Liaoning 5231.135000 Ningxia 432.268333 Qinghai 383.099167 Shaanxi 2658.034167 Shandong 12324.002500 Shanghai 6432.454167 Shanxi 2817.210833 Sichuan 5377.790000 Tianjin 2528.665000 Tibet 170.426667 Xinjiang 1828.896667 Yunnan 2604.054167 Zhejiang 9138.151667 Name: gdp, dtype: float64 . gdp.compute().sort_values() . province Tibet 170.426667 Qinghai 383.099167 Ningxia 432.268333 Hainan 686.714167 Gansu 1397.832500 Guizhou 1422.010833 Xinjiang 1828.896667 Jilin 2274.854167 Jiangxi 2460.782500 Chongqing 2477.712500 Tianjin 2528.665000 Yunnan 2604.054167 Shaanxi 2658.034167 Shanxi 2817.210833 Guangxi 2924.104167 Anhui 3905.870000 Heilongjiang 4041.241667 Beijing 4673.453333 Hunan 4765.891667 Hubei 4772.503333 Fujian 4864.023333 Liaoning 5231.135000 Sichuan 5377.790000 Shanghai 6432.454167 Hebei 6936.825000 Henan 7208.966667 Zhejiang 9138.151667 Jiangsu 10761.846667 Shandong 12324.002500 Guangdong 15358.781667 Name: gdp, dtype: float64 . ddf[ddf.reg.str.contains(&#39;East China&#39;)].head() . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . ec = ddf[ddf.reg.str.contains(&#39;East China&#39;)] mean_gdp_prov = ec.groupby(&#39;province&#39;).gdp.agg([&#39;mean&#39;,&#39;count&#39;]) mean_gdp_prov.compute() . mean count . province . Anhui 3905.870000 | 12 | . Fujian 4864.023333 | 12 | . Jiangsu 10761.846667 | 12 | . Jiangxi 2460.782500 | 12 | . Shandong 12324.002500 | 12 | . Shanghai 6432.454167 | 12 | . Zhejiang 9138.151667 | 12 | . mean_gdp_prov.nlargest(5, &#39;mean&#39;).compute() . mean count . province . Shandong 12324.002500 | 12 | . Jiangsu 10761.846667 | 12 | . Zhejiang 9138.151667 | 12 | . Shanghai 6432.454167 | 12 | . Fujian 4864.023333 | 12 | . mean_gdp_prov.to_csv(&#39;mean_gdp-*.csv&#39;) #the * is where the partition number will go . [&#39;/home/jovyan/work/mean_gdp-0.csv&#39;] . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/10/Dask-api-eda.html",
            "relUrl": "/2020/11/10/Dask-api-eda.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Using Dask and dask-sql",
            "content": "Credit: code from https://github.com/nils-braun/dask-sql and https://coiled.io/blog/getting-started-with-dask-and-sql/ . Dask SQL docs https://dask-sql.readthedocs.io/ . dask-sql . import numpy as np import pandas as pd . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.000000 | 0.000000 | 0.000000 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.000000 | 0.000000 | 0.000000 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.000000 | 0.000000 | 0.000000 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.000000 | 0.000000 | 0.000000 | 1601508 | East China | 1499110 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 355 | Zhejiang | 391292.0 | 260313.0 | 2003 | 9705.02 | 498055 | 1.214286 | 0.035714 | 0.035714 | 6217715 | East China | 2261631 | . 356 356 | Zhejiang | 656175.0 | 276652.0 | 2004 | 11648.70 | 668128 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 3162299 | . 357 357 | Zhejiang | 656175.0 | NaN | 2005 | 13417.68 | 772000 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 2370200 | . 358 358 | Zhejiang | 1017303.0 | 394795.0 | 2006 | 15718.47 | 888935 | 1.214286 | 0.035714 | 0.035714 | 11537149 | East China | 2553268 | . 359 359 | Zhejiang | 844647.0 | 0.0 | 2007 | 18753.73 | 1036576 | 0.047619 | 0.000000 | 0.000000 | 16494981 | East China | 2939778 | . 360 rows × 13 columns . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . ddf . Dask DataFrame Structure: Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . npartitions=5 . 0 int64 | object | float64 | float64 | int64 | float64 | int64 | float64 | float64 | float64 | object | object | int64 | . 72 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/21359/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . # client.restart() . Client . Scheduler: inproc://192.168.1.71/21359/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . ddf.groupby(&quot;province&quot;).gdp.mean().compute() . province Anhui 3905.870000 Beijing 4673.453333 Chongqing 2477.712500 Fujian 4864.023333 Gansu 1397.832500 Guangdong 15358.781667 Guangxi 2924.104167 Guizhou 1422.010833 Hainan 686.714167 Hebei 6936.825000 Heilongjiang 4041.241667 Henan 7208.966667 Hubei 4772.503333 Hunan 4765.891667 Jiangsu 10761.846667 Jiangxi 2460.782500 Jilin 2274.854167 Liaoning 5231.135000 Ningxia 432.268333 Qinghai 383.099167 Shaanxi 2658.034167 Shandong 12324.002500 Shanghai 6432.454167 Shanxi 2817.210833 Sichuan 5377.790000 Tianjin 2528.665000 Tibet 170.426667 Xinjiang 1828.896667 Yunnan 2604.054167 Zhejiang 9138.151667 Name: gdp, dtype: float64 . from dask_sql import Context c = Context() . ddf . Dask DataFrame Structure: Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . npartitions=5 . 0 int64 | object | float64 | float64 | int64 | float64 | int64 | float64 | float64 | float64 | object | object | int64 | . 72 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks c.register_dask_table(ddf, &quot;fiscal&quot;) . result = c.sql(&#39;SELECT count(1) FROM fiscal&#39;) . result . Dask DataFrame Structure: COUNT(1) . npartitions=1 . int64 | . ... | . Dask Name: getitem, 22 tasks result.compute() . COUNT(1) . 0 360 | . result = c.sql(&quot;&quot;&quot; SELECT province, gdp, fdi FROM fiscal AS fiscal &quot;&quot;&quot;) . result.compute() . province gdp fdi . 0 Anhui | 2093.30 | 50661 | . 1 Anhui | 2347.32 | 43443 | . 2 Anhui | 2542.96 | 27673 | . 3 Anhui | 2712.34 | 26131 | . 4 Anhui | 2902.09 | 31847 | . ... ... | ... | ... | . 355 Zhejiang | 9705.02 | 498055 | . 356 Zhejiang | 11648.70 | 668128 | . 357 Zhejiang | 13417.68 | 772000 | . 358 Zhejiang | 15718.47 | 888935 | . 359 Zhejiang | 18753.73 | 1036576 | . 360 rows × 3 columns . print(result.compute()) . province gdp fdi 0 Anhui 2093.30 50661 1 Anhui 2347.32 43443 2 Anhui 2542.96 27673 3 Anhui 2712.34 26131 4 Anhui 2902.09 31847 .. ... ... ... 355 Zhejiang 9705.02 498055 356 Zhejiang 11648.70 668128 357 Zhejiang 13417.68 772000 358 Zhejiang 15718.47 888935 359 Zhejiang 18753.73 1036576 [360 rows x 3 columns] . from dask_sql import Context from dask.datasets import timeseries . print(result.gdp.mean().compute()) . 4428.653416666667 . result . Dask DataFrame Structure: province gdp fdi . npartitions=5 . 0 object | float64 | int64 | . 72 ... | ... | ... | . ... ... | ... | ... | . 288 ... | ... | ... | . 359 ... | ... | ... | . Dask Name: getitem, 30 tasks %%time ddf.groupby(&quot;province&quot;).fdi.mean().compute() . CPU times: user 96.7 ms, sys: 1.99 ms, total: 98.7 ms Wall time: 87.4 ms . province Anhui 7.095308e+04 Beijing 2.573693e+05 Chongqing 4.112783e+04 Fujian 3.744664e+05 Gansu 5.295500e+03 Guangdong 1.194950e+06 Guangxi 5.514783e+04 Guizhou 5.812333e+03 Hainan 6.436600e+04 Hebei 1.322308e+05 Heilongjiang 8.271933e+04 Henan 9.442600e+04 Hubei 1.497132e+05 Hunan 1.321102e+05 Jiangsu 8.736957e+05 Jiangxi 1.037352e+05 Jilin 4.122658e+04 Liaoning 2.859253e+05 Ningxia 3.950417e+03 Qinghai 1.098408e+04 Shaanxi 5.089258e+04 Shandong 5.455843e+05 Shanghai 5.082483e+05 Shanxi 3.862883e+04 Sichuan 6.219717e+04 Tianjin 2.501733e+05 Tibet 8.397500e+02 Xinjiang 4.433083e+03 Yunnan 1.704833e+04 Zhejiang 4.259302e+05 Name: fdi, dtype: float64 . %%time c.sql(&#39;SELECT avg(fdi) FROM fiscal GROUP BY province&#39;).compute() . CPU times: user 206 ms, sys: 5.58 ms, total: 212 ms Wall time: 176 ms . AVG(&quot;fiscal&quot;.&quot;fdi&quot;) . 0 70953 | . 1 257369 | . 2 41127 | . 3 374466 | . 4 5295 | . 5 1194950 | . 6 55147 | . 7 5812 | . 8 64366 | . 9 132230 | . 10 82719 | . 11 94426 | . 12 149713 | . 13 132110 | . 14 873695 | . 15 103735 | . 16 41226 | . 17 285925 | . 18 3950 | . 19 10984 | . 20 50892 | . 21 545584 | . 22 508248 | . 23 38628 | . 24 62197 | . 25 250173 | . 26 839 | . 27 4433 | . 28 17048 | . 29 425930 | . dfp = ddf.persist() . import distributed . cached_tasks = distributed.wait(dfp) print(f&#39;cached {len(cached_tasks[0])} results&#39;) . cached 5 results . c.register_dask_table(dfp, &quot;fiscal_cached&quot;) . result = c.sql(&#39;SELECT count(1) FROM fiscal_cached&#39;) result.compute() . COUNT(1) . 0 360 | . %%time c.sql(&#39;SELECT avg(fdi) FROM fiscal GROUP BY province&#39;).compute() . CPU times: user 208 ms, sys: 12.9 ms, total: 221 ms Wall time: 162 ms . AVG(&quot;fiscal&quot;.&quot;fdi&quot;) . 0 70953 | . 1 257369 | . 2 41127 | . 3 374466 | . 4 5295 | . 5 1194950 | . 6 55147 | . 7 5812 | . 8 64366 | . 9 132230 | . 10 82719 | . 11 94426 | . 12 149713 | . 13 132110 | . 14 873695 | . 15 103735 | . 16 41226 | . 17 285925 | . 18 3950 | . 19 10984 | . 20 50892 | . 21 545584 | . 22 508248 | . 23 38628 | . 24 62197 | . 25 250173 | . 26 839 | . 27 4433 | . 28 17048 | . 29 425930 | . c.sql(&#39;SELECT floor(3.14)&#39;).compute() . FLOOR(3.14) . 0 3.0 | . %%time c.sql(&quot;&quot;&quot; SELECT floor(fdi) AS fdi, avg(gdp) as gdp, count(1) as fiscal_count FROM fiscal_cached WHERE fdi &gt; 50 AND gdp &gt;= 0 GROUP BY floor(fdi) &quot;&quot;&quot;).compute() . distributed.utils_perf - WARNING - full garbage collections took 92% CPU time recently (threshold: 10%) . CPU times: user 356 ms, sys: 20.4 ms, total: 376 ms Wall time: 308 ms . fdi gdp fiscal_count . 0 2000 | 1933.98 | 1 | . 1 2342 | 1399.83 | 1 | . 2 2954 | 2277.35 | 1 | . 3 3539 | 1688.49 | 1 | . 4 3864 | 887.67 | 1 | . ... ... | ... | ... | . 354 527776 | 5252.76 | 1 | . 355 668128 | 11648.70 | 1 | . 356 772000 | 13417.68 | 1 | . 357 888935 | 15718.47 | 1 | . 358 1036576 | 18753.73 | 1 | . 359 rows × 3 columns . And now we can run a query and immediately plot a visualization of the result using Pandas plotting syntax! . c.sql(&quot;&quot;&quot; SELECT floor(fdi) AS fdi, avg(gdp) as gdp FROM fiscal_cached WHERE fdi &gt; 50 AND gdp &gt;= 0 GROUP BY floor(fdi) &quot;&quot;&quot;).compute().plot(x=&#39;fdi&#39;, y=&#39;gdp&#39;) . &lt;AxesSubplot:xlabel=&#39;fdi&#39;&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/09/dask-sql.html",
            "relUrl": "/2020/11/09/dask-sql.html",
            "date": " • Nov 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "T-Tests The Purpose, Assumptions, How it Works, and Corrections.",
            "content": ". Purpose: . The purpose of the T-test is to compare if there are mean differences between two groups of interest. . Assumptions: . Normal distribution of the dependant variable | Independent and identically distributed (i.i.d) variables | . How it works: . T-test comparisons use the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term is known, where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student’s t distribution. This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal. . Corrections: . When we are interested in comparing statistical differences between more than two groups, we may use ANOVA, but if we want to compare differences between more than a single pairwise comparison, we can conduct multiple t-tests. In doing so, we will end up increasing the likelihood of a false positive (type I error) where we are incorrectly rejecting the null hypothesis that there are no statistical differences between groups. One way to address this is to use the Bonferroni correction. . Bonferroni Correction . The Bonferroni correction, the namesake of Carlo Emilio Bonferroni, accounts for what we lose in a p-hacking quest in the experimentation, which is the justification for taking p-values at face value. By intuition, when we go searching for significant differences everywhere, the chance of seeing an apparent significant difference by chance anywhere increases. Using the Bonferroni correction, if the starting alpha/significance level is .05 and we are testing 10 hypotheses, then the corrected alpha/significance level we should use would be .005. Understanding the lack of an incentive to make such an adjustment is straightforward. Another way to address this is to first use ANOVA to detect statistical differences between all groups before deciding whether to use t tests to look for pairwise comparisons between groups. . python ttest.py .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/statistics/2020/11/08/statistics-ttests-post.html",
            "relUrl": "/statistics/2020/11/08/statistics-ttests-post.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "NLP Example bằng tiếng Việt using StackNetClassifier",
            "content": "Credit: Code and Notebooks from https://github.com/ngxbac/aivivn_phanloaisacthaibinhluan . https://www.aivivn.com/ . import pandas as pd import numpy as np from scipy.sparse import hstack, csr_matrix, vstack from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import StratifiedKFold from sklearn.metrics import f1_score from sklearn.ensemble import * from sklearn.linear_model import * from tqdm import * import wordcloud import matplotlib.pyplot as plt import gc import lightgbm as lgb %matplotlib inline . # Load data train_df = pd.read_csv(&quot;./data/train.csv&quot;) test_df = pd.read_csv(&quot;./data/test.csv&quot;) . train_df.head() . id comment label . 0 train_000000 | Dung dc sp tot cam on nshop Đóng gói sản phẩm... | 0 | . 1 train_000001 | Chất lượng sản phẩm tuyệt vời . Son mịn nhưng... | 0 | . 2 train_000002 | Chất lượng sản phẩm tuyệt vời nhưng k có hộp ... | 0 | . 3 train_000003 | :(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ... | 1 | . 4 train_000004 | Lần trước mình mua áo gió màu hồng rất ok mà đ... | 1 | . test_df.head() . id comment . 0 test_000000 | Chưa dùng thử nên chưa biết | . 1 test_000001 | Không đáng tiềnVì ngay đợt sale nên mới mua n... | . 2 test_000002 | Cám ơn shop. Đóng gói sản phẩm rất đẹp và chắc... | . 3 test_000003 | Vải đẹp.phom oki luôn.quá ưng | . 4 test_000004 | Chuẩn hàng đóng gói đẹp | . df = pd.concat([train_df, test_df], axis=0) # del train_df, test_df # gc.collect() . import emoji def extract_emojis(str): return [c for c in str if c in emoji.UNICODE_EMOJI] . good_df = train_df[train_df[&#39;label&#39;] == 0] good_comment = good_df[&#39;comment&#39;].values good_emoji = [] for c in good_comment: good_emoji += extract_emojis(c) good_emoji = np.unique(np.asarray(good_emoji)) . bad_df = train_df[train_df[&#39;label&#39;] == 1] bad_comment = bad_df[&#39;comment&#39;].values bad_emoji = [] for c in bad_comment: bad_emoji += extract_emojis(c) bad_emoji = np.unique(np.asarray(bad_emoji)) . good_emoji . array([&#39;↖&#39;, &#39;↗&#39;, &#39;☀&#39;, &#39;☺&#39;, &#39;♀&#39;, &#39;♥&#39;, &#39;✌&#39;, &#39;✨&#39;, &#39;❌&#39;, &#39;❣&#39;, &#39;❤&#39;, &#39;⭐&#39;, &#39;🆗&#39;, &#39;🌝&#39;, &#39;🌟&#39;, &#39;🌧&#39;, &#39;🌷&#39;, &#39;🌸&#39;, &#39;🌺&#39;, &#39;🌼&#39;, &#39;🍓&#39;, &#39;🎈&#39;, &#39;🎉&#39;, &#39;🏻&#39;, &#39;🏼&#39;, &#39;🏿&#39;, &#39;🐅&#39;, &#39;🐾&#39;, &#39;👉&#39;, &#39;👌&#39;, &#39;👍&#39;, &#39;👏&#39;, &#39;💋&#39;, &#39;💌&#39;, &#39;💐&#39;, &#39;💓&#39;, &#39;💕&#39;, &#39;💖&#39;, &#39;💗&#39;, &#39;💙&#39;, &#39;💚&#39;, &#39;💛&#39;, &#39;💜&#39;, &#39;💞&#39;, &#39;💟&#39;, &#39;💥&#39;, &#39;💪&#39;, &#39;💮&#39;, &#39;💯&#39;, &#39;💰&#39;, &#39;📑&#39;, &#39;🖤&#39;, &#39;😀&#39;, &#39;😁&#39;, &#39;😂&#39;, &#39;😃&#39;, &#39;😄&#39;, &#39;😅&#39;, &#39;😆&#39;, &#39;😇&#39;, &#39;😉&#39;, &#39;😊&#39;, &#39;😋&#39;, &#39;😌&#39;, &#39;😍&#39;, &#39;😎&#39;, &#39;😑&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😖&#39;, &#39;😗&#39;, &#39;😘&#39;, &#39;😙&#39;, &#39;😚&#39;, &#39;😛&#39;, &#39;😜&#39;, &#39;😝&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😡&#39;, &#39;😢&#39;, &#39;😣&#39;, &#39;😥&#39;, &#39;😩&#39;, &#39;😪&#39;, &#39;😫&#39;, &#39;😬&#39;, &#39;😭&#39;, &#39;😯&#39;, &#39;😰&#39;, &#39;😱&#39;, &#39;😲&#39;, &#39;😳&#39;, &#39;😻&#39;, &#39;😿&#39;, &#39;🙁&#39;, &#39;🙂&#39;, &#39;🙃&#39;, &#39;🙄&#39;, &#39;🙆&#39;, &#39;🙌&#39;, &#39;🤑&#39;, &#39;🤔&#39;, &#39;🤗&#39;, &#39;🤙&#39;, &#39;🤝&#39;, &#39;🤣&#39;, &#39;🤤&#39;, &#39;🤨&#39;, &#39;🤪&#39;, &#39;🤭&#39;], dtype=&#39;&lt;U1&#39;) . # Just remove &quot;sad, bad&quot; emoji :D good_emoji_fix = [ &#39;↖&#39;, &#39;↗&#39;, &#39;☀&#39;, &#39;☺&#39;, &#39;♀&#39;, &#39;♥&#39;, &#39;✌&#39;, &#39;✨&#39;, &#39;❣&#39;, &#39;❤&#39;, &#39;⭐&#39;, &#39;🆗&#39;, &#39;🌝&#39;, &#39;🌟&#39;, &#39;🌧&#39;, &#39;🌷&#39;, &#39;🌸&#39;, &#39;🌺&#39;, &#39;🌼&#39;, &#39;🍓&#39;, &#39;🎈&#39;, &#39;🎉&#39;, &#39;🐅&#39;, &#39;🐾&#39;, &#39;👉&#39;, &#39;👌&#39;, &#39;👍&#39;, &#39;👏&#39;, &#39;💋&#39;, &#39;💌&#39;, &#39;💐&#39;, &#39;💓&#39;, &#39;💕&#39;, &#39;💖&#39;, &#39;💗&#39;, &#39;💙&#39;, &#39;💚&#39;, &#39;💛&#39;, &#39;💜&#39;, &#39;💞&#39;, &#39;💟&#39;, &#39;💥&#39;, &#39;💪&#39;, &#39;💮&#39;, &#39;💯&#39;, &#39;💰&#39;, &#39;📑&#39;, &#39;🖤&#39;, &#39;😀&#39;, &#39;😁&#39;, &#39;😂&#39;, &#39;😃&#39;, &#39;😄&#39;, &#39;😅&#39;, &#39;😆&#39;, &#39;😇&#39;, &#39;😉&#39;, &#39;😊&#39;, &#39;😋&#39;, &#39;😌&#39;, &#39;😍&#39;, &#39;😎&#39;, &#39;😑&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😖&#39;, &#39;😗&#39;, &#39;😘&#39;, &#39;😙&#39;, &#39;😚&#39;, &#39;😛&#39;, &#39;😜&#39;, &#39;😝&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😡&#39;, &#39;😯&#39;, &#39;😰&#39;, &#39;😱&#39;, &#39;😲&#39;, &#39;😳&#39;, &#39;😻&#39;, &#39;🙂&#39;, &#39;🙃&#39;, &#39;🙄&#39;, &#39;🙆&#39;, &#39;🙌&#39;, &#39;🤑&#39;, &#39;🤔&#39;, &#39;🤗&#39;, ] . bad_emoji . array([&#39;☹&#39;, &#39;✋&#39;, &#39;❌&#39;, &#39;❓&#39;, &#39;❤&#39;, &#39;⭐&#39;, &#39;🎃&#39;, &#39;🏻&#39;, &#39;🏼&#39;, &#39;🏿&#39;, &#39;👌&#39;, &#39;👍&#39;, &#39;👎&#39;, &#39;👶&#39;, &#39;💀&#39;, &#39;💋&#39;, &#39;😁&#39;, &#39;😂&#39;, &#39;😈&#39;, &#39;😊&#39;, &#39;😌&#39;, &#39;😏&#39;, &#39;😐&#39;, &#39;😑&#39;, &#39;😒&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😖&#39;, &#39;😚&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😠&#39;, &#39;😡&#39;, &#39;😢&#39;, &#39;😣&#39;, &#39;😤&#39;, &#39;😥&#39;, &#39;😧&#39;, &#39;😩&#39;, &#39;😪&#39;, &#39;😫&#39;, &#39;😬&#39;, &#39;😭&#39;, &#39;😳&#39;, &#39;😵&#39;, &#39;😶&#39;, &#39;🙁&#39;, &#39;🙂&#39;, &#39;🙄&#39;, &#39;🤔&#39;, &#39;🤚&#39;, &#39;🤤&#39;], dtype=&#39;&lt;U1&#39;) . # Just remove &quot;good&quot; emoji :D bad_emoji_fix = [ &#39;☹&#39;, &#39;✋&#39;, &#39;❌&#39;, &#39;❓&#39;, &#39;👎&#39;, &#39;👶&#39;, &#39;💀&#39;, &#39;😐&#39;, &#39;😑&#39;, &#39;😒&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😠&#39;, &#39;😡&#39;, &#39;😢&#39;, &#39;😣&#39;, &#39;😤&#39;, &#39;😥&#39;, &#39;😧&#39;, &#39;😩&#39;, &#39;😪&#39;, &#39;😫&#39;, &#39;😬&#39;, &#39;😭&#39;, &#39;😳&#39;, &#39;😵&#39;, &#39;😶&#39;, &#39;🙁&#39;, &#39;🙄&#39;, &#39;🤔&#39;, ] . def count_good_bad_emoji(row): comment = row[&#39;comment&#39;] n_good_emoji = 0 n_bad_emoji = 0 for c in comment: if c in good_emoji_fix: n_good_emoji += 1 if c in bad_emoji_fix: n_bad_emoji += 1 row[&#39;n_good_emoji&#39;] = n_good_emoji row[&#39;n_bad_emoji&#39;] = n_bad_emoji return row . # Some features df[&#39;comment&#39;] = df[&#39;comment&#39;].astype(str).fillna(&#39; &#39;) df[&#39;comment&#39;] = df[&#39;comment&#39;].str.lower() df[&#39;num_words&#39;] = df[&#39;comment&#39;].apply(lambda s: len(s.split())) df[&#39;num_unique_words&#39;] = df[&#39;comment&#39;].apply(lambda s: len(set(w for w in s.split()))) df[&#39;words_vs_unique&#39;] = df[&#39;num_unique_words&#39;] / df[&#39;num_words&#39;] * 100 df = df.apply(count_good_bad_emoji, axis=1) . df[&#39;good_bad_emoji_ratio&#39;] = df[&#39;n_good_emoji&#39;] / df[&#39;n_bad_emoji&#39;] df[&#39;good_bad_emoji_ratio&#39;] = df[&#39;good_bad_emoji_ratio&#39;].replace(np.nan, 0) df[&#39;good_bad_emoji_ratio&#39;] = df[&#39;good_bad_emoji_ratio&#39;].replace(np.inf, 99) df[&#39;good_bad_emoji_diff&#39;] = df[&#39;n_good_emoji&#39;] - df[&#39;n_bad_emoji&#39;] df[&#39;good_bad_emoji_sum&#39;] = df[&#39;n_good_emoji&#39;] + df[&#39;n_bad_emoji&#39;] . train_df = df[~df[&#39;label&#39;].isnull()] test_df = df[df[&#39;label&#39;].isnull()] train_comments = train_df[&#39;comment&#39;].fillna(&quot;none&quot;).values test_comments = test_df[&#39;comment&#39;].fillna(&quot;none&quot;).values y_train = train_df[&#39;label&#39;].values . train_df.head() . id comment label num_words num_unique_words words_vs_unique n_good_emoji n_bad_emoji good_bad_emoji_ratio good_bad_emoji_diff good_bad_emoji_sum . 0 train_000000 | dung dc sp tot cam on nshop đóng gói sản phẩm... | 0.0 | 22 | 20 | 90.909091 | 0 | 0 | 0.0 | 0 | 0 | . 1 train_000001 | chất lượng sản phẩm tuyệt vời . son mịn nhưng... | 0.0 | 18 | 18 | 100.000000 | 0 | 0 | 0.0 | 0 | 0 | . 2 train_000002 | chất lượng sản phẩm tuyệt vời nhưng k có hộp ... | 0.0 | 18 | 14 | 77.777778 | 0 | 0 | 0.0 | 0 | 0 | . 3 train_000003 | :(( mình hơi thất vọng 1 chút vì mình đã kỳ vọ... | 1.0 | 114 | 91 | 79.824561 | 0 | 0 | 0.0 | 0 | 0 | . 4 train_000004 | lần trước mình mua áo gió màu hồng rất ok mà đ... | 1.0 | 26 | 24 | 92.307692 | 0 | 0 | 0.0 | 0 | 0 | . Tạo feature TFIDF đơn giản . tfidf = TfidfVectorizer( min_df = 5, max_df = 0.8, max_features=10000, sublinear_tf=True ) . X_train_tfidf = tfidf.fit_transform(train_comments) X_test_tfidf = tfidf.transform(test_comments) . EXCLUED_COLS = [&#39;id&#39;, &#39;comment&#39;, &#39;label&#39;] static_cols = [c for c in train_df.columns if not c in EXCLUED_COLS] X_train_static = train_df[static_cols].values X_test_static = test_df[static_cols].values . X_train = hstack([X_train_tfidf, csr_matrix(X_train_static)]).tocsr() X_test = hstack([X_test_tfidf, csr_matrix(X_test_static)]).tocsr() # X_train = X_train_tfidf # X_test = X_test_tfidf . X_train.shape, X_test.shape, y_train.shape . ((16087, 2687), (10981, 2687), (16087,)) . Stacking method . models=[ ######## First level ######## [ RandomForestClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), ExtraTreesClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1), LogisticRegression(random_state=1) ], ######## Second level ######## [ RandomForestClassifier (n_estimators=200, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1) ] ] . from pystacknet.pystacknet import StackNetClassifier model = StackNetClassifier( models, metric=&quot;f1&quot;, folds=5, restacking=False, use_retraining=True, use_proba=True, random_state=12345, n_jobs=1, verbose=1 ) model.fit(X_train, y_train) preds=model.predict_proba(X_test) . ====================== Start of Level 0 ====================== Input Dimensionality 2687 at Level 0 4 models included in Level 0 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 1/5 , model 0 , f1===0.789457 Level 0, fold 1/5 , model 1 , f1===0.813472 Level 0, fold 1/5 , model 2 , f1===0.857641 Level 0, fold 1/5 , model 3 , f1===0.865303 =========== end of fold 1 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 2/5 , model 0 , f1===0.812040 Level 0, fold 2/5 , model 1 , f1===0.824688 Level 0, fold 2/5 , model 2 , f1===0.865074 Level 0, fold 2/5 , model 3 , f1===0.872805 =========== end of fold 2 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 3/5 , model 0 , f1===0.804277 Level 0, fold 3/5 , model 1 , f1===0.821736 Level 0, fold 3/5 , model 2 , f1===0.876429 Level 0, fold 3/5 , model 3 , f1===0.876494 =========== end of fold 3 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 4/5 , model 0 , f1===0.800000 Level 0, fold 4/5 , model 1 , f1===0.825202 Level 0, fold 4/5 , model 2 , f1===0.873005 Level 0, fold 4/5 , model 3 , f1===0.863442 =========== end of fold 4 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 5/5 , model 0 , f1===0.802153 Level 0, fold 5/5 , model 1 , f1===0.813953 Level 0, fold 5/5 , model 2 , f1===0.861043 Level 0, fold 5/5 , model 3 , f1===0.873807 =========== end of fold 5 in level 0 =========== Level 0, model 0 , f1===0.801585 Level 0, model 1 , f1===0.819810 Level 0, model 2 , f1===0.866638 Level 0, model 3 , f1===0.870370 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Output dimensionality of level 0 is 4 ====================== End of Level 0 ====================== level 0 lasted 102.148064 seconds ====================== Start of Level 1 ====================== Input Dimensionality 4 at Level 1 1 models included in Level 1 Level 1, fold 1/5 , model 0 , f1===0.872932 =========== end of fold 1 in level 1 =========== Level 1, fold 2/5 , model 0 , f1===0.874336 =========== end of fold 2 in level 1 =========== Level 1, fold 3/5 , model 0 , f1===0.883098 =========== end of fold 3 in level 1 =========== Level 1, fold 4/5 , model 0 , f1===0.875176 =========== end of fold 4 in level 1 =========== Level 1, fold 5/5 , model 0 , f1===0.878459 =========== end of fold 5 in level 1 =========== Level 1, model 0 , f1===0.876800 Output dimensionality of level 1 is 1 ====================== End of Level 1 ====================== level 1 lasted 18.519124 seconds ====================== End of fit ====================== fit() lasted 120.668525 seconds ====================== Start of Level 0 ====================== 1 estimators included in Level 0 ====================== Start of Level 1 ====================== 1 estimators included in Level 1 . pred_cls = np.argmax(preds, axis=1) . # submission = pd.read_csv(&quot;./data/sample_submission.csv&quot;) # submission[&#39;label&#39;] = pred_cls . # submission.head() . # submission.to_csv(&quot;stack_demo.csv&quot;, index=False) . Ensemble method . from sklearn.model_selection import cross_val_predict models = [ RandomForestClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), ExtraTreesClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1), LogisticRegression(random_state=1) ] . def cross_val_and_predict(clf, X, y, X_test, nfolds): kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42) oof_preds = np.zeros((X.shape[0], 2)) sub_preds = np.zeros((X_test.shape[0], 2)) for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)): X_train, y_train = X[train_idx], y[train_idx] X_valid, y_valid = X[valid_idx], y[valid_idx] clf.fit(X_train, y_train) oof_preds[valid_idx] = clf.predict_proba(X_valid) sub_preds += clf.predict_proba(X_test) / kf.n_splits return oof_preds, sub_preds . sub_preds = [] for clf in models: oof_pred, sub_pred = cross_val_and_predict(clf, X_train, y_train, X_test, nfolds=5) oof_pred_cls = oof_pred.argmax(axis=1) oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train) print(clf.__class__) print(f&quot;F1 CV: {oof_f1}&quot;) sub_preds.append(sub_pred) . &lt;class &#39;sklearn.ensemble._forest.RandomForestClassifier&#39;&gt; F1 CV: 0.8036813709933355 &lt;class &#39;sklearn.ensemble._forest.ExtraTreesClassifier&#39;&gt; F1 CV: 0.8206730456291089 &lt;class &#39;sklearn.ensemble._gb.GradientBoostingClassifier&#39;&gt; F1 CV: 0.864902800196505 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; F1 CV: 0.8695652173913043 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . sub_preds = np.asarray(sub_preds) sub_preds = sub_preds.mean(axis=0) sub_pred_cls = sub_preds.argmax(axis=1) . # submission_ensemble = submission.copy() # submission_ensemble[&#39;label&#39;] = sub_pred_cls # submission_ensemble.to_csv(&quot;ensemble.csv&quot;, index=False) . import pandas as pd import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import StratifiedKFold from sklearn.metrics import f1_score import wordcloud import matplotlib.pyplot as plt import gc import lightgbm as lgb %matplotlib inline # Load data train_df = pd.read_csv(&quot;./data/train.csv&quot;) test_df = pd.read_csv(&quot;./data/test.csv&quot;) train_df.head() test_df.head() train_comments = train_df[&#39;comment&#39;].fillna(&quot;none&quot;).values test_comments = test_df[&#39;comment&#39;].fillna(&quot;none&quot;).values y_train = train_df[&#39;label&#39;].values . # Wordcloud of training set cloud = np.array(train_comments).flatten() plt.figure(figsize=(20,10)) word_cloud = wordcloud.WordCloud( max_words=200,background_color =&quot;black&quot;, width=2000,height=1000,mode=&quot;RGB&quot; ).generate(str(cloud)) plt.axis(&quot;off&quot;) plt.imshow(word_cloud) . &lt;matplotlib.image.AxesImage at 0x7f16aaad1f98&gt; . # Wordcloud of test set cloud = np.array(test_comments).flatten() plt.figure(figsize=(20,10)) word_cloud = wordcloud.WordCloud( max_words=100,background_color =&quot;black&quot;, width=2000,height=1000,mode=&quot;RGB&quot; ).generate(str(cloud)) plt.axis(&quot;off&quot;) plt.imshow(word_cloud) . &lt;matplotlib.image.AxesImage at 0x7f16aabc2e48&gt; . tfidf = TfidfVectorizer( min_df=5, max_df= 0.8, max_features=10000, sublinear_tf=True ) X_train = tfidf.fit_transform(train_comments) X_test = tfidf.transform(test_comments) X_train.shape, X_test.shape, y_train.shape def lgb_f1_score(y_hat, data): y_true = data.get_label() y_hat = np.round(y_hat) # scikits f1 doesn&#39;t like probabilities return &#39;f1&#39;, f1_score(y_true, y_hat), True print(&quot;Starting LightGBM. Train shape: {}, test shape: {}&quot;.format(X_train.shape, X_test.shape)) # Cross validation model folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69) # Create arrays and dataframes to store results oof_preds = np.zeros(X_train.shape[0]) sub_preds = np.zeros(X_test.shape[0]) # k-fold for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)): print(&quot;Fold %s&quot; % (n_fold)) train_x, train_y = X_train[train_idx], y_train[train_idx] valid_x, valid_y = X_train[valid_idx], y_train[valid_idx] # set data structure lgb_train = lgb.Dataset(train_x, label=train_y, free_raw_data=False) lgb_test = lgb.Dataset(valid_x, label=valid_y, free_raw_data=False) params = { &#39;objective&#39; :&#39;binary&#39;, &#39;learning_rate&#39; : 0.01, &#39;num_leaves&#39; : 76, &#39;feature_fraction&#39;: 0.64, &#39;bagging_fraction&#39;: 0.8, &#39;bagging_freq&#39;:1, &#39;boosting_type&#39; : &#39;gbdt&#39;, } reg = lgb.train( params, lgb_train, valid_sets=[lgb_train, lgb_test], valid_names=[&#39;train&#39;, &#39;valid&#39;], num_boost_round=10000, verbose_eval=100, early_stopping_rounds=100, feval=lgb_f1_score ) oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration) sub_preds += reg.predict(X_test, num_iteration=reg.best_iteration) / folds.n_splits del reg, train_x, train_y, valid_x, valid_y gc.collect() threshold = 0.5 preds = (sub_preds &gt; threshold).astype(np.uint8) . Starting LightGBM. Train shape: (16087, 2679), test shape: (10981, 2679) Fold 0 [LightGBM] [Info] Number of positive: 5445, number of negative: 7424 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051310 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 52352 [LightGBM] [Info] Number of data points in the train set: 12869, number of used features: 1137 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.423110 -&gt; initscore=-0.310020 [LightGBM] [Info] Start training from score -0.310020 Training until validation scores don&#39;t improve for 100 rounds [100] train&#39;s binary_logloss: 0.393295 train&#39;s f1: 0.874977 valid&#39;s binary_logloss: 0.410023 valid&#39;s f1: 0.851221 [200] train&#39;s binary_logloss: 0.290724 train&#39;s f1: 0.893252 valid&#39;s binary_logloss: 0.323003 valid&#39;s f1: 0.865039 [300] train&#39;s binary_logloss: 0.238792 train&#39;s f1: 0.90528 valid&#39;s binary_logloss: 0.287577 valid&#39;s f1: 0.872143 [400] train&#39;s binary_logloss: 0.205608 train&#39;s f1: 0.919228 valid&#39;s binary_logloss: 0.27182 valid&#39;s f1: 0.872272 [500] train&#39;s binary_logloss: 0.18123 train&#39;s f1: 0.928584 valid&#39;s binary_logloss: 0.265462 valid&#39;s f1: 0.874282 [600] train&#39;s binary_logloss: 0.162145 train&#39;s f1: 0.937332 valid&#39;s binary_logloss: 0.263343 valid&#39;s f1: 0.87482 Early stopping, best iteration is: [522] train&#39;s binary_logloss: 0.176706 train&#39;s f1: 0.930145 valid&#39;s binary_logloss: 0.264713 valid&#39;s f1: 0.875718 Fold 1 [LightGBM] [Info] Number of positive: 5445, number of negative: 7424 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037747 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 52365 [LightGBM] [Info] Number of data points in the train set: 12869, number of used features: 1135 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.423110 -&gt; initscore=-0.310020 [LightGBM] [Info] Start training from score -0.310020 Training until validation scores don&#39;t improve for 100 rounds [100] train&#39;s binary_logloss: 0.392321 train&#39;s f1: 0.875253 valid&#39;s binary_logloss: 0.410153 valid&#39;s f1: 0.849244 [200] train&#39;s binary_logloss: 0.290396 train&#39;s f1: 0.892278 valid&#39;s binary_logloss: 0.323353 valid&#39;s f1: 0.861736 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/08/nlp_tieng_viet.html",
            "relUrl": "/2020/11/08/nlp_tieng_viet.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Spacy in Python for Natural Language Processing (NLP) Example",
            "content": "Notebook and Code from https://github.com/jeffheaton/t81_558_deep_learning . import urllib.request import csv import codecs import numpy as np url = &quot;https://data.heatonresearch.com/data/t81-558/datasets/sonnet_18.txt&quot; with urllib.request.urlopen(url) as urlstream: for line in codecs.iterdecode(urlstream, &#39;utf-8&#39;): print(line.rstrip()) . Sonnet 18 original text William Shakespeare Shall I compare thee to a summer&#39;s day? Thou art more lovely and more temperate: Rough winds do shake the darling buds of May, And summer&#39;s lease hath all too short a date: Sometime too hot the eye of heaven shines, And often is his gold complexion dimm&#39;d; And every fair from fair sometime declines, By chance or nature&#39;s changing course untrimm&#39;d; But thy eternal summer shall not fade Nor lose possession of that fair thou owest; Nor shall Death brag thou wander&#39;st in his shade, When in eternal lines to time thou growest: So long as men can breathe or eyes can see, So long lives this and this gives life to thee. . import spacy nlp = spacy.load(&#39;en&#39;) doc = nlp(line.rstrip()) for token in doc: print(token.text) . So long lives this and this gives life to thee . . import spacy nlp = spacy.load(&#39;en&#39;) doc = nlp(u&quot;Apple is looking at buying a U.K. startup for $1 billion&quot;) for token in doc: print(token.text) . Apple is looking at buying a U.K. startup for $ 1 billion . You can also obtain the part of speech for each word. Common parts of speech include nouns, verbs, pronouns, and adjectives. . for word in doc: print(word.text, word.pos_) . Apple PROPN is AUX looking VERB at ADP buying VERB a DET U.K. PROPN startup NOUN for ADP $ SYM 1 NUM billion NUM . Spacy includes functions to check if parts of a sentence appear to be numbers, acronyms, or other entities. . for word in doc: print(f&quot;{word} is like number? {word.like_num}&quot;) . Apple is like number? False is is like number? False looking is like number? False at is like number? False buying is like number? False a is like number? False U.K. is like number? False startup is like number? False for is like number? False $ is like number? False 1 is like number? True billion is like number? True . import spacy from spacy import displacy nlp = spacy.load(&#39;en&#39;) doc = nlp(u&quot;This is a sentance&quot;) displacy.serve(doc, style=&quot;dep&quot;) . /home/gao/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you&#39;re calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you&#39;re already running a local web server, so there&#39;s no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization. &#34;__main__&#34;, mod_spec) . &lt;!DOCTYPE html&gt; displaCy . This DET is AUX a DET sentance NOUN nsubj det attr . Using the &#39;dep&#39; visualizer Serving on http://0.0.0.0:5000 ... Shutting down server on port 5000. . Note, you will have to manually stop the above cell . print(doc) . This is a sentance . The following code shows how to reduce words to their stems. Here the sentence words are reduced to their most basic form. For example, &quot;striped&quot; to &quot;stripe.&quot; . import spacy # Initialize spacy &#39;en&#39; model, keeping only tagger # component needed for lemmatization nlp = spacy.load(&#39;en&#39;, disable=[&#39;parser&#39;, &#39;ner&#39;]) sentence = &quot;The striped bats are hanging on their feet for best&quot; # Parse the sentence using the loaded &#39;en&#39; model object `nlp` doc = nlp(sentence) # Extract the lemma for each token and join &quot; &quot;.join([token.lemma_ for token in doc]) . &#39;the stripe bat be hang on -PRON- foot for good&#39; . from spacy.lang.en.stop_words import STOP_WORDS print(STOP_WORDS) . {&#39;moreover&#39;, &#39;does&#39;, &#39;becomes&#39;, &#39;though&#39;, &#39;done&#39;, &#39;often&#39;, &#39;all&#39;, &#39;next&#39;, &#39;sometime&#39;, &#39;show&#39;, &#39;your&#39;, &#39;forty&#39;, &#39;am&#39;, &#39;on&#39;, &#39;however&#39;, &#39;empty&#39;, &#39;’m&#39;, &#39;again&#39;, &#39;have&#39;, &#39;up&#39;, &#39;six&#39;, &#39;any&#39;, &#39;ours&#39;, &#39;may&#39;, &#39;mine&#39;, &#39;not&#39;, &#39;upon&#39;, &#39;top&#39;, &#39;twenty&#39;, &#39;please&#39;, &#39;latter&#39;, &#39;noone&#39;, &#39;this&#39;, &#39;make&#39;, &#39;former&#39;, &#39;wherein&#39;, &#39;hereupon&#39;, &#39;nevertheless&#39;, &#34;&#39;ll&#34;, &#39;less&#39;, &#39;nowhere&#39;, &#39;side&#39;, &#39;via&#39;, &#39;whatever&#39;, &#39;’s&#39;, &#39;becoming&#39;, &#39;onto&#39;, &#39;by&#39;, &#39;being&#39;, &#39;n‘t&#39;, &#39;should&#39;, &#39;themselves&#39;, &#39;almost&#39;, &#39;rather&#39;, &#39;nor&#39;, &#39;once&#39;, &#39;hence&#39;, &#39;few&#39;, &#39;unless&#39;, &#39;along&#39;, &#39;off&#39;, &#39;everyone&#39;, &#39;put&#39;, &#39;fifty&#39;, &#39;one&#39;, &#39;hereby&#39;, &#39;neither&#39;, &#39;anyhow&#39;, &#39;whom&#39;, &#39;‘ve&#39;, &#39;it&#39;, &#39;give&#39;, &#39;seemed&#39;, &#39;‘s&#39;, &#39;or&#39;, &#39;first&#39;, &#39;is&#39;, &#34;&#39;ve&#34;, &#39;everything&#39;, &#39;per&#39;, &#39;front&#39;, &#39;whose&#39;, &#39;whoever&#39;, &#39;three&#39;, &#39;’re&#39;, &#39;just&#39;, &#39;could&#39;, &#39;beyond&#39;, &#39;none&#39;, &#39;below&#39;, &#39;you&#39;, &#39;thereupon&#39;, &#39;wherever&#39;, &#39;full&#39;, &#39;a&#39;, &#39;whereupon&#39;, &#39;go&#39;, &#39;then&#39;, &#39;although&#39;, &#39;has&#39;, &#39;yet&#39;, &#39;we&#39;, &#39;call&#39;, &#39;something&#39;, &#39;ten&#39;, &#39;using&#39;, &#39;anything&#39;, &#39;until&#39;, &#39;two&#39;, &#39;but&#39;, &#39;‘d&#39;, &#39;now&#39;, &#39;amongst&#39;, &#39;serious&#39;, &#39;if&#39;, &#39;already&#39;, &#39;some&#39;, &#39;me&#39;, &#39;their&#39;, &#39;latterly&#39;, &#39;part&#39;, &#39;further&#39;, &#39;between&#39;, &#39;down&#39;, &#39;get&#39;, &#39;namely&#39;, &#39;more&#39;, &#39;nothing&#39;, &#39;do&#39;, &#39;back&#39;, &#39;anywhere&#39;, &#39;hers&#39;, &#39;become&#39;, &#39;there&#39;, &#39;always&#39;, &#39;eight&#39;, &#39;anyway&#39;, &#39;sixty&#39;, &#39;’ll&#39;, &#39;around&#39;, &#39;alone&#39;, &#39;who&#39;, &#39;move&#39;, &#39;over&#39;, &#39;well&#39;, &#39;yourself&#39;, &#39;in&#39;, &#34;&#39;d&#34;, &#39;else&#39;, &#39;about&#39;, &#39;name&#39;, &#39;without&#39;, &#39;therefore&#39;, &#39;thence&#39;, &#39;anyone&#39;, &#39;‘m&#39;, &#39;least&#39;, &#39;had&#39;, &#34;&#39;m&#34;, &#39;see&#39;, &#39;last&#39;, &#39;beside&#39;, &#39;i&#39;, &#39;cannot&#39;, &#39;re&#39;, &#39;she&#39;, &#39;therein&#39;, &#39;made&#39;, &#39;must&#39;, &#39;own&#39;, &#39;they&#39;, &#39;became&#39;, &#39;are&#39;, &#39;other&#39;, &#39;at&#39;, &#39;someone&#39;, &#39;never&#39;, &#39;while&#39;, &#39;here&#39;, &#39;when&#39;, &#39;meanwhile&#39;, &#39;each&#39;, &#39;ever&#39;, &#39;his&#39;, &#39;five&#39;, &#39;thru&#39;, &#39;somewhere&#39;, &#39;itself&#39;, &#39;what&#39;, &#39;only&#39;, &#39;than&#39;, &#39;very&#39;, &#39;under&#39;, &#39;many&#39;, &#39;whole&#39;, &#39;’d&#39;, &#39;say&#39;, &#39;together&#39;, &#39;most&#39;, &#39;seeming&#39;, &#39;ca&#39;, &#39;where&#39;, &#39;‘ll&#39;, &#39;eleven&#39;, &#39;among&#39;, &#39;our&#39;, &#39;otherwise&#39;, &#39;of&#39;, &#39;out&#39;, &#39;myself&#39;, &#39;keep&#39;, &#39;her&#39;, &#39;might&#39;, &#39;really&#39;, &#39;why&#39;, &#39;an&#39;, &#39;against&#39;, &#39;him&#39;, &#39;thereby&#39;, &#39;were&#39;, &#39;twelve&#39;, &#39;towards&#39;, &#34;n&#39;t&#34;, &#39;can&#39;, &#39;so&#39;, &#39;also&#39;, &#39;whither&#39;, &#39;hundred&#39;, &#39;seems&#39;, &#39;thereafter&#39;, &#39;whereby&#39;, &#39;behind&#39;, &#39;whether&#39;, &#39;ourselves&#39;, &#39;formerly&#39;, &#39;either&#39;, &#39;afterwards&#39;, &#39;its&#39;, &#39;various&#39;, &#39;whereafter&#39;, &#39;mostly&#39;, &#39;doing&#39;, &#39;those&#39;, &#39;to&#39;, &#39;nobody&#39;, &#39;perhaps&#39;, &#39;with&#39;, &#39;too&#39;, &#39;these&#39;, &#39;seem&#39;, &#39;toward&#39;, &#39;third&#39;, &#39;into&#39;, &#39;be&#39;, &#39;bottom&#39;, &#39;the&#39;, &#39;enough&#39;, &#39;amount&#39;, &#39;four&#39;, &#39;regarding&#39;, &#39;which&#39;, &#39;even&#39;, &#39;before&#39;, &#39;them&#39;, &#39;same&#39;, &#39;after&#39;, &#39;that&#39;, &#39;will&#39;, &#39;would&#39;, &#39;hereafter&#39;, &#39;elsewhere&#39;, &#39;through&#39;, &#39;how&#39;, &#39;whence&#39;, &#39;‘re&#39;, &#39;above&#39;, &#39;take&#39;, &#39;indeed&#39;, &#39;whereas&#39;, &#39;from&#39;, &#39;himself&#39;, &#39;did&#39;, &#39;quite&#39;, &#39;herein&#39;, &#39;he&#39;, &#39;yours&#39;, &#39;was&#39;, &#39;because&#39;, &#39;herself&#39;, &#39;us&#39;, &#39;thus&#39;, &#39;during&#39;, &#39;everywhere&#39;, &#39;been&#39;, &#34;&#39;re&#34;, &#39;another&#39;, &#39;no&#39;, &#39;several&#39;, &#39;much&#39;, &#39;due&#39;, &#39;throughout&#39;, &#39;within&#39;, &#39;still&#39;, &#39;except&#39;, &#39;n’t&#39;, &#39;as&#39;, &#39;my&#39;, &#39;whenever&#39;, &#39;fifteen&#39;, &#39;besides&#39;, &#39;sometimes&#39;, &#39;used&#39;, &#39;nine&#39;, &#34;&#39;s&#34;, &#39;across&#39;, &#39;somehow&#39;, &#39;yourselves&#39;, &#39;both&#39;, &#39;others&#39;, &#39;for&#39;, &#39;every&#39;, &#39;such&#39;, &#39;and&#39;, &#39;since&#39;, &#39;beforehand&#39;, &#39;’ve&#39;} .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/07/spacy_example.html",
            "relUrl": "/2020/11/07/spacy_example.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Kaggle Submission Example",
            "content": "Notebook and Code from https://github.com/jeffheaton/t81_558_deep_learning . import os import pandas as pd from sklearn.model_selection import train_test_split import tensorflow as tf import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from tensorflow.keras.callbacks import EarlyStopping df_train = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_iris_train.csv&quot;, na_values=[&#39;NA&#39;,&#39;?&#39;]) # Encode feature vector df_train.drop(&#39;id&#39;, axis=1, inplace=True) num_classes = len(df_train.groupby(&#39;species&#39;).species.nunique()) print(&quot;Number of classes: {}&quot;.format(num_classes)) # Convert to numpy - Classification x = df_train[[&#39;sepal_l&#39;, &#39;sepal_w&#39;, &#39;petal_l&#39;, &#39;petal_w&#39;]].values dummies = pd.get_dummies(df_train[&#39;species&#39;]) # Classification species = dummies.columns y = dummies.values # Split into train/test x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=45) # Train, with early stopping model = Sequential() model.add(Dense(50, input_dim=x.shape[1], activation=&#39;relu&#39;)) model.add(Dense(25)) model.add(Dense(y.shape[1],activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) monitor = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=1e-3, patience=5, verbose=1, mode=&#39;auto&#39;, restore_best_weights=True) model.fit(x_train,y_train,validation_data=(x_test,y_test), callbacks=[monitor],verbose=0,epochs=1000) . Number of classes: 3 Restoring model weights from the end of the best epoch. Epoch 00055: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x178e5493fc8&gt; . Now that we&#39;ve trained the neural network, we can check its log loss. . from sklearn import metrics # Calculate multi log loss error pred = model.predict(x_test) score = metrics.log_loss(y_test, pred) print(&quot;Log loss score: {}&quot;.format(score)) . Log loss score: 0.3136451941728592 . Now we are ready to generate the Kaggle submission file. We will use the iris test data that does not contain a $y$ target value. It is our job to predict this value and submit to Kaggle. . # Generate Kaggle submit file # Encode feature vector df_test = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_iris_test.csv&quot;, na_values=[&#39;NA&#39;,&#39;?&#39;]) # Convert to numpy - Classification ids = df_test[&#39;id&#39;] df_test.drop(&#39;id&#39;, axis=1, inplace=True) x = df_test[[&#39;sepal_l&#39;, &#39;sepal_w&#39;, &#39;petal_l&#39;, &#39;petal_w&#39;]].values y = dummies.values # Generate predictions pred = model.predict(x) #pred # Create submission data set df_submit = pd.DataFrame(pred) df_submit.insert(0,&#39;id&#39;,ids) df_submit.columns = [&#39;id&#39;,&#39;species-0&#39;,&#39;species-1&#39;,&#39;species-2&#39;] # Write submit file locally df_submit.to_csv(&quot;iris_submit.csv&quot;, index=False) print(df_submit) . id species-0 species-1 species-2 0 100 0.022236 0.533230 0.444534 1 101 0.003699 0.394908 0.601393 2 102 0.004600 0.420394 0.575007 3 103 0.956168 0.040161 0.003672 4 104 0.975333 0.022761 0.001906 5 105 0.966681 0.030938 0.002381 6 106 0.992637 0.007049 0.000314 7 107 0.002810 0.358485 0.638705 8 108 0.026152 0.557480 0.416368 9 109 0.001194 0.350682 0.648124 10 110 0.000649 0.268023 0.731328 11 111 0.994907 0.004923 0.000170 12 112 0.072954 0.587299 0.339747 13 113 0.000571 0.258208 0.741221 14 114 0.977138 0.021400 0.001463 15 115 0.004665 0.449740 0.545596 16 116 0.073553 0.567955 0.358493 17 117 0.968778 0.029240 0.001982 18 118 0.983742 0.015341 0.000918 19 119 0.986016 0.013193 0.000792 20 120 0.023752 0.583601 0.392647 21 121 0.032858 0.584882 0.382260 22 122 0.004007 0.395656 0.600338 23 123 0.000885 0.240763 0.758352 24 124 0.000531 0.271212 0.728256 25 125 0.985742 0.013471 0.000787 26 126 0.001298 0.320333 0.678369 27 127 0.001753 0.342856 0.655391 28 128 0.001147 0.317827 0.681026 29 129 0.981223 0.017589 0.001188 30 130 0.036438 0.578421 0.385140 31 131 0.976528 0.021834 0.001638 32 132 0.003681 0.405441 0.590878 33 133 0.024478 0.539376 0.436146 34 134 0.012039 0.466313 0.521649 35 135 0.963704 0.033453 0.002844 36 136 0.000614 0.244336 0.755050 37 137 0.008160 0.490362 0.501478 38 138 0.976859 0.021646 0.001495 39 139 0.003789 0.317224 0.678987 40 140 0.962254 0.034885 0.002861 41 141 0.000792 0.289380 0.709828 42 142 0.000253 0.239028 0.760719 43 143 0.001390 0.298506 0.700104 44 144 0.968422 0.029224 0.002354 45 145 0.029218 0.524128 0.446654 46 146 0.130497 0.579122 0.290381 47 147 0.023003 0.499443 0.477553 48 148 0.022195 0.527769 0.450036 49 149 0.983695 0.015325 0.000980 50 150 0.942703 0.052154 0.005144 . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from sklearn.model_selection import train_test_split from tensorflow.keras.callbacks import EarlyStopping import pandas as pd import io import os import requests import numpy as np from sklearn import metrics save_path = &quot;.&quot; df = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_auto_train.csv&quot;, na_values=[&#39;NA&#39;, &#39;?&#39;]) cars = df[&#39;name&#39;] # Handle missing value df[&#39;horsepower&#39;] = df[&#39;horsepower&#39;].fillna(df[&#39;horsepower&#39;].median()) # Pandas to Numpy x = df[[&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;]].values y = df[&#39;mpg&#39;].values # regression # Split into train/test x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=42) # Build the neural network model = Sequential() model.add(Dense(25, input_dim=x.shape[1], activation=&#39;relu&#39;)) # Hidden 1 model.add(Dense(10, activation=&#39;relu&#39;)) # Hidden 2 model.add(Dense(1)) # Output model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;) monitor = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=1e-3, patience=5, verbose=1, mode=&#39;auto&#39;, restore_best_weights=True) model.fit(x_train,y_train,validation_data=(x_test,y_test), verbose=2,callbacks=[monitor],epochs=1000) # Predict pred = model.predict(x_test) . Train on 261 samples, validate on 88 samples Epoch 1/1000 261/261 - 0s - loss: 382597.1196 - val_loss: 246687.4858 Epoch 2/1000 261/261 - 0s - loss: 192257.0072 - val_loss: 98804.3558 Epoch 3/1000 261/261 - 0s - loss: 67605.7908 - val_loss: 28617.0703 Epoch 4/1000 261/261 - 0s - loss: 15922.8367 - val_loss: 3325.1682 Epoch 5/1000 261/261 - 0s - loss: 1270.3832 - val_loss: 512.5387 Epoch 6/1000 261/261 - 0s - loss: 1118.9636 - val_loss: 1651.5679 Epoch 7/1000 261/261 - 0s - loss: 1703.0441 - val_loss: 1161.2368 Epoch 8/1000 261/261 - 0s - loss: 900.1420 - val_loss: 452.0660 Epoch 9/1000 261/261 - 0s - loss: 355.7248 - val_loss: 304.3305 Epoch 10/1000 261/261 - 0s - loss: 336.1776 - val_loss: 353.2767 Epoch 11/1000 261/261 - 0s - loss: 364.7770 - val_loss: 337.0882 Epoch 12/1000 261/261 - 0s - loss: 334.1086 - val_loss: 301.5655 Epoch 13/1000 261/261 - 0s - loss: 318.2330 - val_loss: 295.2506 Epoch 14/1000 261/261 - 0s - loss: 315.3628 - val_loss: 294.1454 Epoch 15/1000 261/261 - 0s - loss: 313.4151 - val_loss: 292.0427 Epoch 16/1000 261/261 - 0s - loss: 310.5834 - val_loss: 290.4511 Epoch 17/1000 261/261 - 0s - loss: 308.1132 - val_loss: 289.9176 Epoch 18/1000 261/261 - 0s - loss: 307.3153 - val_loss: 287.1054 Epoch 19/1000 261/261 - 0s - loss: 305.2746 - val_loss: 285.1501 Epoch 20/1000 261/261 - 0s - loss: 303.8164 - val_loss: 283.2582 Epoch 21/1000 261/261 - 0s - loss: 302.2492 - val_loss: 281.4607 Epoch 22/1000 261/261 - 0s - loss: 300.0016 - val_loss: 279.4577 Epoch 23/1000 261/261 - 0s - loss: 296.3905 - val_loss: 279.2795 Epoch 24/1000 261/261 - 0s - loss: 296.2508 - val_loss: 278.0922 Epoch 25/1000 261/261 - 0s - loss: 295.3600 - val_loss: 275.6349 Epoch 26/1000 261/261 - 0s - loss: 291.1920 - val_loss: 271.5592 Epoch 27/1000 261/261 - 0s - loss: 293.0040 - val_loss: 270.6060 Epoch 28/1000 261/261 - 0s - loss: 288.8120 - val_loss: 267.5230 Epoch 29/1000 261/261 - 0s - loss: 285.0153 - val_loss: 267.6846 Epoch 30/1000 261/261 - 0s - loss: 284.5063 - val_loss: 267.5903 Epoch 31/1000 261/261 - 0s - loss: 283.2598 - val_loss: 263.2579 Epoch 32/1000 261/261 - 0s - loss: 279.1897 - val_loss: 259.1413 Epoch 33/1000 261/261 - 0s - loss: 278.0727 - val_loss: 257.1468 Epoch 34/1000 261/261 - 0s - loss: 275.0580 - val_loss: 255.3159 Epoch 35/1000 261/261 - 0s - loss: 275.2246 - val_loss: 257.6078 Epoch 36/1000 261/261 - 0s - loss: 273.1009 - val_loss: 253.1600 Epoch 37/1000 261/261 - 0s - loss: 268.6169 - val_loss: 248.6043 Epoch 38/1000 261/261 - 0s - loss: 266.2035 - val_loss: 246.5989 Epoch 39/1000 261/261 - 0s - loss: 263.9700 - val_loss: 245.5532 Epoch 40/1000 261/261 - 0s - loss: 262.1468 - val_loss: 242.2550 Epoch 41/1000 261/261 - 0s - loss: 259.1994 - val_loss: 239.2889 Epoch 42/1000 261/261 - 0s - loss: 258.9926 - val_loss: 237.0006 Epoch 43/1000 261/261 - 0s - loss: 253.8787 - val_loss: 239.7331 Epoch 44/1000 261/261 - 0s - loss: 255.4787 - val_loss: 234.9061 Epoch 45/1000 261/261 - 0s - loss: 251.2081 - val_loss: 231.0518 Epoch 46/1000 261/261 - 0s - loss: 248.3354 - val_loss: 228.7012 Epoch 47/1000 261/261 - 0s - loss: 246.8801 - val_loss: 225.7509 Epoch 48/1000 261/261 - 0s - loss: 243.6159 - val_loss: 224.8320 Epoch 49/1000 261/261 - 0s - loss: 242.0351 - val_loss: 222.3293 Epoch 50/1000 261/261 - 0s - loss: 240.8072 - val_loss: 218.9842 Epoch 51/1000 261/261 - 0s - loss: 237.3082 - val_loss: 216.6910 Epoch 52/1000 261/261 - 0s - loss: 236.4236 - val_loss: 219.1308 Epoch 53/1000 261/261 - 0s - loss: 233.8834 - val_loss: 213.7722 Epoch 54/1000 261/261 - 0s - loss: 229.9621 - val_loss: 209.7647 Epoch 55/1000 261/261 - 0s - loss: 227.2555 - val_loss: 207.4864 Epoch 56/1000 261/261 - 0s - loss: 226.4306 - val_loss: 204.9454 Epoch 57/1000 261/261 - 0s - loss: 223.0296 - val_loss: 204.7334 Epoch 58/1000 261/261 - 0s - loss: 220.8694 - val_loss: 201.1248 Epoch 59/1000 261/261 - 0s - loss: 217.6376 - val_loss: 197.8849 Epoch 60/1000 261/261 - 0s - loss: 216.9886 - val_loss: 196.0564 Epoch 61/1000 261/261 - 0s - loss: 214.6863 - val_loss: 193.1452 Epoch 62/1000 261/261 - 0s - loss: 210.8178 - val_loss: 190.9064 Epoch 63/1000 261/261 - 0s - loss: 208.5358 - val_loss: 189.0982 Epoch 64/1000 261/261 - 0s - loss: 206.8594 - val_loss: 188.4019 Epoch 65/1000 261/261 - 0s - loss: 204.5793 - val_loss: 184.1434 Epoch 66/1000 261/261 - 0s - loss: 202.2459 - val_loss: 182.0629 Epoch 67/1000 261/261 - 0s - loss: 200.4653 - val_loss: 179.7517 Epoch 68/1000 261/261 - 0s - loss: 199.4847 - val_loss: 181.0924 Epoch 69/1000 261/261 - 0s - loss: 196.1007 - val_loss: 176.6571 Epoch 70/1000 261/261 - 0s - loss: 192.8669 - val_loss: 173.5703 Epoch 71/1000 261/261 - 0s - loss: 192.0731 - val_loss: 171.1448 Epoch 72/1000 261/261 - 0s - loss: 188.9124 - val_loss: 169.1036 Epoch 73/1000 261/261 - 0s - loss: 187.2660 - val_loss: 168.4244 Epoch 74/1000 261/261 - 0s - loss: 184.3366 - val_loss: 164.9515 Epoch 75/1000 261/261 - 0s - loss: 182.0560 - val_loss: 162.9232 Epoch 76/1000 261/261 - 0s - loss: 180.9339 - val_loss: 160.5111 Epoch 77/1000 261/261 - 0s - loss: 177.7289 - val_loss: 160.0768 Epoch 78/1000 261/261 - 0s - loss: 177.0166 - val_loss: 157.8780 Epoch 79/1000 261/261 - 0s - loss: 174.2729 - val_loss: 155.0140 Epoch 80/1000 261/261 - 0s - loss: 174.1473 - val_loss: 152.7101 Epoch 81/1000 261/261 - 0s - loss: 170.2462 - val_loss: 150.7686 Epoch 82/1000 261/261 - 0s - loss: 168.1250 - val_loss: 148.6464 Epoch 83/1000 261/261 - 0s - loss: 165.2611 - val_loss: 147.3025 Epoch 84/1000 261/261 - 0s - loss: 163.6456 - val_loss: 144.6445 Epoch 85/1000 261/261 - 0s - loss: 162.0391 - val_loss: 142.6984 Epoch 86/1000 261/261 - 0s - loss: 159.2869 - val_loss: 142.8578 Epoch 87/1000 261/261 - 0s - loss: 158.4979 - val_loss: 140.0451 Epoch 88/1000 261/261 - 0s - loss: 155.8697 - val_loss: 137.2706 Epoch 89/1000 261/261 - 0s - loss: 153.9711 - val_loss: 135.4351 Epoch 90/1000 261/261 - 0s - loss: 154.6780 - val_loss: 135.3691 Epoch 91/1000 261/261 - 0s - loss: 151.5339 - val_loss: 132.4053 Epoch 92/1000 261/261 - 0s - loss: 149.8378 - val_loss: 129.7334 Epoch 93/1000 261/261 - 0s - loss: 146.4563 - val_loss: 128.3390 Epoch 94/1000 261/261 - 0s - loss: 144.4933 - val_loss: 127.0931 Epoch 95/1000 261/261 - 0s - loss: 142.9235 - val_loss: 124.5410 Epoch 96/1000 261/261 - 0s - loss: 141.2332 - val_loss: 122.6840 Epoch 97/1000 261/261 - 0s - loss: 139.6225 - val_loss: 121.8140 Epoch 98/1000 261/261 - 0s - loss: 137.8158 - val_loss: 119.7630 Epoch 99/1000 261/261 - 0s - loss: 136.0081 - val_loss: 118.2237 Epoch 100/1000 261/261 - 0s - loss: 134.2485 - val_loss: 117.2276 Epoch 101/1000 261/261 - 0s - loss: 132.6553 - val_loss: 114.9724 Epoch 102/1000 261/261 - 0s - loss: 130.9867 - val_loss: 113.3426 Epoch 103/1000 261/261 - 0s - loss: 129.7633 - val_loss: 112.5253 Epoch 104/1000 261/261 - 0s - loss: 127.4988 - val_loss: 109.9802 Epoch 105/1000 261/261 - 0s - loss: 126.5202 - val_loss: 108.6993 Epoch 106/1000 261/261 - 0s - loss: 127.0090 - val_loss: 109.9802 Epoch 107/1000 261/261 - 0s - loss: 123.9040 - val_loss: 105.5228 Epoch 108/1000 261/261 - 0s - loss: 122.4337 - val_loss: 106.0400 Epoch 109/1000 261/261 - 0s - loss: 120.6300 - val_loss: 103.0620 Epoch 110/1000 261/261 - 0s - loss: 118.5036 - val_loss: 101.1414 Epoch 111/1000 261/261 - 0s - loss: 119.0572 - val_loss: 100.2416 Epoch 112/1000 261/261 - 0s - loss: 115.5790 - val_loss: 99.5907 Epoch 113/1000 261/261 - 0s - loss: 114.3071 - val_loss: 96.6901 Epoch 114/1000 261/261 - 0s - loss: 112.3629 - val_loss: 95.6015 Epoch 115/1000 261/261 - 0s - loss: 111.1829 - val_loss: 94.8623 Epoch 116/1000 261/261 - 0s - loss: 110.1737 - val_loss: 92.5723 Epoch 117/1000 261/261 - 0s - loss: 108.3667 - val_loss: 92.2069 Epoch 118/1000 261/261 - 0s - loss: 106.8793 - val_loss: 90.0196 Epoch 119/1000 261/261 - 0s - loss: 111.7453 - val_loss: 89.3325 Epoch 120/1000 261/261 - 0s - loss: 108.2630 - val_loss: 93.4876 Epoch 121/1000 261/261 - 0s - loss: 106.3677 - val_loss: 86.3017 Epoch 122/1000 261/261 - 0s - loss: 101.7241 - val_loss: 85.6503 Epoch 123/1000 261/261 - 0s - loss: 100.5858 - val_loss: 83.9417 Epoch 124/1000 261/261 - 0s - loss: 98.9622 - val_loss: 83.3914 Epoch 125/1000 261/261 - 0s - loss: 97.9784 - val_loss: 81.5708 Epoch 126/1000 261/261 - 0s - loss: 96.6995 - val_loss: 80.4465 Epoch 127/1000 261/261 - 0s - loss: 95.5034 - val_loss: 79.5468 Epoch 128/1000 261/261 - 0s - loss: 93.9933 - val_loss: 78.7416 Epoch 129/1000 261/261 - 0s - loss: 93.2547 - val_loss: 77.2559 Epoch 130/1000 261/261 - 0s - loss: 92.0739 - val_loss: 76.4692 Epoch 131/1000 261/261 - 0s - loss: 91.3897 - val_loss: 75.0902 Epoch 132/1000 261/261 - 0s - loss: 89.5802 - val_loss: 74.2796 Epoch 133/1000 261/261 - 0s - loss: 89.2358 - val_loss: 73.7019 Epoch 134/1000 261/261 - 0s - loss: 89.2894 - val_loss: 71.7912 Epoch 135/1000 261/261 - 0s - loss: 86.9927 - val_loss: 70.9630 Epoch 136/1000 261/261 - 0s - loss: 84.9979 - val_loss: 71.5301 Epoch 137/1000 261/261 - 0s - loss: 85.4751 - val_loss: 69.3716 Epoch 138/1000 261/261 - 0s - loss: 84.5646 - val_loss: 69.2690 Epoch 139/1000 261/261 - 0s - loss: 83.6890 - val_loss: 67.7983 Epoch 140/1000 261/261 - 0s - loss: 80.8676 - val_loss: 66.0073 Epoch 141/1000 261/261 - 0s - loss: 79.7220 - val_loss: 65.3198 Epoch 142/1000 261/261 - 0s - loss: 79.1109 - val_loss: 65.2558 Epoch 143/1000 261/261 - 0s - loss: 78.7909 - val_loss: 63.5800 Epoch 144/1000 261/261 - 0s - loss: 77.2276 - val_loss: 62.5765 Epoch 145/1000 261/261 - 0s - loss: 75.8473 - val_loss: 61.7780 Epoch 146/1000 261/261 - 0s - loss: 74.8493 - val_loss: 60.8583 Epoch 147/1000 261/261 - 0s - loss: 74.0530 - val_loss: 59.8856 Epoch 148/1000 261/261 - 0s - loss: 73.0771 - val_loss: 59.4027 Epoch 149/1000 261/261 - 0s - loss: 72.2401 - val_loss: 58.3119 Epoch 150/1000 261/261 - 0s - loss: 72.1309 - val_loss: 57.5037 Epoch 151/1000 261/261 - 0s - loss: 70.7773 - val_loss: 57.7769 Epoch 152/1000 261/261 - 0s - loss: 70.5883 - val_loss: 56.1087 Epoch 153/1000 261/261 - 0s - loss: 68.6020 - val_loss: 55.3935 Epoch 154/1000 261/261 - 0s - loss: 68.0137 - val_loss: 55.3946 Epoch 155/1000 261/261 - 0s - loss: 68.3630 - val_loss: 54.5067 Epoch 156/1000 261/261 - 0s - loss: 68.1104 - val_loss: 53.5928 Epoch 157/1000 261/261 - 0s - loss: 66.8734 - val_loss: 53.7972 Epoch 158/1000 261/261 - 0s - loss: 64.6184 - val_loss: 51.8031 Epoch 159/1000 261/261 - 0s - loss: 64.5744 - val_loss: 51.4003 Epoch 160/1000 261/261 - 0s - loss: 63.6910 - val_loss: 50.6856 Epoch 161/1000 261/261 - 0s - loss: 64.0145 - val_loss: 49.9536 Epoch 162/1000 261/261 - 0s - loss: 61.8386 - val_loss: 49.8901 Epoch 163/1000 261/261 - 0s - loss: 61.9306 - val_loss: 49.2521 Epoch 164/1000 261/261 - 0s - loss: 60.7556 - val_loss: 47.9911 Epoch 165/1000 261/261 - 0s - loss: 60.2802 - val_loss: 47.2594 Epoch 166/1000 261/261 - 0s - loss: 59.2542 - val_loss: 46.9898 Epoch 167/1000 261/261 - 0s - loss: 58.3004 - val_loss: 46.5502 Epoch 168/1000 261/261 - 0s - loss: 57.8545 - val_loss: 45.7245 Epoch 169/1000 261/261 - 0s - loss: 56.9617 - val_loss: 45.0827 Epoch 170/1000 261/261 - 0s - loss: 56.9749 - val_loss: 45.1476 Epoch 171/1000 261/261 - 0s - loss: 55.8050 - val_loss: 44.0151 Epoch 172/1000 261/261 - 0s - loss: 56.0478 - val_loss: 43.5957 Epoch 173/1000 261/261 - 0s - loss: 55.2461 - val_loss: 43.9503 Epoch 174/1000 261/261 - 0s - loss: 54.0493 - val_loss: 42.5281 Epoch 175/1000 261/261 - 0s - loss: 54.2585 - val_loss: 42.0300 Epoch 176/1000 261/261 - 0s - loss: 52.9849 - val_loss: 42.1091 Epoch 177/1000 261/261 - 0s - loss: 52.6699 - val_loss: 41.1280 Epoch 178/1000 261/261 - 0s - loss: 52.5766 - val_loss: 40.6279 Epoch 179/1000 261/261 - 0s - loss: 51.2797 - val_loss: 41.5560 Epoch 180/1000 261/261 - 0s - loss: 51.3167 - val_loss: 39.8998 Epoch 181/1000 261/261 - 0s - loss: 50.6548 - val_loss: 40.3602 Epoch 182/1000 261/261 - 0s - loss: 49.9360 - val_loss: 38.9575 Epoch 183/1000 261/261 - 0s - loss: 49.3195 - val_loss: 38.5161 Epoch 184/1000 261/261 - 0s - loss: 48.8159 - val_loss: 38.2727 Epoch 185/1000 261/261 - 0s - loss: 48.5230 - val_loss: 38.3134 Epoch 186/1000 261/261 - 0s - loss: 48.1472 - val_loss: 37.5338 Epoch 187/1000 261/261 - 0s - loss: 49.0451 - val_loss: 37.0337 Epoch 188/1000 261/261 - 0s - loss: 46.9509 - val_loss: 37.4614 Epoch 189/1000 261/261 - 0s - loss: 46.8951 - val_loss: 36.4360 Epoch 190/1000 261/261 - 0s - loss: 46.1027 - val_loss: 36.2615 Epoch 191/1000 261/261 - 0s - loss: 45.6384 - val_loss: 35.6610 Epoch 192/1000 261/261 - 0s - loss: 46.9916 - val_loss: 35.5148 Epoch 193/1000 261/261 - 0s - loss: 49.5148 - val_loss: 37.5214 Epoch 194/1000 261/261 - 0s - loss: 46.2516 - val_loss: 35.9050 Epoch 195/1000 261/261 - 0s - loss: 45.1961 - val_loss: 35.2473 Epoch 196/1000 261/261 - 0s - loss: 43.8845 - val_loss: 34.1322 Epoch 197/1000 261/261 - 0s - loss: 43.4610 - val_loss: 33.6880 Epoch 198/1000 261/261 - 0s - loss: 42.6286 - val_loss: 33.7127 Epoch 199/1000 261/261 - 0s - loss: 42.4154 - val_loss: 33.2152 Epoch 200/1000 261/261 - 0s - loss: 42.0020 - val_loss: 32.9451 Epoch 201/1000 261/261 - 0s - loss: 41.6191 - val_loss: 32.5093 Epoch 202/1000 261/261 - 0s - loss: 43.7235 - val_loss: 32.3695 Epoch 203/1000 261/261 - 0s - loss: 43.0863 - val_loss: 34.2041 Epoch 204/1000 261/261 - 0s - loss: 41.0544 - val_loss: 32.0973 Epoch 205/1000 261/261 - 0s - loss: 40.7787 - val_loss: 32.5461 Epoch 206/1000 261/261 - 0s - loss: 41.6360 - val_loss: 31.2820 Epoch 207/1000 261/261 - 0s - loss: 40.7417 - val_loss: 32.2974 Epoch 208/1000 261/261 - 0s - loss: 39.9822 - val_loss: 30.7600 Epoch 209/1000 261/261 - 0s - loss: 39.3857 - val_loss: 32.5769 Epoch 210/1000 261/261 - 0s - loss: 39.1410 - val_loss: 30.4246 Epoch 211/1000 261/261 - 0s - loss: 38.7447 - val_loss: 30.0492 Epoch 212/1000 261/261 - 0s - loss: 37.9753 - val_loss: 29.8627 Epoch 213/1000 261/261 - 0s - loss: 38.3355 - val_loss: 29.6306 Epoch 214/1000 261/261 - 0s - loss: 37.3530 - val_loss: 29.6433 Epoch 215/1000 261/261 - 0s - loss: 37.1885 - val_loss: 29.3205 Epoch 216/1000 261/261 - 0s - loss: 36.7803 - val_loss: 29.0165 Epoch 217/1000 261/261 - 0s - loss: 37.1867 - val_loss: 28.8259 Epoch 218/1000 261/261 - 0s - loss: 36.1244 - val_loss: 29.7593 Epoch 219/1000 261/261 - 0s - loss: 37.7266 - val_loss: 28.7380 Epoch 220/1000 261/261 - 0s - loss: 35.7875 - val_loss: 28.4919 Epoch 221/1000 261/261 - 0s - loss: 35.6227 - val_loss: 28.1351 Epoch 222/1000 261/261 - 0s - loss: 35.3527 - val_loss: 27.9021 Epoch 223/1000 261/261 - 0s - loss: 34.9739 - val_loss: 27.9576 Epoch 224/1000 261/261 - 0s - loss: 34.7204 - val_loss: 27.6015 Epoch 225/1000 261/261 - 0s - loss: 34.7849 - val_loss: 27.3595 Epoch 226/1000 261/261 - 0s - loss: 34.6823 - val_loss: 27.3375 Epoch 227/1000 261/261 - 0s - loss: 34.6561 - val_loss: 27.0810 Epoch 228/1000 261/261 - 0s - loss: 34.7526 - val_loss: 26.9909 Epoch 229/1000 261/261 - 0s - loss: 33.2568 - val_loss: 28.1786 Epoch 230/1000 261/261 - 0s - loss: 33.8126 - val_loss: 26.6188 Epoch 231/1000 261/261 - 0s - loss: 33.7432 - val_loss: 26.5590 Epoch 232/1000 261/261 - 0s - loss: 32.8556 - val_loss: 26.4733 Epoch 233/1000 261/261 - 0s - loss: 32.6516 - val_loss: 26.1711 Epoch 234/1000 261/261 - 0s - loss: 32.9949 - val_loss: 25.9887 Epoch 235/1000 261/261 - 0s - loss: 33.1170 - val_loss: 26.3875 Epoch 236/1000 261/261 - 0s - loss: 32.5374 - val_loss: 25.7300 Epoch 237/1000 261/261 - 0s - loss: 32.6500 - val_loss: 25.6121 Epoch 238/1000 261/261 - 0s - loss: 32.4430 - val_loss: 25.6679 Epoch 239/1000 261/261 - 0s - loss: 32.0512 - val_loss: 25.5607 Epoch 240/1000 261/261 - 0s - loss: 31.8485 - val_loss: 25.3327 Epoch 241/1000 261/261 - 0s - loss: 31.3820 - val_loss: 25.6274 Epoch 242/1000 261/261 - 0s - loss: 32.3983 - val_loss: 24.9405 Epoch 243/1000 261/261 - 0s - loss: 30.7282 - val_loss: 24.9071 Epoch 244/1000 261/261 - 0s - loss: 30.4659 - val_loss: 24.7043 Epoch 245/1000 261/261 - 0s - loss: 30.7127 - val_loss: 24.6449 Epoch 246/1000 261/261 - 0s - loss: 29.9609 - val_loss: 24.4984 Epoch 247/1000 261/261 - 0s - loss: 30.2372 - val_loss: 24.3524 Epoch 248/1000 261/261 - 0s - loss: 30.2689 - val_loss: 24.3986 Epoch 249/1000 261/261 - 0s - loss: 30.7721 - val_loss: 24.2271 Epoch 250/1000 261/261 - 0s - loss: 30.6043 - val_loss: 24.0360 Epoch 251/1000 261/261 - 0s - loss: 30.3024 - val_loss: 24.0987 Epoch 252/1000 261/261 - 0s - loss: 28.9162 - val_loss: 23.7909 Epoch 253/1000 261/261 - 0s - loss: 29.2801 - val_loss: 23.8153 Epoch 254/1000 261/261 - 0s - loss: 29.3222 - val_loss: 23.5515 Epoch 255/1000 261/261 - 0s - loss: 28.5132 - val_loss: 23.8399 Epoch 256/1000 261/261 - 0s - loss: 28.9835 - val_loss: 23.3674 Epoch 257/1000 261/261 - 0s - loss: 28.2271 - val_loss: 23.4548 Epoch 258/1000 261/261 - 0s - loss: 27.8565 - val_loss: 23.1535 Epoch 259/1000 261/261 - 0s - loss: 27.8770 - val_loss: 23.1761 Epoch 260/1000 261/261 - 0s - loss: 27.5445 - val_loss: 22.9507 Epoch 261/1000 261/261 - 0s - loss: 27.6223 - val_loss: 22.8882 Epoch 262/1000 261/261 - 0s - loss: 27.3854 - val_loss: 22.9048 Epoch 263/1000 261/261 - 0s - loss: 27.3946 - val_loss: 22.6476 Epoch 264/1000 261/261 - 0s - loss: 27.0089 - val_loss: 22.5546 Epoch 265/1000 261/261 - 0s - loss: 26.9027 - val_loss: 22.4856 Epoch 266/1000 261/261 - 0s - loss: 26.7630 - val_loss: 22.4675 Epoch 267/1000 261/261 - 0s - loss: 27.0150 - val_loss: 22.3077 Epoch 268/1000 261/261 - 0s - loss: 26.3339 - val_loss: 22.1958 Epoch 269/1000 261/261 - 0s - loss: 26.5861 - val_loss: 22.3650 Epoch 270/1000 261/261 - 0s - loss: 26.3245 - val_loss: 22.0337 Epoch 271/1000 261/261 - 0s - loss: 26.0610 - val_loss: 21.9219 Epoch 272/1000 261/261 - 0s - loss: 25.9908 - val_loss: 22.0404 Epoch 273/1000 261/261 - 0s - loss: 25.7291 - val_loss: 22.0628 Epoch 274/1000 261/261 - 0s - loss: 28.5037 - val_loss: 22.0770 Epoch 275/1000 261/261 - 0s - loss: 26.8031 - val_loss: 21.6196 Epoch 276/1000 261/261 - 0s - loss: 26.1467 - val_loss: 21.5624 Epoch 277/1000 261/261 - 0s - loss: 25.3375 - val_loss: 22.5604 Epoch 278/1000 261/261 - 0s - loss: 25.9187 - val_loss: 21.3272 Epoch 279/1000 261/261 - 0s - loss: 25.1155 - val_loss: 21.4415 Epoch 280/1000 261/261 - 0s - loss: 24.9094 - val_loss: 21.2730 Epoch 281/1000 261/261 - 0s - loss: 24.6625 - val_loss: 21.0808 Epoch 282/1000 261/261 - 0s - loss: 24.9405 - val_loss: 21.0107 Epoch 283/1000 261/261 - 0s - loss: 24.4015 - val_loss: 21.2510 Epoch 284/1000 261/261 - 0s - loss: 24.8920 - val_loss: 20.8377 Epoch 285/1000 261/261 - 0s - loss: 24.1691 - val_loss: 20.7580 Epoch 286/1000 261/261 - 0s - loss: 24.4140 - val_loss: 20.6990 Epoch 287/1000 261/261 - 0s - loss: 24.0014 - val_loss: 20.6096 Epoch 288/1000 261/261 - 0s - loss: 23.7787 - val_loss: 20.5134 Epoch 289/1000 261/261 - 0s - loss: 23.7206 - val_loss: 20.6378 Epoch 290/1000 261/261 - 0s - loss: 24.6226 - val_loss: 20.3711 Epoch 291/1000 261/261 - 0s - loss: 23.3452 - val_loss: 20.4992 Epoch 292/1000 261/261 - 0s - loss: 23.6446 - val_loss: 20.4952 Epoch 293/1000 261/261 - 0s - loss: 24.2056 - val_loss: 20.6786 Epoch 294/1000 261/261 - 0s - loss: 25.1895 - val_loss: 20.4868 Epoch 295/1000 Restoring model weights from the end of the best epoch. 261/261 - 0s - loss: 23.6036 - val_loss: 20.3795 Epoch 00295: early stopping . import numpy as np # Measure RMSE error. RMSE is common for regression. score = np.sqrt(metrics.mean_squared_error(pred,y_test)) print(&quot;Final score (RMSE): {}&quot;.format(score)) . Final score (RMSE): 4.5134384517538795 . import pandas as pd # Generate Kaggle submit file # Encode feature vector df_test = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_auto_test.csv&quot;, na_values=[&#39;NA&#39;,&#39;?&#39;]) # Convert to numpy - regression ids = df_test[&#39;id&#39;] df_test.drop(&#39;id&#39;, axis=1, inplace=True) # Handle missing value df_test[&#39;horsepower&#39;] = df_test[&#39;horsepower&#39;]. fillna(df[&#39;horsepower&#39;].median()) x = df_test[[&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;]].values # Generate predictions pred = model.predict(x) #pred # Create submission data set df_submit = pd.DataFrame(pred) df_submit.insert(0,&#39;id&#39;,ids) df_submit.columns = [&#39;id&#39;,&#39;mpg&#39;] # Write submit file locally df_submit.to_csv(&quot;auto_submit.csv&quot;, index=False) print(df_submit) . id mpg 0 350 29.112602 1 351 27.803200 2 352 27.981804 3 353 30.487831 4 354 27.227440 5 355 26.438324 6 356 27.886986 7 357 29.103935 8 358 26.447609 9 359 30.027260 10 360 30.312553 11 361 30.712151 12 362 23.952263 13 363 24.858467 14 364 23.459129 15 365 22.638985 16 366 26.032127 17 367 26.197884 18 368 28.448906 19 369 28.138954 20 370 27.352821 21 371 27.313377 22 372 26.464119 23 373 26.689583 24 374 26.546562 25 375 27.829781 26 376 27.466354 27 377 30.343369 28 378 29.985909 29 379 27.807251 30 380 28.450882 31 381 26.574844 32 382 28.199501 33 383 29.615051 34 384 29.048317 35 385 29.320534 36 386 29.582710 37 387 24.533165 38 388 24.426888 39 389 24.658607 40 390 21.805504 41 391 26.026482 42 392 24.947670 43 393 26.902489 44 394 26.575218 45 395 33.546684 46 396 24.233910 47 397 28.609993 48 398 28.913261 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/06/example_kaggle_project_submission.html",
            "relUrl": "/2020/11/06/example_kaggle_project_submission.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "World Cup prediction example",
            "content": "Notebook and code from https://www.kaggle.com/agostontorok/soccer-world-cup-2018-winner . Data . FIFA rankings from 1993 to 2018 (courtesy of Tadhg Fitzgerald | International Soccer matches from 1872 to 2018 (courtesy of Mart Jürisoo) | FIFA World Cup 2018 data set (courtesy of Nuggs) | . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from matplotlib import pyplot as plt rankings = pd.read_csv(&#39;fifa_ranking.csv&#39;) rankings = rankings.loc[:,[&#39;rank&#39;, &#39;country_full&#39;, &#39;country_abrv&#39;, &#39;cur_year_avg_weighted&#39;, &#39;rank_date&#39;, &#39;two_year_ago_weighted&#39;, &#39;three_year_ago_weighted&#39;]] rankings = rankings.replace({&quot;IR Iran&quot;: &quot;Iran&quot;}) rankings[&#39;weighted_points&#39;] = rankings[&#39;cur_year_avg_weighted&#39;] + rankings[&#39;two_year_ago_weighted&#39;] + rankings[&#39;three_year_ago_weighted&#39;] rankings[&#39;rank_date&#39;] = pd.to_datetime(rankings[&#39;rank_date&#39;]) . matches = pd.read_csv(&#39;international-football-results-from-1872-to-2017/results.csv&#39;) matches = matches.replace({&#39;Germany DR&#39;: &#39;Germany&#39;, &#39;China&#39;: &#39;China PR&#39;}) matches[&#39;date&#39;] = pd.to_datetime(matches[&#39;date&#39;]) . world_cup = pd.read_csv(&#39;World Cup 2018 Dataset.csv&#39;) world_cup = world_cup.loc[:, [&#39;Team&#39;, &#39;Group&#39;, &#39;First match nagainst&#39;, &#39;Second match n against&#39;, &#39;Third match n against&#39;]] world_cup = world_cup.dropna(how=&#39;all&#39;) world_cup = world_cup.replace({&quot;IRAN&quot;: &quot;Iran&quot;, &quot;Costarica&quot;: &quot;Costa Rica&quot;, &quot;Porugal&quot;: &quot;Portugal&quot;, &quot;Columbia&quot;: &quot;Colombia&quot;, &quot;Korea&quot; : &quot;Korea Republic&quot;}) world_cup = world_cup.set_index(&#39;Team&#39;) . # I want to have the ranks for every day rankings = rankings.set_index([&#39;rank_date&#39;]) .groupby([&#39;country_full&#39;], group_keys=False) .resample(&#39;D&#39;).first() .fillna(method=&#39;ffill&#39;) .reset_index() # join the ranks matches = matches.merge(rankings, left_on=[&#39;date&#39;, &#39;home_team&#39;], right_on=[&#39;rank_date&#39;, &#39;country_full&#39;]) matches = matches.merge(rankings, left_on=[&#39;date&#39;, &#39;away_team&#39;], right_on=[&#39;rank_date&#39;, &#39;country_full&#39;], suffixes=(&#39;_home&#39;, &#39;_away&#39;)) . # feature generation matches[&#39;rank_difference&#39;] = matches[&#39;rank_home&#39;] - matches[&#39;rank_away&#39;] matches[&#39;average_rank&#39;] = (matches[&#39;rank_home&#39;] + matches[&#39;rank_away&#39;])/2 matches[&#39;point_difference&#39;] = matches[&#39;weighted_points_home&#39;] - matches[&#39;weighted_points_away&#39;] matches[&#39;score_difference&#39;] = matches[&#39;home_score&#39;] - matches[&#39;away_score&#39;] matches[&#39;is_won&#39;] = matches[&#39;score_difference&#39;] &gt; 0 # take draw as lost matches[&#39;is_stake&#39;] = matches[&#39;tournament&#39;] != &#39;Friendly&#39; . # I tried earlier the team as well but that did not make a difference either matches[&#39;wc_participant&#39;] = matches[&#39;home_team&#39;] * matches[&#39;home_team&#39;].isin(world_cup.index.tolist()) matches[&#39;wc_participant&#39;] = matches[&#39;wc_participant&#39;].replace({&#39;&#39;:&#39;Other&#39;}) matches = matches.join(pd.get_dummies(matches[&#39;wc_participant&#39;])) . from sklearn import linear_model from sklearn import ensemble from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures X, y = matches.loc[:,[&#39;average_rank&#39;, &#39;rank_difference&#39;, &#39;point_difference&#39;, &#39;is_stake&#39;]], matches[&#39;is_won&#39;] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42) logreg = linear_model.LogisticRegression(C=1e-5) features = PolynomialFeatures(degree=2) model = Pipeline([ (&#39;polynomial_features&#39;, features), (&#39;logistic_regression&#39;, logreg) ]) model = model.fit(X_train, y_train) # figures fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1]) plt.figure(figsize=(15,5)) ax = plt.subplot(1,3,1) ax.plot([0, 1], [0, 1], &#39;k--&#39;) ax.plot(fpr, tpr) ax.set_title(&#39;AUC score is {0:0.2}&#39;.format(roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))) ax.set_aspect(1) ax = plt.subplot(1,3,2) cm = confusion_matrix(y_test, model.predict(X_test)) ax.imshow(cm, cmap=&#39;Blues&#39;, clim = (0, cm.max())) ax.set_xlabel(&#39;Predicted label&#39;) ax.set_title(&#39;Performance on the Test set&#39;) ax = plt.subplot(1,3,3) cm = confusion_matrix(y_train, model.predict(X_train)) ax.imshow(cm, cmap=&#39;Blues&#39;, clim = (0, cm.max())) ax.set_xlabel(&#39;Predicted label&#39;) ax.set_title(&#39;Performance on the Training set&#39;) pass . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . # let&#39;s define the rankings at the time of the World Cup world_cup_rankings = rankings.loc[(rankings[&#39;rank_date&#39;] == rankings[&#39;rank_date&#39;].max()) &amp; rankings[&#39;country_full&#39;].isin(world_cup.index.unique())] world_cup_rankings = world_cup_rankings.set_index([&#39;country_full&#39;]) . import progressbar simulation_results = list() n_simulations = 10000 #bar = progressbar.ProgressBar(max_value=n_simulations) for i in range(n_simulations): #bar.update(i) candidates = [&#39;France&#39;, &#39;Argentina&#39;, &#39;Uruguay&#39;, &#39;Portugal&#39;, &#39;Spain&#39;, &#39;Russia&#39;,&#39;Croatia&#39;, &#39;Denmark&#39;, &#39;Brazil&#39;, &#39;Mexico&#39;, &#39;Belgium&#39;, &#39;Japan&#39;, &#39;Sweden&#39;, &#39;Switzerland&#39;, &#39;Colombia&#39;, &#39;England&#39;] finals = [&#39;round_of_16&#39;, &#39;quarterfinal&#39;, &#39;semifinal&#39;, &#39;final&#39;] for f in finals: iterations = int(len(candidates) / 2) winners = [] for i in range(iterations): home = candidates[i*2] away = candidates[i*2+1] row = pd.DataFrame(np.array([[np.nan, np.nan, np.nan, True]]), columns=X_test.columns) home_rank = world_cup_rankings.loc[home, &#39;rank&#39;] home_points = world_cup_rankings.loc[home, &#39;weighted_points&#39;] opp_rank = world_cup_rankings.loc[away, &#39;rank&#39;] opp_points = world_cup_rankings.loc[away, &#39;weighted_points&#39;] row[&#39;average_rank&#39;] = (home_rank + opp_rank) / 2 row[&#39;rank_difference&#39;] = home_rank - opp_rank row[&#39;point_difference&#39;] = home_points - opp_points home_win_prob = model.predict_proba(row)[:,1][0] # simulation step based on the probability simulated_outcome = np.random.binomial(1, home_win_prob) winners.append(away) if simulated_outcome &lt;= 0.5 else winners.append(home) candidates = winners simulations_results = simulation_results.append(candidates) simulation_results = sum(simulation_results, []) . pd.Series(simulation_results).value_counts().sort_values().divide(n_simulations).plot.barh(figsize=(10,5)) plt.ylabel(&#39;Winning probability&#39;) . Text(0, 0.5, &#39;Winning probability&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/05/soccer-fifa-pred-example.html",
            "relUrl": "/2020/11/05/soccer-fifa-pred-example.html",
            "date": " • Nov 5, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Webscraping Text and Images with BeautifulSoup example",
            "content": "This notebook code is from the app found here: https://github.com/kenichinakanishi/houseplant_classifier/ . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . from urllib.request import Request, urlopen from bs4 import BeautifulSoup def getHTMLContent(link): html = urlopen(link) soup = BeautifulSoup(html, &#39;html.parser&#39;) return soup . req = Request(&#39;https://www.aspca.org/pet-care/animal-poison-control/cats-plant-list&#39;, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}) webpage = urlopen(req).read() # Soupify the webpage soup = BeautifulSoup(webpage, &#39;lxml&#39;) # Search through the parse tree to get all the content from the table content_list = soup.find_all(&#39;span&#39;)[7:-4] # Put it in a dataframe for further processing df_cats = pd.DataFrame(content_list) . /home/gao/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray values = np.array([convert(v) for v in values]) . # Clean up the strings df_cats[0] = df_cats[0].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_cats[4] = df_cats[4].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_cats[1] = df_cats[1].apply(lambda x: str(x).split(&#39;(&#39;)[1][0:-4]) # Get rid of useless columns and rename the columns df_cats = df_cats.drop(columns=[2,3,5,6]).rename(columns = {0:&#39;Name&#39;,1:&#39;Alternative Names&#39;,4:&#39;Scientific Name&#39;,7:&#39;Family&#39;}) # Separate toxic and non-toxic plants df_cats[&#39;Toxic to Cats&#39;] = True first_nontoxic_cats = [index for index in df_cats[df_cats[&#39;Name&#39;].str.startswith(&#39;A&#39;)].index if index&gt;100][0] df_cats.loc[first_nontoxic_cats:,&#39;Toxic to Cats&#39;] = False . df_cats . Name Alternative Names Scientific Name Family Toxic to Cats . 0 Adam-and-Eve | Arum, Lord-and-Ladies, Wake Robin, Starch Root... | Arum maculatum | Araceae | True | . 1 African Wonder Tree | | Ricinus communis | | True | . 2 Alocasia | Elephant&#39;s Ear | Alocasia spp. | Araceae | True | . 3 Aloe | | Aloe vera | Liliaceae | True | . 4 Amaryllis | Many, including: Belladonna lily, Saint Joseph... | Amaryllis spp. | Amaryllidaceae | True | . ... ... | ... | ... | ... | ... | . 980 Yellowrocket | | Barbarea vulgaris | Brassicaceae | False | . 981 Yorba Linda | | Peperomia rotundifolia | Piperaceae | False | . 982 Zebra Haworthia | | Haworthia fasciata | Liliaceae | False | . 983 Zinnia | | Zinnia species | Asteraceae | False | . 984 Zucchini Squash | | Cucurbia pepo cv zucchini | Cucurbitaceae | False | . 985 rows × 5 columns . req = Request(&#39;https://www.aspca.org/pet-care/animal-poison-control/dogs-plant-list&#39;, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}) webpage = urlopen(req).read() soup = BeautifulSoup(webpage, &#39;lxml&#39;) # soupify the webpage content_list = soup.find_all(&#39;span&#39;)[7:-4] # Get all the content from the table df_dogs = pd.DataFrame(content_list) # Put it in a dataframe for processing . /home/gao/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray values = np.array([convert(v) for v in values]) . # Clean up the strings df_dogs[0] = df_dogs[0].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_dogs[4] = df_dogs[4].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_dogs[1] = df_dogs[1].apply(lambda x: str(x).split(&#39;(&#39;)[1][0:-4]) # Get rid of useless columns and rename the columns df_dogs = df_dogs.drop(columns=[2,3,5,6]).rename(columns = {0:&#39;Name&#39;,1:&#39;Alternative Names&#39;,4:&#39;Scientific Name&#39;,7:&#39;Family&#39;}) # Separate toxic and non-toxic plants df_dogs[&#39;Toxic to Dogs&#39;] = True first_nontoxic_dogs = [index for index in df_dogs[df_dogs[&#39;Name&#39;].str.startswith(&#39;A&#39;)].index if index&gt;100][0] df_dogs.loc[first_nontoxic_dogs:,&#39;Toxic to Dogs&#39;] = False . # Merge dataframes into one, outer merge used to retain values that only exist on one side df_catsdogs = df_dogs.merge(df_cats, how=&#39;outer&#39;, on=[&#39;Name&#39;,&#39;Alternative Names&#39;,&#39;Scientific Name&#39;,&#39;Family&#39;]) df_catsdogs = df_catsdogs.fillna(&#39;Unknown&#39;) aspca_df = df_catsdogs.copy() # Assume same toxicity for dogs and cats if unknown aspca_df[&#39;Toxic to Cats&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Dogs&#39;] if (x[&#39;Toxic to Cats&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Cats&#39;], axis=1) aspca_df[&#39;Toxic to Dogs&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Cats&#39;] if (x[&#39;Toxic to Dogs&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Dogs&#39;], axis=1) . # Merge dataframes into one, outer merge used to retain values that only exist on one side df_catsdogs = df_dogs.merge(df_cats, how=&#39;outer&#39;, on=[&#39;Name&#39;,&#39;Alternative Names&#39;,&#39;Scientific Name&#39;,&#39;Family&#39;]) df_catsdogs = df_catsdogs.fillna(&#39;Unknown&#39;) aspca_df = df_catsdogs.copy() # Assume same toxicity for dogs and cats if unknown aspca_df[&#39;Toxic to Cats&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Dogs&#39;] if (x[&#39;Toxic to Cats&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Cats&#39;], axis=1) aspca_df[&#39;Toxic to Dogs&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Cats&#39;] if (x[&#39;Toxic to Dogs&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Dogs&#39;], axis=1) . aspca_df.sample(10) . Name Alternative Names Scientific Name Family Toxic to Dogs Toxic to Cats . 810 Pink Splash | Flamingo Plant, Polka Dot Plant, Measles Plant... | Hypoestes phyllostachya | Acanthaceae | False | False | . 120 English Ivy | Branching Ivy, Glacier Ivy, Needlepoint Ivy, S... | Hedera helix | Araliaceae | True | True | . 564 Crape Myrtle | Crepe Myrtle | Lagerstroemia indica | Lythraceae | False | False | . 201 Japanese Yew | English Yew, Western Yew, Pacific Yew, Anglo-J... | Taxus sp. | Taxaceae | True | True | . 635 Giant Touch-Me-Not | Buzzy Lizzie, Impatience Plant, Patient Lucy, ... | Impatiens spp. | Balsaminaceae | False | False | . 92 Cowbane | Water Hemlock, Poison Parsnip | Cicuta species | Apiaceae | True | True | . 277 Ornamental Pepper | Natal Cherry, Winter Cherry, Jerusalem Cherry | Solanum pseudocapsicum | Solanaceae | True | True | . 513 Carrot Fern | | Onychium japonica | Polypodiaceae | False | False | . 712 Leather Peperomia | | Peperomia crassifolia | Piperaceae | False | False | . 493 California Pitcher Plant | Cobra Orchid, Cobra Plant, Cobra Lily, Chrysam... | Darlingtonia californica | Sarraceniaceae | False | False | . aspca_df = aspca_df.drop_duplicates(&#39;Scientific Name&#39;) # Get rid of duplicates aspca_df = aspca_df.reset_index(drop=True).sort_index() # Reset and sort index . aspca_df = aspca_df.drop(aspca_df[aspca_df[&#39;Scientific Name&#39;].isin([&#39;&#39;,&#39;NONE LISTED&#39;])].index,axis=0).reset_index(drop=True).sort_index() # Fix mistakes in database . # Ensure proper punctuation for each scientific name. def normalize_capitalization(x): first_word, rest = x.split()[0], x.split()[1:] first_word = [first_word.capitalize()] rest = [word.lower() for word in rest] return &#39; &#39;.join(first_word+rest) # Clean up repeated species that have different names def species_normalizer(word): if word.split()[-1] in [&#39;sp&#39;,&#39;species&#39;,&#39;spp&#39;,&#39;sp.&#39;,&#39;spp.&#39;]: word = &#39;&#39;.join(word.split()[:-1]) return word # Remove cv from names, as it is an outdated way of referring to cultivars def cv_remover(word): if &#39;cv&#39; in word: word = word.replace(&#39; cv &#39;,&#39; &#39;) return word # Remove var. from names def var_remover(word): if &#39;var&#39; in word: word = word.replace(&#39; var. &#39;,&#39; &#39;) return word # Apply each of the functions aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(normalize_capitalization) aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(species_normalizer) aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(cv_remover) aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(var_remover) # Remove special characters aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(lambda x: &#39;&#39;.join([character for character in x if character.isalnum() or character.isspace()])) # Reset dataframe for further processing aspca_df = aspca_df.sort_values(&#39;Scientific Name&#39;).drop_duplicates(&#39;Scientific Name&#39;) aspca_df = aspca_df.reset_index(drop=True).sort_index() . aspca_df.sample(10) . Name Alternative Names Scientific Name Family Toxic to Dogs Toxic to Cats . 108 American Bittersweet | Bittersweet, Waxwork, Shrubby Bittersweet, Fal... | Celastrus scandens | Celastraceae | True | True | . 530 Pacific Yew | English Yew, Western Yew, Japanese Yew, Anglo-... | Taxus brevifolia | Taxaceae | True | True | . 467 Pie Plant | Rhubarb | Rheum rhabarbarium | Polygonaceae | True | True | . 164 Pheasant Plant | Zebra Plant | Cryptanthus zonatus | Bromeliaceae | False | False | . 452 Primrose | | Primula vulgaris | Primulaceae | True | True | . 506 Jackson Brier | | Smilax lanceolata | Liliaceae | False | False | . 407 Ivy Peperomia | Plantinum Peperomia, Silver leaf Peperomia, Iv... | Peperomia griseoargentea | Piperaceae | False | False | . 147 Poison Hemlock | Poison Parsley, Spotted Hemlock, Winter Fern, ... | Conium maculatum | Umbelliferae | True | True | . 351 Cardinal Flower | Lobelia, Indian Pink | Lobelia cardinalis | Campanulaceae | True | True | . 236 Pink Brocade | | Episcia cultivar | Gesneriaceae | False | False | . use_cols = [&#39;scientificName&#39;,&#39;taxonRank&#39;,&#39;family&#39;,&#39;genus&#39;,&#39;taxonomicStatus&#39;,&#39;taxonID&#39;, &#39;acceptedNameUsageID&#39;] wfo_df = pd.read_csv(&#39;../classification.txt&#39;, sep=&#39; t&#39;, lineterminator=&#39; n&#39;, usecols=use_cols) wfo_df = wfo_df.sort_values(&#39;taxonomicStatus&#39;) . /home/gao/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . wfo_df.sample(10) . taxonID scientificName taxonRank family genus taxonomicStatus acceptedNameUsageID . 796160 wfo-0000798814 | Peridium oblongifolium | SPECIES | Peraceae | Peridium | Synonym | wfo-0000267144 | . 180708 wfo-0000180970 | Cracca smallii | SPECIES | Fabaceae | Cracca | Synonym | wfo-0000178756 | . 911945 wfo-0000914633 | Thinopyrum turcicum | SPECIES | Poaceae | Thinopyrum | Synonym | wfo-0000866236 | . 167159 wfo-0000167369 | Indigofera cinerea | SPECIES | Fabaceae | Indigofera | Synonym | wfo-0000173646 | . 642316 wfo-0000644639 | Diaphanoptera khorasanica | SPECIES | Caryophyllaceae | Diaphanoptera | Accepted | NaN | . 464965 wfo-0000466716 | Phyllocyclus minutiflorus | SPECIES | Gentianaceae | Phyllocyclus | Doubtful | NaN | . 740337 wfo-0000742945 | Daphne pseudomezereum var. koreana | VARIETY | Thymelaeaceae | Daphne | Synonym | wfo-0000637684 | . 868404 wfo-0000871073 | Festuca montis-aurei | SPECIES | Poaceae | Festuca | Synonym | wfo-0000869683 | . 186218 wfo-0000186502 | Lotononis curvicarpa | SPECIES | Fabaceae | Lotononis | Accepted | NaN | . 552490 wfo-0000554468 | Specklinia casualis | SPECIES | Orchidaceae | Specklinia | Synonym | wfo-0000339564 | . # Don&#39;t need this column, we trust the WFO database more aspca_df.drop(&#39;Family&#39;, axis=1, inplace=True) # Merge dataframes together to get trusted info aspca_df = aspca_df.merge(wfo_df, how = &#39;left&#39;, left_on = [&#39;Scientific Name&#39;], right_on = [&#39;scientificName&#39;]) # Sort by taxonomicStatus and drop duplicates keeping the first - keeping accepted names as priority aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) # Fill NaN&#39;s with Unknown aspca_df = aspca_df.fillna(&#39;Unknown&#39;) . # Clean up and deal with scientific names that are unknown, due to misspellings or otherwise. aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) unknown_idx = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;].index print(len(unknown_idx)) . 101 . def get_closest_name(unknown_name, name_df = wfo_df, name_col = &#39;scientificName&#39;, threshold=0.9, verbose=False): &quot;&quot;&quot; Matches an &#39;unknown_name&#39; against accepted names in a &#39;name_df&#39;. Will return names that are above a &#39;threshold&#39; of closeness. Parameters - unknown_name: str Name we want to match against accepted names. name_df: DataFrame DataFrame containing accepted names. name_col: str, name of name_df column DataFrame column containing accepted names. threshold: int How closely does the unknown_name need to match with the accepted name. If above this threshold, the name is added to a dictionary of possible names. verbose: bool Should the function print the entire list of possible names. Returns: - str Closest name to &#39;unknown_name&#39; that was above the given &#39;threshold&#39;. &quot;&quot;&quot; import operator from difflib import SequenceMatcher def similar(a, b): return SequenceMatcher(None, a, b).ratio() poss_names = {} # Only look through entries with the same first letter to save time for true_sciname in name_df[name_df[name_col].str.startswith(unknown_name[0])][name_col].values: similar_score = similar(unknown_name, true_sciname) if similar_score&gt;threshold: poss_names[true_sciname]=similar_score # If the dict is empty if verbose == True: print(poss_names) if not bool(poss_names): print(f&#39;No names close enough to {unknown_name}.&#39;) return &#39;&#39; else: print(f&#39;{unknown_name} is closest to {max(poss_names.items(), key=operator.itemgetter(1))[0]}, with a score of {max(poss_names.items(), key=operator.itemgetter(1))[1]:.2f}&#39;) return max(poss_names.items(), key=operator.itemgetter(1))[0] . def fix_name(unknown_name, true_name): &quot;&quot;&quot; Fixes the aspca_df entries according to the accepted wfo_df entry. Parameters - unknown_name: str Name we want to fix. true_name: DataFrame Accepted name to use. &quot;&quot;&quot; # Get the series we&#39;re looking to change unknown_data = aspca_df[aspca_df[&#39;Scientific Name&#39;] == unknown_name] # Grab accepted data from wfo database based on ID lookup true_data = wfo_df[wfo_df[&#39;scientificName&#39;] == true_name] true_sciname = true_data.loc[:,&#39;scientificName&#39;].values[0] true_family = true_data.loc[:,&#39;family&#39;].values[0] true_genus = true_data.loc[:,&#39;genus&#39;].values[0] true_taxonomicStatus = true_data.loc[:,&#39;taxonomicStatus&#39;].values[0] # Change scientific name, family, genus and taxonomic status to accepted versions aspca_df.iloc[unknown_data.index,2] = true_sciname aspca_df.iloc[unknown_data.index,8] = true_family aspca_df.iloc[unknown_data.index,9] = true_genus aspca_df.iloc[unknown_data.index,10] = true_taxonomicStatus . unknown_idx = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;].index print(f&#39;{len(unknown_idx)} plants currently cannot be matched.&#39;) from tqdm.notebook import tqdm for i in tqdm(unknown_idx): unknown_name = aspca_df.iloc[i,2] closest_name = get_closest_name(unknown_name) if closest_name == &#39;&#39;: continue fix_name(unknown_name,closest_name) . 101 plants currently cannot be matched. Malus sylvestrus is closest to Malus sylvestris, with a score of 0.94 No names close enough to Maranta insignis. No names close enough to Miltonia roezlii alba. No names close enough to Neoregalia. No names close enough to Nephrolepis exalta bostoniensis. Nephrolepsis exalta is closest to Nephrolepis exaltata, with a score of 0.92 No names close enough to Nephrolepsis cordifolia duffii. No names close enough to Lilium orientalis. No names close enough to Nephrolepsis cordifolia plumosa. Nephrolepis exalta is closest to Nephrolepis exaltata, with a score of 0.95 No names close enough to Lilium asiatica. Hosta plataginea is closest to Hosta plantaginea, with a score of 0.97 No names close enough to Lampranthus piquet. Kalmia poliifolia is closest to Kalmia polifolia, with a score of 0.97 Kalmia augustifolia is closest to Kalmia angustifolia, with a score of 0.95 Jasminium is closest to Jasminum, with a score of 0.94 Hoya publcalyx is closest to Hoya pubicalyx, with a score of 0.93 No names close enough to Hoya carnosa krinkle kurl. No names close enough to Hemigraphis exotica. Gynura aurantica is closest to Gynura aurantiaca, with a score of 0.97 No names close enough to Nolina tuberculata. Guzmania lingulata minor is closest to Guzmania lingulata var. minor, with a score of 0.91 Lavendula angustifolia is closest to Lavandula angustifolia, with a score of 0.95 Onychium japonica is closest to Onychium japonicum, with a score of 0.91 No names close enough to Schefflera or brassia actinoplylla. Paeonis officinalis is closest to Paeonia officinalis, with a score of 0.95 No names close enough to Giant dracaena. Taxus canadensus is closest to Taxus canadensis, with a score of 0.94 Stapelia hirsata is closest to Stapelia hirsuta, with a score of 0.94 Sorghum vulgare var sudanesis is closest to Sorghum vulgare var. sudanense, with a score of 0.92 Smilax walteria is closest to Smilax walteri, with a score of 0.97 Secum weinbergii is closest to Sedum weinbergii, with a score of 0.94 No names close enough to Scindapsusphilodendron. Santpaulia confusa is closest to Saintpaulia confusa, with a score of 0.97 Rhipsalis cassutha is closest to Rhipsalis cassytha, with a score of 0.94 Rheum rhabarbarium is closest to Rheum rhabarbarum, with a score of 0.97 Origanum vulgare hirtum is closest to Origanum vulgare var. hirtum, with a score of 0.90 Tolmeia menziesii is closest to Tolmiea menziesii, with a score of 0.94 Podocarpus macrophylla is closest to Podocarpus macrophyllus, with a score of 0.93 Ploystichum munitum is closest to Polystichum munitum, with a score of 0.95 Plectranthus oetendahlii is closest to Plectranthus oertendahlii, with a score of 0.98 Plantanus occidentalis is closest to Platanus occidentalis, with a score of 0.98 Pilea cadieri is closest to Pilea cadierei, with a score of 0.96 No names close enough to Phoenix robellinii. No names close enough to Peperomia serpens variegata. Peperomia prostata is closest to Peperomia prostrata, with a score of 0.97 Peperomia griseoargentea is closest to Peperomia griseoargentia, with a score of 0.96 Pellonia pulchra is closest to Pellionia pulchra, with a score of 0.97 Rhapis flabelliformus is closest to Rhapis flabelliformis, with a score of 0.95 Fuschsia is closest to Fuchsia, with a score of 0.93 No names close enough to Begonia rex peace. Eriogonium umbellatum is closest to Eriogonum umbellatum, with a score of 0.98 Citrus aurantifolia is closest to Citrus aurantiifolia, with a score of 0.97 Cissus dicolor is closest to Cissus discolor, with a score of 0.97 Chlorophytum bichetti is closest to Chlorophytum bichetii, with a score of 0.95 No names close enough to Ceratostigma larpentiae. Cattleya trianaei is closest to Cattleya trianae, with a score of 0.97 Camellia japonica thea japonica is closest to Camellia japonica var. japonica, with a score of 0.90 Caesalpinia gilliessi is closest to Caesalpinia gilliesii, with a score of 0.95 Borage officinalis is closest to Borago officinalis, with a score of 0.94 No names close enough to Bertolonia mosaica. No names close enough to Begonia semperflorens cultivar. Begonia scharfii is closest to Begonia scharffii, with a score of 0.97 Begonia cleopatra is closest to Begonia cleopatrae, with a score of 0.97 No names close enough to Asparagus densiflorus sprengeri. Arum palestinum is closest to Arum palaestinum, with a score of 0.97 Anthurium scherzeranum is closest to Anthurium scherzerianum, with a score of 0.98 Anthirrhinum multiflorum is closest to Antirrhinum multiflorum, with a score of 0.98 Anoectuchilus setaceus is closest to Anoectochilus setaceus, with a score of 0.95 Anethum graveolena is closest to Anethum graveolens, with a score of 0.94 No names close enough to Albiflora. No names close enough to Acantha. Tradescantia flumeninsis is closest to Tradescantia fluminensis, with a score of 0.92 Citrus aurantium is closest to Citrus ×aurantium, with a score of 0.97 Euonymus atropurpurea is closest to Euonymus atropurpureus, with a score of 0.93 Citrus limonia is closest to Citrus ×limonia, with a score of 0.97 Cleome hasserlana is closest to Cleome hassleriana, with a score of 0.91 Eriogonium inflatum is closest to Eriogonum inflatum, with a score of 0.97 No names close enough to Episcia cultivar. Epidendrum atropurpeum is closest to Epidendrum atropurpureum, with a score of 0.96 Eleagnus is closest to Elaeagnus, with a score of 0.94 No names close enough to Echeveria puloliver. Echeveria pulinata is closest to Echeveria pulvinata, with a score of 0.97 No names close enough to Echevaria. No names close enough to Dypsis lutescens chrysalidocarpus lutescens alternate scientific name. No names close enough to Draceana. No names close enough to Daucus carota sativa. Citrus paradisii is closest to Citrus paradisi, with a score of 0.97 No names close enough to Cycasrevolutazamia. No names close enough to Cucurbita maxima turbaniformis. No names close enough to Cucurbita maxima hubbard. No names close enough to Cucurbita maxima butternut. No names close enough to Cucurbita maxima buttercup. No names close enough to Cucurbita maxima banana. No names close enough to Cucurbia pepo zucchini. No names close enough to Cryptanthus bivattus minor. Coleus ampoinicus is closest to Coleus amboinicus, with a score of 0.94 Clivia minata is closest to Clivia miniata, with a score of 0.96 Clintonia umbelluata is closest to Clintonia umbellulata, with a score of 0.98 No names close enough to Cycasandzamia. Veitchia merillii is closest to Veitchia merrillii, with a score of 0.97 . # Scientific names that don&#39;t match anything on record automatically unknown_df = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;] # Synonyms that don&#39;t have a database link to the accepted name aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) unknown_ids = aspca_df[(aspca_df.acceptedNameUsageID == &#39;Unknown&#39;) &amp; (aspca_df.taxonomicStatus == &#39;Synonym&#39;)] len(unknown_ids) + len(unknown_df) . 52 . # Manually fix some scientific names that don&#39;t match anything on record automatically fix_name(&#39;Nephrolepsis cordifolia plumosa&#39;, &#39;Nephrolepis cordifolia&#39;) fix_name(&#39;Nephrolepsis cordifolia duffii&#39;, &#39;Nephrolepis cordifolia&#39;) fix_name(&#39;Nephrolepis exalta bostoniensis&#39;, &#39;Nephrolepis exaltata&#39;) fix_name(&#39;Neoregalia&#39;, &#39;Neoregelia&#39;) fix_name(&#39;Miltonia roezlii alba&#39;, &#39;Miltonia roezlii&#39;) fix_name(&#39;Maranta insignis&#39;, &#39;Calathea insignis&#39;) fix_name(&#39;Lilium orientalis&#39;, &#39;Lilium japonicum&#39;) fix_name(&#39;Lampranthus piquet&#39;, &#39;Lampranthus piquetbergensis&#39;) fix_name(&#39;Hoya carnosa krinkle kurl&#39;, &#39;Hoya carnosa&#39;) fix_name(&#39;Hemigraphis exotica&#39;, &#39;Hemigraphis alternata&#39;) fix_name(&#39;Lilium asiatica&#39;, &#39;Lilium japonicum&#39;) fix_name(&#39;Nolina tuberculata&#39;, &#39;Beaucarnea recurvata&#39;) fix_name(&#39;Giant dracaena&#39;, &#39;Cordyline australis&#39;) fix_name(&#39;Scindapsusphilodendron&#39;, &#39;Philodendron scandens&#39;) fix_name(&#39;Schefflera or brassia actinoplylla&#39;, &#39;Schefflera actinophylla&#39;) fix_name(&#39;Phoenix robellinii&#39;, &#39;Phoenix roebelenii&#39;) fix_name(&#39;Peperomia serpens variegata&#39;, &#39;Peperomia serpens&#39;) fix_name(&#39;Bertolonia mosaica&#39;, &#39;Fittonia albivenis&#39;) fix_name(&#39;Begonia semperflorens cultivar&#39;, &#39;Begonia semperflorens&#39;) fix_name(&#39;Begonia rex peace&#39;, &#39;Begonia rex&#39;) fix_name(&#39;Asparagus densiflorus sprengeri&#39;, &#39;Asparagus densiflorus&#39;) fix_name(&#39;Albiflora&#39;, &#39;Tradescantia zebrina&#39;) fix_name(&#39;Acantha&#39;, &#39;Acanthus&#39;) fix_name(&#39;Episcia cultivar&#39;, &#39;Episcia&#39;) fix_name(&#39;Echevaria&#39;, &#39;Echeveria&#39;) fix_name(&#39;Echeveria puloliver&#39;, &#39;Echeveria harmsii&#39;) fix_name(&#39;Dypsis lutescens chrysalidocarpus lutescens alternate scientific name&#39;, &#39;Dypsis lutescens&#39;) fix_name(&#39;Draceana&#39;, &#39;Dracaena&#39;) fix_name(&#39;Daucus carota sativa&#39;, &#39;Daucus carota&#39;) fix_name(&#39;Ceratostigma larpentiae&#39;, &#39;Ceratostigma plumbaginoides&#39;) fix_name(&#39;Cycasrevolutazamia&#39;, &#39;Cycas revoluta&#39;) fix_name(&#39;Cucurbita maxima turbaniformis&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima hubbard&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima butternut&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima banana&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima buttercup&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbia pepo zucchini&#39;, &#39;Cucurbita pepo&#39;) fix_name(&#39;Cryptanthus bivattus minor&#39;, &#39;Cryptanthus bivittatus&#39;) fix_name(&#39;Cycasandzamia&#39;, &#39;Cycas&#39;) . # Manually match up synonyms that don&#39;t have a database link to the accepted name fix_name(&#39;Chlorophytum bichetii&#39;, &#39;Chlorophytum laxum&#39;) fix_name(&#39;Rhapis flabelliformis&#39;, &#39;Rhapis excelsa&#39;) fix_name(&#39;Cleome hassleriana&#39;, &#39;Cleome spinosa&#39;) fix_name(&#39;Pellionia pulchra&#39;, &#39;Pellionia repens&#39;) fix_name(&#39;Cissus discolor&#39;, &#39;Cissus javana&#39;) fix_name(&#39;Miltonia roezlii&#39;, &#39;Miltoniopsis roezlii&#39;) fix_name(&#39;Sorghum vulgare var. sudanense&#39;, &#39;Sorghum bicolor&#39;) fix_name(&#39;Camellia japonica var. japonica&#39;, &#39;Camellia japonica&#39;) fix_name(&#39;Onychium japonicum&#39;, &#39;Onychium japonicum&#39;) fix_name(&#39;Epidendrum atropurpureum&#39;, &#39;Psychilis atropurpurea&#39;) fix_name(&#39;Philodendron scandens&#39;, &#39;Philodendron hederaceum&#39;) fix_name(&#39;Origanum vulgare var. hirtum&#39;, &#39;Origanum vulgare subsp. hirtum&#39;) fix_name(&#39;Guzmania lingulata var. minor&#39;, &#39;Guzmania lingulata var. concolor&#39;) fix_name(&#39;Lavandula angustifolia&#39;, &#39;Lavandula angustifolia&#39;) fix_name(&#39;Begonia semperflorens&#39;, &#39;Begonia cucullata&#39;) fix_name(&#39;Calathea insignis&#39;, &#39;Calathea crotalifera&#39;) fix_name(&#39;Citrus ×limonia&#39;, &#39;Citrus limon&#39;) fix_name(&#39;Coleus amboinicus&#39;, &#39;Plectranthus amboinicus&#39;) fix_name(&#39;Rhipsalis cassytha&#39;, &#39;Rhipsalis dichotoma&#39;) fix_name(&#39;Lycopersicon&#39;, &#39;Solanum lycopersicum&#39;) fix_name(&#39;Lachenalia lilacina&#39;, &#39;Iris domestica&#39;) fix_name(&#39;Cymopterus watsonii&#39;, &#39;Cymopterus terebinthinus&#39;) . # Scientific names that don&#39;t match anything on record automatically unknown_df = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;] # Synonyms that don&#39;t have a database link to the accepted name aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) unknown_ids = aspca_df[(aspca_df.acceptedNameUsageID == &#39;Unknown&#39;) &amp; (aspca_df.taxonomicStatus == &#39;Synonym&#39;)] len(unknown_ids) + len(unknown_df) . 1 . synonym_idx = aspca_df[aspca_df[&#39;taxonomicStatus&#39;].values == &#39;Synonym&#39;].index print(f&#39;{len(synonym_idx)} entries have a more acceptable synonym&#39;) . 71 entries have a more acceptable synonym . # Work to update the remaining scientific names that are synonyms for their accepted scientific names aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) synonym_idx = aspca_df[aspca_df[&#39;taxonomicStatus&#39;].values == &#39;Synonym&#39;].index . for i in synonym_idx: # Get the series we&#39;re looking to change synonym_data = aspca_df.iloc[i,:] synonym_name = synonym_data.loc[&#39;Scientific Name&#39;] # Grab accepted data from wfo database based on ID lookup true_data = wfo_df[wfo_df[&#39;taxonID&#39;] == synonym_data.loc[&#39;acceptedNameUsageID&#39;]] true_sciname = true_data.iloc[:,1].values[0] fix_name(synonym_name,true_sciname) . IndexError Traceback (most recent call last) &lt;ipython-input-36-d42f5603c8fc&gt; in &lt;module&gt; 5 # Grab accepted data from wfo database based on ID lookup 6 true_data = wfo_df[wfo_df[&#39;taxonID&#39;] == synonym_data.loc[&#39;acceptedNameUsageID&#39;]] -&gt; 7 true_sciname = true_data.iloc[:,1].values[0] 8 fix_name(synonym_name,true_sciname) IndexError: index 0 is out of bounds for axis 0 with size 0 . synonym_idx = aspca_df[aspca_df[&#39;taxonomicStatus&#39;].values == &#39;Synonym&#39;].index print(f&#39;{len(synonym_idx)} entries have a more acceptable synonym&#39;) . 31 entries have a more acceptable synonym . # Sort and drop again aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;) aspca_df = aspca_df.sort_values(&#39;Scientific Name&#39;).reset_index(drop=True).sort_index() # Set genus of one-word names to be the name, rather than NaN aspca_df.loc[aspca_df.fillna(&#39;Unknown&#39;)[&#39;genus&#39;]==&#39;Unknown&#39;, &#39;genus&#39;] = aspca_df.loc[aspca_df.fillna(&#39;Unknown&#39;)[&#39;genus&#39;]==&#39;Unknown&#39;, &#39;Scientific Name&#39;] # Drop columns we no longer need aspca_df = aspca_df.drop([&#39;taxonID&#39;, &#39;scientificName&#39;, &#39;taxonomicStatus&#39;, &#39;acceptedNameUsageID&#39;, &#39;taxonRank&#39;], axis=1) # Standardize column names aspca_df.rename(columns = {&#39;genus&#39;:&#39;Genus&#39;, &#39;family&#39;:&#39;Family&#39;}, inplace=True) # Reorder columns cols = [&#39;Name&#39;, &#39;Scientific Name&#39;, &#39;Genus&#39;, &#39;Family&#39;, &#39;Alternative Names&#39;, &#39;Toxic to Dogs&#39;, &#39;Toxic to Cats&#39;] aspca_df = aspca_df[cols] . aspca_df.to_csv(&#39;Plant Toxicity - v6.csv&#39;) aspca_df.sample(10) . Name Scientific Name Genus Family Alternative Names Toxic to Dogs Toxic to Cats . 102 Celosia Globosa | Celosia globosa | Celosia | Amaranthaceae | Globe Amarantha, Perpetua | False | False | . 18 Alocasia | Alocasia | Alocasia | Araceae | Elephant&#39;s Ear | True | True | . 386 Variegated Philodendron | Philodendron hederaceum | Philodendron | Araceae | | True | True | . 411 American Mandrake | Podophyllum peltatum | Podophyllum | Berberidaceae | Mayapple, Indian Apple Root, Umbrella Leaf, Wi... | True | True | . 94 Chestnut | Castanea dentata | Castanea | Fagaceae | American Chestnut | False | False | . 291 Butterfly Iris | Iris spuria | Iris | Iridaceae | Spuria Iris | True | True | . 243 Climbing Lily | Gloriosa superba | Gloriosa | Colchicaceae | Gloriosa Lily, Glory Lily, Superb Lily | True | True | . 4 Measles Plant | Acanthus | Acanthus | Acanthaceae | Polka Dot Plant, Flamingo Plant, Baby’s Tears,... | False | False | . 246 Orange Star | Guzmania lingulata var. concolor | Guzmania | Bromeliaceae | | False | False | . 420 Algaroba | Prosopis limensis | Prosopis | Fabaceae | Kiawe, Mesquite | False | False | . aspca_df.head() . Name Scientific Name Genus Family Alternative Names Toxic to Dogs Toxic to Cats . 0 Sand Verbena | Abronia fragrans | Abronia | Nyctaginaceae | Prairie Snowball, Wild Lantana | False | False | . 1 Prayer Bean | Abrus precatorius | Abrus | Fabaceae | Rosary Pea, Buddhist Rosary Bead, Indian Bead,... | True | True | . 2 Copperleaf | Acalypha godseffiana | Acalypha | Euphorbiaceae | Lance Copperleaf | False | False | . 3 Chenille Plant | Acalypha hispida | Acalypha | Euphorbiaceae | Philippine Medusa, Foxtail, Red-hot Cat Tail | False | False | . 4 Measles Plant | Acanthus | Acanthus | Acanthaceae | Polka Dot Plant, Flamingo Plant, Baby’s Tears,... | False | False | . aspca_df[aspca_df[&#39;Toxic to Dogs&#39;] != aspca_df[&#39;Toxic to Cats&#39;]] . Name Scientific Name Genus Family Alternative Names Toxic to Dogs Toxic to Cats . 262 Day Lilies (many varieties) | Hemerocallis | Hemerocallis | Xanthorrhoeaceae | | False | True | . 263 Orange Day Lily | Hemerocallis graminea | Hemerocallis | Xanthorrhoeaceae | | False | True | . 296 Black Walnut | Juglans nigra | Juglans | Juglandaceae | | True | False | . 317 Lily | Lilium | Lilium | Liliaceae | | False | True | . 319 Tiger Lily | Lilium lancifolium | Lilium | Liliaceae | | False | True | . 320 Easter Lily | Lilium longiflorum | Lilium | Liliaceae | | False | True | . 321 Red Lily | Lilium philadelphicum | Lilium | Liliaceae | | False | True | . 322 Japanese Show Lily | Lilium speciosum | Lilium | Liliaceae | | False | True | . aspca_df[[&#39;Family&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Family&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[70:80] . Toxic to Cats Toxic to Dogs . Family . Lauraceae 0.500000 | 0.500000 | . Proteaceae 0.500000 | 0.500000 | . Convolvulaceae 0.500000 | 0.500000 | . Commelinaceae 0.500000 | 0.500000 | . Euphorbiaceae 0.600000 | 0.600000 | . Fabaceae 0.600000 | 0.600000 | . Berberidaceae 0.666667 | 0.666667 | . Polygonaceae 0.666667 | 0.666667 | . Apiaceae 0.666667 | 0.666667 | . Moraceae 0.666667 | 0.666667 | . # How many Families have mixed toxicity len(aspca_df[[&#39;Family&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Family&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[aspca_df[[&#39;Family&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Family&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[&#39;Toxic to Dogs&#39;].apply(lambda x: 0&lt;x&lt;1)]) . 33 . # How many Families len(aspca_df[&#39;Family&#39;].unique()) . 111 . aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[208:218] . Toxic to Cats Toxic to Dogs . Genus . Schefflera 0.666667 | 0.666667 | . Cordyline 0.666667 | 0.666667 | . Iris 0.666667 | 0.666667 | . Aloe 0.666667 | 0.666667 | . Dracaena 0.800000 | 0.800000 | . Aralia 1.000000 | 1.000000 | . Ficus 1.000000 | 1.000000 | . Apocynum 1.000000 | 1.000000 | . Sansevieria 1.000000 | 1.000000 | . Rumex 1.000000 | 1.000000 | . # How many Genuses have mixed toxicity len(aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[&#39;Toxic to Dogs&#39;].apply(lambda x: 0&lt;x&lt;1)]) . 9 . # How many Genuses len(aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)) . 346 . # If running in Colabs !pip install selenium -q !apt-get update # to update ubuntu to correctly run apt install !apt install chromium-chromedriver -q !cp /usr/lib/chromium-browser/chromedriver /usr/bin . WARNING: You are using pip version 20.2.3; however, version 20.2.4 is available. You should consider upgrading via the &#39;/home/gao/anaconda3/bin/python -m pip install --upgrade pip&#39; command. Reading package lists... Done E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied) E: Unable to lock directory /var/lib/apt/lists/ W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied) W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied) E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root? cp: cannot stat &#39;/usr/lib/chromium-browser/chromedriver&#39;: No such file or directory . import sys sys.path.insert(0,&#39;/usr/lib/chromium-browser/chromedriver&#39;) # Import and setup the Selenium webdriver from selenium import webdriver chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;--headless&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;) wd = webdriver.Chrome(&#39;chromedriver&#39;,chrome_options=chrome_options) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/04/Webscraping_Example.html",
            "relUrl": "/2020/11/04/Webscraping_Example.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "MCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis",
            "content": "Metropolis-Hastings . This post includes code and notes from : https://www.tweag.io/blog/2019-10-25-mcmc-intro1/ . https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo . | https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm . | . %matplotlib notebook %matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [10, 6] np.random.seed(42) . state_space = (&quot;sunny&quot;, &quot;cloudy&quot;, &quot;rainy&quot;) . transition_matrix = np.array(((0.6, 0.3, 0.1), (0.3, 0.4, 0.3), (0.2, 0.3, 0.5))) . n_steps = 20000 states = [0] for i in range(n_steps): states.append(np.random.choice((0, 1, 2), p=transition_matrix[states[-1]])) states = np.array(states) . def despine(ax, spines=(&#39;top&#39;, &#39;left&#39;, &#39;right&#39;)): for spine in spines: ax.spines[spine].set_visible(False) fig, ax = plt.subplots() width = 1000 offsets = range(1, n_steps, 5) for i, label in enumerate(state_space): ax.plot(offsets, [np.sum(states[:offset] == i) / offset for offset in offsets], label=label) ax.set_xlabel(&quot;number of steps&quot;) ax.set_ylabel(&quot;likelihood&quot;) ax.legend(frameon=False) despine(ax, (&#39;top&#39;, &#39;right&#39;)) plt.show() . def log_prob(x): return -0.5 * np.sum(x ** 2) . def proposal(x, stepsize): return np.random.uniform(low=x - 0.5 * stepsize, high=x + 0.5 * stepsize, size=x.shape) . def p_acc_MH(x_new, x_old, log_prob): return min(1, np.exp(log_prob(x_new) - log_prob(x_old))) . def sample_MH(x_old, log_prob, stepsize): x_new = proposal(x_old, stepsize) # here we determine whether we accept the new state or not: # we draw a random number uniformly from [0,1] and compare # it with the acceptance probability accept = np.random.random() &lt; p_acc_MH(x_new, x_old, log_prob) if accept: return accept, x_new else: return accept, x_old . def build_MH_chain(init, stepsize, n_total, log_prob): n_accepted = 0 chain = [init] for _ in range(n_total): accept, state = sample_MH(chain[-1], log_prob, stepsize) chain.append(state) n_accepted += accept acceptance_rate = n_accepted / float(n_total) return chain, acceptance_rate . chain, acceptance_rate = build_MH_chain(np.array([2.0]), 3.0, 10000, log_prob) . chain = [state for state, in chain] . print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) last_states = &quot;, &quot;.join(&quot;{:.5f}&quot;.format(state) for state in chain[-10:]) print(&quot;Last ten states of chain: &quot; + last_states) . Acceptance rate: 0.720 Last ten states of chain: 1.05847, 1.59966, 0.14389, -1.13281, 0.24131, -0.77448, -0.59703, 0.67707, 1.47065, 1.27361 . def plot_samples(chain, log_prob, ax, orientation=&#39;vertical&#39;, normalize=True, xlims=(-5, 5), legend=True): from scipy.integrate import quad ax.hist(chain, bins=50, density=True, label=&quot;MCMC samples&quot;, orientation=orientation) # we numerically calculate the normalization constant of our PDF if normalize: Z, _ = quad(lambda x: np.exp(log_prob(x)), -np.inf, np.inf) else: Z = 1.0 xses = np.linspace(xlims[0], xlims[1], 1000) yses = [np.exp(log_prob(x)) / Z for x in xses] if orientation == &#39;horizontal&#39;: (yses, xses) = (xses, yses) ax.plot(xses, yses, label=&quot;true distribution&quot;) if legend: ax.legend(frameon=False) fig, ax = plt.subplots() plot_samples(chain[500:], log_prob, ax) despine(ax) ax.set_yticks(()) plt.show() . def sample_and_display(init_state, stepsize, n_total, n_burnin, log_prob): chain, acceptance_rate = build_MH_chain(init_state, stepsize, n_total, log_prob) print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) fig, ax = plt.subplots() plot_samples([state for state, in chain[n_burnin:]], log_prob, ax) despine(ax) ax.set_yticks(()) plt.show() sample_and_display(np.array([2.0]), 30, 10000, 500, log_prob) . Acceptance rate: 0.104 . sample_and_display(np.array([2.0]), 0.1, 10000, 500, log_prob) . Acceptance rate: 0.985 . sample_and_display(np.array([2.0]), 0.1, 500000, 25000, log_prob) . Acceptance rate: 0.990 . %matplotlib notebook %matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [10, 6] np.random.seed(42) def log_gaussian(x, mu, sigma): # The np.sum() is for compatibility with sample_MH return - 0.5 * np.sum((x - mu) ** 2) / sigma ** 2 - np.log(np.sqrt(2 * np.pi * sigma ** 2)) class BivariateNormal(object): n_variates = 2 def __init__(self, mu1, mu2, sigma1, sigma2): self.mu1, self.mu2 = mu1, mu2 self.sigma1, self.sigma2 = sigma1, sigma2 def log_p_x(self, x): return log_gaussian(x, self.mu1, self.sigma1) def log_p_y(self, x): return log_gaussian(x, self.mu2, self.sigma2) def log_prob(self, x): cov_matrix = np.array([[self.sigma1 ** 2, 0], [0, self.sigma2 ** 2]]) inv_cov_matrix = np.linalg.inv(cov_matrix) kernel = -0.5 * (x - self.mu1) @ inv_cov_matrix @ (x - self.mu2).T normalization = np.log(np.sqrt((2 * np.pi) ** self.n_variates * np.linalg.det(cov_matrix))) return kernel - normalization bivariate_normal = BivariateNormal(mu1=0.0, mu2=0.0, sigma1=1.0, sigma2=0.15) . from mpl_toolkits.axes_grid1 import make_axes_locatable fig, ax = plt.subplots() xses = np.linspace(-2, 2, 200) yses = np.linspace(-0.5, 0.5, 200) log_density_values = [[bivariate_normal.log_prob(np.array((x, y))) for x in xses] for y in yses] dx = (xses[1] - xses[0]) / 2 dy = (yses[1] - yses[0]) / 2 extent = [xses[0] - dx, xses[-1] + dx, yses[0] - dy, yses[-1] + dy] im = ax.imshow(np.exp(log_density_values), extent=extent) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) divider = make_axes_locatable(ax) cax = divider.append_axes(&#39;right&#39;, size=&#39;5%&#39;, pad=0.05) cb = fig.colorbar(im, cax=cax) cb.set_label(&#39;probability density&#39;) plt.show() . def sample_gibbs(old_state, bivariate_dist, stepsizes): &quot;&quot;&quot;Draws a single sample using the systematic Gibbs sampling transition kernel Arguments: - old_state: the old (two-dimensional) state of a Markov chain (a list containing two floats) - bivariate_dist: an object representing a bivariate distribution (in our case, an instance of BivariateNormal) - stepsizes: a list of step sizes &quot;&quot;&quot; x_old, y_old = old_state # for compatibility with sample_MH, change floats to one-dimensional # numpy arrays of length one x_old = np.array([x_old]) y_old = np.array([y_old]) # draw new x conditioned on y p_x_y = bivariate_dist.log_p_x accept_x, x_new = sample_MH(x_old, p_x_y, stepsizes[0]) # draw new y conditioned on x p_y_x = bivariate_dist.log_p_y accept_y, y_new = sample_MH(y_old, p_y_x, stepsizes[1]) # Don&#39;t forget to turn the one-dimensional numpy arrays x_new, y_new # of length one back into floats return (accept_x, accept_y), (x_new[0], y_new[0]) . def build_gibbs_chain(init, stepsizes, n_total, bivariate_dist): &quot;&quot;&quot;Builds a Markov chain by performing repeated transitions using the systematic Gibbs sampling transition kernel Arguments: - init: an initial (two-dimensional) state for the Markov chain (a list containing two floats) - stepsizes: a list of step sizes of type float - n_total: the total length of the Markov chain - bivariate_dist: an object representing a bivariate distribution (in our case, an instance of BivariateNormal) &quot;&quot;&quot; init_x, init_k = init chain = [init] acceptances = [] for _ in range(n_total): accept, new_state = sample_gibbs(chain[-1], bivariate_dist, stepsizes) chain.append(new_state) acceptances.append(accept) acceptance_rates = np.mean(acceptances, 0) print(&quot;Acceptance rates: x: {:.3f}, y: {:.3f}&quot;.format(acceptance_rates[0], acceptance_rates[1])) return chain stepsizes = (6.5, 1.0) initial_state = [2.0, -1.0] chain = build_gibbs_chain(initial_state, stepsizes, 100000, bivariate_normal) chain = np.array(chain) . Acceptance rates: x: 0.462, y: 0.456 . def plot_samples_2D(chain, path_length, burnin, ax, xlims=(-3, 3), ylims=(-0.5, 0.5)): chain = np.array(chain) bins = [np.linspace(xlims[0], xlims[1], 100), np.linspace(ylims[0], ylims[1], 100)] ax.hist2d(*chain[burnin:].T, bins=bins) ax.plot(*chain[:path_length].T, marker=&#39;o&#39;, c=&#39;w&#39;, lw=0.4, markersize=1, alpha=0.75) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_xlim(xlims[0], xlims[1]) ax.set_ylim(ylims[0], ylims[1]) def plot_bivariate_samples(chain, burnin, pdf): fig = plt.figure(figsize=(12,7)) ax_c = plt.subplot2grid((4, 4), (1, 0), rowspan=1, colspan=3) plot_samples_2D(chain, 100, burnin, ax_c) ax_t = plt.subplot2grid((4, 4), (0, 0), rowspan=1, colspan=3, sharex=ax_c) plot_samples(chain[:,0], pdf.log_p_x, ax_t, normalize=False) plt.setp(ax_t.get_xticklabels(), visible=False) ax_t.set_yticks(()) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax_t.spines[spine].set_visible(False) ax_r = plt.subplot2grid((4, 4), (1, 3), rowspan=1, colspan=1, sharey=ax_c) plot_samples(chain[:,1], pdf.log_p_y, ax_r, orientation=&#39;horizontal&#39;, normalize=False, legend=False) plt.setp(ax_r.get_yticklabels(), visible=False) ax_r.set_xticks(()) for spine in (&#39;top&#39;, &#39;bottom&#39;, &#39;right&#39;): ax_r.spines[spine].set_visible(False) plt.show() plot_bivariate_samples(chain, burnin=200, pdf=bivariate_normal) . mix_params = dict(mu1=1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7) . fig, ax = plt.subplots() xspace = np.linspace(-0.5, 3, 200) # densities of both components first_component = [np.exp(log_gaussian(x, mix_params[&#39;mu1&#39;], mix_params[&#39;sigma1&#39;])) for x in xspace] second_component = [np.exp(log_gaussian(x, mix_params[&#39;mu2&#39;], mix_params[&#39;sigma2&#39;])) for x in xspace] # apply component weights first_component = mix_params[&#39;w1&#39;] * np.array(first_component) second_component = mix_params[&#39;w2&#39;] * np.array(second_component) ax.plot(xspace, first_component, color=&#39;black&#39;) ax.fill_between(xspace, first_component, alpha=0.6, label=&quot;1st component&quot;) ax.plot(xspace, second_component, color=&#39;black&#39;) ax.fill_between(xspace, second_component, alpha=0.6, label=&quot;2nd component&quot;) ax.set_xlabel(&#39;x&#39;) ax.set_yticks(()) ax.legend(frameon=False) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax.spines[spine].set_visible(False) plt.show() . class GaussianMixture(object): def __init__(self, mu1, mu2, sigma1, sigma2, w1, w2): self.mu1, self.mu2 = mu1, mu2 self.sigma1, self.sigma2 = sigma1, sigma2 self.w1, self.w2 = w1, w2 def log_prob(self, x): return np.logaddexp(np.log(self.w1) + log_gaussian(x, self.mu1, self.sigma1), np.log(self.w2) + log_gaussian(x, self.mu2, self.sigma2)) def log_p_x_k(self, x, k): # logarithm of p(x|k) mu = (self.mu1, self.mu2)[k] sigma = (self.sigma1, self.sigma2)[k] return log_gaussian(x, mu, sigma) def p_k_x(self, k, x): # p(k|x) using Bayes&#39; theorem mu = (self.mu1, self.mu2)[k] sigma = (self.sigma1, self.sigma2)[k] weight = (self.w1, self.w2)[k] log_normalization = self.log_prob(x) return np.exp(log_gaussian(x, mu, sigma) + np.log(weight) - log_normalization) . def sample_gibbs(old_state, mixture, stepsize): &quot;&quot;&quot;Draws a single sample using the systematic Gibbs sampling transition kernel Arguments: - old_state: the old (two-dimensional) state of a Markov chain (a list containing a float and an integer representing the initial mixture component) - mixture: an object representing a mixture of densities (in our case, an instance of GaussianMixture) - stepsize: a step size of type float &quot;&quot;&quot; x_old, k_old = old_state # for compatibility with sample_MH, change floats to one-dimensional # numpy arrays of length one x_old = np.array([x_old]) # draw new x conditioned on k x_pdf = lambda x: mixture.log_p_x_k(x, k_old) accept, x_new = sample_MH(x_old, x_pdf, stepsize) # ... turn the one-dimensional numpy arrays of length one back # into floats x_new = x_new[0] # draw new k conditioned on x k_probabilities = (mixture.p_k_x(0, x_new), mixture.p_k_x(1, x_new)) jump_probability = k_probabilities[1 - k_old] k_new = np.random.choice((0,1), p=k_probabilities) return accept, jump_probability, (x_new, k_new) def build_gibbs_chain(init, stepsize, n_total, mixture): &quot;&quot;&quot;Builds a Markov chain by performing repeated transitions using the systematic Gibbs sampling transition kernel Arguments: - init: an initial (two-dimensional) state of a Markov chain (a list containing a one-dimensional numpy array of length one and an integer representing the initial mixture component) - stepsize: a step size of type float - n_total: the total length of the Markov chain - mixture: an object representing a mixture of densities (in our case, an instance of GaussianMixture) &quot;&quot;&quot; init_x, init_k = init chain = [init] acceptances = [] jump_probabilities = [] for _ in range(n_total): accept, jump_probability, new_state = sample_gibbs(chain[-1], mixture, stepsize) chain.append(new_state) jump_probabilities.append(jump_probability) acceptances.append(accept) acceptance_rates = np.mean(acceptances) print(&quot;Acceptance rate: x: {:.3f}&quot;.format(acceptance_rates)) print(&quot;Average probability to change mode: {}&quot;.format(np.mean(jump_probabilities))) return chain mixture = GaussianMixture(**mix_params) stepsize = 1.0 initial_state = [2.0, 1] chain = build_gibbs_chain(initial_state, stepsize, 10000, mixture) burnin = 1000 x_states = [state[0] for state in chain[burnin:]] . Acceptance rate: x: 0.631 Average probability to change mode: 0.08629295966662387 . fig, ax = plt.subplots() plot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-1,2.5)) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax.spines[spine].set_visible(False) ax.set_yticks(()) ax.set_xlabel(&#39;x&#39;) plt.show() . mixture = GaussianMixture(mu1=-1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7) stepsize = 1.0 initial_state = [2.0, 1] chain = build_gibbs_chain(initial_state, stepsize, 100000, mixture) burnin = 10000 x_states = [state[0] for state in chain[burnin:]] . Acceptance rate: x: 0.558 Average probability to change mode: 6.139534006013391e-06 . fig, ax = plt.subplots() plot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-2,2.5)) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax.spines[spine].set_visible(False) ax.set_yticks(()) ax.set_xlabel(&#39;x&#39;) plt.show() . Hamiltonian Monte Carlo . import numpy as np import matplotlib.pyplot as plt np.random.seed(42) xspace = np.linspace(-2, 2, 100) unnormalized_probs = np.exp(-0.5 * xspace ** 2) energies = 0.5 * xspace ** 2 fig, ax = plt.subplots(dpi=80) ax.plot(xspace, unnormalized_probs, label=r&quot;$p(x)$&quot;) ax.plot(xspace, energies, label=r&quot;$E(x)=- log p(x)$&quot;) prop = dict(arrowstyle=&quot;-|&gt;,head_width=0.4,head_length=0.8&quot;, shrinkA=0,shrinkB=0) x_index1 = 75 ax.scatter((xspace[x_index1],), (energies[x_index1],), color=&quot;k&quot;) a_start1 = np.array((xspace[x_index1], energies[x_index1])) a_end1 = np.array((xspace[x_index1] - xspace[x_index1], energies[x_index1])) ax.annotate(&quot;&quot;,a_end1, a_start1, arrowprops=prop) text_pos1 = (a_start1[0] + 0.5 * (a_end1[0] - a_start1[0]), a_end1[1] + 0.075) ax.text(*text_pos1, r&quot;force&quot;, horizontalalignment=&quot;center&quot;) x_index2 = 38 ax.scatter((xspace[x_index2],), (energies[x_index2],), color=&quot;k&quot;) a_start2 = np.array((xspace[x_index2], energies[x_index2])) a_end2 = np.array((xspace[x_index2] - xspace[x_index2], energies[x_index2])) ax.annotate(&quot;&quot;,a_end2, a_start2, arrowprops=prop) text_pos2 = (a_start2[0] + 0.5 * (a_end2[0] - a_start2[0]), a_end2[1] + 0.075, ) ax.text(*text_pos2, r&quot;force&quot;, horizontalalignment=&quot;center&quot;) ax.set_xlabel(&quot;x&quot;) ax.set_yticks(()) for spine in (&#39;top&#39;, &#39;right&#39;, &#39;left&#39;): ax.spines[spine].set_visible(False) ax.legend(frameon=False) plt.show() . def leapfrog(x, v, gradient, timestep, trajectory_length): v -= 0.5 * timestep * gradient(x) for _ in range(trajectory_length - 1): x += timestep * v v -= timestep * gradient(x) x += timestep * v v -= 0.5 * timestep * gradient(x) return x, v . def sample_HMC(x_old, log_prob, log_prob_gradient, timestep, trajectory_length): # switch to physics mode! def E(x): return -log_prob(x) def gradient(x): return -log_prob_gradient(x) def K(v): return 0.5 * np.sum(v ** 2) def H(x, v): return K(v) + E(x) # Metropolis acceptance probability, implemented in &quot;logarithmic space&quot; # for numerical stability: def log_p_acc(x_new, v_new, x_old, v_old): return min(0, -(H(x_new, v_new) - H(x_old, v_old))) # give a random kick to particle by drawing its momentum from p(v) v_old = np.random.normal(size=x_old.shape) # approximately calculate position x_new and momentum v_new after # time trajectory_length * timestep x_new, v_new = leapfrog(x_old.copy(), v_old.copy(), gradient, timestep, trajectory_length) # accept / reject based on Metropolis criterion accept = np.log(np.random.random()) &lt; log_p_acc(x_new, v_new, x_old, v_old) # we consider only the position x (meaning, we marginalize out v) if accept: return accept, x_new else: return accept, x_old . def build_HMC_chain(init, timestep, trajectory_length, n_total, log_prob, gradient): n_accepted = 0 chain = [init] for _ in range(n_total): accept, state = sample_HMC(chain[-1].copy(), log_prob, gradient, timestep, trajectory_length) chain.append(state) n_accepted += accept acceptance_rate = n_accepted / float(n_total) return chain, acceptance_rate . def log_prob(x): return -0.5 * np.sum(x ** 2) . def log_prob_gradient(x): return -x . chain, acceptance_rate = build_HMC_chain(np.array([5.0, 1.0]), 1.5, 10, 10000, log_prob, log_prob_gradient) print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) . Acceptance rate: 0.622 . fig, ax = plt.subplots(dpi=80) plot_samples_2D(chain, 100, 200, ax, xlims=(-5.5, 5.5), ylims=(-5.5, 5.5)) plt.show() . chain, acceptance_rate = build_MH_chain(np.array([5.0, 1.0]), 2.6, 10000, log_prob) print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) . Acceptance rate: 0.623 . fig, ax = plt.subplots(dpi=80) plot_samples_2D(chain, 100, 200, ax, xlims=(-5.5, 5.5), ylims=(-5.5, 5.5)) plt.show() . Replica Exchange . mix_params = dict(mu1=-1.5, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7) mixture = GaussianMixture(**mix_params) temperatures = [0.1, 0.4, 0.6, 0.8, 1.0] . from scipy.integrate import quad def plot_tempered_distributions(log_prob, temperatures, axes, xlim=(-4, 4)): xspace = np.linspace(*xlim, 1000) for i, (temp, ax) in enumerate(zip(temperatures, axes)): pdf = lambda x: np.exp(temp * log_prob(x)) Z = quad(pdf, -1000, 1000)[0] ax.plot(xspace, np.array(list(map(pdf, xspace))) / Z) ax.text(0.8, 0.3, r&#39;$ beta={}$&#39;.format(temp), transform=ax.transAxes) ax.text(0.05, 0.3, &#39;replica {}&#39;.format(len(temperatures) - i - 1), transform=ax.transAxes) ax.set_yticks(()) plt.show() fig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True, figsize=(8, 7)) plot_tempered_distributions(mixture.log_prob, temperatures, axes) plt.show() . def handle_left_border(leftmost_old_state, leftmost_temperature, leftmost_stepsize, log_prob, new_multistate): accepted, state = sample_MH(leftmost_old_state, lambda x: leftmost_temperature * log_prob(x), leftmost_stepsize) new_multistate = [state] + new_multistate return new_multistate, accepted def handle_right_border(rightmost_old_state, rightmost_temperature, rightmost_stepsize, log_prob, new_multistate): accepted, state = sample_MH(rightmost_old_state, lambda x: rightmost_temperature * log_prob(x), rightmost_stepsize) new_multistate = new_multistate + [state] return new_multistate, accepted def build_RE_chain(init, stepsizes, n_total, temperatures, swap_interval, log_prob): from itertools import cycle n_replicas = len(temperatures) # a bunch of arrays in which we will store how many # Metropolis-Hastings / swap moves were accepted # and how many there were performed in total accepted_MH_moves = np.zeros(n_replicas) total_MH_moves = np.zeros(n_replicas) accepted_swap_moves = np.zeros(n_replicas - 1) total_swap_moves = np.zeros(n_replicas - 1) cycler = cycle((True, False)) chain = [init] for k in range(n_total): new_multistate = [] if k &gt; 0 and k % swap_interval == 0: # perform RE swap # First, determine the swap partners if next(cycler): # swap (0,1), (2,3), ... partners = [(j-1, j) for j in range(1, n_replicas, 2)] else: # swap (1,2), (3,4), ... partners = [(j-1, j) for j in range(2, len(temperatures), 2)] # Now, for each pair of replicas, attempt an exchange for (i,j) in partners: bi, bj = temperatures[i], temperatures[j] lpi, lpj = log_prob(chain[-1][i]), log_prob(chain[-1][j]) log_p_acc = min(0, bi * lpj - bi * lpi + bj * lpi - bj * lpj) if np.log(np.random.uniform()) &lt; log_p_acc: new_multistate += [chain[-1][j], chain[-1][i]] accepted_swap_moves[i] += 1 else: new_multistate += [chain[-1][i], chain[-1][j]] total_swap_moves[i] += 1 # We might have border cases: if left- / rightmost replicas don&#39;t participate # in swaps, have them draw a sample if partners[0][0] != 0: new_multistate, accepted = handle_left_border(chain[-1][0], temperatures[0], stepsizes[0], log_prob, new_multistate) accepted_MH_moves[0] += accepted total_MH_moves[0] += 1 if partners[-1][1] != len(temperatures) - 1: new_multistate, accepted = handle_right_border(chain[-1][-1], temperatures[-1], stepsizes[-1], log_prob, new_multistate) accepted_MH_moves[-1] += accepted total_MH_moves[-1] += 1 else: # perform sampling in single chains for j, temp in enumerate(temperatures): accepted, state = sample_MH(chain[-1][j], lambda x: temp * log_prob(x), stepsizes[j]) accepted_MH_moves[j] += accepted total_MH_moves[j] += 1 new_multistate.append(state) chain.append(new_multistate) # calculate acceptance rates MH_acceptance_rates = accepted_MH_moves / total_MH_moves # safe division in case of zero total swap moves swap_acceptance_rates = np.divide(accepted_swap_moves, total_swap_moves, out=np.zeros(n_replicas - 1), where=total_swap_moves != 0) return MH_acceptance_rates, swap_acceptance_rates, np.array(chain) . stepsizes = [2.75, 2.5, 2.0, 1.75, 1.6] . def print_MH_acceptance_rates(mh_acceptance_rates): print(&quot;MH acceptance rates: &quot; + &quot;&quot;.join([&quot;{}: {:.3f} &quot;.format(i, x) for i, x in enumerate(mh_acceptance_rates)])) mh_acc_rates, swap_acc_rates, chains = build_RE_chain(np.random.uniform(low=-3, high=3, size=len(temperatures)), stepsizes, 10000, temperatures, 500000000, mixture.log_prob) print_MH_acceptance_rates(mh_acc_rates) . MH acceptance rates: 0: 0.790 1: 0.551 2: 0.415 3: 0.552 4: 0.705 . def plot_RE_samples(chains, axes, bins=np.linspace(-4, 4, 50)): for i, (chain, ax) in enumerate(zip(chains, axes)): ax.hist(chain, bins, density=True, label=&quot;MCMC samples&quot;) fig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True, figsize=(8, 7)) plot_RE_samples(chains[100:].T, axes) plot_tempered_distributions(mixture.log_prob, temperatures, axes) plt.show() . init = np.random.uniform(low=-4, high=4, size=len(temperatures)) mh_acc_rates, swap_acc_rates, chains = build_RE_chain(init, stepsizes, 10000, temperatures, 5, mixture.log_prob) print_MH_acceptance_rates(mh_acc_rates) swap_rate_string = &quot;&quot;.join([&quot;{}&lt;-&gt;{}: {:.3f}, &quot;.format(i, i+1, x) for i, x in enumerate(swap_acc_rates)])[:-2] print(&quot;Swap acceptance rates:&quot;, swap_rate_string) . MH acceptance rates: 0: 0.797 1: 0.552 2: 0.539 3: 0.499 4: 0.466 Swap acceptance rates: 0&lt;-&gt;1: 0.585, 1&lt;-&gt;2: 0.846, 2&lt;-&gt;3: 0.829, 3&lt;-&gt;4: 0.876 . fig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True, figsize=(8, 7)) plot_RE_samples(chains[100:].T, axes) plot_tempered_distributions(mixture.log_prob, temperatures, axes) plt.show() . # Detect swaps. This method works only under the assumption that # when performing local MCMC moves, starting from two different # initial states, you cannot end up with the same state swaps = {} # for each pair of chains... for i in range(len(chains) - 1): # shift one chain by one state to the left. # Where states from both chains match up, a successful exchange # was performed matches = np.where(chains[i, :-1] == chains[i+1, 1:])[0] if len(matches) &gt; 0: swaps[i] = matches # Reconstruct trajectories of single states through the temperature # ladder def reconstruct_trajectory(start_index, chains): res = [] current_ens = start_index for i in range(len(chains)): res.append(current_ens) if i in swaps: if current_ens in swaps[i]: current_ens += 1 elif current_ens in swaps[i] + 1: current_ens -= 1 return np.array(res) def plot_state_trajectories(trajectories, ax, max_samples=300): for trajectory in trajectories: ax.plot(-trajectory[:max_samples] - 1, lw=2) ax.set_xlabel(&quot;# of MCMC samples&quot;) ax.set_ylabel(r&quot;inverse temperature $ beta$&quot;) # make order of temperatures appear as above - whatever it takes... ax.set_yticks(range(-len(temperatures), 0)) ax.set_yticklabels(temperatures[::-1]) # which states to follow start_state_indices = (4, 0) fig, ax = plt.subplots(figsize=(8, 6)) trajectories = np.array([reconstruct_trajectory(i, chains) for i in start_state_indices]) plot_state_trajectories(trajectories, ax) plt.show() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/03/MCMC.html",
            "relUrl": "/2020/11/03/MCMC.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Pandas profiling and Shap values for European Soccer Match Data",
            "content": "This post includes code and notes from this gist and this post. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . %load_ext sql . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . tables = pd.read_sql(&quot;&quot;&quot;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&quot;&quot;&quot;, connection) tables . type name tbl_name rootpage sql . 0 table | sqlite_sequence | sqlite_sequence | 4 | CREATE TABLE sqlite_sequence(name,seq) | . 1 table | Player_Attributes | Player_Attributes | 11 | CREATE TABLE &quot;Player_Attributes&quot; ( n t`id` tIN... | . 2 table | Player | Player | 14 | CREATE TABLE `Player` ( n t`id` tINTEGER PRIMA... | . 3 table | Match | Match | 18 | CREATE TABLE `Match` ( n t`id` tINTEGER PRIMAR... | . 4 table | League | League | 24 | CREATE TABLE `League` ( n t`id` tINTEGER PRIMA... | . 5 table | Country | Country | 26 | CREATE TABLE `Country` ( n t`id` tINTEGER PRIM... | . 6 table | Team | Team | 29 | CREATE TABLE &quot;Team&quot; ( n t`id` tINTEGER PRIMARY... | . 7 table | Team_Attributes | Team_Attributes | 2 | CREATE TABLE `Team_Attributes` ( n t`id` tINTE... | . 8 table | Match_df | Match_df | 3 | CREATE TABLE Match_df( n id INT, n country_n... | . 9 table | Match_Wins | Match_Wins | 308451 | CREATE TABLE Match_Wins( n id INT, n country... | . %%sql SELECT * FROM Match LIMIT 3; . Environment variable $DATABASE_URL not set, and no connect string given. Connection info needed in SQLAlchemy format, example: postgresql://username:password@hostname/dbname or an existing connection: dict_keys([]) . connection . &lt;sqlalchemy.engine.base.Connection at 0x7f785f788c50&gt; . match_wins = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Match_Wins;&quot;&quot;&quot;, connection) . # sql_query = %sql SELECT * FROM Match_Wins # df = sql_query.DataFrame() # df . match_wins . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | 0 | . 25975 25946 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | 1 | . 25976 25947 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | 0 | . 25977 25948 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | 0 | . 25978 25949 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | 1 | . 25979 rows × 11 columns . import matplotlib.pyplot as plt from pandas_profiling import ProfileReport profile = ProfileReport(match_wins, title=&#39;Pandas Profiling Report&#39;) . profile.to_widgets() . . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . 5 24614 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | AC Bellinzona | Neuchâtel Xamax | 1 | 2 | 0 | . 6 24615 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Zürich | FC Luzern | 1 | 0 | 1 | . 7 24616 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Sion | BSC Young Boys | 2 | 1 | 1 | . 8 24617 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Vaduz | FC Aarau | 0 | 2 | 0 | . 9 24668 | Switzerland | Switzerland Super League | 2008/2009 | 3 | 2008-07-26 00:00:00 | FC Basel | AC Bellinzona | 2 | 0 | 1 | . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 25969 25940 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | Grasshopper Club Zürich | FC Thun | 0 | 0 | 0 | . 25970 25941 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | FC Sion | FC Zürich | 2 | 2 | 0 | . 25971 25942 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | FC Vaduz | Lugano | 0 | 0 | 0 | . 25972 25943 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | BSC Young Boys | FC Basel | 2 | 3 | 0 | . 25973 25944 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | FC St. Gallen | FC Luzern | 1 | 4 | 0 | . 25974 25945 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | 0 | . 25975 25946 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | 1 | . 25976 25947 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | 0 | . 25977 25948 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | 0 | . 25978 25949 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | 1 | . profile.to_notebook_iframe() . . profile.to_file(output_file=&quot;pandas_profiling.html&quot;) . . match_wins.head() . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . cols = match_wins.columns colours = [&#39;darkblue&#39;, &#39;red&#39;] sns.heatmap(match_wins[cols].isnull(), cmap=sns.color_palette(colours)) . &lt;AxesSubplot:&gt; . # top = match_wins[&quot;home_team_win&quot;].describe()[&#39;top&#39;] # impute with the most frequent value. # match_wins[&quot;home_team_win&quot;] = match_wins[&quot;home_team_win&quot;].fillna(top) . pct_list = [] for col in match_wins.columns: pct_missing = np.mean(match_wins[col].isnull()) if round(pct_missing*100) &gt;0: pct_list.append([col, round(pct_missing*100)]) print(&#39;{} - {}%&#39;.format(col, round(pct_missing*100))) . id - 0% country_name - 0% league_name - 0% season - 0% stage - 0% date - 0% home_team - 0% away_team - 0% home_team_goal - 0% away_team_goal - 0% home_team_win - 0% . match_wins.country_name . 0 Switzerland 1 Switzerland 2 Switzerland 3 Switzerland 4 Switzerland ... 25974 Switzerland 25975 Switzerland 25976 Switzerland 25977 Switzerland 25978 Switzerland Name: country_name, Length: 25979, dtype: object . # # extracting the titles from the names: # Title = [] # for name in match_wins.country_name: # Title.append(name.split(&quot;,&quot;)[1].split(&quot;.&quot;)[0]) # match_wins[&quot;Team&quot;] = Title . match_wins.groupby([&quot;home_team&quot;, &#39;season&#39;])[&#39;home_team_win&#39;].agg([&#39;sum&#39;]).round(0) . sum . home_team season . 1. FC Kaiserslautern 2010/2011 6 | . 2011/2012 2 | . 1. FC Köln 2008/2009 4 | . 2009/2010 3 | . 2010/2011 11 | . ... ... ... | . Śląsk Wrocław 2011/2012 9 | . 2012/2013 9 | . 2013/2014 5 | . 2014/2015 9 | . 2015/2016 5 | . 1478 rows × 1 columns . df = df.drop(columns = [&quot;Name&quot;]) df = df.drop(columns = [&quot;PassengerId&quot;]) df = df.drop(columns = [&quot;Ticket&quot;]) . match_wins.dtypes . id int64 country_name int8 league_name int8 season int8 stage int64 date int16 home_team int16 away_team int16 home_team_goal int64 away_team_goal int64 home_team_win int64 dtype: object . match_wins.country_name = pd.Categorical(match_wins.country_name) match_wins.league_name = pd.Categorical(match_wins.league_name) match_wins.season = pd.Categorical(match_wins.season) match_wins.date = pd.Categorical(match_wins.date) . match_wins[&quot;country_name&quot;] = match_wins.country_name.cat.codes . match_wins[&quot;league_name&quot;] = match_wins.league_name.cat.codes match_wins[&quot;season&quot;] = match_wins.season.cat.codes match_wins[&quot;date&quot;] = match_wins.date.cat.codes . match_wins.home_team = pd.Categorical(match_wins.home_team) . match_wins.away_team = pd.Categorical(match_wins.away_team) . match_wins[&quot;away_team&quot;] = match_wins.away_team.cat.codes . match_wins[&quot;home_team&quot;] = match_wins.home_team.cat.codes . match_wins[&quot;home_team&quot;] . 0 24 1 72 2 84 3 173 4 76 ... 25974 76 25975 160 25976 84 25977 95 25978 100 Name: home_team, Length: 25979, dtype: int16 . match_wins.date = pd.Categorical(match_wins.date) . match_wins[&quot;date&quot;] = match_wins.date.cat.codes . match_wins . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | 10 | 10 | 0 | 1 | 0 | 24 | 76 | 1 | 2 | 0 | . 1 24560 | 10 | 10 | 0 | 1 | 1 | 72 | 91 | 3 | 1 | 1 | . 2 24561 | 10 | 10 | 0 | 1 | 2 | 84 | 98 | 1 | 2 | 0 | . 3 24562 | 10 | 10 | 0 | 1 | 2 | 173 | 100 | 1 | 2 | 0 | . 4 24613 | 10 | 10 | 0 | 2 | 3 | 76 | 117 | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | 10 | 10 | 7 | 36 | 1693 | 76 | 117 | 0 | 1 | 0 | . 25975 25946 | 10 | 10 | 7 | 36 | 1693 | 160 | 93 | 3 | 0 | 1 | . 25976 25947 | 10 | 10 | 7 | 36 | 1693 | 84 | 91 | 2 | 2 | 0 | . 25977 25948 | 10 | 10 | 7 | 36 | 1693 | 95 | 24 | 0 | 3 | 0 | . 25978 25949 | 10 | 10 | 7 | 36 | 1693 | 100 | 98 | 3 | 1 | 1 | . 25979 rows × 11 columns . match_wins.dtypes . id int64 country_name int8 league_name int8 season int8 stage int64 date int16 home_team int16 away_team int16 home_team_goal int64 away_team_goal int64 home_team_win int64 dtype: object . #match_wins = match_wins.drop(columns = [&quot;Title&quot;]) target = match_wins.home_team_win.values match_wins = match_wins.drop(columns =[&quot;home_team_win&quot;]) . match_wins . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 24559 | 10 | 10 | 0 | 1 | 0 | 24 | 76 | 1 | 2 | . 1 24560 | 10 | 10 | 0 | 1 | 1 | 72 | 91 | 3 | 1 | . 2 24561 | 10 | 10 | 0 | 1 | 2 | 84 | 98 | 1 | 2 | . 3 24562 | 10 | 10 | 0 | 1 | 2 | 173 | 100 | 1 | 2 | . 4 24613 | 10 | 10 | 0 | 2 | 3 | 76 | 117 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | 10 | 10 | 7 | 36 | 1693 | 76 | 117 | 0 | 1 | . 25975 25946 | 10 | 10 | 7 | 36 | 1693 | 160 | 93 | 3 | 0 | . 25976 25947 | 10 | 10 | 7 | 36 | 1693 | 84 | 91 | 2 | 2 | . 25977 25948 | 10 | 10 | 7 | 36 | 1693 | 95 | 24 | 0 | 3 | . 25978 25949 | 10 | 10 | 7 | 36 | 1693 | 100 | 98 | 3 | 1 | . 25979 rows × 10 columns . target . array([0, 1, 0, ..., 0, 0, 1]) . from sklearn.model_selection import train_test_split . x_train, x_test, y_train, y_test = train_test_split(match_wins, target, test_size=0.2, random_state=0) . from sklearn.linear_model import LogisticRegression . LR = LogisticRegression() LR.fit(x_train, y_train) . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression() . LR.score(x_test, y_test) . 0.9736335642802155 . import shap explainer = shap.LinearExplainer(LR, x_train, feature_perturbation=&quot;interventional&quot;) shap_values = explainer.shap_values(x_test) shap.summary_plot(shap_values, x_test) . shap.dependence_plot(&quot;home_team&quot;, shap_values, x_test) . shap.summary_plot(shap_values, x_train, plot_type=&quot;bar&quot;) . shap.initjs() shap.force_plot(explainer.expected_value, shap_values, x_test, link=&quot;logit&quot;) . shap.plots.force is slow for many thousands of rows, try subsampling your data. . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() shap.force_plot(explainer.expected_value, shap_values[0,:], x_test.iloc[0,:], link=&quot;logit&quot;) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() shap.force_plot(explainer.expected_value, shap_values[3,:], x_test.iloc[3,:], link=&quot;logit&quot;) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written.",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/02/pandas_prof_shap_values.html",
            "relUrl": "/2020/11/02/pandas_prof_shap_values.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . Find the quandl api documentation here - . from sklearn.datasets import fetch_california_housing california = fetch_california_housing() X = california.data y = california.target * 100000 print(f&#39;Data shape is {X.shape}&#39;) print(f&#39;Target shape is {y.shape}&#39;) . Data shape is (20640, 8) Target shape is (20640,) . import pandas as pd import numpy as np import matplotlib.pyplot as plt import quandl quandl.ApiConfig.api_key = &#39;&#39; %matplotlib inline . quandl_call = ( &quot;ZILLOW/{category}{code}_{indicator}&quot; ) def download_data(category, code, indicator): &quot;&quot;&quot; Reads in a single dataset from Zillow Quandl API Parameters - category : &quot;Chicago_Area&quot; or &quot;Evanston&quot; code : &quot;Evanston&quot; or &quot;Chicago&quot; indicator : &quot;Sales_Price&quot; or &quot;other&quot; Returns - DataFrame &quot;&quot;&quot; AREA_CATEGORY_dict = {&quot;Evanston&quot;: &quot;C&quot;, &quot;Chicago_Area&quot;: &quot;C&quot;} AREA_CODE_dict = {&quot;Evanston&quot;: &quot;64604&quot;, &quot;Chicago&quot;: &quot;36156&quot;} INDICATOR_CODE_dict = {&quot;Sales_Price&quot;: &quot;SP&quot;} category = AREA_CATEGORY_dict[category] code = AREA_CODE_dict[code] indicator = INDICATOR_CODE_dict[indicator] return quandl.get(quandl_call.format(category=category, code=code, indicator=indicator)) . # data = quandl.get_table(&quot;ZILLOW/REGIONS&quot;, paginate=True) . # col = &#39;region&#39; # mask = np.column_stack([data[col].str.contains(r&quot;Boston&quot;, na=False) for col in data]) # data.loc[mask.any(axis=1)] . # col = &#39;region&#39; # mask = np.column_stack([data[col].str.contains(r&quot;Evanston&quot;, na=False) for col in data]) # df=data.loc[mask.any(axis=1)] #df[&#39;region&#39;] . Chicago and Evanston Home Sale Prices . EV_SP = download_data(&#39;Chicago_Area&#39;, &#39;Evanston&#39;, &#39;Sales_Price&#39;) CH_SP = download_data(&#39;Chicago_Area&#39;, &#39;Chicago&#39;, &#39;Sales_Price&#39;) . CH_SP.query(&quot;Value &gt; 270000&quot;) . Value . Date . 2008-03-31 325100.0 | . 2008-04-30 314800.0 | . 2008-05-31 286900.0 | . 2008-06-30 274600.0 | . 2019-03-31 290800.0 | . 2019-04-30 292000.0 | . 2019-05-31 276000.0 | . 2019-06-30 271500.0 | . 2020-01-31 281400.0 | . 2020-02-29 302900.0 | . 2020-03-31 309200.0 | . from pandasql import sqldf pysqldf = lambda q: sqldf(q, globals()) q = &quot;&quot;&quot;SELECT date ,Value FROM CH_SP WHERE Value &gt; 270000 LIMIT 10;&quot;&quot;&quot; values = pysqldf(q) values . Date Value . 0 2008-03-31 00:00:00.000000 | 325100.0 | . 1 2008-04-30 00:00:00.000000 | 314800.0 | . 2 2008-05-31 00:00:00.000000 | 286900.0 | . 3 2008-06-30 00:00:00.000000 | 274600.0 | . 4 2019-03-31 00:00:00.000000 | 290800.0 | . 5 2019-04-30 00:00:00.000000 | 292000.0 | . 6 2019-05-31 00:00:00.000000 | 276000.0 | . 7 2019-06-30 00:00:00.000000 | 271500.0 | . 8 2020-01-31 00:00:00.000000 | 281400.0 | . 9 2020-02-29 00:00:00.000000 | 302900.0 | . fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Value&#39;) EV_SP[&#39;Value&#39;].plot(label=&#39;Evanston&#39;) CH_SP[&#39;Value&#39;].plot(label=&#39;Chicago&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f3bf2e42c18&gt; . import seaborn as sns from scipy.stats import norm sns.distplot(CH_SP[&#39;Value&#39;], fit=norm); . sns.distplot(EV_SP[&#39;Value&#39;], fit=norm); .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/01/Housing_Prediction.html",
            "relUrl": "/2020/11/01/Housing_Prediction.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
            "content": "This post includes code and notes from data-analysis-using-sql. . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . %load_ext sql . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . connection . &lt;sqlalchemy.engine.base.Connection at 0x7f70c27cc0b8&gt; . tables = pd.read_sql(&quot;&quot;&quot;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&quot;&quot;&quot;, connection) tables . type name tbl_name rootpage sql . 0 table | sqlite_sequence | sqlite_sequence | 4 | CREATE TABLE sqlite_sequence(name,seq) | . 1 table | Player_Attributes | Player_Attributes | 11 | CREATE TABLE &quot;Player_Attributes&quot; ( n t`id` tIN... | . 2 table | Player | Player | 14 | CREATE TABLE `Player` ( n t`id` tINTEGER PRIMA... | . 3 table | Match | Match | 18 | CREATE TABLE `Match` ( n t`id` tINTEGER PRIMAR... | . 4 table | League | League | 24 | CREATE TABLE `League` ( n t`id` tINTEGER PRIMA... | . 5 table | Country | Country | 26 | CREATE TABLE `Country` ( n t`id` tINTEGER PRIM... | . 6 table | Team | Team | 29 | CREATE TABLE &quot;Team&quot; ( n t`id` tINTEGER PRIMARY... | . 7 table | Team_Attributes | Team_Attributes | 2 | CREATE TABLE `Team_Attributes` ( n t`id` tINTE... | . # %%sql # SELECT * # FROM sqlite_master # WHERE type=&#39;table&#39; # ; . engine.execute(&quot;SELECT * FROM Country LIMIT 10&quot;).fetchall() . [(1, &#39;Belgium&#39;), (1729, &#39;England&#39;), (4769, &#39;France&#39;), (7809, &#39;Germany&#39;), (10257, &#39;Italy&#39;), (13274, &#39;Netherlands&#39;), (15722, &#39;Poland&#39;), (17642, &#39;Portugal&#39;), (19694, &#39;Scotland&#39;), (21518, &#39;Spain&#39;)] . %%sql SELECT * FROM Match LIMIT 3; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . 2 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492474 | 10000 | 9994 | 0 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.95 | 3.2 | 3.6 | 1.8 | 3.3 | 3.95 | 1.9 | 3.2 | 3.5 | 1.9 | 3.2 | 3.5 | None | None | None | 1.83 | 3.3 | 3.6 | 1.95 | 3.3 | 3.8 | 2 | 3.25 | 3.25 | 1.85 | 3.25 | 3.75 | 1.91 | 3.25 | 3.6 | . 3 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492475 | 9984 | 8635 | 0 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.38 | 3.3 | 2.75 | 2.4 | 3.3 | 2.55 | 2.6 | 3.1 | 2.3 | 2.5 | 3.2 | 2.5 | None | None | None | 2.5 | 3.25 | 2.4 | 2.63 | 3.3 | 2.5 | 2.35 | 3.25 | 2.65 | 2.5 | 3.2 | 2.5 | 2.3 | 3.2 | 2.75 | . # %%sql # DROP TABLE IF EXISTS Team_table # CREATE TABLE Team_table AS # SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . [] . sql_query = %sql SELECT * FROM Team_table LIMIT 10 df = sql_query.DataFrame() df . * sqlite:///database.sqlite Done. . countries = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Country;&quot;&quot;&quot;, connection) countries.style.highlight_max() . id name . 0 1 | Belgium | . 1 1729 | England | . 2 4769 | France | . 3 7809 | Germany | . 4 10257 | Italy | . 5 13274 | Netherlands | . 6 15722 | Poland | . 7 17642 | Portugal | . 8 19694 | Scotland | . 9 21518 | Spain | . 10 24558 | Switzerland | . # leagues = pd.read_sql(&quot;&quot;&quot;SELECT * # FROM League # JOIN Country ON Country.id = League.country_id;&quot;&quot;&quot;, connection) # leagues . %%sql DROP TABLE IF EXISTS Match_Table; . * sqlite:///database.sqlite Done. . [] . %sql SELECT * FROM Match LIMIT 1; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . %sql SELECT * FROM Country LIMIT 1; . * sqlite:///database.sqlite Done. . id name . 1 | Belgium | . %sql SELECT * FROM League LIMIT 1; . * sqlite:///database.sqlite Done. . id country_id name . 1 | 1 | Belgium Jupiler League | . %%sql DROP TABLE IF EXISTS Match_df; CREATE TABLE Match_df AS SELECT Match.id, Country.name AS country_name, League.name AS league_name, season, stage, date, HT.team_long_name AS home_team, AT.team_long_name AS away_team, home_team_goal, away_team_goal FROM Match JOIN Country on Country.id = Match.country_id JOIN League on League.id = Match.league_id LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id ORDER by date ; . * sqlite:///database.sqlite Done. Done. . [] . %%sql SELECT COUNT(*) FROM Match_df; . * sqlite:///database.sqlite Done. . COUNT(*) . 25979 | . sql_query = %sql SELECT * FROM Match_df LIMIT 10 df = sql_query.DataFrame() df . * sqlite:///database.sqlite Done. . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | . 5 24614 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | AC Bellinzona | Neuchâtel Xamax | 1 | 2 | . 6 24615 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Zürich | FC Luzern | 1 | 0 | . 7 24616 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Sion | BSC Young Boys | 2 | 1 | . 8 24617 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Vaduz | FC Aarau | 0 | 2 | . 9 24668 | Switzerland | Switzerland Super League | 2008/2009 | 3 | 2008-07-26 00:00:00 | FC Basel | AC Bellinzona | 2 | 0 | . Build home team win label for classification . %%sql DROP TABLE IF EXISTS Match_Wins; CREATE TABLE Match_Wins AS SELECT * , CASE WHEN home_team_goal &gt; away_team_goal THEN 1 ELSE 0 END AS home_team_win FROM Match_df ; . * sqlite:///database.sqlite Done. Done. . [] . sql_query = %sql SELECT * FROM Match_Wins df = sql_query.DataFrame() df . * sqlite:///database.sqlite Done. . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | 0 | . 25975 25946 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | 1 | . 25976 25947 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | 0 | . 25977 25948 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | 0 | . 25978 25949 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | 1 | . 25979 rows × 11 columns . from dask import dataframe as dd ddf = dd.from_pandas(df, npartitions=5) . ddf.head() . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . df[&quot;home_team_goal&quot;] = df[&quot;home_team_goal&quot;].astype(float) df[&quot;away_team_goal&quot;] = df[&quot;away_team_goal&quot;].astype(float) df[&quot;stage&quot;] = df[&quot;stage&quot;].astype(float) . feat_list = [ &quot;home_team_goal&quot; ,&quot;away_team_goal&quot; ] . target = [&#39;home_team_win&#39;] . X_train = ddf[feat_list].persist() y_train = ddf[target].persist() . X_train.count().compute() . home_team_goal 25979 away_team_goal 25979 dtype: int64 . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/12521/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask_ml.xgboost import XGBRegressor XGBR = XGBRegressor() XGBR_model = XGBR.fit(X_train,y_train) . # from dask_ml.xgboost import XGBClassifier # XGBC = XGBClassifier() # XGBC_model = XGBC.fit(X_train,y_train) . client.close() . X, y = df.iloc[:, 1:10], df[&quot;home_team_win&quot;] X . country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | . 1 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | . 2 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | . 3 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | . 4 Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | . 25975 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | . 25976 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | . 25977 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | . 25978 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | . 25979 rows × 9 columns . X, y = ddf.iloc[:, 1:10], df[&quot;home_team_win&quot;] X . Dask DataFrame Structure: country_name league_name season stage date home_team away_team home_team_goal away_team_goal . npartitions=5 . 0 object | object | object | int64 | object | object | object | int64 | int64 | . 5196 ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20784 ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25978 ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: getitem, 10 tasks y . 0 0 1 1 2 0 3 0 4 1 5 0 6 1 7 1 8 0 9 1 Name: home_team_win, dtype: int64 . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . # import xgboost # dtrain = xgboost.DMatrix(X_train, y_train) # dtest = xgboost.DMatrix(X_test, y_test) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/31/sql_calls_in_jupyter_Soccer_explore.html",
            "relUrl": "/2020/10/31/sql_calls_in_jupyter_Soccer_explore.html",
            "date": " • Oct 31, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . Find the quandl api documentation here - . import pandas as pd import numpy as np import matplotlib.pyplot as plt import quandl %matplotlib inline . quandl_call = ( &quot;ZILLOW/{category}{code}_{indicator}&quot; ) def download_data(category, code, indicator): &quot;&quot;&quot; Reads in a single dataset from the John Hopkins GitHub repo as a DataFrame Parameters - category : &quot;Chicago_Area&quot; or &quot;Evanston&quot; code : &quot;Evanston&quot; or &quot;Chicago&quot; indicator : &quot;Sales_Price&quot; or &quot;other&quot; Returns - DataFrame &quot;&quot;&quot; AREA_CATEGORY_dict = {&quot;Evanston&quot;: &quot;C&quot;, &quot;Chicago_Area&quot;: &quot;C&quot;} AREA_CODE_dict = {&quot;Evanston&quot;: &quot;64604&quot;, &quot;Chicago&quot;: &quot;36156&quot;} INDICATOR_CODE_dict = {&quot;Sales_Price&quot;: &quot;SP&quot;} category = AREA_CATEGORY_dict[category] code = AREA_CODE_dict[code] indicator = INDICATOR_CODE_dict[indicator] return quandl.get(quandl_call.format(category=category, code=code, indicator=indicator)) . df = download_data(&#39;Chicago_Area&#39;, &#39;Evanston&#39;, &#39;Sales_Price&#39;) . df.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bb6db9ba8&gt; . df[&#39;Value&#39;].plot(label=&#39;Evanston House Prices&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bb4c88cf8&gt; . timeseries = df[&#39;Value&#39;] timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.rolling(12).std().plot(label=&#39;12 Month Rolling Std&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f4bb4a00eb8&gt; . timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f4bb4b0aa90&gt; . from statsmodels.tsa.seasonal import seasonal_decompose decomposition = seasonal_decompose(df[&#39;Value&#39;], freq=12) fig = plt.figure() fig = decomposition.plot() fig.set_size_inches(15, 8) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . &lt;Figure size 432x288 with 0 Axes&gt; . from statsmodels.tsa.arima_model import ARIMA import statsmodels.api as sm . model = sm.tsa.statespace.SARIMAX(df[&#39;Value&#39;],order=(0,1,0), seasonal_order=(1,1,1,12)) results = model.fit() print(results.summary()) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency M will be used. % freq, ValueWarning) . Statespace Model Results ========================================================================================== Dep. Variable: Value No. Observations: 138 Model: SARIMAX(0, 1, 0)x(1, 1, 1, 12) Log Likelihood -1441.360 Date: Mon, 26 Oct 2020 AIC 2888.719 Time: 06:57:44 BIC 2897.204 Sample: 03-31-2008 HQIC 2892.166 - 08-31-2019 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.S.L12 0.3461 0.072 4.832 0.000 0.206 0.487 ma.S.L12 -0.8710 0.113 -7.736 0.000 -1.092 -0.650 sigma2 6.479e+08 1.03e-11 6.3e+19 0.000 6.48e+08 6.48e+08 =================================================================================== Ljung-Box (Q): 120.48 Jarque-Bera (JB): 9.48 Prob(Q): 0.00 Prob(JB): 0.01 Heteroskedasticity (H): 0.66 Skew: -0.60 Prob(H) (two-sided): 0.19 Kurtosis: 3.62 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). [2] Covariance matrix is singular or near-singular, with condition number 5.31e+35. Standard errors may be unstable. . results.resid.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd2947be0&gt; . results.resid.plot(kind=&#39;kde&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd28ffac8&gt; . df[&#39;forecast&#39;] = results.predict(start = 1, end= 200, dynamic= True) df[[&#39;Value&#39;,&#39;forecast&#39;]].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd2689f60&gt; . from pandas.tseries.offsets import DateOffset future_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ] . future_dates . [Timestamp(&#39;2019-08-31 00:00:00&#39;), Timestamp(&#39;2019-09-30 00:00:00&#39;), Timestamp(&#39;2019-10-31 00:00:00&#39;), Timestamp(&#39;2019-11-30 00:00:00&#39;), Timestamp(&#39;2019-12-31 00:00:00&#39;), Timestamp(&#39;2020-01-31 00:00:00&#39;), Timestamp(&#39;2020-02-29 00:00:00&#39;), Timestamp(&#39;2020-03-31 00:00:00&#39;), Timestamp(&#39;2020-04-30 00:00:00&#39;), Timestamp(&#39;2020-05-31 00:00:00&#39;), Timestamp(&#39;2020-06-30 00:00:00&#39;), Timestamp(&#39;2020-07-31 00:00:00&#39;), Timestamp(&#39;2020-08-31 00:00:00&#39;), Timestamp(&#39;2020-09-30 00:00:00&#39;), Timestamp(&#39;2020-10-31 00:00:00&#39;), Timestamp(&#39;2020-11-30 00:00:00&#39;), Timestamp(&#39;2020-12-31 00:00:00&#39;), Timestamp(&#39;2021-01-31 00:00:00&#39;), Timestamp(&#39;2021-02-28 00:00:00&#39;), Timestamp(&#39;2021-03-31 00:00:00&#39;), Timestamp(&#39;2021-04-30 00:00:00&#39;), Timestamp(&#39;2021-05-31 00:00:00&#39;), Timestamp(&#39;2021-06-30 00:00:00&#39;), Timestamp(&#39;2021-07-31 00:00:00&#39;)] . future_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns) . future_df = pd.concat([df,future_dates_df]) . future_df.head() . Value forecast . 2008-03-31 370900.0 | NaN | . 2008-04-30 389600.0 | 370900.0 | . 2008-05-31 367100.0 | 370900.0 | . 2008-06-30 365600.0 | 370900.0 | . 2008-07-31 339000.0 | 370900.0 | . future_df.tail() . Value forecast . 2021-03-31 NaN | NaN | . 2021-04-30 NaN | NaN | . 2021-05-31 NaN | NaN | . 2021-06-30 NaN | NaN | . 2021-07-31 NaN | NaN | . future_df[&#39;forecast&#39;] = results.predict(start = 1, end = 720, dynamic= True) future_df[[&#39;Value&#39;, &#39;forecast&#39;]].plot(figsize=(12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e3573c8&gt; . Exponentially-weighted moving averages . df[&#39;6-month-SMA&#39;]=df[&#39;Value&#39;].rolling(window=6).mean() df[&#39;12-month-SMA&#39;]=df[&#39;Value&#39;].rolling(window=12).mean() . df[&#39;EWMA12&#39;] = df[&#39;Value&#39;].ewm(span=12).mean() . df[[&#39;Value&#39;,&#39;EWMA12&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e350860&gt; . Getting at the trend by removing the cyclical elements of Housing Prices . # Tuple unpacking df_cycle, df_trend = sm.tsa.filters.hpfilter(df.Value) . df_cycle . Date 2008-03-31 1942.830509 2008-04-30 24797.247533 2008-05-31 6450.450288 2008-06-30 9085.726225 2008-07-31 -13417.668736 ... 2019-04-30 25876.617108 2019-05-31 6082.075050 2019-06-30 789.779248 2019-07-31 -4001.523626 2019-08-31 1206.419488 Name: Value, Length: 138, dtype: float64 . df[&quot;trend&quot;] = df_trend . df[[&#39;trend&#39;,&#39;Value&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e275358&gt; . df[[&#39;trend&#39;,&#39;Value&#39;]][&quot;2010-01-31&quot;:].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e20be10&gt; . Chicago and Evanston Home Sale Prices . EV_SP = download_data(&#39;Chicago_Area&#39;, &#39;Evanston&#39;, &#39;Sales_Price&#39;) CH_SP = download_data(&#39;Chicago_Area&#39;, &#39;Chicago&#39;, &#39;Sales_Price&#39;) . fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Value&#39;) EV_SP[&#39;Value&#39;].plot(label=&#39;Evanston&#39;) CH_SP[&#39;Value&#39;].plot(label=&#39;Chicago&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f4b9e0d8828&gt; . CH_SP.plot(figsize=(12,6)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e056470&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/30/Function-for-zillow-data-quandl-api.html",
            "relUrl": "/2020/10/30/Function-for-zillow-data-quandl-api.html",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Creating E-Books (.epub) in python using ebooklib",
            "content": "This post includes code and notes from ebooklib and make-an-ebook. . import os import requests from ebooklib import epub from pyquery import PyQuery as pq . # coding=utf-8 url = &quot;http://example.com/%s.html&quot; build_dir = &quot;build/&quot; if not os.path.exists(build_dir): os.makedirs(build_dir) source_urls = [url % i for i in range(1,2)] urls = [ (build_dir + &quot;%s.html&quot; % i, url % i) for i in range(1,2) ] for filename, url in urls: print(&quot;Getting &quot;, url) response = requests.get(url) with open(filename, &#39;wb&#39;) as f: f.write(response.content) . Getting http://example.com/1.html . if __name__ == &#39;__main__&#39;: book = epub.EpubBook() # add metadata book.set_identifier(&#39;sample123456&#39;) book.set_title(&#39;Sample book&#39;) book.set_language(&#39;en&#39;) book.add_author(&#39;Example Author&#39;) # intro chapter c1 = epub.EpubHtml(title=&#39;Introduction&#39;, file_name=&#39;intro.xhtml&#39;, lang=&#39;en&#39;) c1.content=u&#39;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Introduction&lt;/h1&gt;&lt;p&gt;Introduction paragraph here.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#39; # about chapter c2 = epub.EpubHtml(title=&#39;About this book&#39;, file_name=&#39;about.xhtml&#39;) c2.content=&#39;&lt;h1&gt;About this book&lt;/h1&gt;&lt;p&gt;Text about his book.&lt;/p&gt;&#39; # add chapters to the book book.add_item(c1) book.add_item(c2) # create table of contents # - add section # - add auto created links to chapters book.toc = (epub.Link(&#39;intro.xhtml&#39;, &#39;Introduction&#39;, &#39;intro&#39;), (epub.Section(&#39;Languages&#39;), (c1, c2)) ) # add navigation files book.add_item(epub.EpubNcx()) book.add_item(epub.EpubNav()) # define css style style = &#39;&#39;&#39; @namespace epub &quot;http://www.idpf.org/2007/ops&quot;; body { font-family: Cambria, Liberation Serif, Bitstream Vera Serif, Georgia, Times, Times New Roman, serif; } h2 { text-align: left; text-transform: uppercase; font-weight: 200; } ol { list-style-type: none; } ol &gt; li:first-child { margin-top: 0.3em; } nav[epub|type~=&#39;toc&#39;] &gt; ol &gt; li &gt; ol { list-style-type:square; } nav[epub|type~=&#39;toc&#39;] &gt; ol &gt; li &gt; ol &gt; li { margin-top: 0.3em; } &#39;&#39;&#39; # add css file nav_css = epub.EpubItem(uid=&quot;style_nav&quot;, file_name=&quot;style/nav.css&quot;, media_type=&quot;text/css&quot;, content=style) book.add_item(nav_css) # create spine book.spine = [&#39;nav&#39;, c1, c2] # create epub file epub.write_epub(&#39;test.epub&#39;, book, {}) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/29/pyquery_for_ebooks_python.html",
            "relUrl": "/2020/10/29/pyquery_for_ebooks_python.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Principal component analysis with sklearn",
            "content": "import seaborn as sns from sklearn import datasets import pandas as pd . iris = datasets.load_iris() . target_names = iris.target_names . X = pd.DataFrame(iris.data) . X.columns = [&#39;Sepal Length&#39;, &#39;Sepal Width&#39;, &#39;Petal Length&#39;, &#39;Petal Width&#39;] . for name in X.columns: X[name] = (X[name]-X[name].mean())/X[name].std() . sns.heatmap(X.corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;2.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f026c877780&gt; . import numpy as np . # calculate the Covariance matrix Q = X.cov().to_numpy()# find the eigenvalue and eigenvector of the Covariance matrix D, V = np.linalg.eigh(Q) # print the results np.set_printoptions(precision=2) print(&#39;principal components:&#39;) . principal components: . for i in range(1, len(D)): print(&#39;Feature %d : %2.3f&#39;%(i, D[i])) . Feature 1 : 0.147 Feature 2 : 0.914 Feature 3 : 2.918 . # perform the linear transformation X_new = X.dot(V)# define the columns names to the X_new X_new.columns = [&quot;Feature %d&quot;%i for i in range(1,5)]# The correlation between different features disappear! sns.heatmap(X_new.corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;2.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f023c032940&gt; . # truncation: dimensional reduction X_reduced = X.copy() . V_trun = V[:,2:4] . X_reduced = X_reduced.dot(V_trun) . X_reduced[&#39;Species&#39;] = iris.target . X_reduced.columns = [&#39;Feature 3&#39;, &#39;Feature 4&#39;, &#39;Species&#39;] . for i, t in enumerate(target_names): X_reduced[&#39;Species&#39;].replace(i, t, inplace=True) . sns.scatterplot(data=X_reduced, x=&#39;Feature 3&#39;, y=&#39;Feature 4&#39;, hue=&#39;Species&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f023bf7f400&gt; . from sklearn.decomposition import PCA . X3 = X.to_numpy() pca = PCA(n_components=3, random_state=0) X3_reduced = pca.fit(X3).transform(X3) X3_reduced = pd.DataFrame(X3_reduced) X3_reduced[&#39;species&#39;] = iris.target# plot the results in 3D scatter plot . from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt . fig = plt.figure(figsize=(6,6)) ax = Axes3D(fig) for i in range(3): idx = X3_reduced[&#39;species&#39;]==i ax.scatter(X3_reduced[0][idx], X3_reduced[1][idx], X3_reduced[2][idx], label=target_names[i]) plt.legend() ax.view_init(20,75) plt.xlabel(&#39;Feature 2&#39;) plt.ylabel(&#39;Feature 3&#39;) ax.set_zlabel(&#39;Feature 4&#39;) . Text(0.5, 0, &#39;Feature 4&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/28/PCA-with-sklearn.html",
            "relUrl": "/2020/10/28/PCA-with-sklearn.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
            "content": "This post includes code and notes from data-analysis-using-sql. . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . connection . &lt;sqlalchemy.engine.base.Connection at 0x7fb9c7c07080&gt; . engine.execute(&quot;SELECT * FROM Country LIMIT 10&quot;).fetchall() . [(1, &#39;Belgium&#39;), (1729, &#39;England&#39;), (4769, &#39;France&#39;), (7809, &#39;Germany&#39;), (10257, &#39;Italy&#39;), (13274, &#39;Netherlands&#39;), (15722, &#39;Poland&#39;), (17642, &#39;Portugal&#39;), (19694, &#39;Scotland&#39;), (21518, &#39;Spain&#39;)] . %load_ext sql . %sql sqlite:///database.sqlite . %%sql SELECT * FROM Country LIMIT 10 . * sqlite:///database.sqlite Done. . id name . 1 | Belgium | . 1729 | England | . 4769 | France | . 7809 | Germany | . 10257 | Italy | . 13274 | Netherlands | . 15722 | Poland | . 17642 | Portugal | . 19694 | Scotland | . 21518 | Spain | . %%sql SELECT id ,name FROM Country WHERE name = &quot;England&quot; . * sqlite:///database.sqlite Done. . id name . 1729 | England | . %%sql SELECT * FROM League LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id name . 1 | 1 | Belgium Jupiler League | . 1729 | 1729 | England Premier League | . 4769 | 4769 | France Ligue 1 | . 7809 | 7809 | Germany 1. Bundesliga | . 10257 | 10257 | Italy Serie A | . 13274 | 13274 | Netherlands Eredivisie | . 15722 | 15722 | Poland Ekstraklasa | . 17642 | 17642 | Portugal Liga ZON Sagres | . 19694 | 19694 | Scotland Premier League | . 21518 | 21518 | Spain LIGA BBVA | . %%sql SELECT * FROM Match LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . 2 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492474 | 10000 | 9994 | 0 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.95 | 3.2 | 3.6 | 1.8 | 3.3 | 3.95 | 1.9 | 3.2 | 3.5 | 1.9 | 3.2 | 3.5 | None | None | None | 1.83 | 3.3 | 3.6 | 1.95 | 3.3 | 3.8 | 2 | 3.25 | 3.25 | 1.85 | 3.25 | 3.75 | 1.91 | 3.25 | 3.6 | . 3 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492475 | 9984 | 8635 | 0 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.38 | 3.3 | 2.75 | 2.4 | 3.3 | 2.55 | 2.6 | 3.1 | 2.3 | 2.5 | 3.2 | 2.5 | None | None | None | 2.5 | 3.25 | 2.4 | 2.63 | 3.3 | 2.5 | 2.35 | 3.25 | 2.65 | 2.5 | 3.2 | 2.5 | 2.3 | 3.2 | 2.75 | . 4 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492476 | 9991 | 9998 | 5 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.44 | 3.75 | 7.5 | 1.4 | 4 | 6.8 | 1.4 | 3.9 | 6 | 1.44 | 3.6 | 6.5 | None | None | None | 1.44 | 3.75 | 6 | 1.44 | 4 | 7.5 | 1.45 | 3.75 | 6.5 | 1.5 | 3.75 | 5.5 | 1.44 | 3.75 | 6.5 | . 5 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492477 | 7947 | 9985 | 1 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 5 | 3.5 | 1.65 | 5 | 3.5 | 1.6 | 4 | 3.3 | 1.7 | 4 | 3.4 | 1.72 | None | None | None | 4.2 | 3.4 | 1.7 | 4.5 | 3.5 | 1.73 | 4.5 | 3.4 | 1.65 | 4.5 | 3.5 | 1.65 | 4.75 | 3.3 | 1.67 | . 6 | 1 | 1 | 2008/2009 | 1 | 2008-09-24 00:00:00 | 492478 | 8203 | 8342 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 4.75 | 3.4 | 1.67 | 4.85 | 3.4 | 1.65 | 3.7 | 3.2 | 1.8 | 5 | 3.25 | 1.62 | None | None | None | 4.2 | 3.4 | 1.7 | 5.5 | 3.75 | 1.67 | 4.35 | 3.4 | 1.7 | 4.5 | 3.4 | 1.7 | None | None | None | . 7 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492479 | 9999 | 8571 | 2 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.1 | 3.2 | 3.3 | 2.05 | 3.25 | 3.15 | 1.85 | 3.2 | 3.5 | 1.83 | 3.3 | 3.6 | None | None | None | 1.83 | 3.3 | 3.6 | 1.91 | 3.4 | 3.6 | 2.1 | 3.25 | 3 | 1.85 | 3.25 | 3.75 | 2.1 | 3.25 | 3.1 | . 8 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492480 | 4049 | 9996 | 1 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 3.2 | 3.4 | 2.2 | 2.55 | 3.3 | 2.4 | 2.4 | 3.2 | 2.4 | 2.5 | 3.2 | 2.5 | None | None | None | 2.7 | 3.25 | 2.25 | 2.6 | 3.4 | 2.4 | 2.8 | 3.25 | 2.25 | 2.8 | 3.2 | 2.25 | 2.88 | 3.25 | 2.2 | . 9 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492481 | 10001 | 9986 | 1 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.25 | 3.25 | 2.88 | 2.3 | 3.25 | 2.7 | 2.1 | 3.1 | 3 | 2.25 | 3.2 | 2.75 | None | None | None | 2.2 | 3.25 | 2.75 | 2.2 | 3.3 | 3.1 | 2.25 | 3.25 | 2.8 | 2.2 | 3.3 | 2.8 | 2.25 | 3.2 | 2.8 | . 10 | 1 | 1 | 2008/2009 | 10 | 2008-11-01 00:00:00 | 492564 | 8342 | 8571 | 4 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.3 | 5.25 | 9.5 | 1.25 | 5 | 10 | 1.3 | 4.2 | 8 | 1.25 | 4.5 | 10 | None | None | None | 1.35 | 4.2 | 7 | 1.27 | 5 | 10 | 1.3 | 4.35 | 8.5 | 1.25 | 5 | 10 | 1.29 | 4.5 | 9 | . %%sql SELECT * FROM Player LIMIT 10; . * sqlite:///database.sqlite Done. . id player_api_id player_name player_fifa_api_id birthday height weight . 1 | 505942 | Aaron Appindangoye | 218353 | 1992-02-29 00:00:00 | 182.88 | 187 | . 2 | 155782 | Aaron Cresswell | 189615 | 1989-12-15 00:00:00 | 170.18 | 146 | . 3 | 162549 | Aaron Doran | 186170 | 1991-05-13 00:00:00 | 170.18 | 163 | . 4 | 30572 | Aaron Galindo | 140161 | 1982-05-08 00:00:00 | 182.88 | 198 | . 5 | 23780 | Aaron Hughes | 17725 | 1979-11-08 00:00:00 | 182.88 | 154 | . 6 | 27316 | Aaron Hunt | 158138 | 1986-09-04 00:00:00 | 182.88 | 161 | . 7 | 564793 | Aaron Kuhl | 221280 | 1996-01-30 00:00:00 | 172.72 | 146 | . 8 | 30895 | Aaron Lennon | 152747 | 1987-04-16 00:00:00 | 165.1 | 139 | . 9 | 528212 | Aaron Lennox | 206592 | 1993-02-19 00:00:00 | 190.5 | 181 | . 10 | 101042 | Aaron Meijers | 188621 | 1987-10-28 00:00:00 | 175.26 | 170 | . %%sql SELECT * FROM Player_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id player_fifa_api_id player_api_id date overall_rating potential preferred_foot attacking_work_rate defensive_work_rate crossing finishing heading_accuracy short_passing volleys dribbling curve free_kick_accuracy long_passing ball_control acceleration sprint_speed agility reactions balance shot_power jumping stamina strength long_shots aggression interceptions positioning vision penalties marking standing_tackle sliding_tackle gk_diving gk_handling gk_kicking gk_positioning gk_reflexes . 1 | 218353 | 505942 | 2016-02-18 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 2 | 218353 | 505942 | 2015-11-19 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 3 | 218353 | 505942 | 2015-09-21 00:00:00 | 62 | 66 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 63 | 41 | 45 | 54 | 48 | 65 | 66 | 69 | 6 | 11 | 10 | 8 | 8 | . 4 | 218353 | 505942 | 2015-03-20 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 5 | 218353 | 505942 | 2007-02-22 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 6 | 189615 | 155782 | 2016-04-21 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 40 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 62 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 7 | 189615 | 155782 | 2016-04-07 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 32 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 60 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 8 | 189615 | 155782 | 2016-01-07 00:00:00 | 73 | 75 | left | high | medium | 79 | 52 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 59 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 9 | 189615 | 155782 | 2015-12-24 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 10 | 189615 | 155782 | 2015-12-17 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . %%sql SELECT * FROM Team LIMIT 10; . * sqlite:///database.sqlite Done. . id team_api_id team_fifa_api_id team_long_name team_short_name . 1 | 9987 | 673 | KRC Genk | GEN | . 2 | 9993 | 675 | Beerschot AC | BAC | . 3 | 10000 | 15005 | SV Zulte-Waregem | ZUL | . 4 | 9994 | 2007 | Sporting Lokeren | LOK | . 5 | 9984 | 1750 | KSV Cercle Brugge | CEB | . 6 | 8635 | 229 | RSC Anderlecht | AND | . 7 | 9991 | 674 | KAA Gent | GEN | . 8 | 9998 | 1747 | RAEC Mons | MON | . 9 | 7947 | None | FCV Dender EH | DEN | . 10 | 9985 | 232 | Standard de Liège | STL | . %%sql SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id team_fifa_api_id team_api_id date buildUpPlaySpeed buildUpPlaySpeedClass buildUpPlayDribbling buildUpPlayDribblingClass buildUpPlayPassing buildUpPlayPassingClass buildUpPlayPositioningClass chanceCreationPassing chanceCreationPassingClass chanceCreationCrossing chanceCreationCrossingClass chanceCreationShooting chanceCreationShootingClass chanceCreationPositioningClass defencePressure defencePressureClass defenceAggression defenceAggressionClass defenceTeamWidth defenceTeamWidthClass defenceDefenderLineClass . 1 | 434 | 9930 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 50 | Mixed | Organised | 60 | Normal | 65 | Normal | 55 | Normal | Organised | 50 | Medium | 55 | Press | 45 | Normal | Cover | . 2 | 434 | 9930 | 2014-09-19 00:00:00 | 52 | Balanced | 48 | Normal | 56 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 3 | 434 | 9930 | 2015-09-10 00:00:00 | 47 | Balanced | 41 | Normal | 54 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 4 | 77 | 8485 | 2010-02-22 00:00:00 | 70 | Fast | None | Little | 70 | Long | Organised | 70 | Risky | 70 | Lots | 70 | Lots | Organised | 60 | Medium | 70 | Double | 70 | Wide | Cover | . 5 | 77 | 8485 | 2011-02-22 00:00:00 | 47 | Balanced | None | Little | 52 | Mixed | Organised | 53 | Normal | 48 | Normal | 52 | Normal | Organised | 47 | Medium | 47 | Press | 52 | Normal | Cover | . 6 | 77 | 8485 | 2012-02-22 00:00:00 | 58 | Balanced | None | Little | 62 | Mixed | Organised | 45 | Normal | 70 | Lots | 55 | Normal | Organised | 40 | Medium | 40 | Press | 60 | Normal | Cover | . 7 | 77 | 8485 | 2013-09-20 00:00:00 | 62 | Balanced | None | Little | 45 | Mixed | Organised | 40 | Normal | 50 | Normal | 55 | Normal | Organised | 42 | Medium | 42 | Press | 60 | Normal | Cover | . 8 | 77 | 8485 | 2014-09-19 00:00:00 | 58 | Balanced | 64 | Normal | 62 | Mixed | Organised | 56 | Normal | 68 | Lots | 57 | Normal | Organised | 41 | Medium | 42 | Press | 60 | Normal | Cover | . 9 | 77 | 8485 | 2015-09-10 00:00:00 | 59 | Balanced | 64 | Normal | 53 | Mixed | Organised | 51 | Normal | 72 | Lots | 63 | Normal | Free Form | 49 | Medium | 45 | Press | 63 | Normal | Cover | . 10 | 614 | 8576 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 40 | Mixed | Organised | 45 | Normal | 35 | Normal | 55 | Normal | Organised | 30 | Deep | 70 | Double | 30 | Narrow | Offside Trap | . %%sql CREATE TABLE Team_table AS SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . [] . %%sql DROP TABLE IF EXISTS Team_table . * sqlite:///database.sqlite Done. . [] . sql_query = %sql SELECT * FROM Team LIMIT 10 df = sql_query.DataFrame() . * sqlite:///database.sqlite Done. . df . id team_api_id team_fifa_api_id team_long_name team_short_name . 0 1 | 9987 | 673.0 | KRC Genk | GEN | . 1 2 | 9993 | 675.0 | Beerschot AC | BAC | . 2 3 | 10000 | 15005.0 | SV Zulte-Waregem | ZUL | . 3 4 | 9994 | 2007.0 | Sporting Lokeren | LOK | . 4 5 | 9984 | 1750.0 | KSV Cercle Brugge | CEB | . 5 6 | 8635 | 229.0 | RSC Anderlecht | AND | . 6 7 | 9991 | 674.0 | KAA Gent | GEN | . 7 8 | 9998 | 1747.0 | RAEC Mons | MON | . 8 9 | 7947 | NaN | FCV Dender EH | DEN | . 9 10 | 9985 | 232.0 | Standard de Liège | STL | . import matplotlib.pyplot as plt plt.figure(figsize=(20,7)) plot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name plot.bar(); . * sqlite:///database.sqlite Done. . plot.pie(); . type(plot) . sql.run.ResultSet . # #Imports # import numpy as np # linear algebra # import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import sqlite3 # import matplotlib.pyplot as plt # # Input data files are available in the &quot;../input/&quot; directory. # # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory # path = &quot;../input/&quot; #Insert path here # database = path + &#39;database.sqlite&#39; . First we will create the connection to the DB, and see what tables we have . tables = pd.read_sql(&quot;&quot;&quot;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&quot;&quot;&quot;, connection) tables . type name tbl_name rootpage sql . 0 table | sqlite_sequence | sqlite_sequence | 4 | CREATE TABLE sqlite_sequence(name,seq) | . 1 table | Player_Attributes | Player_Attributes | 11 | CREATE TABLE &quot;Player_Attributes&quot; ( n t`id` tIN... | . 2 table | Player | Player | 14 | CREATE TABLE `Player` ( n t`id` tINTEGER PRIMA... | . 3 table | Match | Match | 18 | CREATE TABLE `Match` ( n t`id` tINTEGER PRIMAR... | . 4 table | League | League | 24 | CREATE TABLE `League` ( n t`id` tINTEGER PRIMA... | . 5 table | Country | Country | 26 | CREATE TABLE `Country` ( n t`id` tINTEGER PRIM... | . 6 table | Team | Team | 29 | CREATE TABLE &quot;Team&quot; ( n t`id` tINTEGER PRIMARY... | . 7 table | Team_Attributes | Team_Attributes | 2 | CREATE TABLE `Team_Attributes` ( n t`id` tINTE... | . countries = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Country;&quot;&quot;&quot;, connection) countries . id name . 0 1 | Belgium | . 1 1729 | England | . 2 4769 | France | . 3 7809 | Germany | . 4 10257 | Italy | . 5 13274 | Netherlands | . 6 15722 | Poland | . 7 17642 | Portugal | . 8 19694 | Scotland | . 9 21518 | Spain | . 10 24558 | Switzerland | . leagues = pd.read_sql(&quot;&quot;&quot;SELECT * FROM League JOIN Country ON Country.id = League.country_id;&quot;&quot;&quot;, connection) leagues . id country_id name id name . 0 1 | 1 | Belgium Jupiler League | 1 | Belgium | . 1 1729 | 1729 | England Premier League | 1729 | England | . 2 4769 | 4769 | France Ligue 1 | 4769 | France | . 3 7809 | 7809 | Germany 1. Bundesliga | 7809 | Germany | . 4 10257 | 10257 | Italy Serie A | 10257 | Italy | . 5 13274 | 13274 | Netherlands Eredivisie | 13274 | Netherlands | . 6 15722 | 15722 | Poland Ekstraklasa | 15722 | Poland | . 7 17642 | 17642 | Portugal Liga ZON Sagres | 17642 | Portugal | . 8 19694 | 19694 | Scotland Premier League | 19694 | Scotland | . 9 21518 | 21518 | Spain LIGA BBVA | 21518 | Spain | . 10 24558 | 24558 | Switzerland Super League | 24558 | Switzerland | . teams = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Team ORDER BY team_long_name LIMIT 10;&quot;&quot;&quot;, connection) teams . id team_api_id team_fifa_api_id team_long_name team_short_name . 0 16848 | 8350 | 29 | 1. FC Kaiserslautern | KAI | . 1 15624 | 8722 | 31 | 1. FC Köln | FCK | . 2 16239 | 8165 | 171 | 1. FC Nürnberg | NUR | . 3 16243 | 9905 | 169 | 1. FSV Mainz 05 | MAI | . 4 11817 | 8576 | 614 | AC Ajaccio | AJA | . 5 11074 | 108893 | 111989 | AC Arles-Avignon | ARL | . 6 49116 | 6493 | 1714 | AC Bellinzona | BEL | . 7 26560 | 10217 | 650 | ADO Den Haag | HAA | . 8 9537 | 8583 | 57 | AJ Auxerre | AUX | . 9 9547 | 9829 | 69 | AS Monaco | MON | . detailed_matches = pd.read_sql(&quot;&quot;&quot;SELECT Match.id, Country.name AS country_name, League.name AS league_name, season, stage, date, HT.team_long_name AS home_team, AT.team_long_name AS away_team, home_team_goal, away_team_goal FROM Match JOIN Country on Country.id = Match.country_id JOIN League on League.id = Match.league_id LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id WHERE country_name = &#39;Spain&#39; ORDER by date LIMIT 10;&quot;&quot;&quot;, connection) detailed_matches . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 21518 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-30 00:00:00 | Valencia CF | RCD Mallorca | 3 | 0 | . 1 21525 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-30 00:00:00 | RCD Espanyol | Real Valladolid | 1 | 0 | . 2 21519 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | CA Osasuna | Villarreal CF | 1 | 1 | . 3 21520 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | RC Deportivo de La Coruña | Real Madrid CF | 2 | 1 | . 4 21521 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | CD Numancia | FC Barcelona | 1 | 0 | . 5 21522 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Racing Santander | Sevilla FC | 1 | 1 | . 6 21523 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Real Sporting de Gijón | Getafe CF | 1 | 2 | . 7 21524 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Real Betis Balompié | RC Recreativo | 0 | 1 | . 8 21526 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Athletic Club de Bilbao | UD Almería | 1 | 3 | . 9 21527 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Atlético Madrid | Málaga CF | 4 | 0 | . # Star with Spain Real Madrid CF, barcelonat . leages_by_season = pd.read_sql(&quot;&quot;&quot;SELECT Country.name AS country_name, League.name AS league_name, season, count(distinct stage) AS number_of_stages, count(distinct HT.team_long_name) AS number_of_teams, avg(home_team_goal) AS avg_home_team_scors, avg(away_team_goal) AS avg_away_team_goals, avg(home_team_goal-away_team_goal) AS avg_goal_dif, avg(home_team_goal+away_team_goal) AS avg_goals, sum(home_team_goal+away_team_goal) AS total_goals FROM Match JOIN Country on Country.id = Match.country_id JOIN League on League.id = Match.league_id LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id WHERE country_name in (&#39;Spain&#39;, &#39;Germany&#39;, &#39;France&#39;, &#39;Italy&#39;, &#39;England&#39;) GROUP BY Country.name, League.name, season HAVING count(distinct stage) &gt; 10 ORDER BY Country.name, League.name, season DESC ;&quot;&quot;&quot;, connection) leages_by_season . country_name league_name season number_of_stages number_of_teams avg_home_team_scors avg_away_team_goals avg_goal_dif avg_goals total_goals . 0 England | England Premier League | 2015/2016 | 38 | 20 | 1.492105 | 1.207895 | 0.284211 | 2.700000 | 1026 | . 1 England | England Premier League | 2014/2015 | 38 | 20 | 1.473684 | 1.092105 | 0.381579 | 2.565789 | 975 | . 2 England | England Premier League | 2013/2014 | 38 | 20 | 1.573684 | 1.194737 | 0.378947 | 2.768421 | 1052 | . 3 England | England Premier League | 2012/2013 | 38 | 20 | 1.557895 | 1.239474 | 0.318421 | 2.797368 | 1063 | . 4 England | England Premier League | 2011/2012 | 38 | 20 | 1.589474 | 1.215789 | 0.373684 | 2.805263 | 1066 | . 5 England | England Premier League | 2010/2011 | 38 | 20 | 1.623684 | 1.173684 | 0.450000 | 2.797368 | 1063 | . 6 England | England Premier League | 2009/2010 | 38 | 20 | 1.697368 | 1.073684 | 0.623684 | 2.771053 | 1053 | . 7 England | England Premier League | 2008/2009 | 38 | 20 | 1.400000 | 1.078947 | 0.321053 | 2.478947 | 942 | . 8 France | France Ligue 1 | 2015/2016 | 38 | 20 | 1.436842 | 1.089474 | 0.347368 | 2.526316 | 960 | . 9 France | France Ligue 1 | 2014/2015 | 38 | 20 | 1.410526 | 1.081579 | 0.328947 | 2.492105 | 947 | . 10 France | France Ligue 1 | 2013/2014 | 38 | 20 | 1.415789 | 1.039474 | 0.376316 | 2.455263 | 933 | . 11 France | France Ligue 1 | 2012/2013 | 38 | 20 | 1.468421 | 1.076316 | 0.392105 | 2.544737 | 967 | . 12 France | France Ligue 1 | 2011/2012 | 38 | 20 | 1.473684 | 1.042105 | 0.431579 | 2.515789 | 956 | . 13 France | France Ligue 1 | 2010/2011 | 38 | 20 | 1.342105 | 1.000000 | 0.342105 | 2.342105 | 890 | . 14 France | France Ligue 1 | 2009/2010 | 38 | 20 | 1.389474 | 1.021053 | 0.368421 | 2.410526 | 916 | . 15 France | France Ligue 1 | 2008/2009 | 38 | 20 | 1.286842 | 0.971053 | 0.315789 | 2.257895 | 858 | . 16 Germany | Germany 1. Bundesliga | 2015/2016 | 34 | 18 | 1.565359 | 1.264706 | 0.300654 | 2.830065 | 866 | . 17 Germany | Germany 1. Bundesliga | 2014/2015 | 34 | 18 | 1.588235 | 1.166667 | 0.421569 | 2.754902 | 843 | . 18 Germany | Germany 1. Bundesliga | 2013/2014 | 34 | 18 | 1.748366 | 1.411765 | 0.336601 | 3.160131 | 967 | . 19 Germany | Germany 1. Bundesliga | 2012/2013 | 34 | 18 | 1.591503 | 1.343137 | 0.248366 | 2.934641 | 898 | . 20 Germany | Germany 1. Bundesliga | 2011/2012 | 34 | 18 | 1.660131 | 1.199346 | 0.460784 | 2.859477 | 875 | . 21 Germany | Germany 1. Bundesliga | 2010/2011 | 34 | 18 | 1.647059 | 1.274510 | 0.372549 | 2.921569 | 894 | . 22 Germany | Germany 1. Bundesliga | 2009/2010 | 34 | 18 | 1.513072 | 1.316993 | 0.196078 | 2.830065 | 866 | . 23 Germany | Germany 1. Bundesliga | 2008/2009 | 34 | 18 | 1.699346 | 1.222222 | 0.477124 | 2.921569 | 894 | . 24 Italy | Italy Serie A | 2015/2016 | 38 | 20 | 1.471053 | 1.105263 | 0.365789 | 2.576316 | 979 | . 25 Italy | Italy Serie A | 2014/2015 | 38 | 20 | 1.498681 | 1.187335 | 0.311346 | 2.686016 | 1018 | . 26 Italy | Italy Serie A | 2013/2014 | 38 | 20 | 1.536842 | 1.186842 | 0.350000 | 2.723684 | 1035 | . 27 Italy | Italy Serie A | 2012/2013 | 38 | 20 | 1.494737 | 1.144737 | 0.350000 | 2.639474 | 1003 | . 28 Italy | Italy Serie A | 2011/2012 | 38 | 20 | 1.511173 | 1.072626 | 0.438547 | 2.583799 | 925 | . 29 Italy | Italy Serie A | 2010/2011 | 38 | 20 | 1.431579 | 1.081579 | 0.350000 | 2.513158 | 955 | . 30 Italy | Italy Serie A | 2009/2010 | 38 | 20 | 1.542105 | 1.068421 | 0.473684 | 2.610526 | 992 | . 31 Italy | Italy Serie A | 2008/2009 | 38 | 20 | 1.521053 | 1.078947 | 0.442105 | 2.600000 | 988 | . 32 Spain | Spain LIGA BBVA | 2015/2016 | 38 | 20 | 1.618421 | 1.126316 | 0.492105 | 2.744737 | 1043 | . 33 Spain | Spain LIGA BBVA | 2014/2015 | 38 | 20 | 1.536842 | 1.118421 | 0.418421 | 2.655263 | 1009 | . 34 Spain | Spain LIGA BBVA | 2013/2014 | 38 | 20 | 1.631579 | 1.118421 | 0.513158 | 2.750000 | 1045 | . 35 Spain | Spain LIGA BBVA | 2012/2013 | 38 | 20 | 1.686842 | 1.184211 | 0.502632 | 2.871053 | 1091 | . 36 Spain | Spain LIGA BBVA | 2011/2012 | 38 | 20 | 1.678947 | 1.084211 | 0.594737 | 2.763158 | 1050 | . 37 Spain | Spain LIGA BBVA | 2010/2011 | 38 | 20 | 1.636842 | 1.105263 | 0.531579 | 2.742105 | 1042 | . 38 Spain | Spain LIGA BBVA | 2009/2010 | 38 | 20 | 1.600000 | 1.113158 | 0.486842 | 2.713158 | 1031 | . 39 Spain | Spain LIGA BBVA | 2008/2009 | 38 | 20 | 1.660526 | 1.236842 | 0.423684 | 2.897368 | 1101 | . df = pd.DataFrame(index=np.sort(leages_by_season[&#39;season&#39;].unique()), columns=leages_by_season[&#39;country_name&#39;].unique()) df.loc[:,&#39;Germany&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Germany&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;Spain&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Spain&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;France&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;France&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;Italy&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Italy&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;England&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;England&#39;,&#39;avg_goals&#39;]) df.plot(figsize=(12,5),title=&#39;Average Goals per Game Over Time&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb9c3770780&gt; . df = pd.DataFrame(index=np.sort(leages_by_season[&#39;season&#39;].unique()), columns=leages_by_season[&#39;country_name&#39;].unique()) df.loc[:,&#39;Germany&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Germany&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;Spain&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Spain&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;France&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;France&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;Italy&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Italy&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;England&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;England&#39;,&#39;avg_goal_dif&#39;]) df.plot(figsize=(12,5),title=&#39;Average Goals Difference Home vs Out&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb9c39687f0&gt; . players_height = pd.read_sql(&quot;&quot;&quot;SELECT CASE WHEN ROUND(height)&lt;165 then 165 WHEN ROUND(height)&gt;195 then 195 ELSE ROUND(height) END AS calc_height, COUNT(height) AS distribution, (avg(PA_Grouped.avg_overall_rating)) AS avg_overall_rating, (avg(PA_Grouped.avg_potential)) AS avg_potential, AVG(weight) AS avg_weight FROM PLAYER LEFT JOIN (SELECT Player_Attributes.player_api_id, avg(Player_Attributes.overall_rating) AS avg_overall_rating, avg(Player_Attributes.potential) AS avg_potential FROM Player_Attributes GROUP BY Player_Attributes.player_api_id) AS PA_Grouped ON PLAYER.player_api_id = PA_Grouped.player_api_id GROUP BY calc_height ORDER BY calc_height ;&quot;&quot;&quot;, connection) players_height . calc_height distribution avg_overall_rating avg_potential avg_weight . 0 165.0 | 74 | 67.365543 | 73.327754 | 139.459459 | . 1 168.0 | 118 | 67.500518 | 73.124182 | 144.127119 | . 2 170.0 | 403 | 67.726903 | 73.379056 | 147.799007 | . 3 173.0 | 530 | 66.980272 | 72.848746 | 152.824528 | . 4 175.0 | 1188 | 66.805204 | 72.258774 | 156.111953 | . 5 178.0 | 1489 | 66.367212 | 71.943339 | 160.665547 | . 6 180.0 | 1388 | 66.419053 | 71.846394 | 165.261527 | . 7 183.0 | 1954 | 66.634380 | 71.754555 | 170.167861 | . 8 185.0 | 1278 | 66.928964 | 71.833475 | 174.636933 | . 9 188.0 | 1305 | 67.094253 | 72.151949 | 179.278161 | . 10 191.0 | 652 | 66.997649 | 71.846159 | 184.791411 | . 11 193.0 | 470 | 67.485141 | 72.459225 | 188.795745 | . 12 195.0 | 211 | 67.425619 | 72.615373 | 196.464455 | . players_height.calc_height . 0 165.0 1 168.0 2 170.0 3 173.0 4 175.0 5 178.0 6 180.0 7 183.0 8 185.0 9 188.0 10 191.0 11 193.0 12 195.0 Name: calc_height, dtype: float64 . # players_height.plot(x=[&#39;calc_height&#39;],y=[&#39;avg_overall_rating&#39;],figsize=(12,5),title=&#39;Potential vs Height&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/27/sql_calls_in_jupyter_Soccer_Pred.html",
            "relUrl": "/2020/10/27/sql_calls_in_jupyter_Soccer_Pred.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Data Science Links",
            "content": "On the borders of Statistics and Machine learning: Discussion of conscious v. unconscious processes and Statistics and Machine learning, causal inference v. pattern recognition. | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20links/2020/10/26/links.html",
            "relUrl": "/data%20science%20links/2020/10/26/links.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Working with sqlite databases in Jupyter for European Soccer Match Data",
            "content": "This post includes code and notes from data-analysis-using-sql. . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . connection . &lt;sqlalchemy.engine.base.Connection at 0x7fb9c3356780&gt; . engine.execute(&quot;SELECT * FROM Country LIMIT 10&quot;).fetchall() . [(1, &#39;Belgium&#39;), (1729, &#39;England&#39;), (4769, &#39;France&#39;), (7809, &#39;Germany&#39;), (10257, &#39;Italy&#39;), (13274, &#39;Netherlands&#39;), (15722, &#39;Poland&#39;), (17642, &#39;Portugal&#39;), (19694, &#39;Scotland&#39;), (21518, &#39;Spain&#39;)] . %load_ext sql . The sql extension is already loaded. To reload it, use: %reload_ext sql . %sql sqlite:///database.sqlite . %%sql SELECT * FROM Country LIMIT 10 . * sqlite:///database.sqlite Done. . id name . 1 | Belgium | . 1729 | England | . 4769 | France | . 7809 | Germany | . 10257 | Italy | . 13274 | Netherlands | . 15722 | Poland | . 17642 | Portugal | . 19694 | Scotland | . 21518 | Spain | . %%sql SELECT id ,name FROM Country WHERE name = &quot;England&quot; . * sqlite:///database.sqlite Done. . id name . 1729 | England | . %%sql SELECT * FROM League LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id name . 1 | 1 | Belgium Jupiler League | . 1729 | 1729 | England Premier League | . 4769 | 4769 | France Ligue 1 | . 7809 | 7809 | Germany 1. Bundesliga | . 10257 | 10257 | Italy Serie A | . 13274 | 13274 | Netherlands Eredivisie | . 15722 | 15722 | Poland Ekstraklasa | . 17642 | 17642 | Portugal Liga ZON Sagres | . 19694 | 19694 | Scotland Premier League | . 21518 | 21518 | Spain LIGA BBVA | . %%sql SELECT * FROM Match LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . 2 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492474 | 10000 | 9994 | 0 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.95 | 3.2 | 3.6 | 1.8 | 3.3 | 3.95 | 1.9 | 3.2 | 3.5 | 1.9 | 3.2 | 3.5 | None | None | None | 1.83 | 3.3 | 3.6 | 1.95 | 3.3 | 3.8 | 2 | 3.25 | 3.25 | 1.85 | 3.25 | 3.75 | 1.91 | 3.25 | 3.6 | . 3 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492475 | 9984 | 8635 | 0 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.38 | 3.3 | 2.75 | 2.4 | 3.3 | 2.55 | 2.6 | 3.1 | 2.3 | 2.5 | 3.2 | 2.5 | None | None | None | 2.5 | 3.25 | 2.4 | 2.63 | 3.3 | 2.5 | 2.35 | 3.25 | 2.65 | 2.5 | 3.2 | 2.5 | 2.3 | 3.2 | 2.75 | . 4 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492476 | 9991 | 9998 | 5 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.44 | 3.75 | 7.5 | 1.4 | 4 | 6.8 | 1.4 | 3.9 | 6 | 1.44 | 3.6 | 6.5 | None | None | None | 1.44 | 3.75 | 6 | 1.44 | 4 | 7.5 | 1.45 | 3.75 | 6.5 | 1.5 | 3.75 | 5.5 | 1.44 | 3.75 | 6.5 | . 5 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492477 | 7947 | 9985 | 1 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 5 | 3.5 | 1.65 | 5 | 3.5 | 1.6 | 4 | 3.3 | 1.7 | 4 | 3.4 | 1.72 | None | None | None | 4.2 | 3.4 | 1.7 | 4.5 | 3.5 | 1.73 | 4.5 | 3.4 | 1.65 | 4.5 | 3.5 | 1.65 | 4.75 | 3.3 | 1.67 | . 6 | 1 | 1 | 2008/2009 | 1 | 2008-09-24 00:00:00 | 492478 | 8203 | 8342 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 4.75 | 3.4 | 1.67 | 4.85 | 3.4 | 1.65 | 3.7 | 3.2 | 1.8 | 5 | 3.25 | 1.62 | None | None | None | 4.2 | 3.4 | 1.7 | 5.5 | 3.75 | 1.67 | 4.35 | 3.4 | 1.7 | 4.5 | 3.4 | 1.7 | None | None | None | . 7 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492479 | 9999 | 8571 | 2 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.1 | 3.2 | 3.3 | 2.05 | 3.25 | 3.15 | 1.85 | 3.2 | 3.5 | 1.83 | 3.3 | 3.6 | None | None | None | 1.83 | 3.3 | 3.6 | 1.91 | 3.4 | 3.6 | 2.1 | 3.25 | 3 | 1.85 | 3.25 | 3.75 | 2.1 | 3.25 | 3.1 | . 8 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492480 | 4049 | 9996 | 1 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 3.2 | 3.4 | 2.2 | 2.55 | 3.3 | 2.4 | 2.4 | 3.2 | 2.4 | 2.5 | 3.2 | 2.5 | None | None | None | 2.7 | 3.25 | 2.25 | 2.6 | 3.4 | 2.4 | 2.8 | 3.25 | 2.25 | 2.8 | 3.2 | 2.25 | 2.88 | 3.25 | 2.2 | . 9 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492481 | 10001 | 9986 | 1 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.25 | 3.25 | 2.88 | 2.3 | 3.25 | 2.7 | 2.1 | 3.1 | 3 | 2.25 | 3.2 | 2.75 | None | None | None | 2.2 | 3.25 | 2.75 | 2.2 | 3.3 | 3.1 | 2.25 | 3.25 | 2.8 | 2.2 | 3.3 | 2.8 | 2.25 | 3.2 | 2.8 | . 10 | 1 | 1 | 2008/2009 | 10 | 2008-11-01 00:00:00 | 492564 | 8342 | 8571 | 4 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.3 | 5.25 | 9.5 | 1.25 | 5 | 10 | 1.3 | 4.2 | 8 | 1.25 | 4.5 | 10 | None | None | None | 1.35 | 4.2 | 7 | 1.27 | 5 | 10 | 1.3 | 4.35 | 8.5 | 1.25 | 5 | 10 | 1.29 | 4.5 | 9 | . %%sql SELECT * FROM Player LIMIT 10; . * sqlite:///database.sqlite Done. . id player_api_id player_name player_fifa_api_id birthday height weight . 1 | 505942 | Aaron Appindangoye | 218353 | 1992-02-29 00:00:00 | 182.88 | 187 | . 2 | 155782 | Aaron Cresswell | 189615 | 1989-12-15 00:00:00 | 170.18 | 146 | . 3 | 162549 | Aaron Doran | 186170 | 1991-05-13 00:00:00 | 170.18 | 163 | . 4 | 30572 | Aaron Galindo | 140161 | 1982-05-08 00:00:00 | 182.88 | 198 | . 5 | 23780 | Aaron Hughes | 17725 | 1979-11-08 00:00:00 | 182.88 | 154 | . 6 | 27316 | Aaron Hunt | 158138 | 1986-09-04 00:00:00 | 182.88 | 161 | . 7 | 564793 | Aaron Kuhl | 221280 | 1996-01-30 00:00:00 | 172.72 | 146 | . 8 | 30895 | Aaron Lennon | 152747 | 1987-04-16 00:00:00 | 165.1 | 139 | . 9 | 528212 | Aaron Lennox | 206592 | 1993-02-19 00:00:00 | 190.5 | 181 | . 10 | 101042 | Aaron Meijers | 188621 | 1987-10-28 00:00:00 | 175.26 | 170 | . %%sql SELECT * FROM Player_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id player_fifa_api_id player_api_id date overall_rating potential preferred_foot attacking_work_rate defensive_work_rate crossing finishing heading_accuracy short_passing volleys dribbling curve free_kick_accuracy long_passing ball_control acceleration sprint_speed agility reactions balance shot_power jumping stamina strength long_shots aggression interceptions positioning vision penalties marking standing_tackle sliding_tackle gk_diving gk_handling gk_kicking gk_positioning gk_reflexes . 1 | 218353 | 505942 | 2016-02-18 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 2 | 218353 | 505942 | 2015-11-19 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 3 | 218353 | 505942 | 2015-09-21 00:00:00 | 62 | 66 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 63 | 41 | 45 | 54 | 48 | 65 | 66 | 69 | 6 | 11 | 10 | 8 | 8 | . 4 | 218353 | 505942 | 2015-03-20 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 5 | 218353 | 505942 | 2007-02-22 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 6 | 189615 | 155782 | 2016-04-21 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 40 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 62 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 7 | 189615 | 155782 | 2016-04-07 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 32 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 60 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 8 | 189615 | 155782 | 2016-01-07 00:00:00 | 73 | 75 | left | high | medium | 79 | 52 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 59 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 9 | 189615 | 155782 | 2015-12-24 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 10 | 189615 | 155782 | 2015-12-17 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . %%sql SELECT * FROM Team LIMIT 10; . * sqlite:///database.sqlite Done. . id team_api_id team_fifa_api_id team_long_name team_short_name . 1 | 9987 | 673 | KRC Genk | GEN | . 2 | 9993 | 675 | Beerschot AC | BAC | . 3 | 10000 | 15005 | SV Zulte-Waregem | ZUL | . 4 | 9994 | 2007 | Sporting Lokeren | LOK | . 5 | 9984 | 1750 | KSV Cercle Brugge | CEB | . 6 | 8635 | 229 | RSC Anderlecht | AND | . 7 | 9991 | 674 | KAA Gent | GEN | . 8 | 9998 | 1747 | RAEC Mons | MON | . 9 | 7947 | None | FCV Dender EH | DEN | . 10 | 9985 | 232 | Standard de Liège | STL | . %%sql SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id team_fifa_api_id team_api_id date buildUpPlaySpeed buildUpPlaySpeedClass buildUpPlayDribbling buildUpPlayDribblingClass buildUpPlayPassing buildUpPlayPassingClass buildUpPlayPositioningClass chanceCreationPassing chanceCreationPassingClass chanceCreationCrossing chanceCreationCrossingClass chanceCreationShooting chanceCreationShootingClass chanceCreationPositioningClass defencePressure defencePressureClass defenceAggression defenceAggressionClass defenceTeamWidth defenceTeamWidthClass defenceDefenderLineClass . 1 | 434 | 9930 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 50 | Mixed | Organised | 60 | Normal | 65 | Normal | 55 | Normal | Organised | 50 | Medium | 55 | Press | 45 | Normal | Cover | . 2 | 434 | 9930 | 2014-09-19 00:00:00 | 52 | Balanced | 48 | Normal | 56 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 3 | 434 | 9930 | 2015-09-10 00:00:00 | 47 | Balanced | 41 | Normal | 54 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 4 | 77 | 8485 | 2010-02-22 00:00:00 | 70 | Fast | None | Little | 70 | Long | Organised | 70 | Risky | 70 | Lots | 70 | Lots | Organised | 60 | Medium | 70 | Double | 70 | Wide | Cover | . 5 | 77 | 8485 | 2011-02-22 00:00:00 | 47 | Balanced | None | Little | 52 | Mixed | Organised | 53 | Normal | 48 | Normal | 52 | Normal | Organised | 47 | Medium | 47 | Press | 52 | Normal | Cover | . 6 | 77 | 8485 | 2012-02-22 00:00:00 | 58 | Balanced | None | Little | 62 | Mixed | Organised | 45 | Normal | 70 | Lots | 55 | Normal | Organised | 40 | Medium | 40 | Press | 60 | Normal | Cover | . 7 | 77 | 8485 | 2013-09-20 00:00:00 | 62 | Balanced | None | Little | 45 | Mixed | Organised | 40 | Normal | 50 | Normal | 55 | Normal | Organised | 42 | Medium | 42 | Press | 60 | Normal | Cover | . 8 | 77 | 8485 | 2014-09-19 00:00:00 | 58 | Balanced | 64 | Normal | 62 | Mixed | Organised | 56 | Normal | 68 | Lots | 57 | Normal | Organised | 41 | Medium | 42 | Press | 60 | Normal | Cover | . 9 | 77 | 8485 | 2015-09-10 00:00:00 | 59 | Balanced | 64 | Normal | 53 | Mixed | Organised | 51 | Normal | 72 | Lots | 63 | Normal | Free Form | 49 | Medium | 45 | Press | 63 | Normal | Cover | . 10 | 614 | 8576 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 40 | Mixed | Organised | 45 | Normal | 35 | Normal | 55 | Normal | Organised | 30 | Deep | 70 | Double | 30 | Narrow | Offside Trap | . %%sql CREATE TABLE Team_table AS SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . [] . %%sql DROP TABLE IF EXISTS Team_table . * sqlite:///database.sqlite Done. . [] . sql_query = %sql SELECT * FROM Team LIMIT 10 df = sql_query.DataFrame() . * sqlite:///database.sqlite Done. . df . id team_api_id team_fifa_api_id team_long_name team_short_name . 0 1 | 9987 | 673.0 | KRC Genk | GEN | . 1 2 | 9993 | 675.0 | Beerschot AC | BAC | . 2 3 | 10000 | 15005.0 | SV Zulte-Waregem | ZUL | . 3 4 | 9994 | 2007.0 | Sporting Lokeren | LOK | . 4 5 | 9984 | 1750.0 | KSV Cercle Brugge | CEB | . 5 6 | 8635 | 229.0 | RSC Anderlecht | AND | . 6 7 | 9991 | 674.0 | KAA Gent | GEN | . 7 8 | 9998 | 1747.0 | RAEC Mons | MON | . 8 9 | 7947 | NaN | FCV Dender EH | DEN | . 9 10 | 9985 | 232.0 | Standard de Liège | STL | . import matplotlib.pyplot as plt plt.figure(figsize=(20,7)) plot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name plot.bar(); . * sqlite:///database.sqlite Done. . plot.pie(); . type(plot) . sql.run.ResultSet .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/26/Working_with_sqlite3_dbs_in_jupyter_Soccer_Pred.html",
            "relUrl": "/2020/10/26/Working_with_sqlite3_dbs_in_jupyter_Soccer_Pred.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader import datetime import pandas_datareader.data as web import statsmodels.api as sm import quandl . start = datetime.datetime(1960, 1, 1) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . ... ... | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . 740 rows × 1 columns . df = SP500 . df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.tail() . Value . Date . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . Clean Up . Let&#39;s clean this up just a little! . df.columns = [&#39;Value&#39;] df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.describe().transpose() . count mean std min 25% 50% 75% max . Value 740.0 | 761.732932 | 834.566138 | 53.73 | 100.9 | 349.425 | 1239.415 | 3526.65 | . Step 2: Visualize the Data . Let&#39;s visualize this data with a few methods. . df.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8873251b38&gt; . timeseries = df[&#39;Value&#39;] . timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.rolling(12).std().plot(label=&#39;12 Month Rolling Std&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f88731e22e8&gt; . timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f887315f630&gt; . Decomposition . ETS decomposition allows us to see the individual parts! . from statsmodels.tsa.seasonal import seasonal_decompose decomposition = seasonal_decompose(df[&#39;Value&#39;], freq=12) fig = plt.figure() fig = decomposition.plot() fig.set_size_inches(15, 8) . &lt;Figure size 432x288 with 0 Axes&gt; . Testing for Stationarity . df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . from statsmodels.tsa.stattools import adfuller . result = adfuller(df[&#39;Value&#39;]) . print(&#39;Augmented Dickey-Fuller Test:&#39;) labels = [&#39;ADF Test Statistic&#39;,&#39;p-value&#39;,&#39;#Lags Used&#39;,&#39;Number of Observations Used&#39;] for value,label in zip(result,labels): print(label+&#39; : &#39;+str(value) ) if result[1] &lt;= 0.05: print(&quot;strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary&quot;) else: print(&quot;weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary &quot;) . Augmented Dickey-Fuller Test: ADF Test Statistic : 1.7247353245135 p-value : 0.9981874531215522 #Lags Used : 20 Number of Observations Used : 719 weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary . def adf_check(time_series): &quot;&quot;&quot; Pass in a time series, returns ADF report &quot;&quot;&quot; result = adfuller(time_series) print(&#39;Augmented Dickey-Fuller Test:&#39;) labels = [&#39;ADF Test Statistic&#39;,&#39;p-value&#39;,&#39;#Lags Used&#39;,&#39;Number of Observations Used&#39;] for value,label in zip(result,labels): print(label+&#39; : &#39;+str(value) ) if result[1] &lt;= 0.05: print(&quot;strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary&quot;) else: print(&quot;weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary &quot;) . First Difference . df[&#39;Value First Difference&#39;] = df[&#39;Value&#39;] - df[&#39;Value&#39;].shift(1) . adf_check(df[&#39;Value First Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -4.267790128581322 p-value : 0.0005048563860225925 #Lags Used : 20 Number of Observations Used : 718 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . df[&#39;Value First Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f886d63feb8&gt; . Second Difference . df[&#39;Value Second Difference&#39;] = df[&#39;Value First Difference&#39;] - df[&#39;Value First Difference&#39;].shift(1) . adf_check(df[&#39;Value Second Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -12.29955077642857 p-value : 7.504260735615441e-23 #Lags Used : 18 Number of Observations Used : 719 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . df[&#39;Value Second Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8877f726d8&gt; . Seasonal Difference . df[&#39;Seasonal Difference&#39;] = df[&#39;Value&#39;] - df[&#39;Value&#39;].shift(12) df[&#39;Seasonal Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8877fb79e8&gt; . adf_check(df[&#39;Seasonal Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -5.239903673260254 p-value : 7.284266188346342e-06 #Lags Used : 20 Number of Observations Used : 707 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . Seasonal First Difference . df[&#39;Seasonal First Difference&#39;] = df[&#39;Value First Difference&#39;] - df[&#39;Value First Difference&#39;].shift(12) df[&#39;Seasonal First Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f88acb0c2b0&gt; . adf_check(df[&#39;Seasonal First Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -6.196739887980032 p-value : 5.940155101037563e-08 #Lags Used : 20 Number of Observations Used : 706 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . from statsmodels.graphics.tsaplots import plot_acf,plot_pacf . # Check out: https://stackoverflow.com/questions/21788593/statsmodels-duplicate-charts # https://github.com/statsmodels/statsmodels/issues/1265 fig_first = plot_acf(df[&quot;Value First Difference&quot;].dropna()) . fig_seasonal_first = plot_acf(df[&quot;Seasonal First Difference&quot;].dropna()) . Pandas also has this functionality built in, but only for ACF, not PACF. So I recommend using statsmodels, as ACF and PACF is more core to its functionality than it is to pandas&#39; functionality. . from pandas.plotting import autocorrelation_plot autocorrelation_plot(df[&#39;Seasonal First Difference&#39;].dropna()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f886d660358&gt; . We can then plot this relationship: . result = plot_pacf(df[&quot;Seasonal First Difference&quot;].dropna()) . fig = plt.figure(figsize=(12,8)) ax1 = fig.add_subplot(211) fig = sm.graphics.tsa.plot_acf(df[&#39;Seasonal First Difference&#39;].iloc[13:], lags=40, ax=ax1) ax2 = fig.add_subplot(212) fig = sm.graphics.tsa.plot_pacf(df[&#39;Seasonal First Difference&#39;].iloc[13:], lags=40, ax=ax2) . from statsmodels.tsa.arima_model import ARIMA . help(ARIMA) . Help on class ARIMA in module statsmodels.tsa.arima_model: class ARIMA(ARMA) | ARIMA(endog, order, exog=None, dates=None, freq=None, missing=&#39;none&#39;) | | Autoregressive Integrated Moving Average ARIMA(p,d,q) Model | | Parameters | - | endog : array-like | The endogenous variable. | order : iterable | The (p,d,q) order of the model for the number of AR parameters, | differences, and MA parameters to use. | exog : array-like, optional | An optional array of exogenous variables. This should *not* include a | constant or trend. You can specify this in the `fit` method. | dates : array-like of datetime, optional | An array-like object of datetime objects. If a pandas object is given | for endog or exog, it is assumed to have a DateIndex. | freq : str, optional | The frequency of the time-series. A Pandas offset or &#39;B&#39;, &#39;D&#39;, &#39;W&#39;, | &#39;M&#39;, &#39;A&#39;, or &#39;Q&#39;. This is optional if dates are given. | | | Notes | -- | If exogenous variables are given, then the model that is fit is | | .. math:: | | phi(L)(y_t - X_t beta) = theta(L) epsilon_t | | where :math:` phi` and :math:` theta` are polynomials in the lag | operator, :math:`L`. This is the regression model with ARMA errors, | or ARMAX model. This specification is used, whether or not the model | is fit using conditional sum of square or maximum-likelihood, using | the `method` argument in | :meth:`statsmodels.tsa.arima_model.ARIMA.fit`. Therefore, for | now, `css` and `mle` refer to estimation methods only. This may | change for the case of the `css` model in future versions. | | Method resolution order: | ARIMA | ARMA | statsmodels.tsa.base.tsa_model.TimeSeriesModel | statsmodels.base.model.LikelihoodModel | statsmodels.base.model.Model | builtins.object | | Methods defined here: | | __getnewargs__(self) | | __init__(self, endog, order, exog=None, dates=None, freq=None, missing=&#39;none&#39;) | Initialize self. See help(type(self)) for accurate signature. | | fit(self, start_params=None, trend=&#39;c&#39;, method=&#39;css-mle&#39;, transparams=True, solver=&#39;lbfgs&#39;, maxiter=500, full_output=1, disp=5, callback=None, start_ar_lags=None, **kwargs) | Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter. | | Parameters | - | start_params : array-like, optional | Starting parameters for ARMA(p,q). If None, the default is given | by ARMA._fit_start_params. See there for more information. | transparams : bool, optional | Whehter or not to transform the parameters to ensure stationarity. | Uses the transformation suggested in Jones (1980). If False, | no checking for stationarity or invertibility is done. | method : str {&#39;css-mle&#39;,&#39;mle&#39;,&#39;css&#39;} | This is the loglikelihood to maximize. If &#34;css-mle&#34;, the | conditional sum of squares likelihood is maximized and its values | are used as starting values for the computation of the exact | likelihood via the Kalman filter. If &#34;mle&#34;, the exact likelihood | is maximized via the Kalman Filter. If &#34;css&#34; the conditional sum | of squares likelihood is maximized. All three methods use | `start_params` as starting parameters. See above for more | information. | trend : str {&#39;c&#39;,&#39;nc&#39;} | Whether to include a constant or not. &#39;c&#39; includes constant, | &#39;nc&#39; no constant. | solver : str or None, optional | Solver to be used. The default is &#39;lbfgs&#39; (limited memory | Broyden-Fletcher-Goldfarb-Shanno). Other choices are &#39;bfgs&#39;, | &#39;newton&#39; (Newton-Raphson), &#39;nm&#39; (Nelder-Mead), &#39;cg&#39; - | (conjugate gradient), &#39;ncg&#39; (non-conjugate gradient), and | &#39;powell&#39;. By default, the limited memory BFGS uses m=12 to | approximate the Hessian, projected gradient tolerance of 1e-8 and | factr = 1e2. You can change these by using kwargs. | maxiter : int, optional | The maximum number of function evaluations. Default is 500. | tol : float | The convergence tolerance. Default is 1e-08. | full_output : bool, optional | If True, all output from solver will be available in | the Results object&#39;s mle_retvals attribute. Output is dependent | on the solver. See Notes for more information. | disp : int, optional | If True, convergence information is printed. For the default | l_bfgs_b solver, disp controls the frequency of the output during | the iterations. disp &lt; 0 means no output in this case. | callback : function, optional | Called after each iteration as callback(xk) where xk is the current | parameter vector. | start_ar_lags : int, optional | Parameter for fitting start_params. When fitting start_params, | residuals are obtained from an AR fit, then an ARMA(p,q) model is | fit via OLS using these residuals. If start_ar_lags is None, fit | an AR process according to best BIC. If start_ar_lags is not None, | fits an AR process with a lag length equal to start_ar_lags. | See ARMA._fit_start_params_hr for more information. | kwargs | See Notes for keyword arguments that can be passed to fit. | | Returns | - | `statsmodels.tsa.arima.ARIMAResults` class | | See Also | -- | statsmodels.base.model.LikelihoodModel.fit : for more information | on using the solvers. | ARIMAResults : results class returned by fit | | Notes | -- | If fit by &#39;mle&#39;, it is assumed for the Kalman Filter that the initial | unknown state is zero, and that the initial variance is | P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel(&#39;F&#39;)).reshape(r, | r, order = &#39;F&#39;) | | predict(self, params, start=None, end=None, exog=None, typ=&#39;linear&#39;, dynamic=False) | ARIMA model in-sample and out-of-sample prediction | | Parameters | - | params : array-like | The fitted parameters of the model. | start : int, str, or datetime | Zero-indexed observation number at which to start forecasting, ie., | the first forecast is start. Can also be a date string to | parse or a datetime type. | end : int, str, or datetime | Zero-indexed observation number at which to end forecasting, ie., | the first forecast is start. Can also be a date string to | parse or a datetime type. However, if the dates index does not | have a fixed frequency, end must be an integer index if you | want out of sample prediction. | exog : array-like, optional | If the model is an ARMAX and out-of-sample forecasting is | requested, exog must be given. Note that you&#39;ll need to pass | `k_ar` additional lags for any exogenous variables. E.g., if you | fit an ARMAX(2, q) model and want to predict 5 steps, you need 7 | observations to do this. | dynamic : bool, optional | The `dynamic` keyword affects in-sample prediction. If dynamic | is False, then the in-sample lagged values are used for | prediction. If `dynamic` is True, then in-sample forecasts are | used in place of lagged dependent variables. The first forecasted | value is `start`. | typ : str {&#39;linear&#39;, &#39;levels&#39;} | | - &#39;linear&#39; : Linear prediction in terms of the differenced | endogenous variables. | - &#39;levels&#39; : Predict the levels of the original endogenous | variables. | | | Returns | - | predict : array | The predicted values. | | | | Notes | -- | Use the results predict method instead. | | - | Static methods defined here: | | __new__(cls, endog, order, exog=None, dates=None, freq=None, missing=&#39;none&#39;) | Create and return a new object. See help(type) for accurate signature. | | - | Methods inherited from ARMA: | | geterrors(self, params) | Get the errors of the ARMA process. | | Parameters | - | params : array-like | The fitted ARMA parameters | order : array-like | 3 item iterable, with the number of AR, MA, and exogenous | parameters, including the trend | | hessian(self, params) | Compute the Hessian at params, | | Notes | -- | This is a numerical approximation. | | loglike(self, params, set_sigma2=True) | Compute the log-likelihood for ARMA(p,q) model | | Notes | -- | Likelihood used depends on the method set in fit | | loglike_css(self, params, set_sigma2=True) | Conditional Sum of Squares likelihood function. | | loglike_kalman(self, params, set_sigma2=True) | Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter. | | score(self, params) | Compute the score function at params. | | Notes | -- | This is a numerical approximation. | | - | Class methods inherited from ARMA: | | from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type | Create a Model from a formula and dataframe. | | Parameters | - | formula : str or generic Formula object | The formula specifying the model | data : array-like | The data for the model. See Notes. | subset : array-like | An array-like object of booleans, integers, or index values that | indicate the subset of df to use in the model. Assumes df is a | `pandas.DataFrame` | drop_cols : array-like | Columns to drop from the design matrix. Cannot be used to | drop terms involving categoricals. | args : extra arguments | These are passed to the model | kwargs : extra keyword arguments | These are passed to the model with one exception. The | ``eval_env`` keyword is passed to patsy. It can be either a | :class:`patsy:patsy.EvalEnvironment` object or an integer | indicating the depth of the namespace to use. For example, the | default ``eval_env=0`` uses the calling namespace. If you wish | to use a &#34;clean&#34; environment set ``eval_env=-1``. | | Returns | - | model : Model instance | | Notes | -- | data must define __getitem__ with the keys in the formula terms | args and kwargs are passed on to the model instantiation. E.g., | a numpy structured or rec array, a dictionary, or a pandas DataFrame. | | - | Data descriptors inherited from statsmodels.tsa.base.tsa_model.TimeSeriesModel: | | exog_names | | - | Methods inherited from statsmodels.base.model.LikelihoodModel: | | information(self, params) | Fisher information matrix of model | | Returns -Hessian of loglike evaluated at params. | | initialize(self) | Initialize (possibly re-initialize) a Model instance. For | instance, the design matrix of a linear model may change | and some things must be recomputed. | | - | Data descriptors inherited from statsmodels.base.model.Model: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | endog_names | Names of endogenous variables . model = sm.tsa.statespace.SARIMAX(df[&#39;Value&#39;],order=(0,1,0), seasonal_order=(1,1,1,12)) results = model.fit() print(results.summary()) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:219: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting. &#39; ignored when e.g. forecasting.&#39;, ValueWarning) . Statespace Model Results ========================================================================================== Dep. Variable: Value No. Observations: 740 Model: SARIMAX(0, 1, 0)x(1, 1, 1, 12) Log Likelihood -3719.108 Date: Fri, 23 Oct 2020 AIC 7444.215 Time: 09:03:22 BIC 7457.982 Sample: 0 HQIC 7449.528 - 740 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.S.L12 0.0043 0.021 0.201 0.840 -0.037 0.046 ma.S.L12 -0.9513 0.018 -53.297 0.000 -0.986 -0.916 sigma2 1563.8216 30.663 51.001 0.000 1503.724 1623.919 =================================================================================== Ljung-Box (Q): 116.83 Jarque-Bera (JB): 9251.13 Prob(Q): 0.00 Prob(JB): 0.00 Heteroskedasticity (H): 410.23 Skew: -1.86 Prob(H) (two-sided): 0.00 Kurtosis: 20.08 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . results.resid.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872daf940&gt; . results.resid.plot(kind=&#39;kde&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872d863c8&gt; . Prediction of Future Values . Firts we can get an idea of how well our model performs by just predicting for values that we actually already know: . df[&#39;forecast&#39;] = results.predict(start = 1, end= 720, dynamic= True) df[[&#39;Value&#39;,&#39;forecast&#39;]].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f88ace0d7b8&gt; . Forecasting . df.tail() . Value Value First Difference Value Second Difference Seasonal Difference Seasonal First Difference forecast . Date . 2020-08-01 3391.71 | 120.59 | 57.09 | 166.67 | 173.75 | NaN | . 2020-08-31 3500.31 | 108.60 | -11.99 | 223.00 | 56.33 | NaN | . 2020-09-01 3526.65 | 26.34 | -82.26 | 571.84 | 348.84 | NaN | . 2020-09-30 3363.00 | -163.65 | -189.99 | 710.61 | 138.77 | NaN | . 2020-10-01 3380.80 | 17.80 | 181.45 | 796.21 | 85.60 | NaN | . from pandas.tseries.offsets import DateOffset . future_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ] . future_dates . [Timestamp(&#39;2020-10-01 00:00:00&#39;), Timestamp(&#39;2020-11-01 00:00:00&#39;), Timestamp(&#39;2020-12-01 00:00:00&#39;), Timestamp(&#39;2021-01-01 00:00:00&#39;), Timestamp(&#39;2021-02-01 00:00:00&#39;), Timestamp(&#39;2021-03-01 00:00:00&#39;), Timestamp(&#39;2021-04-01 00:00:00&#39;), Timestamp(&#39;2021-05-01 00:00:00&#39;), Timestamp(&#39;2021-06-01 00:00:00&#39;), Timestamp(&#39;2021-07-01 00:00:00&#39;), Timestamp(&#39;2021-08-01 00:00:00&#39;), Timestamp(&#39;2021-09-01 00:00:00&#39;), Timestamp(&#39;2021-10-01 00:00:00&#39;), Timestamp(&#39;2021-11-01 00:00:00&#39;), Timestamp(&#39;2021-12-01 00:00:00&#39;), Timestamp(&#39;2022-01-01 00:00:00&#39;), Timestamp(&#39;2022-02-01 00:00:00&#39;), Timestamp(&#39;2022-03-01 00:00:00&#39;), Timestamp(&#39;2022-04-01 00:00:00&#39;), Timestamp(&#39;2022-05-01 00:00:00&#39;), Timestamp(&#39;2022-06-01 00:00:00&#39;), Timestamp(&#39;2022-07-01 00:00:00&#39;), Timestamp(&#39;2022-08-01 00:00:00&#39;), Timestamp(&#39;2022-09-01 00:00:00&#39;)] . future_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns) . future_df = pd.concat([df,future_dates_df]) . future_df.head() . Value Value First Difference Value Second Difference Seasonal Difference Seasonal First Difference forecast . 1960-01-01 58.03 | NaN | NaN | NaN | NaN | NaN | . 1960-02-01 55.78 | -2.25 | NaN | NaN | NaN | 58.03 | . 1960-03-01 55.02 | -0.76 | 1.49 | NaN | NaN | 58.03 | . 1960-04-01 55.73 | 0.71 | 1.47 | NaN | NaN | 58.03 | . 1960-05-01 55.22 | -0.51 | -1.22 | NaN | NaN | 58.03 | . future_df.tail() . Value Value First Difference Value Second Difference Seasonal Difference Seasonal First Difference forecast . 2022-05-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-06-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-07-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-08-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-09-01 NaN | NaN | NaN | NaN | NaN | NaN | . future_df[&#39;forecast&#39;] = results.predict(start = 1, end = 720, dynamic= True) future_df[[&#39;Value&#39;, &#39;forecast&#39;]].plot(figsize=(12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872997fd0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/25/ARIMA-ADF-FORECASTING.html",
            "relUrl": "/2020/10/25/ARIMA-ADF-FORECASTING.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
            "content": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader import datetime import pandas_datareader.data as web import statsmodels.api as sm import quandl . start = datetime.datetime(1960, 1, 1) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . ... ... | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . 740 rows × 1 columns . SP500 . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . ... ... | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . 740 rows × 1 columns . SP500.dropna(inplace=True) SP500.index = pd.to_datetime(SP500.index) . SP500.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . Simple Moving Averages . SP500[&#39;6-month-SMA&#39;]=SP500[&#39;Value&#39;].rolling(window=6).mean() SP500[&#39;12-month-SMA&#39;]=SP500[&#39;Value&#39;].rolling(window=12).mean() . SP500.head() . Value 6-month-SMA 12-month-SMA . Date . 1960-01-01 58.03 | NaN | NaN | . 1960-02-01 55.78 | NaN | NaN | . 1960-03-01 55.02 | NaN | NaN | . 1960-04-01 55.73 | NaN | NaN | . 1960-05-01 55.22 | NaN | NaN | . SP500.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f51bf16ab00&gt; . Exponentially-weighted moving averages . SP500[&#39;EWMA12&#39;] = SP500[&#39;Value&#39;].ewm(span=12).mean() . SP500[[&#39;Value&#39;,&#39;EWMA12&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f51bf0608d0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/24/SNP-SMA-EWMA.html",
            "relUrl": "/2020/10/24/SNP-SMA-EWMA.html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Stock Market Analysis of the S&P 500 Index",
            "content": "This post includes code and notes adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader import datetime import pandas_datareader.data as web . import statsmodels.api as sm import quandl . start = datetime.datetime(1960, 1, 1) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . ... ... | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . 740 rows × 1 columns . SP500.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f36bb9a7208&gt; . SP500.index . DatetimeIndex([&#39;1960-01-01&#39;, &#39;1960-02-01&#39;, &#39;1960-03-01&#39;, &#39;1960-04-01&#39;, &#39;1960-05-01&#39;, &#39;1960-06-01&#39;, &#39;1960-07-01&#39;, &#39;1960-08-01&#39;, &#39;1960-09-01&#39;, &#39;1960-10-01&#39;, ... &#39;2020-05-01&#39;, &#39;2020-06-01&#39;, &#39;2020-06-30&#39;, &#39;2020-07-01&#39;, &#39;2020-07-31&#39;, &#39;2020-08-01&#39;, &#39;2020-08-31&#39;, &#39;2020-09-01&#39;, &#39;2020-09-30&#39;, &#39;2020-10-01&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, length=740, freq=None) . SP500.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . SP500[&#39;Value&#39;].plot() plt.ylabel(&quot;SP500 Value&quot;) . Text(0, 0.5, &#39;SP500 Value&#39;) . Getting at the trend by removing the cyclical elements of the S&amp;P 500 . The Hodrick–Prescott filter can be used to &#39;remove the cyclical component of a time series from raw data&#39;. It takes from a time-series y_t, provides a trend τ_t and separates out the cyclical component ζt: $y_t = tau_t + zeta_t$ These are found by minimizing the quadratic loss function of: $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$. . # Tuple unpacking SP500_cycle, SP500_trend = sm.tsa.filters.hpfilter(SP500.Value) . SP500_cycle . Date 1960-01-01 3.642040 1960-02-01 0.893330 1960-03-01 -0.367657 1960-04-01 -0.163754 1960-05-01 -1.187567 ... 2020-08-01 134.761963 2020-08-31 213.461317 2020-09-01 209.742540 2020-09-30 16.019072 2020-10-01 3.743264 Name: Value, Length: 740, dtype: float64 . type(SP500_cycle) . pandas.core.series.Series . SP500[&quot;trend&quot;] = SP500_trend . SP500[[&#39;trend&#39;,&#39;Value&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f36902e1898&gt; . SP500[[&#39;trend&#39;,&#39;Value&#39;]][&quot;2000-03-31&quot;:].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3690252470&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/23/SNP-trend-minus-cycle.html",
            "relUrl": "/2020/10/23/SNP-trend-minus-cycle.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Machine Learning in Healthcare notes",
            "content": "Machine Learning for Healthcare . +Machine Learning for Healthcare . Mycin system from the 1970s: ‘MYCIN was an early backward chaining expert system that used artificial intelligence to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for patient’s body weight — the name derived from the antibiotics themselves, as many antibiotics have the suffix “-mycin”. The’ . | Internist-I: For primary care. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/10/23/ML-for-HC-notes.html",
            "relUrl": "/data%20science%20content/2020/10/23/ML-for-HC-notes.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Stock Market Analysis of Microsoft, Zoom, and Snowflake",
            "content": "Daily Return and Cumulative Returns . Cumulative Return: $ i_i = (1+r_t) * i_{t-1} $ | . This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . import pandas_datareader import datetime . import pandas_datareader.data as web . start = datetime.datetime(2019, 1, 1) end = datetime.datetime(2021, 1, 1) . MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . . Recreate this linear plot of all the stocks&#39; Open price ! Hint: For the legend, use label parameter and plt.legend() . # Code Here . MSFT_stock[&#39;Open&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8),title=&#39;Open Price&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;ZOOM_stock&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;SNOW_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f5545f36dd8&gt; . . MSFT_stock[&#39;Volume&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8),title=&#39;Volume Traded&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;ZOOM_stock&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;SNOW_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f55463855c0&gt; . MSFT_stock[&#39;Volume&#39;].argmax() . 291 . MSFT_stock[&#39;Total Traded&#39;] = MSFT_stock[&#39;Open&#39;]*MSFT_stock[&#39;Volume&#39;] SNOW_stock[&#39;Total Traded&#39;] = SNOW_stock[&#39;Open&#39;]*SNOW_stock[&#39;Volume&#39;] ZOOM_stock[&#39;Total Traded&#39;] = ZOOM_stock[&#39;Open&#39;]*ZOOM_stock[&#39;Volume&#39;] . MSFT_stock[&#39;Total Traded&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8)) ZOOM_stock[&#39;Total Traded&#39;].plot(label=&#39;ZOOM_stock&#39;) SNOW_stock[&#39;Total Traded&#39;].plot(label=&#39;SNOW_stock&#39;) plt.legend() plt.ylabel(&#39;Total Traded&#39;) . Text(0, 0.5, &#39;Total Traded&#39;) . ZOOM_stock[&#39;Total Traded&#39;].argmax() . 346 . MA (Moving Averages) . ZOOM_stock[&#39;MA50&#39;] = ZOOM_stock[&#39;Open&#39;].rolling(50).mean() ZOOM_stock[&#39;MA200&#39;] = ZOOM_stock[&#39;Open&#39;].rolling(200).mean() ZOOM_stock[[&#39;Open&#39;,&#39;MA50&#39;,&#39;MA200&#39;]].plot(label=&#39;ZOOM_stock&#39;,figsize=(16,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5545f034e0&gt; . from pandas.plotting import scatter_matrix car_comp = pd.concat([MSFT_stock[&#39;Open&#39;],ZOOM_stock[&#39;Open&#39;],SNOW_stock[&#39;Open&#39;]],axis=1) car_comp.columns = [&#39;MSFT_stock Open&#39;,&#39;ZOOM_stock Open&#39;,&#39;SNOW_stock Open&#39;] . # You can use a semi-colon to remove the axes print outs scatter_matrix(car_comp,figsize=(8,8),alpha=0.2,hist_kwds={&#39;bins&#39;:50}); . from mpl_finance import candlestick_ohlc from matplotlib.dates import DateFormatter, date2num, WeekdayLocator, DayLocator, MONDAY . # Rest the index to get a column of January Dates MSFT_stock_reset = MSFT_stock.loc[&#39;2019-01&#39;:&#39;2019-01&#39;].reset_index() . # Create a new column of numerical &quot;date&quot; values for matplotlib to use MSFT_stock_reset[&#39;date_ax&#39;] = MSFT_stock_reset[&#39;Date&#39;].apply(lambda date: date2num(date)) MSFT_stock_values = [tuple(vals) for vals in MSFT_stock_reset[[&#39;date_ax&#39;, &#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]].values] . mondays = WeekdayLocator(MONDAY) # major ticks on the mondays alldays = DayLocator() # minor ticks on the days weekFormatter = DateFormatter(&#39;%b %d&#39;) # e.g., Jan 12 dayFormatter = DateFormatter(&#39;%d&#39;) # e.g., 12 . fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) ax.xaxis.set_major_locator(mondays) ax.xaxis.set_minor_locator(alldays) ax.xaxis.set_major_formatter(weekFormatter) candlestick_ohlc(ax, MSFT_stock_values, width=0.6, colorup=&#39;g&#39;,colordown=&#39;r&#39;); . # Method 1: Using shift MSFT_stock[&#39;returns&#39;] = (MSFT_stock[&#39;Close&#39;] / MSFT_stock[&#39;Close&#39;].shift(1) ) - 1 MSFT_stock.head() MSFT_stock[&#39;returns&#39;] = MSFT_stock[&#39;Close&#39;].pct_change(1) MSFT_stock.head() . High Low Open Close Volume Adj Close Total Traded returns . Date . 2019-01-02 101.750000 | 98.940002 | 99.550003 | 101.120003 | 35329300.0 | 98.860214 | 3.517032e+09 | NaN | . 2019-01-03 100.190002 | 97.199997 | 100.099998 | 97.400002 | 42579100.0 | 95.223351 | 4.262168e+09 | -0.036788 | . 2019-01-04 102.510002 | 98.930000 | 99.720001 | 101.930000 | 44060600.0 | 99.652115 | 4.393723e+09 | 0.046509 | . 2019-01-07 103.269997 | 100.980003 | 101.639999 | 102.059998 | 35656100.0 | 99.779205 | 3.624086e+09 | 0.001275 | . 2019-01-08 103.970001 | 101.709999 | 103.040001 | 102.800003 | 31514400.0 | 100.502670 | 3.247244e+09 | 0.007251 | . SNOW_stock[&#39;returns&#39;] = SNOW_stock[&#39;Close&#39;].pct_change(1) ZOOM_stock[&#39;returns&#39;] = ZOOM_stock[&#39;Close&#39;].pct_change(1) . SNOW_stock.head() . High Low Open Close Volume Adj Close Total Traded returns . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | 8.844426e+09 | NaN | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | 2.747775e+09 | -0.103926 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | 1.756719e+09 | 0.054760 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | 1.270727e+09 | -0.046458 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | 9.275504e+08 | 0.027573 | . ZOOM_stock.head() . High Low Open Close Volume Adj Close Total Traded MA50 MA200 returns . Date . 2019-04-18 66.000000 | 60.320999 | 65.000000 | 62.000000 | 25764700 | 62.000000 | 1.674706e+09 | NaN | NaN | NaN | . 2019-04-22 68.900002 | 59.939999 | 61.000000 | 65.699997 | 9949700 | 65.699997 | 6.069317e+08 | NaN | NaN | 0.059677 | . 2019-04-23 74.168999 | 65.550003 | 66.870003 | 69.000000 | 6786500 | 69.000000 | 4.538133e+08 | NaN | NaN | 0.050228 | . 2019-04-24 71.500000 | 63.160000 | 71.400002 | 63.200001 | 4973500 | 63.200001 | 3.551079e+08 | NaN | NaN | -0.084058 | . 2019-04-25 66.849998 | 62.599998 | 64.739998 | 65.000000 | 3863300 | 65.000000 | 2.501100e+08 | NaN | NaN | 0.028481 | . SNOW_stock[&#39;returns&#39;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5545c0de10&gt; . MSFT_stock[&#39;returns&#39;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5545d664a8&gt; . ZOOM_stock[&#39;returns&#39;].hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5545e78080&gt; . MSFT_stock[&#39;returns&#39;].hist(bins=100,label=&#39;MSFT_stock&#39;,figsize=(10,8),alpha=0.5) ZOOM_stock[&#39;returns&#39;].hist(bins=100,label=&#39;ZOOM_stock&#39;,alpha=0.5) SNOW_stock[&#39;returns&#39;].hist(bins=100,label=&#39;SNOW_stock&#39;,alpha=0.5) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f5546496eb8&gt; . MSFT_stock[&#39;returns&#39;].plot(kind=&#39;kde&#39;,label=&#39;MSFT_stock&#39;,figsize=(12,6)) ZOOM_stock[&#39;returns&#39;].plot(kind=&#39;kde&#39;,label=&#39;ZOOM_stock&#39;) SNOW_stock[&#39;returns&#39;].plot(kind=&#39;kde&#39;,label=&#39;SNOW_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f5545ccd470&gt; . box_df = pd.concat([MSFT_stock[&#39;returns&#39;],ZOOM_stock[&#39;returns&#39;],SNOW_stock[&#39;returns&#39;]],axis=1) box_df.columns = [&#39;MSFT_stock Returns&#39;,&#39; ZOOM_stock Returns&#39;,&#39;SNOW_stock Returns&#39;] box_df.plot(kind=&#39;box&#39;,figsize=(8,11),colormap=&#39;jet&#39;) . /home/gao/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5545ce25c0&gt; . scatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={&#39;bins&#39;:50}); . scatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={&#39;bins&#39;:50}); . box_df.plot(kind=&#39;scatter&#39;,x=&#39; ZOOM_stock Returns&#39;,y=&#39;MSFT_stock Returns&#39;,alpha=0.4,figsize=(10,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f55453f23c8&gt; . Daily Return and Cumulative Return . Cumulative Return: $ i_i = (1+r_t) * i_{t-1} $ . MSFT_stock[&#39;Cumulative Return&#39;] = (1 + MSFT_stock[&#39;returns&#39;]).cumprod() MSFT_stock.head() . High Low Open Close Volume Adj Close Total Traded returns Cumulative Return . Date . 2019-01-02 101.750000 | 98.940002 | 99.550003 | 101.120003 | 35329300.0 | 98.860214 | 3.517032e+09 | NaN | NaN | . 2019-01-03 100.190002 | 97.199997 | 100.099998 | 97.400002 | 42579100.0 | 95.223351 | 4.262168e+09 | -0.036788 | 0.963212 | . 2019-01-04 102.510002 | 98.930000 | 99.720001 | 101.930000 | 44060600.0 | 99.652115 | 4.393723e+09 | 0.046509 | 1.008010 | . 2019-01-07 103.269997 | 100.980003 | 101.639999 | 102.059998 | 35656100.0 | 99.779205 | 3.624086e+09 | 0.001275 | 1.009296 | . 2019-01-08 103.970001 | 101.709999 | 103.040001 | 102.800003 | 31514400.0 | 100.502670 | 3.247244e+09 | 0.007251 | 1.016614 | . SNOW_stock[&#39;Cumulative Return&#39;] = (1 + SNOW_stock[&#39;returns&#39;]).cumprod() ZOOM_stock[&#39;Cumulative Return&#39;] = (1 + ZOOM_stock[&#39;returns&#39;]).cumprod() . MSFT_stock[&#39;Cumulative Return&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8),title=&#39;Cumulative Return&#39;) SNOW_stock[&#39;Cumulative Return&#39;].plot(label=&#39;SNOW_stock&#39;) ZOOM_stock[&#39;Cumulative Return&#39;].plot(label=&#39;ZOOM_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f55453d8f28&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/22/stock-market-returns.html",
            "relUrl": "/2020/10/22/stock-market-returns.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Data Science Content",
            "content": "Machine Learning . Pyspark Content . Pyspark Style Guide . | Spark Joins . | . Style Guides . Python Style Guide | . Reinforcement Learning . Reinforcement Learning Project | . Principal Component Analysis (PCA) . Principal Component Analysis (PCA) with Scikit-learn . Data Engineering Resources . Data Engineering Resources | . NLP . BERT Using BERT for Unlabeled Data | | . | . Docker . Using Docker for Data Science | | . Github Roadmaps . VS Code Roadmap | . APIs . News API | . Startups . Startup Financial Modeling | . Content . Explainable Machine Learning . LIME explanations for classification results | . Mapping with Python . Hex Maps for Spatial Data . | Choropleth using the Plot.ly . | . Organization . Organizing Folders with Python . Economics and Data Science . An Economist’s Value in Data Science Covers Econometrics. Economists ‘have a unique set of skills that allows them to apply statistics in a very similar fashion to data scientists, while also being able to evaluate bias, externalities, and comprehend the math behind algorithms. Additionally, their experience analyzing businesses provide an ability to question and discuss the value of models relative to business goals.’ . Statistics .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/10/22/ds-content.html",
            "relUrl": "/data%20science%20content/2020/10/22/ds-content.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Using the Quandl API and Pandas Datareader API to call Microsoft, Apple, Zoom, Snowflake stocks and other finance data",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader.data as web import datetime . start = datetime.datetime(2020, 1, 1) end = pd.to_datetime(&#39;today&#39;) . AAPL_stock = web.DataReader(&#39;AAPL&#39;, &#39;yahoo&#39;, start, end) AAPL_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) AAPL_stock[&#39;Open&#39;].plot(label=&#39;Apple&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) AAPL_stock[&#39;Volume&#39;].plot(label=&#39;Apple&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f900d632d68&gt; . import pandas_datareader.data as web import datetime gdp = web.DataReader(&quot;GDP&quot;, &quot;fred&quot;, start, end) . gdp.head() . GDP . DATE . 2020-01-01 21561.139 | . 2020-04-01 19520.114 | . import quandl . #quandl.ApiConfig.api_key = &#39;&#39; . mydata = quandl.get(&quot;EIA/PET_RWTC_D&quot;) mydata.head() . Value . Date . 1986-01-02 25.56 | . 1986-01-03 26.00 | . 1986-01-06 26.53 | . 1986-01-07 25.85 | . 1986-01-08 25.87 | . mydata.plot(figsize=(12,6)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f900ffb6a58&gt; . #mydata = quandl.get(&quot;EIA/PET_RWTC_D&quot;, returns=&quot;numpy&quot;,start_date=start,end_date=end) . mydata = quandl.get(&quot;FRED/GDP&quot;,start_date=start,end_date=end) . mydata.head() . Value . Date . 2002-01-01 10788.952 | . 2002-04-01 10893.207 | . 2002-07-01 10992.051 | . 2002-10-01 11071.463 | . 2003-01-01 11183.507 | . mydata = quandl.get([&quot;NSE/OIL.1&quot;, &quot;WIKI/AAPL.4&quot;],start_date=start,end_date=end) . mydata.head() . NSE/OIL - Open WIKI/AAPL - Close . mydata = quandl.get(&quot;FRED/GDP&quot;) . mydata = quandl.get(&#39;WIKI/FB&#39;,start_date=start,end_date=end) . mydata.head() . Open High Low Close Volume Ex-Dividend Split Ratio Adj. Open Adj. High Adj. Low Adj. Close Adj. Volume . Date . mydata = quandl.get(&#39;WIKI/FB.1&#39;,start_date=start,end_date=end) mydata.head() . Open . Date . mydata = quandl.get(&#39;WIKI/FB.7&#39;,start_date=start,end_date=end) mydata.head() . Split Ratio . Date . # Homes . houses = quandl.get(&#39;ZILLOW/M11_ZRIAH&#39;,start_date=start,end_date=end) . houses.head() . Value . Date . 2020-01-31 3342.0 | . 2020-02-29 3358.0 | . houses.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f900d58fef0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/21/quant_data_datareader_quandl.html",
            "relUrl": "/2020/10/21/quant_data_datareader_quandl.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
            "content": "The purpose of the T-test is to compare if there are mean differences between two groups of interest. When we are interested in comparing statistical differences between more than two groups, and we conduct multiple t-tests, we will end up increasing the likelihood of a false positive (type I error) where we are incorrectly rejecting the null hypothesis that there are no statistical differences between groups. One way to address this is to use the Bonferroni correction. The Bonferroni correction, the namesake of Carlo Emilio Bonferroni, accounts for what we lose in a p-hacking quest in the experimentation, which is the justification for taking p-values at face value. By intuition, when we go searching for significant differences everywhere, the chance of seeing an apparent significant difference by chance anywhere increases. Using the Bonferroni correction, if the starting alpha/significance level is .05 and we are testing 10 hypotheses, then the corrected alpha/significance level we should use would be .005. Understanding the lack of an incentive to make such an adjustment is straightforward. Another way to address this is to first use ANOVA to detect statistical differences between all groups before deciding whether to use t tests to look for pairwise comparisons between groups. . T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student&#39;s t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd import seaborn as sns import numpy as np import pandas as pd from statsmodels.stats.power import NormalIndPower, TTestIndPower from scipy.stats import ttest_ind_from_stats import numpy as np import scipy . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;]] df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;] . df=df_subset df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . # Add distributions by region import matplotlib.pyplot as plt #fig, axes = plt.subplots(nrows=3, ncols=3) test_cells = [&#39;East China&#39;, &#39;North China&#39;] metrics = [&#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;] for test_cell in test_cells: for metric in metrics: df.loc[df[&quot;region&quot;] == test_cell].hist(column=[metric], bins=60) print(test_cell) print(metric) . East China gdp East China fdi East China it North China gdp North China fdi North China it . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec7cbdd8&gt;]], dtype=object) . Distributions of Dependant Variables . Right skew . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec6e00f0&gt;]], dtype=object) . sns.distplot(df[&#39;gdp&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec5d6a90&gt; . sns.distplot(df[&#39;fdi&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4a4d30&gt; . sns.distplot(df[&#39;it&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4df278&gt; . sns.distplot(df[&#39;specific&#39;].dropna()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec3e09e8&gt; . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec31ccc0&gt;]], dtype=object) . Removal of GDP value outliers more than 3 standard deviations away from the mean . outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean . import scipy.stats as stats . df[&#39;gdp_zscore&#39;] = stats.zscore(df[&#39;gdp&#39;]) . these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped . df[abs(df[&#39;gdp_zscore&#39;])&gt;3].hist(column = [&#39;gdp&#39;]) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec873208&gt;]], dtype=object) . df_no_gdp_outliers=df[abs(df[&#39;gdp_zscore&#39;])&lt;3] . df_no_gdp_outliers . year region province gdp fdi it specific gdp_zscore . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | -0.521466 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | -0.464746 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | -0.421061 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | -0.383239 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | -0.340870 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 354 2002 | East China | Zhejiang | 8003.67 | 307610 | 1962633 | 365437.0 | 0.798274 | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | 1.178172 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | 1.612181 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | 2.007180 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | 2.520929 | . 350 rows × 8 columns . df_no_gdp_outliers.hist(column=[&#39;gdp&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec95e4e0&gt;]], dtype=object) . counts_fiscal=df.groupby(&#39;region&#39;).count() counts_fiscal . year province gdp fdi it specific gdp_zscore . region . East China 84 | 84 | 84 | 84 | 84 | 84 | 84 | . North China 48 | 48 | 48 | 48 | 48 | 47 | 48 | . Northeast China 36 | 36 | 36 | 36 | 36 | 36 | 36 | . Northwest China 60 | 60 | 60 | 60 | 60 | 60 | 60 | . South Central China 72 | 72 | 72 | 72 | 72 | 72 | 72 | . Southwest China 60 | 60 | 60 | 60 | 60 | 57 | 60 | . counts_fiscal=df.groupby(&#39;province&#39;).count() counts_fiscal . year region gdp fdi it specific gdp_zscore . province . Anhui 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Beijing 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Chongqing 12 | 12 | 12 | 12 | 12 | 9 | 12 | . Fujian 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Gansu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangdong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guizhou 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hainan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hebei 12 | 12 | 12 | 12 | 12 | 11 | 12 | . Heilongjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Henan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hubei 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hunan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangsu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jilin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Liaoning 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Ningxia 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Qinghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shaanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shandong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Sichuan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tianjin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tibet 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Xinjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Yunnan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Zhejiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . #df_no_gdp_outliers.pivot_table(index=&#39;grouping column 1&#39;, columns=&#39;grouping column 2&#39;, values=&#39;aggregating column&#39;, aggfunc=&#39;sum&#39;) . #pd.crosstab(df_no_gdp_outliers, &#39;year&#39;) . df_no_gdp_outliers_subset = df_no_gdp_outliers[[&#39;region&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;]] df_no_gdp_outliers_subset . region gdp fdi it . 0 East China | 2093.30 | 50661 | 631930 | . 1 East China | 2347.32 | 43443 | 657860 | . 2 East China | 2542.96 | 27673 | 889463 | . 3 East China | 2712.34 | 26131 | 1227364 | . 4 East China | 2902.09 | 31847 | 1499110 | . ... ... | ... | ... | ... | . 354 East China | 8003.67 | 307610 | 1962633 | . 355 East China | 9705.02 | 498055 | 2261631 | . 356 East China | 11648.70 | 668128 | 3162299 | . 357 East China | 13417.68 | 772000 | 2370200 | . 358 East China | 15718.47 | 888935 | 2553268 | . 350 rows × 4 columns . def aggregate_and_ttest(dataset, groupby_feature=&#39;region&#39;, alpha=.05, test_cells = [0, 1]): #Imports from tqdm import tqdm from scipy.stats import ttest_ind_from_stats metrics = [&#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;] feature_size = &#39;size&#39; feature_mean = &#39;mean&#39; feature_std = &#39;std&#39; for metric in tqdm(metrics): #print(metric) crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg([&#39;size&#39;, &#39;mean&#39;, &#39;std&#39;]) print(crosstab) treatment = crosstab.index[test_cells[0]] control = crosstab.index[test_cells[1]] counts_control = crosstab.loc[control, feature_size] counts_treatment = crosstab.loc[treatment, feature_size] mean_control = crosstab.loc[control, feature_mean] mean_treatment = crosstab.loc[treatment, feature_mean] standard_deviation_control = crosstab.loc[control, feature_std] standard_deviation_treatment = crosstab.loc[treatment, feature_std] t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control) #fstring to print the p value and t statistic print(f&quot;The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} is {t_statistic} and the p value is {p_value}.&quot;) #f string to say of the comparison is significant at a given alpha level if p_value &lt; alpha: print(f&#39;The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}&#39;) else: print(f&#39;The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}&#39;) . aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1]) . 100%|██████████| 3/3 [00:00&lt;00:00, 115.78it/s] . size mean std region East China 78 6070.604231 3500.372702 North China 48 4239.038542 2866.705149 Northeast China 36 3849.076944 1948.531835 Northwest China 60 1340.026167 1174.399739 South Central China 68 4835.540882 3697.129915 Southwest China 60 2410.398833 2144.589994 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 355577.897436 275635.866746 North China 48 169600.583333 127011.475909 Northeast China 36 136623.750000 142734.495232 Northwest China 60 15111.133333 22954.193559 South Central China 68 218931.426471 339981.399823 Southwest China 60 25405.083333 31171.373876 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 1.775615e+06 1.153030e+06 North China 48 1.733719e+06 1.548794e+06 Northeast China 36 2.665148e+06 1.768442e+06 Northwest China 60 1.703538e+06 1.446408e+06 South Central China 68 2.500962e+06 2.196436e+06 Southwest China 60 2.424971e+06 2.002198e+06 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372. The comparison between East China and North China is not statistically significant at the threshold of 0.05 . . from tqdm import tqdm for i in tqdm(range(10000)): ... . 100%|██████████| 10000/10000 [00:00&lt;00:00, 2169617.21it/s] . EastvNorth=pd.DataFrame() EastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1]) EastvNorth . 100%|██████████| 3/3 [00:00&lt;00:00, 135.00it/s] . size mean std region East China 78 6070.604231 3500.372702 North China 48 4239.038542 2866.705149 Northeast China 36 3849.076944 1948.531835 Northwest China 60 1340.026167 1174.399739 South Central China 68 4835.540882 3697.129915 Southwest China 60 2410.398833 2144.589994 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 355577.897436 275635.866746 North China 48 169600.583333 127011.475909 Northeast China 36 136623.750000 142734.495232 Northwest China 60 15111.133333 22954.193559 South Central China 68 218931.426471 339981.399823 Southwest China 60 25405.083333 31171.373876 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 1.775615e+06 1.153030e+06 North China 48 1.733719e+06 1.548794e+06 Northeast China 36 2.665148e+06 1.768442e+06 Northwest China 60 1.703538e+06 1.446408e+06 South Central China 68 2.500962e+06 2.196436e+06 Southwest China 60 2.424971e+06 2.002198e+06 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372. The comparison between East China and North China is not statistically significant at the threshold of 0.05 . . Genearate an experimental_crosstab to be used in statistical tests . experimental_crosstab = df_no_gdp_outliers_subset.groupby(&#39;region&#39;).agg([&#39;size&#39;, &#39;mean&#39;, &#39;std&#39;]) . experimental_crosstab.index . Index([&#39;East China&#39;, &#39;North China&#39;, &#39;Northeast China&#39;, &#39;Northwest China&#39;, &#39;South Central China&#39;, &#39;Southwest China&#39;], dtype=&#39;object&#39;, name=&#39;region&#39;) . df = experimental_crosstab.T df . region East China North China Northeast China Northwest China South Central China Southwest China . gdp size 7.800000e+01 | 4.800000e+01 | 3.600000e+01 | 6.000000e+01 | 6.800000e+01 | 6.000000e+01 | . mean 6.070604e+03 | 4.239039e+03 | 3.849077e+03 | 1.340026e+03 | 4.835541e+03 | 2.410399e+03 | . std 3.500373e+03 | 2.866705e+03 | 1.948532e+03 | 1.174400e+03 | 3.697130e+03 | 2.144590e+03 | . fdi size 7.800000e+01 | 4.800000e+01 | 3.600000e+01 | 6.000000e+01 | 6.800000e+01 | 6.000000e+01 | . mean 3.555779e+05 | 1.696006e+05 | 1.366238e+05 | 1.511113e+04 | 2.189314e+05 | 2.540508e+04 | . std 2.756359e+05 | 1.270115e+05 | 1.427345e+05 | 2.295419e+04 | 3.399814e+05 | 3.117137e+04 | . it size 7.800000e+01 | 4.800000e+01 | 3.600000e+01 | 6.000000e+01 | 6.800000e+01 | 6.000000e+01 | . mean 1.775615e+06 | 1.733719e+06 | 2.665148e+06 | 1.703538e+06 | 2.500962e+06 | 2.424971e+06 | . std 1.153030e+06 | 1.548794e+06 | 1.768442e+06 | 1.446408e+06 | 2.196436e+06 | 2.002198e+06 | . #experimental_crosstab.reset_index().unstack() . experimental_crosstab.iloc[0,1] . 6070.604230769231 . experimental_crosstab.index . Index([&#39;East China&#39;, &#39;North China&#39;, &#39;Northeast China&#39;, &#39;Northwest China&#39;, &#39;South Central China&#39;, &#39;Southwest China&#39;], dtype=&#39;object&#39;, name=&#39;region&#39;) . experimental_crosstab . gdp fdi it . size mean std size mean std size mean std . region . East China 78 | 6070.604231 | 3500.372702 | 78 | 355577.897436 | 275635.866746 | 78 | 1.775615e+06 | 1.153030e+06 | . North China 48 | 4239.038542 | 2866.705149 | 48 | 169600.583333 | 127011.475909 | 48 | 1.733719e+06 | 1.548794e+06 | . Northeast China 36 | 3849.076944 | 1948.531835 | 36 | 136623.750000 | 142734.495232 | 36 | 2.665148e+06 | 1.768442e+06 | . Northwest China 60 | 1340.026167 | 1174.399739 | 60 | 15111.133333 | 22954.193559 | 60 | 1.703538e+06 | 1.446408e+06 | . South Central China 68 | 4835.540882 | 3697.129915 | 68 | 218931.426471 | 339981.399823 | 68 | 2.500962e+06 | 2.196436e+06 | . Southwest China 60 | 2410.398833 | 2144.589994 | 60 | 25405.083333 | 31171.373876 | 60 | 2.424971e+06 | 2.002198e+06 | . experimental_crosstab.columns = [&#39;_&#39;.join(col) for col in experimental_crosstab.columns.values] . experimental_crosstab . gdp_size gdp_mean gdp_std fdi_size fdi_mean fdi_std it_size it_mean it_std . region . East China 78 | 6070.604231 | 3500.372702 | 78 | 355577.897436 | 275635.866746 | 78 | 1.775615e+06 | 1.153030e+06 | . North China 48 | 4239.038542 | 2866.705149 | 48 | 169600.583333 | 127011.475909 | 48 | 1.733719e+06 | 1.548794e+06 | . Northeast China 36 | 3849.076944 | 1948.531835 | 36 | 136623.750000 | 142734.495232 | 36 | 2.665148e+06 | 1.768442e+06 | . Northwest China 60 | 1340.026167 | 1174.399739 | 60 | 15111.133333 | 22954.193559 | 60 | 1.703538e+06 | 1.446408e+06 | . South Central China 68 | 4835.540882 | 3697.129915 | 68 | 218931.426471 | 339981.399823 | 68 | 2.500962e+06 | 2.196436e+06 | . Southwest China 60 | 2410.398833 | 2144.589994 | 60 | 25405.083333 | 31171.373876 | 60 | 2.424971e+06 | 2.002198e+06 | . experimental_crosstab.loc[&#39;East China&#39;, &#39;gdp_size&#39;] . 78 . experimental_crosstab.to_csv(&#39;fiscal_experimental_crosstab.csv&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/19/ttests-distributions-crosstabs-functions.html",
            "relUrl": "/2020/10/19/ttests-distributions-crosstabs-functions.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
            "content": "import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd import seaborn as sns . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;]] df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;] . df=df_subset df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . Distributions of Dependant Variables . Right skew . sns.distplot(df[&#39;gdp&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1043c20588&gt; . sns.distplot(df[&#39;fdi&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f10437f74a8&gt; . sns.distplot(df[&#39;it&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1043a09ef0&gt; . sns.distplot(df[&#39;specific&#39;].dropna()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f10439b7a20&gt; . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f10439a19e8&gt;]], dtype=object) . Removal of GDP value outliers more than 3 standard deviations away from the mean . outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean . import scipy.stats as stats . df[&#39;gdp_zscore&#39;] = stats.zscore(df[&#39;gdp&#39;]) . these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped . df[abs(df[&#39;gdp_zscore&#39;])&gt;3].hist(column = [&#39;gdp&#39;]) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f104364e0f0&gt;]], dtype=object) . df_no_gdp_outliers=df[abs(df[&#39;gdp_zscore&#39;])&lt;3] . df_no_gdp_outliers . year region province gdp fdi it specific gdp_zscore . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | -0.521466 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | -0.464746 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | -0.421061 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | -0.383239 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | -0.340870 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 354 2002 | East China | Zhejiang | 8003.67 | 307610 | 1962633 | 365437.0 | 0.798274 | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | 1.178172 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | 1.612181 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | 2.007180 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | 2.520929 | . 350 rows × 8 columns . df_no_gdp_outliers.hist(column=[&#39;gdp&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f10429f5ba8&gt;]], dtype=object) . counts_fiscal=df.groupby(&#39;region&#39;).count() counts_fiscal . year province gdp fdi it specific gdp_zscore . region . East China 84 | 84 | 84 | 84 | 84 | 84 | 84 | . North China 48 | 48 | 48 | 48 | 48 | 47 | 48 | . Northeast China 36 | 36 | 36 | 36 | 36 | 36 | 36 | . Northwest China 60 | 60 | 60 | 60 | 60 | 60 | 60 | . South Central China 72 | 72 | 72 | 72 | 72 | 72 | 72 | . Southwest China 60 | 60 | 60 | 60 | 60 | 57 | 60 | . counts_fiscal=df.groupby(&#39;province&#39;).count() counts_fiscal . year region gdp fdi it specific gdp_zscore . province . Anhui 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Beijing 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Chongqing 12 | 12 | 12 | 12 | 12 | 9 | 12 | . Fujian 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Gansu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangdong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guizhou 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hainan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hebei 12 | 12 | 12 | 12 | 12 | 11 | 12 | . Heilongjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Henan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hubei 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hunan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangsu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jilin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Liaoning 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Ningxia 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Qinghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shaanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shandong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Sichuan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tianjin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tibet 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Xinjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Yunnan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Zhejiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Subset by needed columns . df_no_gdp_outliers.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;, &#39;gdp_zscore&#39;], dtype=&#39;object&#39;) . df_no_gdp_outliers_subset = df_no_gdp_outliers[[&#39;region&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;]] df_no_gdp_outliers_subset . region gdp fdi it . 0 East China | 2093.30 | 50661 | 631930 | . 1 East China | 2347.32 | 43443 | 657860 | . 2 East China | 2542.96 | 27673 | 889463 | . 3 East China | 2712.34 | 26131 | 1227364 | . 4 East China | 2902.09 | 31847 | 1499110 | . ... ... | ... | ... | ... | . 354 East China | 8003.67 | 307610 | 1962633 | . 355 East China | 9705.02 | 498055 | 2261631 | . 356 East China | 11648.70 | 668128 | 3162299 | . 357 East China | 13417.68 | 772000 | 2370200 | . 358 East China | 15718.47 | 888935 | 2553268 | . 350 rows × 4 columns . Genearate an experimental_crosstab to be used in statistical tests . experimental_crosstab = df_no_gdp_outliers_subset.groupby(&#39;region&#39;).agg([&#39;size&#39;, &#39;mean&#39;, &#39;std&#39;]) . experimental_crosstab.index . Index([&#39;East China&#39;, &#39;North China&#39;, &#39;Northeast China&#39;, &#39;Northwest China&#39;, &#39;South Central China&#39;, &#39;Southwest China&#39;], dtype=&#39;object&#39;, name=&#39;region&#39;) . experimental_crosstab = experimental_crosstab.reset_index() . experimental_crosstab . region gdp fdi it . size mean std size mean std size mean std . 0 East China | 78 | 6070.604231 | 3500.372702 | 78 | 355577.897436 | 275635.866746 | 78 | 1.775615e+06 | 1.153030e+06 | . 1 North China | 48 | 4239.038542 | 2866.705149 | 48 | 169600.583333 | 127011.475909 | 48 | 1.733719e+06 | 1.548794e+06 | . 2 Northeast China | 36 | 3849.076944 | 1948.531835 | 36 | 136623.750000 | 142734.495232 | 36 | 2.665148e+06 | 1.768442e+06 | . 3 Northwest China | 60 | 1340.026167 | 1174.399739 | 60 | 15111.133333 | 22954.193559 | 60 | 1.703538e+06 | 1.446408e+06 | . 4 South Central China | 68 | 4835.540882 | 3697.129915 | 68 | 218931.426471 | 339981.399823 | 68 | 2.500962e+06 | 2.196436e+06 | . 5 Southwest China | 60 | 2410.398833 | 2144.589994 | 60 | 25405.083333 | 31171.373876 | 60 | 2.424971e+06 | 2.002198e+06 | . experimental_crosstab.to_csv(&#39;fiscal_experimental_crosstab.csv&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/18/ttests-distributions-crosstabs.html",
            "relUrl": "/2020/10/18/ttests-distributions-crosstabs.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Using Dask with dask.bag and regex to parse Notes from the Underground from project gutenberg",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import dask.bag as db import re . book_bag = db.from_url(&#39;https://www.gutenberg.org/cache/epub/600/pg600.txt&#39;) . book_bag.take(5) . (b&#34; xef xbb xbfProject Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky r n&#34;, b&#39; r n&#39;, b&#39;This eBook is for the use of anyone anywhere at no cost and with r n&#39;, b&#39;almost no restrictions whatsoever. You may copy it, give it away or r n&#39;, b&#39;re-use it under the terms of the Project Gutenberg License included r n&#39;) . remove_spaces = book_bag.map(lambda x:x.strip()) . remove_spaces.take(10) . (b&#34; xef xbb xbfProject Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky&#34;, b&#39;&#39;, b&#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, b&#39;almost no restrictions whatsoever. You may copy it, give it away or&#39;, b&#39;re-use it under the terms of the Project Gutenberg License included&#39;, b&#39;with this eBook or online at www.gutenberg.net&#39;, b&#39;&#39;, b&#39;&#39;, b&#39;Title: Notes from the Underground&#39;, b&#39;&#39;) . def decode_to_ascii(x): return x.decode(&quot;ascii&quot;,&quot;ignore&quot;) . ascii_text = remove_spaces.map(decode_to_ascii) . ascii_text.take(10) . (&#34;Project Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky&#34;, &#39;&#39;, &#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever. You may copy it, give it away or&#39;, &#39;re-use it under the terms of the Project Gutenberg License included&#39;, &#39;with this eBook or online at www.gutenberg.net&#39;, &#39;&#39;, &#39;&#39;, &#39;Title: Notes from the Underground&#39;, &#39;&#39;) . def remove_punctuation(x): return re.sub(r&#39;[^ w s]&#39;,&#39;&#39;,x) . remove_punctuation = ascii_text.map(remove_punctuation) . remove_punctuation.take(10) . (&#39;Project Gutenbergs Notes from the Underground by Feodor Dostoevsky&#39;, &#39;&#39;, &#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever You may copy it give it away or&#39;, &#39;reuse it under the terms of the Project Gutenberg License included&#39;, &#39;with this eBook or online at wwwgutenbergnet&#39;, &#39;&#39;, &#39;&#39;, &#39;Title Notes from the Underground&#39;, &#39;&#39;) . lower_text = remove_punctuation.map(str.lower) . lower_text.take(10) . (&#39;project gutenbergs notes from the underground by feodor dostoevsky&#39;, &#39;&#39;, &#39;this ebook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever you may copy it give it away or&#39;, &#39;reuse it under the terms of the project gutenberg license included&#39;, &#39;with this ebook or online at wwwgutenbergnet&#39;, &#39;&#39;, &#39;&#39;, &#39;title notes from the underground&#39;, &#39;&#39;) . split_word_list = lower_text.map(lambda x: x.split(&#39; &#39;)) . split_word_list.take(10) . ([&#39;project&#39;, &#39;gutenbergs&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;, &#39;by&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;], [&#39;&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;], [&#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;], [&#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;, &#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;], [&#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;, &#39;wwwgutenbergnet&#39;], [&#39;&#39;], [&#39;&#39;], [&#39;title&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;], [&#39;&#39;]) . def remove_empty_words(word_list): return list(filter(lambda a: a != &#39;&#39;, word_list)) non_empty_words = split_word_list.filter(remove_empty_words) . non_empty_words.take(10) . ([&#39;project&#39;, &#39;gutenbergs&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;, &#39;by&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;], [&#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;], [&#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;, &#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;], [&#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;, &#39;wwwgutenbergnet&#39;], [&#39;title&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;], [&#39;author&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;], [&#39;posting&#39;, &#39;date&#39;, &#39;september&#39;, &#39;13&#39;, &#39;2008&#39;, &#39;ebook&#39;, &#39;600&#39;], [&#39;release&#39;, &#39;date&#39;, &#39;july&#39;, &#39;1996&#39;], [&#39;language&#39;, &#39;english&#39;]) . all_words = non_empty_words.flatten() . type(all_words) . dask.bag.core.Bag . all_words.take(30) . (&#39;project&#39;, &#39;gutenbergs&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;, &#39;by&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;&#39;, &#39;you&#39;, &#39;may&#39;) . change_to_key_value = all_words.map(lambda x: (x, 1)) . change_to_key_value.take(4) . ((&#39;project&#39;, 1), (&#39;gutenbergs&#39;, 1), (&#39;notes&#39;, 1), (&#39;from&#39;, 1)) . grouped_words = all_words.groupby(lambda x:x) . grouped_words.take(1) . ((&#39;project&#39;, [&#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;]),) . word_count = grouped_words.map(lambda x: (x[0], len(x[1]))) . word_count.take(10) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;from&#39;, 186), (&#39;the&#39;, 1555), (&#39;underground&#39;, 26), (&#39;by&#39;, 153), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;this&#39;, 237)) . change_to_key_value.take(10) . ((&#39;project&#39;, 1), (&#39;gutenbergs&#39;, 1), (&#39;notes&#39;, 1), (&#39;from&#39;, 1), (&#39;the&#39;, 1), (&#39;underground&#39;, 1), (&#39;by&#39;, 1), (&#39;feodor&#39;, 1), (&#39;dostoevsky&#39;, 1), (&#39;this&#39;, 1)) . # Take a running count of a word # In this case, the default value of # count needs to be provided def add_bin_op(count, x): return count + x[1] # Take the output from multiple bin_op(s) # and add them to get the total count of # a word def add_combine_op(x, y): return x + y word_count = change_to_key_value.foldby(lambda x: x[0], add_bin_op, 0, add_combine_op) . word_count.take(10) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;from&#39;, 186), (&#39;the&#39;, 1555), (&#39;underground&#39;, 26), (&#39;by&#39;, 153), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;this&#39;, 237)) . much_easier = all_words.frequencies() . much_easier.take(10) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;from&#39;, 186), (&#39;the&#39;, 1555), (&#39;underground&#39;, 26), (&#39;by&#39;, 153), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;this&#39;, 237)) . Removing stop words in top word frequency counts . from spacy.lang.en import STOP_WORDS . without_stopwords = all_words.filter(lambda x: x not in STOP_WORDS) . new_freq = without_stopwords.frequencies() . new_freq.take(20) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;underground&#39;, 26), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;ebook&#39;, 9), (&#39;use&#39;, 18), (&#39;cost&#39;, 5), (&#39;restrictions&#39;, 3), (&#39;whatsoever&#39;, 2), (&#39;&#39;, 1896), (&#39;copy&#39;, 12), (&#39;away&#39;, 59), (&#39;reuse&#39;, 2), (&#39;terms&#39;, 24), (&#39;gutenberg&#39;, 28), (&#39;license&#39;, 15), (&#39;included&#39;, 6), (&#39;online&#39;, 4)) . new_freq.topk(10) . dask.bag&lt;topk-aggregate, npartitions=1&gt; . new_freq.topk(10, key=lambda x: x[1]).compute() . [(&#39;&#39;, 1896), (&#39;man&#39;, 122), (&#39;know&#39;, 90), (&#39;project&#39;, 87), (&#39;time&#39;, 83), (&#39;like&#39;, 82), (&#39;come&#39;, 74), (&#39;course&#39;, 73), (&#39;love&#39;, 72), (&#39;life&#39;, 69)] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/17/dask-NLP-gutenberg-books.html",
            "relUrl": "/2020/10/17/dask-NLP-gutenberg-books.html",
            "date": " • Oct 17, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Friday Links",
            "content": "Spacy V3 . | why-congress-should-invest-in-open-source-software . | National Research Cloud AI/ML . | Favicons . | Babies’ random choices become their preferences: “We assume we choose things that we like, but research suggests that’s sometimes backward: We like things because we choose them, and we dislike things that we don’t choose.” . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/16/links.html",
            "relUrl": "/links/2020/10/16/links.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Using Dask with dask.bag and regex to parse The Brothers Karamazov from project gutenberg",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import dask.bag as db import re . book_bag = db.from_url(&#39;https://www.gutenberg.org/files/28054/28054-0.txt&#39;) . book_bag.take(5) . (b&#39; xef xbb xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor r n&#39;, b&#39;Dostoyevsky r n&#39;, b&#39; r n&#39;, b&#39; r n&#39;, b&#39; r n&#39;) . remove_spaces = book_bag.map(lambda x:x.strip()) . remove_spaces.take(10) . (b&#39; xef xbb xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor&#39;, b&#39;Dostoyevsky&#39;, b&#39;&#39;, b&#39;&#39;, b&#39;&#39;, b&#39;This ebook is for the use of anyone anywhere in the United States and most&#39;, b&#39;other parts of the world at no cost and with almost no restrictions&#39;, b&#39;whatsoever. You may copy it, give it away or re xe2 x80 x90use it under the terms of&#39;, b&#39;the Project Gutenberg License included with this eBook or online at&#39;, b&#39;http://www.gutenberg.org/license. If you are not located in the United&#39;) . def decode_to_ascii(x): return x.decode(&quot;ascii&quot;,&quot;ignore&quot;) . ascii_text = remove_spaces.map(decode_to_ascii) . ascii_text.take(10) . (&#39;The Project Gutenberg EBook of The Brothers Karamazov by Fyodor&#39;, &#39;Dostoyevsky&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;This ebook is for the use of anyone anywhere in the United States and most&#39;, &#39;other parts of the world at no cost and with almost no restrictions&#39;, &#39;whatsoever. You may copy it, give it away or reuse it under the terms of&#39;, &#39;the Project Gutenberg License included with this eBook or online at&#39;, &#39;http://www.gutenberg.org/license. If you are not located in the United&#39;) . def remove_punctuation(x): return re.sub(r&#39;[^ w s]&#39;,&#39;&#39;,x) . remove_punctuation = ascii_text.map(remove_punctuation) . remove_punctuation.take(10) . (&#39;The Project Gutenberg EBook of The Brothers Karamazov by Fyodor&#39;, &#39;Dostoyevsky&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;This ebook is for the use of anyone anywhere in the United States and most&#39;, &#39;other parts of the world at no cost and with almost no restrictions&#39;, &#39;whatsoever You may copy it give it away or reuse it under the terms of&#39;, &#39;the Project Gutenberg License included with this eBook or online at&#39;, &#39;httpwwwgutenbergorglicense If you are not located in the United&#39;) . lower_text = remove_punctuation.map(str.lower) . lower_text.take(10) . (&#39;the project gutenberg ebook of the brothers karamazov by fyodor&#39;, &#39;dostoyevsky&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;this ebook is for the use of anyone anywhere in the united states and most&#39;, &#39;other parts of the world at no cost and with almost no restrictions&#39;, &#39;whatsoever you may copy it give it away or reuse it under the terms of&#39;, &#39;the project gutenberg license included with this ebook or online at&#39;, &#39;httpwwwgutenbergorglicense if you are not located in the united&#39;) . split_word_list = lower_text.map(lambda x: x.split(&#39; &#39;)) . split_word_list.take(10) . ([&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;, &#39;by&#39;, &#39;fyodor&#39;], [&#39;dostoyevsky&#39;], [&#39;&#39;], [&#39;&#39;], [&#39;&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;, &#39;states&#39;, &#39;and&#39;, &#39;most&#39;], [&#39;other&#39;, &#39;parts&#39;, &#39;of&#39;, &#39;the&#39;, &#39;world&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;], [&#39;whatsoever&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;, &#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;], [&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;, &#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;], [&#39;httpwwwgutenbergorglicense&#39;, &#39;if&#39;, &#39;you&#39;, &#39;are&#39;, &#39;not&#39;, &#39;located&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;]) . def remove_empty_words(word_list): return list(filter(lambda a: a != &#39;&#39;, word_list)) non_empty_words = split_word_list.filter(remove_empty_words) . non_empty_words.take(10) . ([&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;, &#39;by&#39;, &#39;fyodor&#39;], [&#39;dostoyevsky&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;, &#39;states&#39;, &#39;and&#39;, &#39;most&#39;], [&#39;other&#39;, &#39;parts&#39;, &#39;of&#39;, &#39;the&#39;, &#39;world&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;], [&#39;whatsoever&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;, &#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;], [&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;, &#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;], [&#39;httpwwwgutenbergorglicense&#39;, &#39;if&#39;, &#39;you&#39;, &#39;are&#39;, &#39;not&#39;, &#39;located&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;], [&#39;states&#39;, &#39;youll&#39;, &#39;have&#39;, &#39;to&#39;, &#39;check&#39;, &#39;the&#39;, &#39;laws&#39;, &#39;of&#39;, &#39;the&#39;, &#39;country&#39;, &#39;where&#39;, &#39;you&#39;, &#39;are&#39;, &#39;located&#39;], [&#39;before&#39;, &#39;using&#39;, &#39;this&#39;, &#39;ebook&#39;], [&#39;title&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;]) . all_words = non_empty_words.flatten() . type(all_words) . dask.bag.core.Bag . all_words.take(30) . (&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;, &#39;by&#39;, &#39;fyodor&#39;, &#39;dostoyevsky&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;, &#39;states&#39;, &#39;and&#39;, &#39;most&#39;, &#39;other&#39;, &#39;parts&#39;, &#39;of&#39;, &#39;the&#39;) . change_to_key_value = all_words.map(lambda x: (x, 1)) . change_to_key_value.take(4) . ((&#39;the&#39;, 1), (&#39;project&#39;, 1), (&#39;gutenberg&#39;, 1), (&#39;ebook&#39;, 1)) . grouped_words = all_words.groupby(lambda x:x) . grouped_words.take(1) . ((&#39;the&#39;, [&#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, ...]),) . word_count = grouped_words.map(lambda x: (x[0], len(x[1]))) . word_count.take(10) . ((&#39;the&#39;, 15379), (&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;of&#39;, 7410), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;by&#39;, 1165), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3)) . change_to_key_value.take(10) . ((&#39;the&#39;, 1), (&#39;project&#39;, 1), (&#39;gutenberg&#39;, 1), (&#39;ebook&#39;, 1), (&#39;of&#39;, 1), (&#39;the&#39;, 1), (&#39;brothers&#39;, 1), (&#39;karamazov&#39;, 1), (&#39;by&#39;, 1), (&#39;fyodor&#39;, 1)) . # Take a running count of a word # In this case, the default value of # count needs to be provided def add_bin_op(count, x): return count + x[1] # Take the output from multiple bin_op(s) # and add them to get the total count of # a word def add_combine_op(x, y): return x + y word_count = change_to_key_value.foldby(lambda x: x[0], add_bin_op, 0, add_combine_op) . word_count.take(10) . ((&#39;the&#39;, 15379), (&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;of&#39;, 7410), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;by&#39;, 1165), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3)) . much_easier = all_words.frequencies() . much_easier.take(10) . ((&#39;the&#39;, 15379), (&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;of&#39;, 7410), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;by&#39;, 1165), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3)) . Removing stop words in top word frequency counts . from spacy.lang.en import STOP_WORDS . without_stopwords = all_words.filter(lambda x: x not in STOP_WORDS) . new_freq = without_stopwords.frequencies() . new_freq.take(20) . ((&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3), (&#39;use&#39;, 77), (&#39;united&#39;, 24), (&#39;states&#39;, 21), (&#39;parts&#39;, 19), (&#39;world&#39;, 182), (&#39;cost&#39;, 12), (&#39;restrictions&#39;, 2), (&#39;whatsoever&#39;, 5), (&#39;copy&#39;, 16), (&#39;away&#39;, 445), (&#39;reuse&#39;, 2), (&#39;terms&#39;, 33), (&#39;license&#39;, 14)) . new_freq.topk(10) . dask.bag&lt;topk-aggregate, npartitions=1&gt; . new_freq.topk(10, key=lambda x: x[1]).compute() . [(&#39;alyosha&#39;, 1176), (&#39;said&#39;, 993), (&#39;know&#39;, 843), (&#39;man&#39;, 842), (&#39;mitya&#39;, 814), (&#39;dont&#39;, 784), (&#39;come&#39;, 772), (&#39;father&#39;, 721), (&#39;ivan&#39;, 677), (&#39;time&#39;, 669)] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/16/dask-NLP-gutenberg-books.html",
            "relUrl": "/2020/10/16/dask-NLP-gutenberg-books.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Linear Regression using Dask Data Frames",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&quot;sqlite:///fiscal.db&quot;) connection = engine.connect() metadata = db.MetaData() . #engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . client.restart() . Client . Scheduler: inproc://192.168.1.71/9672/30 | Dashboard: http://192.168.1.71:46451/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object float64 int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . client.id . &#39;Client-4c6edbde-0e23-11eb-a5c8-4b14c8b4f4db&#39; . Selecting Features and Target . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . #OHE from dask_ml.preprocessing import OneHotEncoder . ddf = ddf.categorize(cat_feat_list) . ohe = OneHotEncoder(sparse=False) . ohe_ddf = ohe.fit_transform(ddf[cat_feat_list]) . feat_list = feat_list + ohe_ddf.columns.tolist() feat_list = [f for f in feat_list if f not in cat_feat_list] . ddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target]) . ddf_processed.compute() . year fdi region_East China region_North China region_Southwest China region_Northwest China region_South Central China region_Northeast China province_Anhui province_Beijing ... province_Shandong province_Shanghai province_Shanxi province_Sichuan province_Tianjin province_Tibet province_Xinjiang province_Yunnan province_Zhejiang gdp . 0 1996 | 50661.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2093.30 | . 1 1997 | 43443.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2347.32 | . 2 1998 | 27673.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2542.96 | . 3 1999 | 26131.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2712.34 | . 4 2000 | 31847.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2902.09 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 2003 | 498055.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 9705.02 | . 356 2004 | 668128.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 11648.70 | . 357 2005 | 772000.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 13417.68 | . 358 2006 | 888935.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 15718.47 | . 359 2007 | 1036576.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 18753.73 | . 360 rows × 75 columns . feat_list . [&#39;year&#39;, &#39;fdi&#39;, &#39;region_East China&#39;, &#39;region_North China&#39;, &#39;region_Southwest China&#39;, &#39;region_Northwest China&#39;, &#39;region_South Central China&#39;, &#39;region_Northeast China&#39;, &#39;province_Anhui&#39;, &#39;province_Beijing&#39;, &#39;province_Chongqing&#39;, &#39;province_Fujian&#39;, &#39;province_Gansu&#39;, &#39;province_Guangdong&#39;, &#39;province_Guangxi&#39;, &#39;province_Guizhou&#39;, &#39;province_Hainan&#39;, &#39;province_Hebei&#39;, &#39;province_Heilongjiang&#39;, &#39;province_Henan&#39;, &#39;province_Hubei&#39;, &#39;province_Hunan&#39;, &#39;province_Jiangsu&#39;, &#39;province_Jiangxi&#39;, &#39;province_Jilin&#39;, &#39;province_Liaoning&#39;, &#39;province_Ningxia&#39;, &#39;province_Qinghai&#39;, &#39;province_Shaanxi&#39;, &#39;province_Shandong&#39;, &#39;province_Shanghai&#39;, &#39;province_Shanxi&#39;, &#39;province_Sichuan&#39;, &#39;province_Tianjin&#39;, &#39;province_Tibet&#39;, &#39;province_Xinjiang&#39;, &#39;province_Yunnan&#39;, &#39;province_Zhejiang&#39;, &#39;region_East China&#39;, &#39;region_North China&#39;, &#39;region_Southwest China&#39;, &#39;region_Northwest China&#39;, &#39;region_South Central China&#39;, &#39;region_Northeast China&#39;, &#39;province_Anhui&#39;, &#39;province_Beijing&#39;, &#39;province_Chongqing&#39;, &#39;province_Fujian&#39;, &#39;province_Gansu&#39;, &#39;province_Guangdong&#39;, &#39;province_Guangxi&#39;, &#39;province_Guizhou&#39;, &#39;province_Hainan&#39;, &#39;province_Hebei&#39;, &#39;province_Heilongjiang&#39;, &#39;province_Henan&#39;, &#39;province_Hubei&#39;, &#39;province_Hunan&#39;, &#39;province_Jiangsu&#39;, &#39;province_Jiangxi&#39;, &#39;province_Jilin&#39;, &#39;province_Liaoning&#39;, &#39;province_Ningxia&#39;, &#39;province_Qinghai&#39;, &#39;province_Shaanxi&#39;, &#39;province_Shandong&#39;, &#39;province_Shanghai&#39;, &#39;province_Shanxi&#39;, &#39;province_Sichuan&#39;, &#39;province_Tianjin&#39;, &#39;province_Tibet&#39;, &#39;province_Xinjiang&#39;, &#39;province_Yunnan&#39;, &#39;province_Zhejiang&#39;] . Dask Linear Regression . X=ddf_processed[feat_list].persist() y=ddf_processed[target].persist() . from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge . X . Dask DataFrame Structure: year fdi region_East China region_East China region_North China region_North China region_Southwest China region_Southwest China region_Northwest China region_Northwest China region_South Central China region_South Central China region_Northeast China region_Northeast China province_Anhui province_Anhui province_Beijing province_Beijing province_Chongqing province_Chongqing province_Fujian province_Fujian province_Gansu province_Gansu province_Guangdong province_Guangdong province_Guangxi province_Guangxi province_Guizhou province_Guizhou province_Hainan province_Hainan province_Hebei province_Hebei province_Heilongjiang province_Heilongjiang province_Henan province_Henan province_Hubei province_Hubei province_Hunan province_Hunan province_Jiangsu province_Jiangsu province_Jiangxi province_Jiangxi province_Jilin province_Jilin province_Liaoning province_Liaoning province_Ningxia province_Ningxia province_Qinghai province_Qinghai province_Shaanxi province_Shaanxi province_Shandong province_Shandong province_Shanghai province_Shanghai province_Shanxi province_Shanxi province_Sichuan province_Sichuan province_Tianjin province_Tianjin province_Tibet province_Tibet province_Xinjiang province_Xinjiang province_Yunnan province_Yunnan province_Zhejiang province_Zhejiang region_East China region_East China region_North China region_North China region_Southwest China region_Southwest China region_Northwest China region_Northwest China region_South Central China region_South Central China region_Northeast China region_Northeast China province_Anhui province_Anhui province_Beijing province_Beijing province_Chongqing province_Chongqing province_Fujian province_Fujian province_Gansu province_Gansu province_Guangdong province_Guangdong province_Guangxi province_Guangxi province_Guizhou province_Guizhou province_Hainan province_Hainan province_Hebei province_Hebei province_Heilongjiang province_Heilongjiang province_Henan province_Henan province_Hubei province_Hubei province_Hunan province_Hunan province_Jiangsu province_Jiangsu province_Jiangxi province_Jiangxi province_Jilin province_Jilin province_Liaoning province_Liaoning province_Ningxia province_Ningxia province_Qinghai province_Qinghai province_Shaanxi province_Shaanxi province_Shandong province_Shandong province_Shanghai province_Shanghai province_Shanxi province_Shanxi province_Sichuan province_Sichuan province_Tianjin province_Tianjin province_Tibet province_Tibet province_Xinjiang province_Xinjiang province_Yunnan province_Yunnan province_Zhejiang province_Zhejiang . npartitions=5 . 0 int64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | . 72 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: getitem, 5 tasks X.compute() . year fdi region_East China region_East China region_North China region_North China region_Southwest China region_Southwest China region_Northwest China region_Northwest China ... province_Tianjin province_Tianjin province_Tibet province_Tibet province_Xinjiang province_Xinjiang province_Yunnan province_Yunnan province_Zhejiang province_Zhejiang . 0 1996 | 50661.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 1997 | 43443.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1998 | 27673.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1999 | 26131.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 2000 | 31847.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 2003 | 498055.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 356 2004 | 668128.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 357 2005 | 772000.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 358 2006 | 888935.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 359 2007 | 1036576.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 360 rows × 146 columns . y . Dask DataFrame Structure: gdp . npartitions=5 . 0 float64 | . 72 ... | . ... ... | . 288 ... | . 359 ... | . Dask Name: getitem, 5 tasks LinReg = LinearRegression() . LinReg.fit(X, y) . LinearRegression() . RidgeReg = Ridge() RidgeReg.fit(x, y) . Ridge() . LinReg.predict(x)[:5] . array([[1830.87851079], [2076.99855135], [2220.28956053], [2534.65768132], [2936.29581027]]) . RidgeReg.predict(x)[:5] . array([[1804.41754025], [2053.19939587], [2200.05297844], [2516.48507702], [2919.42271884]]) . client.restart() . Client . Scheduler: inproc://192.168.1.71/9672/30 | Dashboard: http://192.168.1.71:46451/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/15/dask-xgboost-Linear-Regresion-Copy1.html",
            "relUrl": "/2020/10/15/dask-xgboost-Linear-Regresion-Copy1.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Using dask_ml.preprocessing and OneHotEncoder for categorical encoding with Dask",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&quot;sqlite:///fiscal.db&quot;) connection = engine.connect() metadata = db.MetaData() . #engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/9390/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.restart() . Client . Scheduler: inproc://192.168.1.71/9390/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object float64 int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . client.id . &#39;Client-0ac0cc94-0e22-11eb-a4ae-d71460f30774&#39; . # Selecting Features and Target . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . #OHE from dask_ml.preprocessing import OneHotEncoder . ddf = ddf.categorize(cat_feat_list) . ohe = OneHotEncoder(sparse=False) . ohe_ddf = ohe.fit_transform(ddf[cat_feat_list]) . feat_list = feat_list + ohe_ddf.columns.tolist() feat_list = [f for f in feat_list if f not in cat_feat_list] #client.close() . ddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target]) . ddf_processed.compute() . year fdi region_East China region_North China region_Southwest China region_Northwest China region_South Central China region_Northeast China province_Anhui province_Beijing ... province_Shandong province_Shanghai province_Shanxi province_Sichuan province_Tianjin province_Tibet province_Xinjiang province_Yunnan province_Zhejiang gdp . 0 1996 | 50661.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2093.30 | . 1 1997 | 43443.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2347.32 | . 2 1998 | 27673.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2542.96 | . 3 1999 | 26131.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2712.34 | . 4 2000 | 31847.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2902.09 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 2003 | 498055.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 9705.02 | . 356 2004 | 668128.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 11648.70 | . 357 2005 | 772000.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 13417.68 | . 358 2006 | 888935.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 15718.47 | . 359 2007 | 1036576.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 18753.73 | . 360 rows × 39 columns . client.restart() . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/14/dask-xgboost-HT-OHE.html",
            "relUrl": "/2020/10/14/dask-xgboost-HT-OHE.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Moving Dask XGboost with Fiscal Data, saving and loading Dask XGboost models",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal.db&#39;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_table LIMIT 10&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, 631930, 147002, 2093.3, 50661), (1997, &#39;East China&#39;, &#39;Anhui&#39;, 657860, 151981, 2347.32, 43443), (1998, &#39;East China&#39;, &#39;Anhui&#39;, 889463, 174930, 2542.96, 27673), (1999, &#39;East China&#39;, &#39;Anhui&#39;, 1227364, 285324, 2712.34, 26131), (2000, &#39;East China&#39;, &#39;Anhui&#39;, 1499110, 195580, 2902.09, 31847), (2001, &#39;East China&#39;, &#39;Anhui&#39;, 2165189, 250898, 3246.71, 33672), (2002, &#39;East China&#39;, &#39;Anhui&#39;, 2404936, 434149, 3519.72, 38375), (2003, &#39;East China&#39;, &#39;Anhui&#39;, 2815820, 619201, 3923.11, 36720), (2004, &#39;East China&#39;, &#39;Anhui&#39;, 3422176, 898441, 4759.3, 54669), (2005, &#39;East China&#39;, &#39;Anhui&#39;, 3874846, 898441, 5350.17, 69000)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . df.gdp.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2677958128&gt; . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/13442/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.restart() . Client . Scheduler: inproc://192.168.1.71/13442/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object float64 int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . client.id . &#39;Client-e79fe0fe-0d59-11eb-b482-f9dc9eaa58ee&#39; . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) # ddf[&quot;province&quot;] = ddf[&quot;province&quot;].astype(float) # ddf[&quot;region&quot;] = ddf[&quot;region&quot;].astype(float) . x=ddf[feat_list].persist() y=ddf[target].persist() . x . Dask DataFrame Structure: year fdi . npartitions=5 . 0 int64 | float64 | . 72 ... | ... | . ... ... | ... | . 288 ... | ... | . 359 ... | ... | . Dask Name: getitem, 5 tasks y.compute() . gdp . 0 2093.30 | . 1 2347.32 | . 2 2542.96 | . 3 2712.34 | . 4 2902.09 | . ... ... | . 355 9705.02 | . 356 11648.70 | . 357 13417.68 | . 358 15718.47 | . 359 18753.73 | . 360 rows × 1 columns . print(x.shape,y.shape) . (Delayed(&#39;int-97d0cf00-db85-425b-a0d2-08297142db86&#39;), 2) (Delayed(&#39;int-01cdae78-a995-48c1-9b93-277a008ad57a&#39;), 1) . x.count().compute() . year 360 fdi 360 dtype: int64 . from dask_ml.xgboost import XGBRegressor . XGBR = XGBRegressor() . %%time XGBR_model = XGBR.fit(x,y) . CPU times: user 54.2 s, sys: 1.02 s, total: 55.2 s Wall time: 18.4 s . XGBR_model . XGBRegressor() . XGBR_model.save_model(&#39;fiscal_model&#39;) . XGBR_model.load_model(&#39;fiscal_model&#39;) . [08:43:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/13/dask-xgboost-fiscal-data.html",
            "relUrl": "/2020/10/13/dask-xgboost-fiscal-data.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Monday Links",
            "content": "Nobel Prize in Economics 2020, Auction Theory . | The Promise of Prediction Markets . | ENTRY DETERRENCE by Milgrom and John Roberts (1981) . | Auction Theory by Milgrom . | How Computer Science Informs Modern Auction Design . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/12/links.html",
            "relUrl": "/links/2020/10/12/links.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Visualizing Operations with Dask Dataframes on Fiscal Data",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&quot;sqlite:///fiscal_data.db&quot;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;, 50661, 631930, 147002)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.7 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . df.gdp.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0272396c50&gt; . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/5995/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object object int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.describe().visualize(filename=&#39;describe.png&#39;) . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . max_gdp_per_region = ddf.groupby(&#39;region&#39;)[&#39;gdp&#39;].max() . max_gdp_per_region.visualize() . max_gdp_per_region.compute() . region East China 9705.02 North China 9846.81 Northwest China 956.32 South Central China 9439.6 Southwest China 937.5 Northeast China 9304.52 Name: gdp, dtype: object . ddf . Dask DataFrame Structure: year region province gdp fdi it specific . npartitions=5 . 0 int64 | object | object | object | int64 | int64 | float64 | . 72 ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks ddf.npartitions . 5 . ddf.npartitions . 5 . len(ddf) . 360 . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&quot;4GB&quot;) client . /home/gao/anaconda3/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use. Perhaps you already have a cluster running? Hosting the HTTP server on port 39701 instead http_address[&#34;port&#34;], self.http_server.port . Client . Scheduler: inproc://192.168.1.71/5995/20 | Dashboard: http://192.168.1.71:39701/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.id . &#39;Client-9f9a71c2-0c90-11eb-976b-cff3b7a8059e&#39; . ddf.describe().compute() . year fdi it specific . count 360.000000 | 3.600000e+02 | 3.600000e+02 | 3.560000e+02 | . mean 2001.500000 | 1.961394e+05 | 2.165819e+06 | 5.834707e+05 | . std 3.456857 | 3.030440e+05 | 1.769294e+06 | 6.540553e+05 | . min 1996.000000 | 2.000000e+00 | 1.478970e+05 | 8.964000e+03 | . 25% 1998.750000 | 3.309900e+04 | 1.077466e+06 | 2.237530e+05 | . 50% 2001.500000 | 1.411025e+05 | 2.020634e+06 | 4.243700e+05 | . 75% 2004.250000 | 4.065125e+05 | 3.375492e+06 | 1.011846e+06 | . max 2007.000000 | 1.743140e+06 | 1.053331e+07 | 3.937966e+06 | . ddf.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) # ddf[&quot;province&quot;] = ddf[&quot;province&quot;].astype(float) # ddf[&quot;region&quot;] = ddf[&quot;region&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . ddf . Dask DataFrame Structure: year region province gdp fdi it specific . npartitions=5 . 0 int64 | object | object | float64 | float64 | float64 | float64 | . 72 ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | . Dask Name: assign, 65 tasks ddf.nlargest(20, &#39;gdp&#39;).compute() . year region province gdp fdi it specific . 71 2007 | South Central China | Guangdong | 31777.01 | 1712603.0 | 4947824.0 | 859482.0 | . 70 2006 | South Central China | Guangdong | 26587.76 | 1451065.0 | 4559252.0 | 1897575.0 | . 263 2007 | East China | Shandong | 25776.91 | 1101159.0 | 6357869.0 | 2121243.0 | . 69 2005 | South Central China | Guangdong | 22557.37 | 1236400.0 | 4327217.0 | 1491588.0 | . 262 2006 | East China | Shandong | 21900.19 | 1000069.0 | 5304833.0 | 1204547.0 | . 179 2007 | East China | Jiangsu | 21742.05 | 1743140.0 | 3557071.0 | 1188989.0 | . 68 2004 | South Central China | Guangdong | 18864.62 | 1001158.0 | 5193902.0 | 1491588.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576.0 | 2939778.0 | 844647.0 | . 178 2006 | East China | Jiangsu | 18598.69 | 1318339.0 | 2926542.0 | 1388043.0 | . 261 2005 | East China | Shandong | 18366.87 | 897000.0 | 4142859.0 | 1011203.0 | . 67 2003 | South Central China | Guangdong | 15844.64 | 782294.0 | 4073606.0 | 1550764.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935.0 | 2553268.0 | 1017303.0 | . 260 2004 | East China | Shandong | 15021.84 | 870064.0 | 3732990.0 | 1011203.0 | . 143 2007 | South Central China | Henan | 15012.46 | 306162.0 | 10533312.0 | 3860764.0 | . 177 2005 | East China | Jiangsu | 15003.60 | 1213800.0 | 3479548.0 | 1483371.0 | . 119 2007 | North China | Hebei | 13607.32 | 241621.0 | 7537692.0 | 2981235.0 | . 66 2002 | South Central China | Guangdong | 13502.42 | 1133400.0 | 3545004.0 | 1235386.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000.0 | 2370200.0 | 656175.0 | . 275 2007 | East China | Shanghai | 12494.01 | 792000.0 | 2386339.0 | 272744.0 | . 176 2004 | East China | Jiangsu | 12442.87 | 1056365.0 | 2410257.0 | 1483371.0 | . without_ec = ddf[ddf.region !=&#39;East China&#39;] . without_ec.nlargest(20, &#39;gdp&#39;).compute() . year region province gdp fdi it specific . 71 2007 | South Central China | Guangdong | 31777.01 | 1712603.0 | 4947824.0 | 859482.0 | . 70 2006 | South Central China | Guangdong | 26587.76 | 1451065.0 | 4559252.0 | 1897575.0 | . 69 2005 | South Central China | Guangdong | 22557.37 | 1236400.0 | 4327217.0 | 1491588.0 | . 68 2004 | South Central China | Guangdong | 18864.62 | 1001158.0 | 5193902.0 | 1491588.0 | . 67 2003 | South Central China | Guangdong | 15844.64 | 782294.0 | 4073606.0 | 1550764.0 | . 143 2007 | South Central China | Henan | 15012.46 | 306162.0 | 10533312.0 | 3860764.0 | . 119 2007 | North China | Hebei | 13607.32 | 241621.0 | 7537692.0 | 2981235.0 | . 66 2002 | South Central China | Guangdong | 13502.42 | 1133400.0 | 3545004.0 | 1235386.0 | . 142 2006 | South Central China | Henan | 12362.79 | 184526.0 | 7601825.0 | 2018158.0 | . 65 2001 | South Central China | Guangdong | 12039.25 | 1193203.0 | 2152243.0 | 1257232.0 | . 118 2006 | North China | Hebei | 11467.60 | 201434.0 | 5831974.0 | 1253141.0 | . 64 2000 | South Central China | Guangdong | 10741.25 | 1128091.0 | 1927102.0 | 714572.0 | . 141 2005 | South Central China | Henan | 10587.42 | 123000.0 | 5676863.0 | 1171796.0 | . 299 2007 | Southwest China | Sichuan | 10562.39 | 149322.0 | 10384846.0 | 3937966.0 | . 117 2005 | North China | Hebei | 10012.11 | 191000.0 | 4503640.0 | 859056.0 | . 23 2007 | North China | Beijing | 9846.81 | 506572.0 | 1962192.0 | 752279.0 | . 167 2007 | South Central China | Hunan | 9439.60 | 327051.0 | 8340692.0 | 3156087.0 | . 155 2007 | South Central China | Hubei | 9333.40 | 276622.0 | 7666512.0 | 2922784.0 | . 215 2007 | Northeast China | Liaoning | 9304.52 | 598554.0 | 5502192.0 | 3396397.0 | . 63 1999 | South Central China | Guangdong | 9250.68 | 1165750.0 | 1789235.0 | 988521.0 | . ddf[&#39;province&#39;].compute() . 0 Anhui 1 Anhui 2 Anhui 3 Anhui 4 Anhui ... 355 Zhejiang 356 Zhejiang 357 Zhejiang 358 Zhejiang 359 Zhejiang Name: province, Length: 360, dtype: object . ddf.where(ddf[&#39;province&#39;]==&#39;Zhejiang&#39;).compute() . year region province gdp fdi it specific . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003.0 | East China | Zhejiang | 9705.02 | 498055.0 | 2261631.0 | 391292.0 | . 356 2004.0 | East China | Zhejiang | 11648.70 | 668128.0 | 3162299.0 | 656175.0 | . 357 2005.0 | East China | Zhejiang | 13417.68 | 772000.0 | 2370200.0 | 656175.0 | . 358 2006.0 | East China | Zhejiang | 15718.47 | 888935.0 | 2553268.0 | 1017303.0 | . 359 2007.0 | East China | Zhejiang | 18753.73 | 1036576.0 | 2939778.0 | 844647.0 | . 360 rows × 7 columns . mask_after_2010 = ddf.where(ddf[&#39;year&#39;]&gt;2000) . mask_after_2010.compute() . year region province gdp fdi it specific . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003.0 | East China | Zhejiang | 9705.02 | 498055.0 | 2261631.0 | 391292.0 | . 356 2004.0 | East China | Zhejiang | 11648.70 | 668128.0 | 3162299.0 | 656175.0 | . 357 2005.0 | East China | Zhejiang | 13417.68 | 772000.0 | 2370200.0 | 656175.0 | . 358 2006.0 | East China | Zhejiang | 15718.47 | 888935.0 | 2553268.0 | 1017303.0 | . 359 2007.0 | East China | Zhejiang | 18753.73 | 1036576.0 | 2939778.0 | 844647.0 | . 360 rows × 7 columns . def add_some_text(cname, *args, **kwargs): return &quot;Region name is &quot; + cname dummy_values = ddf[&#39;region&#39;].apply(add_some_text, axis=1) . /home/gao/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py:3208: UserWarning: You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly. To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using. Before: .apply(func) After: .apply(func, meta=(&#39;region&#39;, &#39;object&#39;)) warnings.warn(meta_warning(meta)) . dummy_values . Dask Series Structure: npartitions=5 0 object 72 ... ... 288 ... 359 ... Name: region, dtype: object Dask Name: apply, 75 tasks . dummy_values.visualize() . dummy_values.compute() . 0 Region name is East China 1 Region name is East China 2 Region name is East China 3 Region name is East China 4 Region name is East China ... 355 Region name is East China 356 Region name is East China 357 Region name is East China 358 Region name is East China 359 Region name is East China Name: region, Length: 360, dtype: object . max_per_region_yr = ddf.groupby(&#39;region&#39;).apply(lambda x: x.loc[x[&#39;gdp&#39;].idxmax(), &#39;year&#39;]) . /home/gao/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected. Before: .apply(func) After: .apply(func, meta={&#39;x&#39;: &#39;f8&#39;, &#39;y&#39;: &#39;f8&#39;}) for dataframe result or: .apply(func, meta=(&#39;x&#39;, &#39;f8&#39;)) for series result &#34;&#34;&#34;Entry point for launching an IPython kernel. . max_per_region_yr.visualize() . max_per_region_yr.compute() . region North China 2007 Northeast China 2007 Northwest China 2007 South Central China 2007 East China 2007 Southwest China 2007 dtype: int64 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/12/dask-xgboost-fiscal-data.html",
            "relUrl": "/2020/10/12/dask-xgboost-fiscal-data.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Moving Fiscal Data from a sqlite db to a dask dataframe",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal_data.db&#39;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;, 50661, 631930, 147002)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.7 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . df.gdp.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f41255eacc0&gt; . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . ddf.npartitions . ddf.npartitions . len(ddf) . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . ddf.describe().compute() . ddf.columns . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(float) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) #ddf[&quot;province&quot;] = ddf[&quot;province&quot;].astype(float) #ddf[&quot;region&quot;] = ddf[&quot;region&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . type(target) . x=ddf[feat_list].persist() y=ddf[target].persist() . x . y.compute() . print(x.shape,y.shape) . x.count().compute() . from dask_ml.xgboost import XGBRegressor . XGBR = XGBRegressor() . %%time XGBR_model = XGBR.fit(x,y) . XGBR_model . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/11/dask-xgboost-fiscal-data.html",
            "relUrl": "/2020/10/11/dask-xgboost-fiscal-data.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Working with dask data frames. Reading Fiscal Data from a sqlite db to a dask dataframe. Computing, visualizing and groupby with dask dataframes. Using dask.distributed locally.",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal_data.db&#39;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;, 50661, 631930, 147002)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.7 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object object int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.npartitions . 5 . ddf.npartitions . 5 . len(ddf) . 360 . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/12451/1 | Dashboard: http://localhost:8787/status &lt;/ul&gt; &lt;/td&gt; Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; ddf.describe().compute() . year fdi it specific . count 360.000000 | 3.600000e+02 | 3.600000e+02 | 3.560000e+02 | . mean 2001.500000 | 1.961394e+05 | 2.165819e+06 | 5.834707e+05 | . std 3.456857 | 3.030440e+05 | 1.769294e+06 | 6.540553e+05 | . min 1996.000000 | 2.000000e+00 | 1.478970e+05 | 8.964000e+03 | . 25% 1998.750000 | 3.309900e+04 | 1.077466e+06 | 2.237530e+05 | . 50% 2001.500000 | 1.411025e+05 | 2.020634e+06 | 4.243700e+05 | . 75% 2004.250000 | 4.065125e+05 | 3.375492e+06 | 1.011846e+06 | . max 2007.000000 | 1.743140e+06 | 1.053331e+07 | 3.937966e+06 | . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . groupby_yr = ddf.groupby(&#39;year&#39;).count() . groupby_yr.compute() . region province gdp fdi it specific . year . 1996 30 | 30 | 30 | 30 | 30 | 29 | . 1997 30 | 30 | 30 | 30 | 30 | 28 | . 1998 30 | 30 | 30 | 30 | 30 | 30 | . 1999 30 | 30 | 30 | 30 | 30 | 30 | . 2000 30 | 30 | 30 | 30 | 30 | 29 | . 2001 30 | 30 | 30 | 30 | 30 | 30 | . 2002 30 | 30 | 30 | 30 | 30 | 30 | . 2003 30 | 30 | 30 | 30 | 30 | 30 | . 2004 30 | 30 | 30 | 30 | 30 | 30 | . 2005 30 | 30 | 30 | 30 | 30 | 30 | . 2006 30 | 30 | 30 | 30 | 30 | 30 | . 2007 30 | 30 | 30 | 30 | 30 | 30 | . group_region = ddf.groupby(&#39;region&#39;)[&#39;gdp&#39;].sum() . group_region.compute() . region East China 2093.32347.322542.962712.342902.093246.713519.... North China 1789.22077.092377.182678.823161.663707.964315.... Northwest China 722.52793.57887.67956.321052.881125.371232.031... South Central China 6834.977774.538530.889250.6810741.2512039.2513... Southwest China 1315.121509.751602.381663.21791.01976.862232.8... Northeast China 2370.52667.52774.42866.33151.43390.13637.24057... Name: gdp, dtype: object . ddf.nlargest(5, &#39;fdi&#39;).compute() . year region province gdp fdi it specific . 179 2007 | East China | Jiangsu | 21742.05 | 1743140 | 3557071 | 1188989.0 | . 71 2007 | South Central China | Guangdong | 31777.01 | 1712603 | 4947824 | 859482.0 | . 70 2006 | South Central China | Guangdong | 26587.76 | 1451065 | 4559252 | 1897575.0 | . 178 2006 | East China | Jiangsu | 18598.69 | 1318339 | 2926542 | 1388043.0 | . 69 2005 | South Central China | Guangdong | 22557.37 | 1236400 | 4327217 | 1491588.0 | . ddf.sum().visualize() . ddf.sum().visualize(rankdir=&quot;LR&quot;) . (ddf).visualize(rankdir=&quot;LR&quot;) . ddf.visualize(rankdir=&quot;LR&quot;) . client.close() . &lt;/div&gt; | . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/10/Dask_Fiscal-db-to-dask-dataframe.html",
            "relUrl": "/2020/10/10/Dask_Fiscal-db-to-dask-dataframe.html",
            "date": " • Oct 10, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Moving fiscal data from a pandas dataframe to a sqlite local database",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import pandas as pd . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df.columns . Index([&#39;Unnamed: 0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;], dtype=&#39;object&#39;) . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;]] df_subset . year reg province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;] . df_subset . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal_data.db&#39;) connection = engine.connect() metadata = db.MetaData() . fiscal_data = db.Table(&#39;fiscal_data&#39;, metadata, db.Column(&#39;year&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;region&#39;,db.String, nullable=True), db.Column(&#39;province&#39;,db.String, nullable=True), db.Column(&#39;gdp&#39;,db.String, nullable=True), db.Column(&#39;fdi&#39;,db.Integer, nullable=True), db.Column(&#39;it&#39;,db.Integer, nullable=True), db.Column(&#39;specific&#39;, db.Integer, nullable=True) ) . metadata.create_all(engine) #Creates the table . fiscal_data . Table(&#39;fiscal_data&#39;, MetaData(bind=None), Column(&#39;year&#39;, Integer(), table=&lt;fiscal_data&gt;), Column(&#39;region&#39;, String(), table=&lt;fiscal_data&gt;), Column(&#39;province&#39;, String(), table=&lt;fiscal_data&gt;), Column(&#39;gdp&#39;, String(), table=&lt;fiscal_data&gt;), Column(&#39;fdi&#39;, Integer(), table=&lt;fiscal_data&gt;), Column(&#39;it&#39;, Integer(), table=&lt;fiscal_data&gt;), Column(&#39;specific&#39;, Integer(), table=&lt;fiscal_data&gt;), schema=None) . df_subset.to_sql(&#39;fiscal_data&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT year, region, province, gdp FROM fiscal_data LIMIT 10&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;), (1997, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2347.32&#39;), (1998, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2542.96&#39;), (1999, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2712.34&#39;), (2000, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2902.09&#39;), (2001, &#39;East China&#39;, &#39;Anhui&#39;, &#39;3246.71&#39;), (2002, &#39;East China&#39;, &#39;Anhui&#39;, &#39;3519.72&#39;), (2003, &#39;East China&#39;, &#39;Anhui&#39;, &#39;3923.11&#39;), (2004, &#39;East China&#39;, &#39;Anhui&#39;, &#39;4759.3&#39;), (2005, &#39;East China&#39;, &#39;Anhui&#39;, &#39;5350.17&#39;)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df.tail(30) . year region province gdp . 330 2002 | Northwest China | Xinjiang | 1612.65 | . 331 2003 | Northwest China | Xinjiang | 1886.35 | . 332 2004 | Northwest China | Xinjiang | 2209.09 | . 333 2005 | Northwest China | Xinjiang | 2604.19 | . 334 2006 | Northwest China | Xinjiang | 3045.26 | . 335 2007 | Northwest China | Xinjiang | 3523.16 | . 336 1996 | Southwest China | Yunnan | 1517.69 | . 337 1997 | Southwest China | Yunnan | 1676.17 | . 338 1998 | Southwest China | Yunnan | 1831.33 | . 339 1999 | Southwest China | Yunnan | 1899.82 | . 340 2000 | Southwest China | Yunnan | 2011.19 | . 341 2001 | Southwest China | Yunnan | 2138.31 | . 342 2002 | Southwest China | Yunnan | 2312.82 | . 343 2003 | Southwest China | Yunnan | 2556.02 | . 344 2004 | Southwest China | Yunnan | 3081.91 | . 345 2005 | Southwest China | Yunnan | 3462.73 | . 346 2006 | Southwest China | Yunnan | 3988.14 | . 347 2007 | Southwest China | Yunnan | 4772.52 | . 348 1996 | East China | Zhejiang | 4188.53 | . 349 1997 | East China | Zhejiang | 4686.11 | . 350 1998 | East China | Zhejiang | 5052.62 | . 351 1999 | East China | Zhejiang | 5443.92 | . 352 2000 | East China | Zhejiang | 6141.03 | . 353 2001 | East China | Zhejiang | 6898.34 | . 354 2002 | East China | Zhejiang | 8003.67 | . 355 2003 | East China | Zhejiang | 9705.02 | . 356 2004 | East China | Zhejiang | 11648.7 | . 357 2005 | East China | Zhejiang | 13417.68 | . 358 2006 | East China | Zhejiang | 15718.47 | . 359 2007 | East China | Zhejiang | 18753.73 | . #http://manpages.ubuntu.com/manpages/precise/man1/sqlite3.1.html . # sqlite3 fiscal_data.db # create table memos(text, priority INTEGER); # insert into memos values(&#39;example 1&#39;, 10); # insert into memos values(&#39;example 2&#39;, 100); # select * from memos; # sqlite3 -line fiscal_data.db &#39;select * from memos where priority &gt; 20;&#39; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/09/Dask_Fiscal-Data-Sqlite-db.html",
            "relUrl": "/2020/10/09/Dask_Fiscal-Data-Sqlite-db.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Using Dask for Arrays",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask. . import numpy as np import dask.array as da . np_arr = np.random.randint(20, size=20) np_arr . array([12, 18, 17, 7, 5, 9, 11, 3, 5, 15, 13, 13, 5, 12, 11, 16, 4, 10, 9, 7]) . dask_arr = da.random.randint(20, size=20, chunks=5) . dask_arr . | Array Chunk . Bytes 160 B | 40 B | . Shape (20,) | (5,) | . Count 4 Tasks | 4 Chunks | . Type int64 | numpy.ndarray | . | 20 1 | . ## This is simply because Dask does lazy evaluaion. ### You need to call `compute()` to start the execution . dask_arr.compute() . array([ 3, 17, 5, 11, 19, 14, 14, 11, 9, 18, 9, 7, 10, 13, 10, 10, 11, 10, 9, 2]) . dask_arr.chunks . ((5, 5, 5, 5),) . dask_arr_from_np = da.from_array(np_arr, chunks=5) . dask_arr_from_np . | Array Chunk . Bytes 160 B | 40 B | . Shape (20,) | (5,) | . Count 5 Tasks | 4 Chunks | . Type int64 | numpy.ndarray | . | 20 1 | . dask_arr_from_np.compute() . array([12, 18, 17, 7, 5, 9, 11, 3, 5, 15, 13, 13, 5, 12, 11, 16, 4, 10, 9, 7]) . ### array operations into a graph to tasks #### See : http://docs.dask.org/en/latest/graphviz.html . dask_arr_from_np.sum().visualize() . dask_arr_from_np.sum().visualize(rankdir=&quot;LR&quot;) . (dask_arr_from_np+1).visualize(rankdir=&quot;LR&quot;) . dask_arr_mean = da.mean(dask_arr_from_np) dask_arr_mean.compute() . 10.1 . dask_arr_mean.visualize(rankdir=&quot;LR&quot;) . x = da.random.random(10, chunks=2) y = da.random.random(10, chunks=2) sum_x_y = da.add(x, y) #similar to numpy.add mean_x_y = da.mean(sum_x_y) . sum_x_y.compute() . array([0.96028343, 0.55946179, 1.11161829, 1.28233368, 0.53130934, 0.86805782, 0.20173099, 0.77596276, 0.92576765, 1.04750609]) . sum_x_y.visualize() . mean_x_y.visualize() . da_arr_large = da.random.randint(10000, size=(50000, 50000), chunks=(5000, 1000)) da_sum_large = da_arr_large.sum() . ### Get no. bytes using `nbytes` : http://docs.dask.org/en/latest/array-api.html#dask.array.Array.nbytes . da_arr_large.nbytes . 20000000000 . ### Convert bytes to GB, 1Gb = 1e+9 bytes . da_arr_large.nbytes/1e+9 . 20.0 . da_sum_large.compute() . 12498643590734 . # Dask 2 . size_tuple = (500,500) chunks_tuple = (10,500) . da_arr = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) da_arr2 = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) . def random_func(x): return np.mean((((x * 2).T)**2),axis=0) . gufoo = da.gufunc(random_func, signature=&quot;(i)-&gt;()&quot;, output_dtypes=float, vectorize=True) . random_op_arr = gufoo(da_arr) random_op_arr.compute() . array([112.056, 107.44 , 111.024, 109.656, 118.832, 109.84 , 117.2 , 111.952, 116.312, 117.368, 128.568, 111.144, 110.656, 112.648, 115.24 , 114.624, 113.912, 109.632, 112.864, 113.488, 119.248, 121.4 , 108.272, 118.784, 114.968, 115.216, 107.872, 113.6 , 112.456, 112.48 , 114.864, 119.28 , 112.656, 110.208, 109.728, 120.576, 119.632, 118.12 , 112.888, 116.384, 113.192, 106.84 , 111.72 , 115.928, 106.08 , 114.568, 121.512, 115.384, 113.864, 107.104, 114.32 , 116.176, 117.28 , 116.976, 117.784, 110.088, 121.696, 114.2 , 113.864, 116.072, 112.344, 113.808, 113.968, 110.472, 119.536, 113.84 , 109.328, 116.552, 119.056, 113.84 , 117.872, 114.928, 116.336, 115.192, 115.808, 106.984, 116.984, 114.536, 116.496, 111.968, 115.216, 108.24 , 119.52 , 116.136, 111.144, 111.712, 119.224, 114.312, 110.464, 110.216, 111.288, 119.6 , 108.264, 114.456, 119.016, 107.032, 114.832, 108.056, 105.712, 110.64 , 103.4 , 106.768, 118.216, 112.44 , 113.728, 114.6 , 117.832, 108.288, 117.92 , 113.12 , 121.984, 112.776, 123.144, 115.968, 112.44 , 115.712, 112.144, 108.448, 114.752, 108.376, 101.296, 102.992, 117.872, 114.056, 115.736, 115.528, 122.072, 130.168, 106.992, 109.912, 117.872, 112.152, 112.184, 113.544, 116.496, 112.832, 108.712, 116.96 , 120.984, 117.808, 112.272, 111.816, 118.872, 116.376, 118.992, 112.344, 124.672, 97.576, 112.496, 117.92 , 102.392, 109.992, 112.016, 117.92 , 108.352, 112.376, 121.008, 117.808, 113.504, 125.592, 114.936, 111.456, 116.488, 104.744, 114.136, 114. , 107.256, 117.84 , 111.872, 109.152, 118.752, 112.32 , 116.16 , 106.696, 109.472, 111.968, 118.264, 115.088, 112.864, 110.016, 111.888, 111.84 , 118.488, 107.952, 121.52 , 126.52 , 112.12 , 110.952, 115.328, 110.064, 106.36 , 118.96 , 109.68 , 117.776, 107.112, 111.152, 113.888, 113.408, 114.992, 117.632, 116.648, 117.112, 118.2 , 116.36 , 113.104, 113.6 , 112.208, 112.592, 117.192, 102.832, 112.08 , 113.744, 116.048, 117.368, 113.96 , 111.24 , 121.824, 112.56 , 110.192, 130.776, 111.656, 119.984, 113.592, 113.592, 106.664, 125.192, 113.6 , 117.12 , 106.24 , 112.856, 114.544, 117.16 , 108.344, 112.208, 109.112, 124.824, 109.824, 106.352, 115.568, 112.64 , 112.904, 112.736, 112.52 , 124.808, 120.32 , 114.472, 119.528, 113.456, 112.448, 118.672, 110.016, 116.16 , 122.048, 111.088, 114.56 , 107.448, 115.328, 111.656, 108.688, 116.904, 110.8 , 108.896, 112.136, 115.896, 111.848, 108.808, 114.504, 124.552, 116.248, 114.576, 110.56 , 112.152, 117.576, 125.44 , 110.72 , 108.072, 115.192, 116.048, 107.76 , 111.376, 121.608, 115.256, 113.84 , 105.672, 115.024, 115.864, 114.304, 123.344, 114.624, 115.696, 113.288, 116.688, 109.048, 125.264, 118.8 , 112.2 , 114.312, 109.728, 116.064, 113.808, 106.912, 109.288, 117. , 114.632, 114.456, 110.168, 111.976, 117.816, 110.04 , 103.048, 113.656, 112.504, 113.8 , 120.04 , 120.224, 110.68 , 110.096, 116.12 , 113.424, 107.408, 111.296, 111.512, 117.432, 105.96 , 115.992, 118.44 , 110.024, 119.216, 111.664, 119.184, 109.824, 116.736, 116.76 , 107.544, 120.44 , 115.08 , 110.136, 112.144, 113.888, 111.32 , 109.952, 117.096, 111.152, 115.728, 110.832, 113.312, 113.664, 112.016, 111.952, 114.896, 114.728, 107.848, 108.832, 122.384, 111.824, 107.384, 117.504, 117.344, 110.144, 109.568, 101.36 , 111.944, 105.512, 115.792, 112.08 , 104.568, 109.008, 108.992, 114.936, 113.008, 120.088, 117.328, 117.008, 107.584, 111.688, 115.664, 108.416, 119.48 , 107.336, 120.184, 111.952, 115.824, 113.928, 117.064, 114.296, 111.56 , 120.04 , 112.256, 115.368, 109.112, 112.184, 112.128, 111.288, 117.856, 109.184, 113.128, 119.888, 110.656, 111.992, 116.704, 107.696, 111.608, 121.504, 110.296, 111.008, 112.072, 117.072, 115.68 , 108.888, 117.704, 113.112, 101.144, 112.36 , 122.688, 112.016, 111.64 , 113.992, 117.08 , 109.976, 108.048, 110.504, 112.936, 111.776, 117.392, 116.568, 106.896, 105.224, 115.512, 117. , 116.192, 113.344, 111.776, 114.312, 113.008, 114.768, 121.712, 112.528, 108.976, 106.648, 107.8 , 122.696, 104.064, 117.072, 119.064, 111.472, 112.752, 109.52 , 123.712, 114.032, 120.888, 109.84 , 123.36 , 111.576, 118.56 , 116.328, 113.048, 111.68 , 106.072, 109.752, 112.32 , 114.344, 114.976, 114.072, 121.792, 113.024, 109.864, 115.84 , 115.752, 118.648, 107.52 , 116.104, 112.464, 123.232, 112.32 , 116.952, 106.32 , 110.992, 111.256, 113.616, 111.344, 115.216, 121.504, 117.504, 115.816, 116.6 , 111.08 , 108.776, 110.672, 109.464, 107.096, 112.928, 106.8 , 110.4 , 112.576, 114.648, 113.272, 112.504, 112.888, 112.84 , 111.496]) . random_op_arr.shape . (500,) . @da.as_gufunc(signature=&quot;(m,n),(n,j)-&gt;(m,j)&quot;, output_dtypes=int, allow_rechunk=True) def random_func(x, y): return np.matmul(x, y)**2 . da_arr3 = da.random.randint(10, size=(200, 100), chunks=(10, 100)) da_arr4 = da.random.randint(10, size=(100, 300), chunks=(5,5)) . # random_matmul = random_func(da_arr3, da_arr4) # random_matmul.compute() . random_matmul.shape . (200, 300) . # Dask 3 . my_arr = da.random.randint(10, size=20, chunks=3) . my_arr.compute() . array([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5]) . my_hundred_arr = my_arr + 100 my_hundred_arr.compute() . array([103, 108, 108, 107, 104, 101, 105, 107, 102, 107, 104, 104, 108, 100, 109, 103, 106, 107, 101, 105]) . (my_arr * (-1)).compute() . array([-3, -8, -8, -7, -4, -1, -5, -7, -2, -7, -4, -4, -8, 0, -9, -3, -6, -7, -1, -5]) . dask_sum = my_arr.sum() dask_sum . | Array Chunk . Bytes 8 B | 8 B | . Shape () | () | . Count 17 Tasks | 1 Chunks | . Type int64 | numpy.ndarray | . | | . my_arr.compute() . array([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5]) . dask_sum.compute() . 99 . my_ones_arr = da.ones((10,10), chunks=2, dtype=int) . my_ones_arr.compute() . array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) . my_ones_arr.mean(axis=0).compute() . array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) . my_custom_array = da.random.randint(10, size=(4,4), chunks=(1,4)) . my_custom_array.compute() . array([[0, 1, 7, 6], [0, 1, 2, 4], [6, 3, 5, 3], [3, 2, 2, 6]]) . my_custom_array.mean(axis=0).compute() . array([2.25, 1.75, 4. , 4.75]) . my_custom_array.mean(axis=1).compute() . array([3.5 , 1.75, 4.25, 3.25]) . ## Slicing . my_custom_array[1:3, 2:4] . | Array Chunk . Bytes 32 B | 16 B | . Shape (2, 2) | (1, 2) | . Count 6 Tasks | 2 Chunks | . Type int64 | numpy.ndarray | . | 2 2 | . my_custom_array[1:3, 2:4].compute() . array([[2, 4], [5, 3]]) . ## Broadcasting . my_custom_array.compute() . array([[0, 1, 7, 6], [0, 1, 2, 4], [6, 3, 5, 3], [3, 2, 2, 6]]) . my_small_arr = da.ones(4, chunks=2) my_small_arr.compute() . array([1., 1., 1., 1.]) . brd_example1 = da.add(my_custom_array, my_small_arr) . brd_example1.compute() . array([[1., 2., 8., 7.], [1., 2., 3., 5.], [7., 4., 6., 4.], [4., 3., 3., 7.]]) . ten_arr = da.full_like(my_small_arr, 10) . ten_arr.compute() . array([10., 10., 10., 10.]) . brd_example2 = da.add(my_custom_array, ten_arr) . brd_example2.compute() . array([[10., 11., 17., 16.], [10., 11., 12., 14.], [16., 13., 15., 13.], [13., 12., 12., 16.]]) . ## Reshaping . my_custom_array.shape . (4, 4) . custom_arr_1d = my_custom_array.reshape(16) . custom_arr_1d . | Array Chunk . Bytes 128 B | 32 B | . Shape (16,) | (4,) | . Count 8 Tasks | 4 Chunks | . Type int64 | numpy.ndarray | . | 16 1 | . custom_arr_1d.compute() . array([0, 1, 7, 6, 0, 1, 2, 4, 6, 3, 5, 3, 3, 2, 2, 6]) . # Stacking . stacked_arr = da.stack([brd_example1, brd_example2]) . stacked_arr.compute() . array([[[ 1., 2., 8., 7.], [ 1., 2., 3., 5.], [ 7., 4., 6., 4.], [ 4., 3., 3., 7.]], [[10., 11., 17., 16.], [10., 11., 12., 14.], [16., 13., 15., 13.], [13., 12., 12., 16.]]]) . another_stacked = da.stack([brd_example1, brd_example2], axis=1) . another_stacked.compute() . array([[[ 1., 2., 8., 7.], [10., 11., 17., 16.]], [[ 1., 2., 3., 5.], [10., 11., 12., 14.]], [[ 7., 4., 6., 4.], [16., 13., 15., 13.]], [[ 4., 3., 3., 7.], [13., 12., 12., 16.]]]) . # Concatenate . concate_arr = da.concatenate([brd_example1, brd_example2]) . concate_arr.compute() . array([[ 1., 2., 8., 7.], [ 1., 2., 3., 5.], [ 7., 4., 6., 4.], [ 4., 3., 3., 7.], [10., 11., 17., 16.], [10., 11., 12., 14.], [16., 13., 15., 13.], [13., 12., 12., 16.]]) . another_concate_arr = da.concatenate([brd_example1, brd_example2],axis=1) . another_concate_arr.compute() . array([[ 1., 2., 8., 7., 10., 11., 17., 16.], [ 1., 2., 3., 5., 10., 11., 12., 14.], [ 7., 4., 6., 4., 16., 13., 15., 13.], [ 4., 3., 3., 7., 13., 12., 12., 16.]]) . # Dask 4 . import numpy as np import dask.array as da . size_tuple = (18000,18000) np_arr = np.random.randint(10, size=size_tuple) np_arr2 = np.random.randint(10, size=size_tuple) . %time (((np_arr * 2).T)**2 + np_arr2 + 100).sum(axis=1).mean() . MemoryError Traceback (most recent call last) &lt;timed eval&gt; in &lt;module&gt; MemoryError: . chunks_tuple = (500, 500) da_arr = da.from_array(np_arr, chunks=chunks_tuple) da_arr2 = da.from_array(np_arr2, chunks=chunks_tuple) . %time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute() . CPU times: user 10.1 s, sys: 362 ms, total: 10.5 s Wall time: 2.47 s . 3933124.5174444444 . size_tuple = (50000, 50000) np_arr = np.random.randint(10, size=size_tuple) np_arr2 = np.random.randint(10, size=size_tuple) . MemoryError Traceback (most recent call last) &lt;ipython-input-5-9ce9976b2eaf&gt; in &lt;module&gt; 1 size_tuple = (50000, 50000) -&gt; 2 np_arr = np.random.randint(10, size=size_tuple) 3 np_arr2 = np.random.randint(10, size=size_tuple) mtrand.pyx in mtrand.RandomState.randint() mtrand.pyx in mtrand.RandomState.randint() randint_helpers.pxi in mtrand._rand_int64() MemoryError: . chunks_tuple = (5000, 5000) da_arr = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) da_arr2 = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) . %time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute() . CPU times: user 3min 10s, sys: 10.5 s, total: 3min 20s Wall time: 28.2 s . 10925051.41748 . da_arr.nbytes/1e+9 . 20.0 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/08/Dask-Day-1.html",
            "relUrl": "/2020/10/08/Dask-Day-1.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Fiscal Data",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . df=pd.read_csv(&#39;df_panel_fix.csv&#39;) . df . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.000000 | 0.000000 | 0.000000 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.000000 | 0.000000 | 0.000000 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.000000 | 0.000000 | 0.000000 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.000000 | 0.000000 | 0.000000 | 1601508 | East China | 1499110 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 355 | Zhejiang | 391292.0 | 260313.0 | 2003 | 9705.02 | 498055 | 1.214286 | 0.035714 | 0.035714 | 6217715 | East China | 2261631 | . 356 356 | Zhejiang | 656175.0 | 276652.0 | 2004 | 11648.70 | 668128 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 3162299 | . 357 357 | Zhejiang | 656175.0 | NaN | 2005 | 13417.68 | 772000 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 2370200 | . 358 358 | Zhejiang | 1017303.0 | 394795.0 | 2006 | 15718.47 | 888935 | 1.214286 | 0.035714 | 0.035714 | 11537149 | East China | 2553268 | . 359 359 | Zhejiang | 844647.0 | 0.0 | 2007 | 18753.73 | 1036576 | 0.047619 | 0.000000 | 0.000000 | 16494981 | East China | 2939778 | . 360 rows × 13 columns . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;it&quot;, &quot;specific&quot;, &#39;gdp&#39;,&quot;fdi&quot;]] df_subset . year reg province it specific gdp fdi . 0 1996 | East China | Anhui | 631930 | 147002.0 | 2093.30 | 50661 | . 1 1997 | East China | Anhui | 657860 | 151981.0 | 2347.32 | 43443 | . 2 1998 | East China | Anhui | 889463 | 174930.0 | 2542.96 | 27673 | . 3 1999 | East China | Anhui | 1227364 | 285324.0 | 2712.34 | 26131 | . 4 2000 | East China | Anhui | 1499110 | 195580.0 | 2902.09 | 31847 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 2261631 | 391292.0 | 9705.02 | 498055 | . 356 2004 | East China | Zhejiang | 3162299 | 656175.0 | 11648.70 | 668128 | . 357 2005 | East China | Zhejiang | 2370200 | 656175.0 | 13417.68 | 772000 | . 358 2006 | East China | Zhejiang | 2553268 | 1017303.0 | 15718.47 | 888935 | . 359 2007 | East China | Zhejiang | 2939778 | 844647.0 | 18753.73 | 1036576 | . 360 rows × 7 columns . df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;it&quot;, &quot;specific&quot;, &#39;gdp&#39;,&quot;fdi&quot;] . df_subset . year region province it specific gdp fdi . 0 1996 | East China | Anhui | 631930 | 147002.0 | 2093.30 | 50661 | . 1 1997 | East China | Anhui | 657860 | 151981.0 | 2347.32 | 43443 | . 2 1998 | East China | Anhui | 889463 | 174930.0 | 2542.96 | 27673 | . 3 1999 | East China | Anhui | 1227364 | 285324.0 | 2712.34 | 26131 | . 4 2000 | East China | Anhui | 1499110 | 195580.0 | 2902.09 | 31847 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 2261631 | 391292.0 | 9705.02 | 498055 | . 356 2004 | East China | Zhejiang | 3162299 | 656175.0 | 11648.70 | 668128 | . 357 2005 | East China | Zhejiang | 2370200 | 656175.0 | 13417.68 | 772000 | . 358 2006 | East China | Zhejiang | 2553268 | 1017303.0 | 15718.47 | 888935 | . 359 2007 | East China | Zhejiang | 2939778 | 844647.0 | 18753.73 | 1036576 | . 360 rows × 7 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal.db&#39;) connection = engine.connect() metadata = db.MetaData() . fiscal_table = db.Table(&#39;fiscal_table&#39;, metadata, db.Column(&#39;year&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;region&#39;,db.Integer, nullable=True), db.Column(&#39;province&#39;,db.Integer, nullable=True), db.Column(&#39;it&#39;,db.Integer, nullable=True), db.Column(&#39;specific&#39;,db.Integer, nullable=True), db.Column(&#39;gdp&#39;,db.Integer, nullable=True), db.Column(&#39;fdi&#39;, db.Numeric, nullable=True) ) . metadata.create_all(engine) #Creates the table . df_subset.to_sql(&#39;fiscal_table&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT * FROM fiscal_table LIMIT 10&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, 631930, 147002, 2093.3, 50661), (1997, &#39;East China&#39;, &#39;Anhui&#39;, 657860, 151981, 2347.32, 43443), (1998, &#39;East China&#39;, &#39;Anhui&#39;, 889463, 174930, 2542.96, 27673), (1999, &#39;East China&#39;, &#39;Anhui&#39;, 1227364, 285324, 2712.34, 26131), (2000, &#39;East China&#39;, &#39;Anhui&#39;, 1499110, 195580, 2902.09, 31847), (2001, &#39;East China&#39;, &#39;Anhui&#39;, 2165189, 250898, 3246.71, 33672), (2002, &#39;East China&#39;, &#39;Anhui&#39;, 2404936, 434149, 3519.72, 38375), (2003, &#39;East China&#39;, &#39;Anhui&#39;, 2815820, 619201, 3923.11, 36720), (2004, &#39;East China&#39;, &#39;Anhui&#39;, 3422176, 898441, 4759.3, 54669), (2005, &#39;East China&#39;, &#39;Anhui&#39;, 3874846, 898441, 5350.17, 69000)] . sql = &quot;&quot;&quot; SELECT year , region , province , it --, CURRENT_DATE() FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df.tail(30) . year region province it . 330 2002 | Northwest China | Xinjiang | 2150325 | . 331 2003 | Northwest China | Xinjiang | 2355164 | . 332 2004 | Northwest China | Xinjiang | 2838346 | . 333 2005 | Northwest China | Xinjiang | 3421743 | . 334 2006 | Northwest China | Xinjiang | 4686125 | . 335 2007 | Northwest China | Xinjiang | 5502470 | . 336 1996 | Southwest China | Yunnan | 1374111 | . 337 1997 | Southwest China | Yunnan | 1452425 | . 338 1998 | Southwest China | Yunnan | 1617463 | . 339 1999 | Southwest China | Yunnan | 1888666 | . 340 2000 | Southwest China | Yunnan | 2254281 | . 341 2001 | Southwest China | Yunnan | 2856307 | . 342 2002 | Southwest China | Yunnan | 3035767 | . 343 2003 | Southwest China | Yunnan | 3388449 | . 344 2004 | Southwest China | Yunnan | 3957158 | . 345 2005 | Southwest China | Yunnan | 4280994 | . 346 2006 | Southwest China | Yunnan | 5046865 | . 347 2007 | Southwest China | Yunnan | 6832541 | . 348 1996 | East China | Zhejiang | 740327 | . 349 1997 | East China | Zhejiang | 814253 | . 350 1998 | East China | Zhejiang | 923455 | . 351 1999 | East China | Zhejiang | 1001703 | . 352 2000 | East China | Zhejiang | 1135215 | . 353 2001 | East China | Zhejiang | 1203372 | . 354 2002 | East China | Zhejiang | 1962633 | . 355 2003 | East China | Zhejiang | 2261631 | . 356 2004 | East China | Zhejiang | 3162299 | . 357 2005 | East China | Zhejiang | 2370200 | . 358 2006 | East China | Zhejiang | 2553268 | . 359 2007 | East China | Zhejiang | 2939778 | . #df[&#39;it&#39;].plot(figsize = (12, 8)) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/07/Fiscal_data-sqlitedb-Copy1.html",
            "relUrl": "/2020/10/07/Fiscal_data-sqlitedb-Copy1.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for NLP",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . df=pd.read_csv(&#39;nf_complete.csv&#39;) . df.columns . Index([&#39;Unnamed: 0&#39;, &#39;year&#39;, &#39;title&#39;, &#39;abstract&#39;, &#39;theme&#39;, &#39;China&#39;, &#39;Russia&#39;, &#39;War&#39;, &#39;President&#39;, &#39;US&#39;, &#39;Vietnam&#39;, &#39;Cold War&#39;, &#39;World War&#39;, &#39;Vietnam War&#39;, &#39;Korean War&#39;, &#39;Survey&#39;, &#39;Case Study&#39;, &#39;Trade&#39;, &#39;Humanitarian&#39;, &#39;fixed_effects&#39;, &#39;instrumental_variable&#39;, &#39;regression&#39;, &#39;experimental&#39;], dtype=&#39;object&#39;) . df[[&quot;year&quot;,&quot;title&quot;]] . year title . 0 2000 | &quot;Institutions at the Domestic/International Ne... | . 1 2000 | Born to Lose and Doomed to Survive: State Deat... | . 2 2000 | The significance of “allegiance” in internatio... | . 3 2000 | The significance of “allegiance” in internatio... | . 4 2000 | Truth-Telling and Mythmaking in Post-Soviet Ru... | . ... ... | ... | . 121 2018 | Planning for the Short Haul: Trade Among Belli... | . 122 2018 | Clinging to the Anti-Imperial Mantle: The Repu... | . 123 2018 | The New Navy&#39;s Pacific Wars: Peripheral Confl... | . 124 2018 | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | . 125 2018 | Unexpected Humanitarians: Albania, the U.S. Mi... | . 126 rows × 2 columns . df_subset = df[[&quot;year&quot;, &quot;title&quot;, &quot;abstract&quot;, &quot;theme&quot;, &quot;War&quot;, &#39;Cold War&#39;,&quot;Trade&quot;]] df_subset . year title abstract theme War Cold War Trade . 0 2000 | &quot;Institutions at the Domestic/International Ne... | Civil-military relations are frequently studie... | IR scholarship | 1 | 0 | 0 | . 1 2000 | Born to Lose and Doomed to Survive: State Deat... | Under what conditions do states die, or exit t... | IR scholarship | 1 | 1 | 0 | . 2 2000 | The significance of “allegiance” in internatio... | My dissertation employs original and secondary... | IR scholarship | 1 | 0 | 0 | . 3 2000 | The significance of “allegiance” in internatio... | nThis study revises prevailing interpretation... | Conflit Between States | 0 | 1 | 0 | . 4 2000 | Truth-Telling and Mythmaking in Post-Soviet Ru... | Can distorted and pernicious ideas about histo... | Conflict Between States | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 121 2018 | Planning for the Short Haul: Trade Among Belli... | In times of war, why do belligerents continue ... | Conflict between states | 1 | 0 | 1 | . 122 2018 | Clinging to the Anti-Imperial Mantle: The Repu... | My dissertation project, Clinging to the Anti-... | Cold War | 0 | 1 | 0 | . 123 2018 | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Using a transnational methodology and sources ... | Military History | 1 | 0 | 0 | . 124 2018 | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | There is a dilemma at the heart of coercion. S... | IR Scholarship | 0 | 0 | 1 | . 125 2018 | Unexpected Humanitarians: Albania, the U.S. Mi... | Using archives and oral history, this disserta... | Military History | 0 | 0 | 0 | . 126 rows × 7 columns . df_subset.columns = [&quot;year&quot;, &quot;title&quot;, &quot;abstract&quot;, &quot;theme&quot;, &quot;War&quot;, &#39;Cold War&#39;,&quot;Trade&quot;] . df_subset . year title abstract theme War Cold War Trade . 0 2000 | &quot;Institutions at the Domestic/International Ne... | Civil-military relations are frequently studie... | IR scholarship | 1 | 0 | 0 | . 1 2000 | Born to Lose and Doomed to Survive: State Deat... | Under what conditions do states die, or exit t... | IR scholarship | 1 | 1 | 0 | . 2 2000 | The significance of “allegiance” in internatio... | My dissertation employs original and secondary... | IR scholarship | 1 | 0 | 0 | . 3 2000 | The significance of “allegiance” in internatio... | nThis study revises prevailing interpretation... | Conflit Between States | 0 | 1 | 0 | . 4 2000 | Truth-Telling and Mythmaking in Post-Soviet Ru... | Can distorted and pernicious ideas about histo... | Conflict Between States | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 121 2018 | Planning for the Short Haul: Trade Among Belli... | In times of war, why do belligerents continue ... | Conflict between states | 1 | 0 | 1 | . 122 2018 | Clinging to the Anti-Imperial Mantle: The Repu... | My dissertation project, Clinging to the Anti-... | Cold War | 0 | 1 | 0 | . 123 2018 | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Using a transnational methodology and sources ... | Military History | 1 | 0 | 0 | . 124 2018 | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | There is a dilemma at the heart of coercion. S... | IR Scholarship | 0 | 0 | 1 | . 125 2018 | Unexpected Humanitarians: Albania, the U.S. Mi... | Using archives and oral history, this disserta... | Military History | 0 | 0 | 0 | . 126 rows × 7 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///nf_nlp.db&#39;) connection = engine.connect() metadata = db.MetaData() . nf_nlp_table = db.Table(&#39;nf_nlp_table&#39;, metadata, db.Column(&#39;year&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;title&#39;,db.String, nullable=True), db.Column(&#39;abstract&#39;,db.String, nullable=True), db.Column(&#39;theme&#39;,db.String, nullable=True), db.Column(&#39;War&#39;,db.Integer, nullable=True), db.Column(&#39;Cold War&#39;,db.Integer, nullable=True), db.Column(&#39;Trade&#39;, db.Integer, nullable=True) ) . metadata.create_all(engine) #Creates the table . nf_nlp_table . Table(&#39;nf_nlp_table&#39;, MetaData(bind=None), Column(&#39;year&#39;, Integer(), table=&lt;nf_nlp_table&gt;), Column(&#39;title&#39;, String(), table=&lt;nf_nlp_table&gt;), Column(&#39;abstract&#39;, String(), table=&lt;nf_nlp_table&gt;), Column(&#39;theme&#39;, String(), table=&lt;nf_nlp_table&gt;), Column(&#39;War&#39;, Integer(), table=&lt;nf_nlp_table&gt;), Column(&#39;Cold War&#39;, Integer(), table=&lt;nf_nlp_table&gt;), Column(&#39;Trade&#39;, Integer(), table=&lt;nf_nlp_table&gt;), schema=None) . df_subset.to_sql(&#39;nf_nlp_table&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT year, theme, title FROM nf_nlp_table LIMIT 10&quot;).fetchall() . [(2000, &#39;IR scholarship&#39;, &#39;&#34;Institutions at the Domestic/International Nexus: the political-military origins of military effectiveness, strategic integration and war&#39;), (2000, &#39;IR scholarship&#39;, &#39;Born to Lose and Doomed to Survive: State Death and Survival in the International System&#39;), (2000, &#39;IR scholarship&#39;, &#39;The significance of “allegiance” in international relations&#39;), (2000, &#39;Conflit Between States&#39;, &#39;The significance of “allegiance” in international relations&#39;), (2000, &#39;Conflict Between States&#39;, &#39;Truth-Telling and Mythmaking in Post-Soviet Russia: Historical Ideas, Mass Education, and Interstate Conflict&#39;), (2000, &#39;Domestic Military History&#39;, &#39;Building a Cape Fear Metropolis: Fort Bragg, Fayetteville, and the Sandhills of North Carolina&#39;), (2000, &#39;Culture&#39;, &#39;The Glories and the Sadness: Shaping the national Memory of the First World War in Great Britain, Canada and Australia&#39;), (2000, &#39;Culture / Peace Process&#39;, &#39;What leads longstanding adversaries to engage in conflict resolution&#39;), (2001, &#39;Military History&#39;, &#39;A School for the Nation: Military Institutions and the Boundaries of Nationality&#39;), (2001, &#39;Military History&#39;, &#34;The &#39;American Century&#39; Army: The Origins of the U.S. Cold War Army, 1949-1959&#34;)] . sql = &quot;&quot;&quot; SELECT year , theme , title FROM nf_nlp_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df.tail(30) . year theme title . 96 2014 | IR Scholarship | “Multiparty Mediation: Identifying Characteris... | . 97 2014 | IR Scholarship | The Justice Dilemma: International Criminal Ac... | . 98 2014 | IR Scholarship | Beyond Revolution and Repression: U.S. Foreign... | . 99 2014 | IR Scholarship | Protection States Trust?: Major Power Patronag... | . 100 2014 | Nuclear Weapons | The Constraining Power of the Nuclear Nonproli... | . 101 2015 | Military History | Selling Her the Military: Recruiting Women int... | . 102 2015 | IR Scholarship | American Evangelicals, Israel, and Modern Chri... | . 103 2015 | Non-state | Who Can Keep the Peace? Insurgent Organization... | . 104 2015 | IR Scholarship | Credibility in Crisis: The Role of Leadership ... | . 105 2015 | IR Scholarship | Evaluating the Changing of the Guards: Survey ... | . 106 2015 | Soviet Union | Extracting the Eagle’s Talons: The Soviet Unio... | . 107 2015 | IR Scholarship | The Control War: Communist Revolutionary Warfa... | . 108 2015 | Nuclear Weapons | Nuclear Weapons and Foreign Policy | . 109 2016 | Civ-Mil | Securing Control and Controlling Security: Civ... | . 110 2016 | Military History | Digging for Victory: The Stalinist State’s Mob... | . 111 2016 | Non-state | Persuading Power: Insurgent Diplomacy and the ... | . 112 2016 | Conflict between states | A Prelude to Violence? The Effect of Nationali... | . 113 2016 | Conflict between states | Engaging the ‘Evil Empire’: East – West Relati... | . 114 2017 | IR Scholarship | More Talk, Less Action: Why Costless Diplomacy... | . 115 2017 | Cold War | Experiments in Peace: Asian Neutralism, Human ... | . 116 2017 | IR Scholarship | Fully Committed? Religiously Committed State P... | . 117 2017 | Military History | Straddling the Threshold of Two Worlds: Soldie... | . 118 2017 | Military History | U.S. Army’s Investigation and Adjudication of ... | . 119 2017 | IR Scholarship | Grand Strategic Crucibles: The Lasting Effects... | . 120 2018 | Nuclear Weapons | Trust in International Politics: The Role of L... | . 121 2018 | Conflict between states | Planning for the Short Haul: Trade Among Belli... | . 122 2018 | Cold War | Clinging to the Anti-Imperial Mantle: The Repu... | . 123 2018 | Military History | The New Navy&#39;s Pacific Wars: Peripheral Confl... | . 124 2018 | IR Scholarship | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | . 125 2018 | Military History | Unexpected Humanitarians: Albania, the U.S. Mi... | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/06/NLP-sqlitedb.html",
            "relUrl": "/2020/10/06/NLP-sqlitedb.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . start = pd.to_datetime(&#39;2020-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Open&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Volume&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f2d82804f60&gt; . FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . stocks = pd.concat([MSFT_stock[&#39;Open&#39;], ZOOM_stock[&#39;Open&#39;], SNOW_stock[&#39;Open&#39;], FXAIX_stock[&#39;Open&#39;]], axis = 1) . stocks.reset_index(level=0, inplace=True) . stocks . Date Open Open Open Open . 0 2020-01-02 | 158.779999 | 68.800003 | NaN | 112.980003 | . 1 2020-01-03 | 158.320007 | 67.620003 | NaN | 112.190002 | . 2 2020-01-06 | 157.080002 | 66.629997 | NaN | 112.589996 | . 3 2020-01-07 | 159.320007 | 70.290001 | NaN | 112.290001 | . 4 2020-01-08 | 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | ... | . 187 2020-09-29 | 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 188 2020-09-30 | 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 189 2020-10-01 | 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 190 2020-10-02 | 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 2020-10-05 | 207.220001 | 493.970001 | 231.970001 | NaN | . 192 rows × 5 columns . stocks.columns = [&#39;Date&#39;,&#39;MSFT_stock&#39;,&#39;ZOOM_stock&#39;,&#39;SNOW_stock&#39;,&#39;FXAIX_stock&#39;] . stocks . Date MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . 0 2020-01-02 | 158.779999 | 68.800003 | NaN | 112.980003 | . 1 2020-01-03 | 158.320007 | 67.620003 | NaN | 112.190002 | . 2 2020-01-06 | 157.080002 | 66.629997 | NaN | 112.589996 | . 3 2020-01-07 | 159.320007 | 70.290001 | NaN | 112.290001 | . 4 2020-01-08 | 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | ... | . 187 2020-09-29 | 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 188 2020-09-30 | 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 189 2020-10-01 | 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 190 2020-10-02 | 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 2020-10-05 | 207.220001 | 493.970001 | 231.970001 | NaN | . 192 rows × 5 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///stocks.db&#39;) connection = engine.connect() metadata = db.MetaData() . stocks_table = db.Table(&#39;stocks_table&#39;, metadata, db.Column(&#39;Date&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;MSFT_stock&#39;,db.Integer, nullable=True), db.Column(&#39;ZOOM_stock&#39;,db.Integer, nullable=True), db.Column(&#39;SNOW_stock&#39;,db.Integer, nullable=True), db.Column(&#39;FXAIX_stock&#39;, db.Numeric, nullable=True) ) . metadata.create_all(engine) #Creates the table . stocks_table . Table(&#39;stocks_table&#39;, MetaData(bind=None), Column(&#39;Date&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;MSFT_stock&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;ZOOM_stock&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;SNOW_stock&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;FXAIX_stock&#39;, Numeric(), table=&lt;stocks_table&gt;), schema=None) . stocks.to_sql(&#39;stocks_table&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT * FROM stocks_table LIMIT 10&quot;).fetchall() . [(&#39;2020-01-02 00:00:00.000000&#39;, 158.77999877929688, 68.80000305175781, None, 112.9800033569336), (&#39;2020-01-03 00:00:00.000000&#39;, 158.32000732421875, 67.62000274658203, None, 112.19000244140625), (&#39;2020-01-06 00:00:00.000000&#39;, 157.0800018310547, 66.62999725341797, None, 112.58999633789062), (&#39;2020-01-07 00:00:00.000000&#39;, 159.32000732421875, 70.29000091552734, None, 112.29000091552734), (&#39;2020-01-08 00:00:00.000000&#39;, 158.92999267578125, 71.80999755859375, None, 112.83999633789062), (&#39;2020-01-09 00:00:00.000000&#39;, 161.83999633789062, 73.98999786376953, None, 113.62000274658203), (&#39;2020-01-10 00:00:00.000000&#39;, 162.82000732421875, 73.08000183105469, None, 113.30000305175781), (&#39;2020-01-13 00:00:00.000000&#39;, 161.75999450683594, 73.88999938964844, None, 114.08999633789062), (&#39;2020-01-14 00:00:00.000000&#39;, 163.38999938964844, 74.31999969482422, None, 113.93000030517578), (&#39;2020-01-15 00:00:00.000000&#39;, 162.6199951171875, 73.27999877929688, None, 114.13999938964844)] . sql = &quot;&quot;&quot; SELECT DATE(date) AS DATE , FXAIX_stock , MSFT_stock , SNOW_stock , row_number() OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_NBR , COUNT(*) OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_CNT , CASE WHEN FXAIX_stock &gt;= 120 THEN &#39;SNP_High&#39; ELSE &#39;SNP_low&#39; END AS SNP_HIGH_LOW FROM stocks_table --WHERE FXAIX_stock &gt;= 120 &quot;&quot;&quot; cnxn = connection . stocks = pd.read_sql(sql, cnxn) . stocks.tail(30) . DATE FXAIX_stock MSFT_stock SNOW_stock REC_NBR REC_CNT SNP_HIGH_LOW . 162 2020-08-24 | 119.260002 | 214.789993 | NaN | 1 | 1 | SNP_low | . 163 2020-08-25 | 119.690002 | 213.100006 | NaN | 1 | 1 | SNP_low | . 164 2020-08-26 | 120.910004 | 217.880005 | NaN | 1 | 1 | SNP_High | . 165 2020-08-27 | 121.120003 | 222.889999 | NaN | 1 | 1 | SNP_High | . 166 2020-08-28 | 121.940002 | 228.179993 | NaN | 1 | 1 | SNP_High | . 167 2020-08-31 | 121.690002 | 227.000000 | NaN | 1 | 1 | SNP_High | . 168 2020-09-01 | 122.610001 | 225.509995 | NaN | 1 | 1 | SNP_High | . 169 2020-09-02 | 124.510002 | 227.970001 | NaN | 1 | 1 | SNP_High | . 170 2020-09-03 | 120.150002 | 229.270004 | NaN | 1 | 1 | SNP_High | . 171 2020-09-04 | 119.180000 | 215.100006 | NaN | 1 | 1 | SNP_low | . 172 2020-09-08 | 115.879997 | 206.500000 | NaN | 1 | 1 | SNP_low | . 173 2020-09-09 | 118.220001 | 207.600006 | NaN | 1 | 1 | SNP_low | . 174 2020-09-10 | 116.139999 | 213.399994 | NaN | 1 | 1 | SNP_low | . 175 2020-09-11 | 116.209999 | 207.199997 | NaN | 1 | 1 | SNP_low | . 176 2020-09-14 | 117.730003 | 204.240005 | NaN | 1 | 1 | SNP_low | . 177 2020-09-15 | 118.339996 | 208.419998 | NaN | 1 | 1 | SNP_low | . 178 2020-09-16 | 117.800003 | 210.619995 | 245.000000 | 1 | 1 | SNP_low | . 179 2020-09-17 | 116.809998 | 200.050003 | 230.759995 | 1 | 1 | SNP_low | . 180 2020-09-18 | 115.510002 | 202.800003 | 235.000000 | 1 | 1 | SNP_low | . 181 2020-09-21 | 114.180000 | 197.190002 | 230.000000 | 1 | 1 | SNP_low | . 182 2020-09-22 | 115.379997 | 205.059998 | 238.500000 | 1 | 1 | SNP_low | . 183 2020-09-23 | 112.650002 | 207.899994 | 243.500000 | 1 | 1 | SNP_low | . 184 2020-09-24 | 112.989998 | 199.850006 | 213.509995 | 1 | 1 | SNP_low | . 185 2020-09-25 | 114.800003 | 203.550003 | 228.119995 | 1 | 1 | SNP_low | . 186 2020-09-28 | 116.650002 | 210.880005 | 235.929993 | 1 | 1 | SNP_low | . 187 2020-09-29 | 116.099998 | 209.350006 | 255.000000 | 1 | 1 | SNP_low | . 188 2020-09-30 | 117.070000 | 207.729996 | 261.500000 | 1 | 1 | SNP_low | . 189 2020-10-01 | 117.699997 | 213.490005 | 255.250000 | 1 | 1 | SNP_low | . 190 2020-10-02 | 116.120003 | 208.000000 | 232.440002 | 1 | 1 | SNP_low | . 191 2020-10-05 | NaN | 207.220001 | 231.970001 | 1 | 1 | SNP_low | . stocks[&#39;FXAIX_stock&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 in 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 in 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/05/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "relUrl": "/2020/10/05/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020 with pandas_datareader and writing to at sqlite database",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . # # start = datetime.datetime(2016, 1, 1) # # end = datetime.datetime(2017, 5, 17) # start = datetime.datetime(2010, 1, 1) # end = datetime.datetime(2020, 1, 1) . start = pd.to_datetime(&#39;2020-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Open&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Volume&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fae549b8ba8&gt; . FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . stocks = pd.concat([MSFT_stock[&#39;Open&#39;], ZOOM_stock[&#39;Open&#39;], SNOW_stock[&#39;Open&#39;], FXAIX_stock[&#39;Open&#39;]], axis = 1) . stocks . Open Open Open Open . Date . 2020-01-02 158.779999 | 68.800003 | NaN | 112.980003 | . 2020-01-03 158.320007 | 67.620003 | NaN | 112.190002 | . 2020-01-06 157.080002 | 66.629997 | NaN | 112.589996 | . 2020-01-07 159.320007 | 70.290001 | NaN | 112.290001 | . 2020-01-08 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | . 2020-09-28 210.880005 | 502.410004 | 235.929993 | 116.650002 | . 2020-09-29 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 2020-09-30 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 2020-10-01 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 2020-10-02 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 rows × 4 columns . stocks.columns = [&#39;MSFT_stock&#39;,&#39;ZOOM_stock&#39;,&#39;SNOW_stock&#39;,&#39;FXAIX_stock&#39;] . stocks . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 158.779999 | 68.800003 | NaN | 112.980003 | . 2020-01-03 158.320007 | 67.620003 | NaN | 112.190002 | . 2020-01-06 157.080002 | 66.629997 | NaN | 112.589996 | . 2020-01-07 159.320007 | 70.290001 | NaN | 112.290001 | . 2020-01-08 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | . 2020-09-28 210.880005 | 502.410004 | 235.929993 | 116.650002 | . 2020-09-29 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 2020-09-30 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 2020-10-01 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 2020-10-02 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 rows × 4 columns . mean_daily_ret = stocks.pct_change(1).mean() mean_daily_ret . MSFT_stock 0.001751 ZOOM_stock 0.011973 SNOW_stock -0.002546 FXAIX_stock 0.000440 dtype: float64 . stocks.pct_change(1).corr() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . MSFT_stock 1.000000 | 0.209041 | 0.661827 | 0.382807 | . ZOOM_stock 0.209041 | 1.000000 | 0.095052 | 0.127526 | . SNOW_stock 0.661827 | 0.095052 | 1.000000 | 0.292117 | . FXAIX_stock 0.382807 | 0.127526 | 0.292117 | 1.000000 | . stock_normed = stocks/stocks.iloc[0] stock_normed.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fae54a74a90&gt; . stock_daily_ret = stocks.pct_change(1) stock_daily_ret.head() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 NaN | NaN | NaN | NaN | . 2020-01-03 -0.002897 | -0.017151 | NaN | -0.006992 | . 2020-01-06 -0.007832 | -0.014641 | NaN | 0.003565 | . 2020-01-07 0.014260 | 0.054930 | NaN | -0.002664 | . 2020-01-08 -0.002448 | 0.021625 | NaN | 0.004898 | . log_ret = np.log(stocks / stocks.shift(1)) log_ret.head() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 NaN | NaN | NaN | NaN | . 2020-01-03 -0.002901 | -0.017300 | NaN | -0.007017 | . 2020-01-06 -0.007863 | -0.014749 | NaN | 0.003559 | . 2020-01-07 0.014160 | 0.053475 | NaN | -0.002668 | . 2020-01-08 -0.002451 | 0.021394 | NaN | 0.004886 | . log_ret.hist(bins = 100, figsize = (12, 6)); plt.tight_layout() . log_ret.describe().transpose() . count mean std min 25% 50% 75% max . MSFT_stock 190.0 | 0.001421 | 0.025752 | -0.087821 | -0.012115 | 0.004000 | 0.016980 | 0.081248 | . ZOOM_stock 190.0 | 0.010279 | 0.056461 | -0.142569 | -0.017014 | 0.011119 | 0.035968 | 0.368600 | . SNOW_stock 12.0 | -0.004386 | 0.063753 | -0.131433 | -0.033113 | 0.019477 | 0.034320 | 0.077728 | . FXAIX_stock 190.0 | 0.000144 | 0.024461 | -0.127150 | -0.007774 | 0.002806 | 0.010082 | 0.089894 | . log_ret.mean() * 252 . MSFT_stock 0.358130 ZOOM_stock 2.590236 SNOW_stock -1.105148 FXAIX_stock 0.036359 dtype: float64 . log_ret.cov() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . MSFT_stock 0.000663 | 0.000323 | 0.001291 | 0.000245 | . ZOOM_stock 0.000323 | 0.003188 | 0.000290 | 0.000184 | . SNOW_stock 0.001291 | 0.000290 | 0.004064 | 0.000231 | . FXAIX_stock 0.000245 | 0.000184 | 0.000231 | 0.000598 | . # Set seed (optional) np.random.seed(101) # Stock Columns print(&#39;Stocks&#39;) print(stocks.columns) print(&#39; n&#39;) # Create Random Weights print(&#39;Creating Random Weights&#39;) weights = np.array(np.random.random(4)) print(weights) print(&#39; n&#39;) # Rebalance Weights print(&#39;Rebalance to sum to 1.0&#39;) weights = weights / np.sum(weights) print(weights) print(&#39; n&#39;) # Expected Return print(&#39;Expected Portfolio Return&#39;) exp_ret = np.sum(log_ret.mean() * weights) *252 print(exp_ret) print(&#39; n&#39;) # Expected Variance print(&#39;Expected Volatility&#39;) exp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) print(exp_vol) print(&#39; n&#39;) # Sharpe Ratio SR = exp_ret/exp_vol print(&#39;Sharpe Ratio&#39;) print(SR) . Stocks Index([&#39;MSFT_stock&#39;, &#39;ZOOM_stock&#39;, &#39;SNOW_stock&#39;, &#39;FXAIX_stock&#39;], dtype=&#39;object&#39;) Creating Random Weights [0.51639863 0.57066759 0.02847423 0.17152166] Rebalance to sum to 1.0 [0.40122278 0.44338777 0.02212343 0.13326603] Expected Portfolio Return 1.272564336318203 Expected Volatility 0.4864366288684257 Sharpe Ratio 2.6160948020680697 . num_ports = 15000 all_weights = np.zeros((num_ports, len(stocks.columns))) ret_arr = np.zeros(num_ports) vol_arr = np.zeros(num_ports) sharpe_arr = np.zeros(num_ports) for ind in range(num_ports): # Create Random Weights weights = np.array(np.random.random(4)) # Rebalance Weights weights = weights / np.sum(weights) # Save Weights all_weights[ind,:] = weights # Expected Return ret_arr[ind] = np.sum((log_ret.mean() * weights) *252) # Expected Variance vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) # Sharpe Ratio sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind] . sharpe_arr.max() . 2.8667995807841824 . sharpe_arr.argmax() . 5483 . all_weights[10619,:] . array([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01]) . max_sr_ret = ret_arr[1419] max_sr_vol = vol_arr[1419] . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add red dot for max SR plt.scatter(max_sr_vol, max_sr_ret, c = &#39;red&#39;, s = 50, edgecolors = &#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7fae54366048&gt; . def get_ret_vol_sr(weights): &quot;&quot;&quot; Takes in weights, returns array or return,volatility, sharpe ratio &quot;&quot;&quot; weights = np.array(weights) ret = np.sum(log_ret.mean() * weights) * 252 vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) sr = ret/vol return np.array([ret, vol, sr]) from scipy.optimize import minimize import numpy as np def neg_sharpe(weights): return get_ret_vol_sr(weights)[2] * -1 # Contraints def check_sum(weights): &#39;&#39;&#39; Returns 0 if sum of weights is 1.0 &#39;&#39;&#39; return np.sum(weights) - 1 # By convention of minimize function it should be a function that returns zero for conditions cons = ({&#39;type&#39; : &#39;eq&#39;, &#39;fun&#39;: check_sum}) # 0-1 bounds for each weight bounds = ((0, 1), (0, 1), (0, 1), (0, 1)) # Initial Guess (equal distribution) init_guess = [0.25, 0.25, 0.25, 0.25] # Sequential Least Squares opt_results = minimize(neg_sharpe, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) opt_results . fun: -2.8998675936504807 jac: array([-3.57061625e-04, 6.75618649e-05, 1.98669076e+00, 1.90789163e-01]) message: &#39;Optimization terminated successfully.&#39; nfev: 42 nit: 7 njev: 7 status: 0 success: True x: array([1.59222977e-01, 8.40777023e-01, 7.68699340e-16, 0.00000000e+00]) . opt_results.x get_ret_vol_sr(opt_results.x) . array([2.23483308, 0.77066728, 2.89986759]) . frontier_y = np.linspace(0, 0.3, 100) . def minimize_volatility(weights): return get_ret_vol_sr(weights)[1] frontier_volatility = [] for possible_return in frontier_y: # function for return cons = ({&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: check_sum}, {&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: lambda w: get_ret_vol_sr(w)[0] - possible_return}) result = minimize(minimize_volatility, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) frontier_volatility.append(result[&#39;fun&#39;]) . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add frontier line plt.plot(frontier_volatility, frontier_y, &#39;g--&#39;, linewidth = 3) . [&lt;matplotlib.lines.Line2D at 0x7fae542ed9e8&gt;] . stocks[&#39;FXAIX_stock&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 in 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 in 2020 Value&#39;) . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . stocks . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 158.779999 | 68.800003 | NaN | 112.980003 | . 2020-01-03 158.320007 | 67.620003 | NaN | 112.190002 | . 2020-01-06 157.080002 | 66.629997 | NaN | 112.589996 | . 2020-01-07 159.320007 | 70.290001 | NaN | 112.290001 | . 2020-01-08 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | . 2020-09-28 210.880005 | 502.410004 | 235.929993 | 116.650002 | . 2020-09-29 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 2020-09-30 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 2020-10-01 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 2020-10-02 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 rows × 4 columns . engine = db.create_engine(&#39;sqlite:///stocks.sqlite&#39;) . connection = engine.connect() metadata = db.MetaData() . stocks.to_sql(&#39;stocks&#39;, con=engine, if_exists=&#39;append&#39;, index=True) . engine.execute(&quot;SELECT * FROM stocks LIMIT 10&quot;).fetchall() . [(158.77999877929688, 68.80000305175781, None, 112.9800033569336), (158.32000732421875, 67.62000274658203, None, 112.19000244140625), (157.0800018310547, 66.62999725341797, None, 112.58999633789062), (159.32000732421875, 70.29000091552734, None, 112.29000091552734), (158.92999267578125, 71.80999755859375, None, 112.83999633789062), (161.83999633789062, 73.98999786376953, None, 113.62000274658203), (162.82000732421875, 73.08000183105469, None, 113.30000305175781), (161.75999450683594, 73.88999938964844, None, 114.08999633789062), (163.38999938964844, 74.31999969482422, None, 113.93000030517578), (162.6199951171875, 73.27999877929688, None, 114.13999938964844)] . engine.execute(&quot;SELECT FXAIX_stock FROM stocks LIMIT 10&quot;).fetchall() . [(112.9800033569336,), (112.19000244140625,), (112.58999633789062,), (112.29000091552734,), (112.83999633789062,), (113.62000274658203,), (113.30000305175781,), (114.08999633789062,), (113.93000030517578,), (114.13999938964844,)] . # df = pd.DataFrame({&#39;name&#39; : [&#39;User 1&#39;, &#39;User 2&#39;, &#39;User 3&#39;]}) # df # df.to_sql(&#39;users&#39;, con=engine) # engine.execute(&quot;SELECT * FROM users&quot;).fetchall() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/04/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "relUrl": "/2020/10/04/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Friday Links",
            "content": "Escaping strings in Bash using !:q / | . bash # This string ‘has single’ “and double” quotes and a $ bash !:q .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/02/links2.html",
            "relUrl": "/links/2020/10/02/links2.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "Friday Books",
            "content": "21 Lessons for the 21st Century/ . | Outliers: The Story of Success/ . | David and Goliath: Underdogs, Misfits, and the Art of Battling Giants / . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/books/2020/10/02/links.html",
            "relUrl": "/books/2020/10/02/links.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post46": {
            "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite",
            "content": "This post includes code adapted from SQLAlchemy Example. . import sqlalchemy as db import sqlite3 import pandas as pd . from sqlalchemy import Column, Integer, String, ForeignKey, create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import relationship, backref, sessionmaker, joinedload # For this example we will use an in-memory sqlite DB. # Let&#39;s also configure it to echo everything it does to the screen. engine = create_engine(&#39;sqlite:///:memory:&#39;, echo=True) . # The base class which our objects will be defined on. Base = declarative_base() # Our User object, mapped to the &#39;users&#39; table class User(Base): __tablename__ = &#39;users&#39; # Every SQLAlchemy table should have a primary key named &#39;id&#39; id = Column(Integer, primary_key=True) name = Column(String) fullname = Column(String) password = Column(String) # Lets us print out a user object conveniently. def __repr__(self): return &quot;&lt;User(name=&#39;%s&#39;, fullname=&#39;%s&#39;, password&#39;%s&#39;)&gt;&quot; % ( self.name, self.fullname, self.password) . # The Address object stores the addresses # of a user in the &#39;adressess&#39; table. class Address(Base): __tablename__ = &#39;addresses&#39; id = Column(Integer, primary_key=True) email_address = Column(String, nullable=False) # Since we have a 1:n relationship, we need to store a foreign key # to the users table. user_id = Column(Integer, ForeignKey(&#39;users.id&#39;)) # Defines the 1:n relationship between users and addresses. # Also creates a backreference which is accessible from a User object. user = relationship(&quot;User&quot;, backref=backref(&#39;addresses&#39;)) # Lets us print out an address object conveniently. def __repr__(self): return &quot;&lt;Address(email_address=&#39;%s&#39;)&gt;&quot; % self.email_address . # Create all tables by issuing CREATE TABLE commands to the DB. Base.metadata.create_all(engine) # Creates a new session to the database by using the engine we described. Session = sessionmaker(bind=engine) session = Session() # Let&#39;s create a user and add two e-mail addresses to that user. example_user = User(name=&#39;example&#39;, fullname=&#39;example last_name_example&#39;, password=&#39;examplepassword&#39;) example_user.addresses = [Address(email_address=&#39;example@gmail.com&#39;), Address(email_address=&#39;example@yahoo.com&#39;)] # Let&#39;s add the user and its addresses we&#39;ve created to the DB and commit. session.add(example_user) session.commit() # Now let&#39;s query the user that has the e-mail address ed@google.com # SQLAlchemy will construct a JOIN query automatically. user_by_email = session.query(User) .filter(Address.email_address==&#39;example@gmail.com&#39;) .first() . 2020-10-02 08:55:48,507 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(&#34;users&#34;) 2020-10-02 08:55:48,508 INFO sqlalchemy.engine.base.Engine () 2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(&#34;addresses&#34;) 2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine () 2020-10-02 08:55:48,513 INFO sqlalchemy.engine.base.Engine BEGIN (implicit) 2020-10-02 08:55:48,514 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name, fullname, password) VALUES (?, ?, ?) 2020-10-02 08:55:48,515 INFO sqlalchemy.engine.base.Engine (&#39;example&#39;, &#39;example last_name_example&#39;, &#39;examplepassword&#39;) 2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?) 2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine (&#39;example@gmail.com&#39;, 1) 2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?) 2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine (&#39;example@yahoo.com&#39;, 1) 2020-10-02 08:55:48,520 INFO sqlalchemy.engine.base.Engine COMMIT 2020-10-02 08:55:48,522 INFO sqlalchemy.engine.base.Engine BEGIN (implicit) 2020-10-02 08:55:48,523 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password FROM users, addresses WHERE addresses.email_address = ? LIMIT ? OFFSET ? 2020-10-02 08:55:48,524 INFO sqlalchemy.engine.base.Engine (&#39;example@gmail.com&#39;, 1, 0) . print(user_by_email) . &lt;User(name=&#39;example&#39;, fullname=&#39;example last_name_example&#39;, password&#39;examplepassword&#39;)&gt; . # This will cause an additional query by lazy loading from the DB. print(user_by_email.addresses) . 2020-10-02 08:55:53,081 INFO sqlalchemy.engine.base.Engine SELECT addresses.id AS addresses_id, addresses.email_address AS addresses_email_address, addresses.user_id AS addresses_user_id FROM addresses WHERE ? = addresses.user_id 2020-10-02 08:55:53,083 INFO sqlalchemy.engine.base.Engine (1,) [&lt;Address(email_address=&#39;example@gmail.com&#39;)&gt;, &lt;Address(email_address=&#39;example@yahoo.com&#39;)&gt;] . # To avoid querying again when getting all addresses of a user, # we use the joinedload option. SQLAlchemy will load all results and hide # the duplicate entries from us, so we can then get for # the user&#39;s addressess without an additional query to the DB. user_by_email = session.query(User) .filter(Address.email_address==&#39;example@gmail.com&#39;) .options(joinedload(User.addresses)) .first() . 2020-10-02 08:56:04,305 INFO sqlalchemy.engine.base.Engine SELECT anon_1.users_id AS anon_1_users_id, anon_1.users_name AS anon_1_users_name, anon_1.users_fullname AS anon_1_users_fullname, anon_1.users_password AS anon_1_users_password, addresses_1.id AS addresses_1_id, addresses_1.email_address AS addresses_1_email_address, addresses_1.user_id AS addresses_1_user_id FROM (SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password FROM users, addresses WHERE addresses.email_address = ? LIMIT ? OFFSET ?) AS anon_1 LEFT OUTER JOIN addresses AS addresses_1 ON anon_1.users_id = addresses_1.user_id 2020-10-02 08:56:04,306 INFO sqlalchemy.engine.base.Engine (&#39;example@gmail.com&#39;, 1, 0) . print(user_by_email) . &lt;User(name=&#39;example&#39;, fullname=&#39;example last_name_example&#39;, password&#39;examplepassword&#39;)&gt; . print(user_by_email.addresses) . [&lt;Address(email_address=&#39;example@gmail.com&#39;)&gt;, &lt;Address(email_address=&#39;example@yahoo.com&#39;)&gt;] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/02/databases_sqllite_sqlalchemy_27-Copy1.html",
            "relUrl": "/2020/10/02/databases_sqllite_sqlalchemy_27-Copy1.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post47": {
            "title": "Thursday Links",
            "content": "Test-Driven Data Science Development - From TWD. Using pytest. . | Top 10 Essential Data Science Topics to Real-World Application from the Industry Perspectives - from the BBC. Review of Data Science and Causal inference. . | Parser for Kindle My Clippings.txt file . | Forecasting Newsletter. . | High Replicability of Newly-Discovered Social-behavioral Findings is Achievable - ‘Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal’ . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/01/links.html",
            "relUrl": "/links/2020/10/01/links.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post48": {
            "title": "Stock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_YIELD_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-31 1.98 | . 2010-02-28 2.03 | . 2010-03-31 1.90 | . 2010-04-30 1.83 | . 2010-05-31 1.95 | . ... ... | . 2020-07-01 1.91 | . 2020-07-31 1.86 | . 2020-08-31 1.76 | . 2020-09-01 1.69 | . 2020-09-30 1.69 | . 136 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_PE_RATIO_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 20.70 | . 2010-02-01 18.91 | . 2010-03-01 18.91 | . 2010-04-01 19.01 | . 2010-05-01 17.30 | . ... ... | . 2020-07-01 27.57 | . 2020-07-31 28.12 | . 2020-08-01 29.16 | . 2020-08-31 30.09 | . 2020-09-01 30.32 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_PE_RATIO_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 20.70 | . 2010-02-01 18.91 | . 2010-03-01 18.91 | . 2010-04-01 19.01 | . 2010-05-01 17.30 | . ... ... | . 2020-07-01 27.57 | . 2020-07-31 28.12 | . 2020-08-01 29.16 | . 2020-08-31 30.09 | . 2020-09-01 30.32 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_EARNINGS_YIELD_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 4.83 | . 2010-02-01 5.29 | . 2010-03-01 5.29 | . 2010-04-01 5.26 | . 2010-05-01 5.78 | . ... ... | . 2020-07-01 3.63 | . 2020-07-31 3.56 | . 2020-08-01 3.43 | . 2020-08-31 3.32 | . 2020-09-01 3.30 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_INFLADJ_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 1343.51 | . 2010-02-01 1302.03 | . 2010-03-01 1371.58 | . 2010-04-01 1423.00 | . 2010-05-01 1336.08 | . ... ... | . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-31 26.59 | . 2010-02-28 26.38 | . 2010-03-31 26.08 | . 2010-04-30 26.09 | . 2010-05-31 26.12 | . ... ... | . 2020-02-29 59.23 | . 2020-03-31 59.81 | . 2020-04-30 60.25 | . 2020-05-31 60.28 | . 2020-06-30 59.99 | . 126 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_YEAR&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-12-31 26.87 | . 2011-12-31 30.34 | . 2012-12-31 35.26 | . 2013-12-31 38.90 | . 2014-12-31 43.52 | . 2015-12-31 47.53 | . 2016-12-31 49.05 | . 2017-12-31 51.43 | . 2018-12-31 55.43 | . 2019-03-31 55.35 | . 2019-06-30 56.17 | . 2019-09-30 57.32 | . 2019-12-31 58.72 | . 2020-03-31 59.18 | . 2020-06-30 59.99 | . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_GROWTH_YEAR&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-12-31 1.45 | . 2011-12-31 16.26 | . 2012-12-31 18.25 | . 2013-12-31 11.99 | . 2014-12-31 12.72 | . 2015-12-31 10.00 | . 2016-12-31 5.33 | . 2017-12-31 7.07 | . 2018-12-31 9.84 | . 2019-03-31 9.87 | . 2019-06-30 9.98 | . 2019-09-30 9.32 | . 2019-12-31 8.36 | . 2020-03-31 8.45 | . 2020-06-30 6.43 | . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_GROWTH_QUARTER&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-03-31 -19.63 | . 2010-06-30 -13.90 | . 2010-09-30 -6.48 | . 2010-12-31 1.45 | . 2011-03-31 6.97 | . 2011-06-30 10.46 | . 2011-09-30 12.65 | . 2011-12-31 16.26 | . 2012-03-31 16.74 | . 2012-06-30 16.35 | . 2012-09-30 17.51 | . 2012-12-31 18.25 | . 2013-03-31 17.40 | . 2013-06-30 17.47 | . 2013-09-30 16.27 | . 2013-12-31 11.99 | . 2014-03-31 12.82 | . 2014-06-30 12.37 | . 2014-09-30 11.89 | . 2014-12-31 12.72 | . 2015-03-31 12.64 | . 2015-06-30 11.67 | . 2015-09-30 10.43 | . 2015-12-31 10.00 | . 2016-03-31 7.52 | . 2016-06-30 6.51 | . 2016-09-30 5.92 | . 2016-12-31 5.33 | . 2017-03-31 5.71 | . 2017-06-30 6.21 | . 2017-09-30 6.99 | . 2017-12-31 7.07 | . 2018-03-31 7.81 | . 2018-06-30 7.99 | . 2018-09-30 8.65 | . 2018-12-31 9.84 | . 2019-03-31 9.87 | . 2019-06-30 9.98 | . 2019-09-30 9.32 | . 2019-12-31 8.36 | . 2020-03-31 8.45 | . 2020-06-30 6.43 | . SP500.head() . Value . Date . 2010-01-01 1123.58 | . 2010-02-01 1089.16 | . 2010-03-01 1152.05 | . 2010-04-01 1197.32 | . 2010-05-01 1125.06 | . SP500.tail() . Value . Date . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . import matplotlib.pyplot as plt %matplotlib inline . SP500[&#39;Value&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/01/StockMarketPortfolioAnaylsis_snp+.html",
            "relUrl": "/2020/10/01/StockMarketPortfolioAnaylsis_snp+.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post49": {
            "title": "Wednesday Links",
            "content": "Artificial Intelligence, Algorithmic Pricing, and Collusion - From American Economic Review. Abstract - ‘Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty.’ . | 北極之最：格陵蘭史帕特冰川大片冰舌脫落 後果多嚴重 - from the BBC. . | Vì sao Ròm trở thành hiện tượng phòng vé nhưng lại gây chia rẽ khán giả? - From BBC. . | Ròm - From IMDB. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/30/links.html",
            "relUrl": "/links/2020/09/30/links.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post50": {
            "title": "Stock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) . SP500.head() . Value . Date . 2010-01-01 1123.58 | . 2010-02-01 1089.16 | . 2010-03-01 1152.05 | . 2010-04-01 1197.32 | . 2010-05-01 1125.06 | . SP500.tail() . Value . Date . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . import matplotlib.pyplot as plt %matplotlib inline . SP500[&#39;Value&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/30/StockMarketPortfolioAnaylsis_snp.html",
            "relUrl": "/2020/09/30/StockMarketPortfolioAnaylsis_snp.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post51": {
            "title": "Tuesday Links",
            "content": "A Satellite Account for Health in the United States - From NBER. The paper ‘measures the change in medical spending and health outcomes for a comprehensive set of 80 conditions’ . | 全球暖化下的西伯利亞：BBC採訪團隊在地見聞 - from the BBC. . | Dùng sầu riêng và mít để sạc điện thoại - From BBC. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/29/links.html",
            "relUrl": "/links/2020/09/29/links.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post52": {
            "title": "Monday Links",
            "content": "澳大利亞也可以看到絢麗的極光 - from the BBC. . | Does Machine Translation Affect International Trade? Evidence from a Large Digital Platform - From NBER. . | 3.How to digitize your lab notebooks - From Nature. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/28/links.html",
            "relUrl": "/links/2020/09/28/links.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite3",
            "content": "This post includes code adapted from these sqlalchemy and sqlite gists and the sqlite3 documentation. . import sqlalchemy as db import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///music.sqlite&#39;) . connection = engine.connect() metadata = db.MetaData() music = db.Table(&#39;music&#39;, metadata, db.Column(&#39;Id&#39;, db.Integer()), db.Column(&#39;song&#39;, db.String(255), nullable=False), db.Column(&#39;album&#39;, db.String(255), nullable=False), db.Column(&#39;artist&#39;, db.String(255), nullable=False) ) metadata.create_all(engine) . #Inserting one record query = db.insert(music).values(Id=1, song=&#39;song3&#39;, album=&#39;album3&#39;, artist=&#39;artist3&#39;) ResultProxy = connection.execute(query) . #Inserting many records query = db.insert(music) values_list = [{&#39;Id&#39;:&#39;2&#39;, &#39;song&#39;:&#39;song1&#39;, &#39;album&#39;:&#39;album1&#39;, &#39;artist&#39;:&#39;artist1&#39;}, {&#39;Id&#39;:&#39;3&#39;, &#39;song&#39;:&#39;song2&#39;, &#39;album&#39;:&#39;album2&#39;, &#39;artist&#39;:&#39;artist2&#39;}] ResultProxy = connection.execute(query,values_list) results = connection.execute(db.select([music])).fetchall() df = pd.DataFrame(results) df.columns = results[0].keys() df.head(10) . Id song album artist . 0 1 | song3 | album3 | artist3 | . 1 2 | song1 | album1 | artist1 | . 2 3 | song2 | album2 | artist2 | . 3 2 | song1 | album1 | artist1 | . 4 3 | song2 | album2 | artist2 | . results = connection.execute(db.select([music])).fetchall() df = pd.DataFrame(results) df.columns = results[0].keys() df.head(4) . query = db.select([music]).where(db.and_(music.columns.song == &#39;song3&#39;, music.columns.artist == &#39;artist3&#39;)) result = connection.execute(query).fetchall() result[:3] . [(1, &#39;song3&#39;, &#39;album3&#39;, &#39;artist3&#39;)] . conn = sqlite3.connect(&#39;music.sqlite&#39;) . c = conn.cursor() # Create table c.execute(&#39;&#39;&#39;CREATE TABLE stockmarket (date text, trans text, symbol text, qty real, price real)&#39;&#39;&#39;) # Insert a row of data c.execute(&quot;INSERT INTO stockmarket VALUES (&#39;2006-01-05&#39;,&#39;BUY&#39;,&#39;RHAT&#39;,100,35.14)&quot;) # Save (commit) the changes conn.commit() # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn.close() . conn = sqlite3.connect(&#39;music.sqlite&#39;) c = conn.cursor() . symbol = &#39;RHAT&#39; c.execute(&quot;SELECT * FROM stockmarket WHERE symbol = &#39;%s&#39;&quot; % symbol) . &lt;sqlite3.Cursor at 0x7f0098ae0180&gt; . t = (&#39;RHAT&#39;,) c.execute(&#39;SELECT * FROM stockmarket WHERE symbol=?&#39;, t) print(c.fetchone()) . (&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14) . # Larger example that inserts many records at a time purchases = [(&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000, 45.00), (&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000, 72.00), (&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500, 53.00), ] c.executemany(&#39;INSERT INTO stockmarket VALUES (?,?,?,?,?)&#39;, purchases) . &lt;sqlite3.Cursor at 0x7f0098ae0180&gt; . for row in c.execute(&#39;SELECT * FROM stockmarket ORDER BY price&#39;): print(row) . (&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14) (&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000.0, 45.0) (&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500.0, 53.0) (&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000.0, 72.0) . # Use dbeaver to examine .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/28/databases_sqllite_sqlalchemy_27.html",
            "relUrl": "/2020/09/28/databases_sqllite_sqlalchemy_27.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post54": {
            "title": "What I've been reading.",
            "content": "Gödel, Escher, Bach - | 21 Lessons for the 21st Century | Grit: The Power of Passion and Perseverance Hardcover | The notebooks for The possessed- Published 1968 by University of Chicago Press. Found at local used book store. |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/books/2020/09/27/reading.html",
            "relUrl": "/books/2020/09/27/reading.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "Sat Links",
            "content": "What Is Your List of 10 Challenges in Data Science? - from the HDSR. . | What’s Wrong with Social Science and How to Fix It - ‘surely notice that all you have is a n=23, p=0.049 three-way interaction effect (one of dozens you tested, and with no multiple testing adjustments of course’ . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/26/links.html",
            "relUrl": "/links/2020/09/26/links.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post56": {
            "title": "Friday Links",
            "content": "What quantum computers reveal about innovation - from the Economist. . | The Past, Present, and (Near) Future of Gene Therapy and Gene Editing - from NEJM . | 中国首富换人做 农夫山泉钟睒睒登顶富豪榜 - 半小时的首富 - 来自BBC。 . | Messi bị kéo vào vụ chỉ trích tuyển Argentina - Vnexpress. . | Earth, Wind &amp; Fire - September . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/25/links.html",
            "relUrl": "/links/2020/09/25/links.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post57": {
            "title": "Analyzing Size of Armed Forces From 1947 - 1963 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.longley.load_pandas().data #print(sm.datasets.longley.NOTE) . df.head() . TOTEMP GNPDEFL GNP UNEMP ARMED POP YEAR . 0 60323.0 | 83.0 | 234289.0 | 2356.0 | 1590.0 | 107608.0 | 1947.0 | . 1 61122.0 | 88.5 | 259426.0 | 2325.0 | 1456.0 | 108632.0 | 1948.0 | . 2 60171.0 | 88.2 | 258054.0 | 3682.0 | 1616.0 | 109773.0 | 1949.0 | . 3 61187.0 | 89.5 | 284599.0 | 3351.0 | 1650.0 | 110929.0 | 1950.0 | . 4 63221.0 | 96.2 | 328975.0 | 2099.0 | 3099.0 | 112075.0 | 1951.0 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1947&#39;, &#39;1962&#39;)) df.index = index df.head() . TOTEMP GNPDEFL GNP UNEMP ARMED POP YEAR . 1947-12-31 60323.0 | 83.0 | 234289.0 | 2356.0 | 1590.0 | 107608.0 | 1947.0 | . 1948-12-31 61122.0 | 88.5 | 259426.0 | 2325.0 | 1456.0 | 108632.0 | 1948.0 | . 1949-12-31 60171.0 | 88.2 | 258054.0 | 3682.0 | 1616.0 | 109773.0 | 1949.0 | . 1950-12-31 61187.0 | 89.5 | 284599.0 | 3351.0 | 1650.0 | 110929.0 | 1950.0 | . 1951-12-31 63221.0 | 96.2 | 328975.0 | 2099.0 | 3099.0 | 112075.0 | 1951.0 | . df[&#39;ARMED&#39;].plot() plt.ylabel(&quot;ARMED&quot;) . Text(0, 0.5, &#39;ARMED&#39;) . # unpacking cycle, trend = sm.tsa.filters.hpfilter(df.ARMED) cycle . 1947-12-31 -497.642333 1948-12-31 -713.661033 1949-12-31 -635.368706 1950-12-31 -682.008289 1951-12-31 688.574390 1952-12-31 1108.959755 1953-12-31 992.297873 1954-12-31 731.045710 1955-12-31 370.040046 1956-12-31 124.660757 1957-12-31 15.056446 1958-12-31 -193.702199 1959-12-31 -324.553899 1960-12-31 -407.316313 1961-12-31 -393.604252 1962-12-31 -182.777954 Name: ARMED, dtype: float64 . type(cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = trend df[[&#39;trend&#39;,&#39;ARMED&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;ARMED&#39;]][&quot;1950-01-01&quot;:&quot;1955-01-01&quot;].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2d5af13940&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/25/AnalyzingSizeofArmedForces.html",
            "relUrl": "/2020/09/25/AnalyzingSizeofArmedForces.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post58": {
            "title": "Analyzing US Real Interest Rate From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;realint&#39;].plot() plt.ylabel(&quot;realint&quot;) . Text(0, 0.5, &#39;realint&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking cycle, trend = sm.tsa.filters.hpfilter(df.realint) cycle . 1959-03-31 -1.195751 1959-06-30 -0.505792 1959-09-30 -0.205086 1959-12-31 2.717430 1960-03-31 -0.197051 ... 2008-09-30 4.330269 2008-12-31 8.961987 2009-03-31 -0.596183 2009-06-30 -3.008487 2009-09-30 -3.188797 Name: realint, Length: 203, dtype: float64 . type(cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = trend df[[&#39;trend&#39;,&#39;realint&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;realint&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2a8100438&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/24/AnalyzingUSRealInterestRate.html",
            "relUrl": "/2020/09/24/AnalyzingUSRealInterestRate.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post59": {
            "title": "Analyzing US Inflation From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;infl&#39;].plot() plt.ylabel(&quot;infl&quot;) . Text(0, 0.5, &#39;infl&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking infl_cycle, infl_trend = sm.tsa.filters.hpfilter(df.infl) infl_cycle . 1959-03-31 -1.206811 1959-06-30 1.141499 1959-09-30 1.550564 1959-12-31 -0.909577 1960-03-31 1.140149 ... 2008-09-30 -5.064733 2008-12-31 -10.550048 2009-03-31 -0.681429 2009-06-30 1.883255 2009-09-30 2.206560 Name: infl, Length: 203, dtype: float64 . type(infl_cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = infl_trend df[[&#39;trend&#39;,&#39;infl&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;infl&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fec38616be0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/23/AnalyzingUSInflation.html",
            "relUrl": "/2020/09/23/AnalyzingUSInflation.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "Analyzing US Unemployment From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;unemp&#39;].plot() plt.ylabel(&quot;unemp&quot;) . Text(0, 0.5, &#39;unemp&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking unemp_cycle, unemp_trend = sm.tsa.filters.hpfilter(df.unemp) unemp_cycle . 1959-03-31 0.011338 1959-06-30 -0.702548 1959-09-30 -0.516441 1959-12-31 -0.229910 1960-03-31 -0.642198 ... 2008-09-30 -0.481666 2008-12-31 0.198598 2009-03-31 1.171440 2009-06-30 2.040247 2009-09-30 2.207674 Name: unemp, Length: 203, dtype: float64 . type(unemp_cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = unemp_trend df[[&#39;trend&#39;,&#39;unemp&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;unemp&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdb5c2ecdd8&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/22/AnalyzingUSUnemployment.html",
            "relUrl": "/2020/09/22/AnalyzingUSUnemployment.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "Stock Market and Optimal Portfolio Anaylsis scipy and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import numpy as np import quandl %matplotlib inline . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . # Grabbing a bunch of tech stocks for our portfolio COST = quandl.get(&#39;WIKI/COST.11&#39;, start_date = start, end_date = end) NLSN = quandl.get(&#39;WIKI/NLSN.11&#39;, start_date = start, end_date = end) NKE = quandl.get(&#39;WIKI/NKE.11&#39;, start_date = start, end_date = end) DIS = quandl.get(&#39;WIKI/DIS.11&#39;, start_date = start, end_date = end) . stocks = pd.concat([COST, NLSN, NKE, DIS], axis = 1) stocks.columns = [&#39;COST&#39;,&#39;NLSN&#39;,&#39;NKE&#39;,&#39;DIS&#39;] . stocks . COST NLSN NKE DIS . Date . 2010-01-04 49.085078 | NaN | 14.751122 | 28.960651 | . 2010-01-05 48.936361 | NaN | 14.809811 | 28.888407 | . 2010-01-06 49.572542 | NaN | 14.719521 | 28.734890 | . 2010-01-07 49.332941 | NaN | 14.863985 | 28.743920 | . 2010-01-08 48.977671 | NaN | 14.834641 | 28.789072 | . ... ... | ... | ... | ... | . 2018-03-21 186.070000 | 32.44 | 66.350000 | 101.820000 | . 2018-03-22 182.640000 | 31.82 | 64.420000 | 100.600000 | . 2018-03-23 180.840000 | 31.51 | 64.630000 | 98.540000 | . 2018-03-26 187.220000 | 32.03 | 65.900000 | 100.650000 | . 2018-03-27 183.150000 | 32.09 | 66.170000 | 99.360000 | . 2071 rows × 4 columns . mean_daily_ret = stocks.pct_change(1).mean() mean_daily_ret . COST 0.000699 NLSN 0.000312 NKE 0.000833 DIS 0.000683 dtype: float64 . stocks.pct_change(1).corr() . COST NLSN NKE DIS . COST 1.000000 | 0.265003 | 0.370978 | 0.415377 | . NLSN 0.265003 | 1.000000 | 0.312192 | 0.392808 | . NKE 0.370978 | 0.312192 | 1.000000 | 0.446150 | . DIS 0.415377 | 0.392808 | 0.446150 | 1.000000 | . stocks.head() . COST NLSN NKE DIS . Date . 2010-01-04 49.085078 | NaN | 14.751122 | 28.960651 | . 2010-01-05 48.936361 | NaN | 14.809811 | 28.888407 | . 2010-01-06 49.572542 | NaN | 14.719521 | 28.734890 | . 2010-01-07 49.332941 | NaN | 14.863985 | 28.743920 | . 2010-01-08 48.977671 | NaN | 14.834641 | 28.789072 | . stock_normed = stocks/stocks.iloc[0] stock_normed.plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . stock_daily_ret = stocks.pct_change(1) stock_daily_ret.head() . COST NLSN NKE DIS . Date . 2010-01-04 NaN | NaN | NaN | NaN | . 2010-01-05 -0.003030 | NaN | 0.003979 | -0.002495 | . 2010-01-06 0.013000 | NaN | -0.006097 | -0.005314 | . 2010-01-07 -0.004833 | NaN | 0.009814 | 0.000314 | . 2010-01-08 -0.007201 | NaN | -0.001974 | 0.001571 | . log_ret = np.log(stocks / stocks.shift(1)) log_ret.head() . COST NLSN NKE DIS . Date . 2010-01-04 NaN | NaN | NaN | NaN | . 2010-01-05 -0.003034 | NaN | 0.003971 | -0.002498 | . 2010-01-06 0.012916 | NaN | -0.006115 | -0.005328 | . 2010-01-07 -0.004845 | NaN | 0.009767 | 0.000314 | . 2010-01-08 -0.007228 | NaN | -0.001976 | 0.001570 | . log_ret.hist(bins = 100, figsize = (12, 6)); plt.tight_layout() . log_ret.describe().transpose() . count mean std min 25% 50% 75% max . COST 2068.0 | 0.000633 | 0.011172 | -0.083110 | -0.005293 | 0.000413 | 0.006618 | 0.060996 | . NLSN 1801.0 | 0.000198 | 0.015121 | -0.185056 | -0.007131 | 0.000000 | 0.008051 | 0.095201 | . NKE 2070.0 | 0.000725 | 0.014682 | -0.098743 | -0.006602 | 0.000656 | 0.008155 | 0.115342 | . DIS 2070.0 | 0.000596 | 0.013220 | -0.096190 | -0.005710 | 0.000776 | 0.007453 | 0.073531 | . log_ret.mean() * 252 . COST 0.159439 NLSN 0.049979 NKE 0.182719 DIS 0.150081 dtype: float64 . # Compute pairwise covariance of columns log_ret.cov() . COST NLSN NKE DIS . COST 0.000125 | 0.000045 | 0.000061 | 0.000061 | . NLSN 0.000045 | 0.000229 | 0.000070 | 0.000077 | . NKE 0.000061 | 0.000070 | 0.000216 | 0.000087 | . DIS 0.000061 | 0.000077 | 0.000087 | 0.000175 | . # Set seed (optional) np.random.seed(101) # Stock Columns print(&#39;Stocks&#39;) print(stocks.columns) print(&#39; n&#39;) # Create Random Weights print(&#39;Creating Random Weights&#39;) weights = np.array(np.random.random(4)) print(weights) print(&#39; n&#39;) # Rebalance Weights print(&#39;Rebalance to sum to 1.0&#39;) weights = weights / np.sum(weights) print(weights) print(&#39; n&#39;) # Expected Return print(&#39;Expected Portfolio Return&#39;) exp_ret = np.sum(log_ret.mean() * weights) *252 print(exp_ret) print(&#39; n&#39;) # Expected Variance print(&#39;Expected Volatility&#39;) exp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) print(exp_vol) print(&#39; n&#39;) # Sharpe Ratio SR = exp_ret/exp_vol print(&#39;Sharpe Ratio&#39;) print(SR) . Stocks Index([&#39;COST&#39;, &#39;NLSN&#39;, &#39;NKE&#39;, &#39;DIS&#39;], dtype=&#39;object&#39;) Creating Random Weights [0.51639863 0.57066759 0.02847423 0.17152166] Rebalance to sum to 1.0 [0.40122278 0.44338777 0.02212343 0.13326603] Expected Portfolio Return 0.11017373023155777 Expected Volatility 0.16110487214223854 Sharpe Ratio 0.6838634286260817 . num_ports = 15000 all_weights = np.zeros((num_ports, len(stocks.columns))) ret_arr = np.zeros(num_ports) vol_arr = np.zeros(num_ports) sharpe_arr = np.zeros(num_ports) for ind in range(num_ports): # Create Random Weights weights = np.array(np.random.random(4)) # Rebalance Weights weights = weights / np.sum(weights) # Save Weights all_weights[ind,:] = weights # Expected Return ret_arr[ind] = np.sum((log_ret.mean() * weights) *252) # Expected Variance vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) # Sharpe Ratio sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind] . sharpe_arr.max() . 1.042687299617254 . sharpe_arr.argmax() . 10619 . all_weights[10619,:] . array([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01]) . max_sr_ret = ret_arr[1419] max_sr_vol = vol_arr[1419] . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add red dot for max SR plt.scatter(max_sr_vol, max_sr_ret, c = &#39;red&#39;, s = 50, edgecolors = &#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7f703b26fd00&gt; . def get_ret_vol_sr(weights): &quot;&quot;&quot; Takes in weights, returns array or return,volatility, sharpe ratio &quot;&quot;&quot; weights = np.array(weights) ret = np.sum(log_ret.mean() * weights) * 252 vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) sr = ret/vol return np.array([ret, vol, sr]) from scipy.optimize import minimize import numpy as np def neg_sharpe(weights): return get_ret_vol_sr(weights)[2] * -1 # Contraints def check_sum(weights): &#39;&#39;&#39; Returns 0 if sum of weights is 1.0 &#39;&#39;&#39; return np.sum(weights) - 1 # By convention of minimize function it should be a function that returns zero for conditions cons = ({&#39;type&#39; : &#39;eq&#39;, &#39;fun&#39;: check_sum}) # 0-1 bounds for each weight bounds = ((0, 1), (0, 1), (0, 1), (0, 1)) # Initial Guess (equal distribution) init_guess = [0.25, 0.25, 0.25, 0.25] # Sequential Least Squares opt_results = minimize(neg_sharpe, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) opt_results . fun: -1.0442236428192482 jac: array([-1.85623765e-04, 3.00063133e-01, 3.43203545e-04, 1.72853470e-05]) message: &#39;Optimization terminated successfully&#39; nfev: 20 nit: 4 njev: 4 status: 0 success: True x: array([0.53438392, 0. , 0.27969302, 0.18592306]) . opt_results.x get_ret_vol_sr(opt_results.x) . array([0.16421049, 0.15725605, 1.04422364]) . frontier_y = np.linspace(0, 0.3, 100) . def minimize_volatility(weights): return get_ret_vol_sr(weights)[1] frontier_volatility = [] for possible_return in frontier_y: # function for return cons = ({&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: check_sum}, {&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: lambda w: get_ret_vol_sr(w)[0] - possible_return}) result = minimize(minimize_volatility, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) frontier_volatility.append(result[&#39;fun&#39;]) . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add frontier line plt.plot(frontier_volatility, frontier_y, &#39;g--&#39;, linewidth = 3) . [&lt;matplotlib.lines.Line2D at 0x7f703b1949a0&gt;] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/21/StockMarketPortfolioAnaylsis2.html",
            "relUrl": "/2020/09/21/StockMarketPortfolioAnaylsis2.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post62": {
            "title": "Stock Market and Portfolio Anaylsis with pandas_datareader and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd from pandas_datareader import data, wb import datetime . start = pd.to_datetime(&#39;2020-02-04&#39;) end = pd.to_datetime(&#39;today&#39;) MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f557129fa90&gt; . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . # Grabbing a bunch of tech stocks for our portfolio COST = quandl.get(&#39;WIKI/COST.11&#39;, start_date = start, end_date = end) NLSN = quandl.get(&#39;WIKI/NLSN.11&#39;, start_date = start, end_date = end) NKE = quandl.get(&#39;WIKI/NKE.11&#39;, start_date = start, end_date = end) DIS = quandl.get(&#39;WIKI/DIS.11&#39;, start_date = start, end_date = end) # Example COST.iloc[0][&#39;Adj. Close&#39;] for stock_df in (COST, NLSN, NKE, DIS): stock_df[&#39;Normed Return&#39;] = stock_df[&#39;Adj. Close&#39;] / stock_df.iloc[0][&#39;Adj. Close&#39;] COST.head() COST.tail() ## Allocations for stock_df,allo in zip([COST,NLSN,NKE,DIS],[.2, .1, .4, .3]): stock_df[&#39;Allocation&#39;] = stock_df[&#39;Normed Return&#39;] * allo COST.head() ## Investment for stock_df in [COST,NLSN,NKE,DIS]: stock_df[&#39;Position Values&#39;] = stock_df[&#39;Allocation&#39;] * 1000000 ## Total Portfolio Value portfolio_val = pd.concat([COST[&#39;Position Values&#39;], NLSN[&#39;Position Values&#39;], NKE[&#39;Position Values&#39;], DIS[&#39;Position Values&#39;]], axis = 1) portfolio_val.head() portfolio_val.columns = [&#39;COST Pos&#39;, &#39;NLSN Pos&#39;, &#39;NKE Pos&#39;, &#39;DIS Pos&#39;] portfolio_val.head() portfolio_val[&#39;Total Pos&#39;] = portfolio_val.sum(axis = 1) portfolio_val.head() import matplotlib.pyplot as plt %matplotlib inline portfolio_val[&#39;Total Pos&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total Portfolio Value&#39;) portfolio_val.drop(&#39;Total Pos&#39;, axis = 1).plot(kind = &#39;line&#39;, figsize = (12, 8)) portfolio_val.tail() . COST Pos NLSN Pos NKE Pos DIS Pos Total Pos . Date . 2018-03-21 758153.014553 | 144489.827125 | 1.799185e+06 | 1.054741e+06 | 3.756569e+06 | . 2018-03-22 744177.280475 | 141728.307617 | 1.746850e+06 | 1.042104e+06 | 3.674859e+06 | . 2018-03-23 736843.076002 | 140347.547864 | 1.752545e+06 | 1.020764e+06 | 3.650500e+06 | . 2018-03-26 762838.756299 | 142663.660999 | 1.786983e+06 | 1.042622e+06 | 3.735107e+06 | . 2018-03-27 746255.305075 | 142930.904822 | 1.794304e+06 | 1.029259e+06 | 3.712749e+06 | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/19/StockMarketPortfolioAnaylsis.html",
            "relUrl": "/2020/09/19/StockMarketPortfolioAnaylsis.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post63": {
            "title": "NLP Heatmaps with Seaborn",
            "content": "from jupyterthemes import jtplot import warnings from imblearn.over_sampling import SMOTE import seaborn as sns from sklearn.model_selection import train_test_split import pandas as pd import numpy as np import pandas_profiling from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn import preprocessing import matplotlib.pyplot as plt %matplotlib inline # ignore warnings warnings.filterwarnings(&#39;ignore&#39;) jtplot.style(theme=&#39;oceans16&#39;, context=&#39;notebook&#39;, ticks=True, grid=False, figsize=(10, 9)) . df=pd.read_csv(&#39;../processed_data/nf_complete.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 126 entries, 0 to 125 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 Unnamed: 0 126 non-null int64 1 year 126 non-null int64 2 title 126 non-null object 3 abstract 126 non-null object 4 theme 126 non-null object 5 China 126 non-null int64 6 Russia 126 non-null int64 7 War 126 non-null int64 8 President 126 non-null int64 9 US 126 non-null int64 10 Vietnam 126 non-null int64 11 Cold War 126 non-null int64 12 World War 126 non-null int64 13 Vietnam War 126 non-null int64 14 Korean War 126 non-null int64 15 Survey 126 non-null int64 16 Case Study 126 non-null int64 17 Trade 126 non-null int64 18 Humanitarian 126 non-null int64 19 fixed_effects 126 non-null int64 20 instrumental_variable 126 non-null int64 21 regression 126 non-null int64 22 experimental 126 non-null int64 dtypes: int64(20), object(3) memory usage: 22.8+ KB . import plotly_express as ple ple.histogram(df.sort_values(&#39;year&#39;).groupby([&#39;year&#39;,&#39;theme&#39;])[&#39;Cold War&#39;].sum().reset_index(), x=&quot;year&quot;, y=&quot;Cold War&quot;, histfunc=&quot;sum&quot;, color=&quot;theme&quot;) . # ple.lidifferences(dfm_regional.sort_values(&#39;year&#39;).groupby([&#39;year&#39;,&#39;theme&#39;])[&#39;Cold War&#39;].sum().reset_index(), # x=&#39;year&#39;, # y=&#39;Cold War&#39;, # line_group=&#39;theme&#39;, # color=&#39;theme&#39; # ) . # Create the crosstab DataFrame pd_crosstab = pd.crosstab(df[&quot;theme&quot;], df[&quot;year&quot;]) # Plot a heatmap of the table with no color bar and using the BuGn palette sns.heatmap(pd_crosstab, cbar=False, cmap=&quot;GnBu&quot;, linewidths=0.3) # Rotate tick marks for visibility plt.yticks(rotation=0) plt.xticks(rotation=90) plt.tight_layout() #plt.savefig(&#39;./img/theme_heat_1.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500) #Show the plot plt.show() plt.clf() . &lt;Figure size 720x648 with 0 Axes&gt; . sns.clustermap(pd_crosstab, cmap=&#39;Greens&#39;, robust=True) # plot using a color palette #sns.heatmap(df, cmap=&quot;YlGnBu&quot;) #sns.heatmap(df, cmap=&quot;Blues&quot;) #sns.heatmap(df, cmap=&quot;BuPu&quot;) #sns.heatmap(df, cmap=&quot;Greens&quot;) . &lt;seaborn.matrix.ClusterGrid at 0x7f978c07a470&gt; . # Import seaborn library import seaborn as sns # Get correlation matrix of the meat DataFrame corr_meat = df.corr(method=&#39;pearson&#39;) # Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels fig = sns.clustermap(pd_crosstab, row_cluster=True, col_cluster=True, figsize=(10, 10)) plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90) plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0) plt.show() . data_normalized = pd_crosstab # Standardize the mean and variance within a stat, so different stats can be comparable # (This is the same as changing all the columns to Z-scores) data_normalized = (data_normalized - data_normalized.mean())/data_normalized.var() # Normalize these values to range from -1 to 1 data_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min()) data_normalized = data_normalized.T # Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram. sns.clustermap(data_normalized, cmap=&#39;Blues&#39;); . data_normalized = pd_crosstab # Standardize the mean and variance within a stat, so different stats can be comparable # (This is the same as changing all the columns to Z-scores) data_normalized = (data_normalized - data_normalized.mean())/data_normalized.var() # Normalize these values to range from -1 to 1 data_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min()) #data_normalized = data_normalized.T # Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram. sns.clustermap(data_normalized, cmap=&#39;BuPu&#39;); . import matplotlib.pyplot as plt sns.clustermap(data_normalized); fig = plt.gcf() fig.savefig(&#39;clusteredheatmap_bbox_tight.png&#39;, bbox_inches=&#39;tight&#39;) . tidy_df = pd.melt(df.reset_index(), id_vars=&#39;index&#39;) df.T.head() . 0 1 2 3 4 5 6 7 8 9 ... 116 117 118 119 120 121 122 123 124 125 . Unnamed: 0 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 14 | 13 | ... | 128 | 130 | 123 | 125 | 131 | 132 | 133 | 134 | 135 | 136 | . year 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2001 | 2001 | ... | 2017 | 2017 | 2017 | 2017 | 2018 | 2018 | 2018 | 2018 | 2018 | 2018 | . title &quot;Institutions at the Domestic/International Ne... | Born to Lose and Doomed to Survive: State Deat... | The significance of “allegiance” in internatio... | The significance of “allegiance” in internatio... | Truth-Telling and Mythmaking in Post-Soviet Ru... | Building a Cape Fear Metropolis: Fort Bragg, F... | The Glories and the Sadness: Shaping the natio... | What leads longstanding adversaries to engage ... | A School for the Nation: Military Institution... | The &#39;American Century&#39; Army: The Origins of t... | ... | Fully Committed? Religiously Committed State P... | Straddling the Threshold of Two Worlds: Soldie... | U.S. Army’s Investigation and Adjudication of ... | Grand Strategic Crucibles: The Lasting Effects... | Trust in International Politics: The Role of L... | Planning for the Short Haul: Trade Among Belli... | Clinging to the Anti-Imperial Mantle: The Repu... | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | Unexpected Humanitarians: Albania, the U.S. Mi... | . abstract Civil-military relations are frequently studie... | Under what conditions do states die, or exit t... | My dissertation employs original and secondary... | nThis study revises prevailing interpretation... | Can distorted and pernicious ideas about histo... | My dissertation examines the cultural and econ... | In my dissertation I compare the ways in whic... | This dissertation develops a socio-psychoanal... | Beginning in Europe in the latter half of the ... | This dissertation covers the period 1949-1959 ... | ... | This dissertation argues that the higher the l... | This dissertation explores how American soldie... | This dissertation examines the U.S. Army’s res... | When and how do military interventions shape g... | In my dissertation, I focus on how leader rela... | In times of war, why do belligerents continue ... | My dissertation project, Clinging to the Anti-... | Using a transnational methodology and sources ... | There is a dilemma at the heart of coercion. S... | Using archives and oral history, this disserta... | . theme IR scholarship | IR scholarship | IR scholarship | Conflit Between States | Conflict Between States | Domestic Military History | Culture | Culture / Peace Process | Military History | Military History | ... | IR Scholarship | Military History | Military History | IR Scholarship | Nuclear Weapons | Conflict between states | Cold War | Military History | IR Scholarship | Military History | . 5 rows × 126 columns .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/03/NLP_Seaborn_Heatmaps.html",
            "relUrl": "/2020/09/03/NLP_Seaborn_Heatmaps.html",
            "date": " • Sep 3, 2020"
        }
        
    
  
    
        ,"post64": {
            "title": "NLP ngrams With Python",
            "content": "&#39;In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.&#39;, from wikipedia . import pandas as pd df=pd.read_csv(&#39;../../processed_data/nf_complete.csv&#39;) . Pre-processing text . def preprocessor(text): text = re.sub(&#39;&lt;[^&gt;]*&gt;&#39;, &#39;&#39;, text) emoticons = re.findall(&#39;(?::|;|=)(?:-)?(?: )| (|D|P)&#39;, text) text = re.sub(&#39;[ W]+&#39;, &#39; &#39;, text.lower()) + &#39; &#39;.join(emoticons).replace(&#39;-&#39;, &#39;&#39;) return text . Find Total Word Count . text = &quot; &quot;.join(review for review in df.abstract) print (&quot;There are {} words in the combination of all abstracts.&quot;.format(len(text))) . There are 272025 words in the combination of all abstracts. . from urllib.request import urlopen from random import randint def wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sum def retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return word def buildWordDict(text): # Remove newlines and quotes text = text.replace(&#39; n&#39;, &#39; &#39;); text = text.replace(&#39;&quot;&#39;, &#39;&#39;); # Make sure punctuation marks are treated as their own &quot;words,&quot; # so that they will be included in the Markov chain punctuation = [&#39;,&#39;,&#39;.&#39;,&#39;;&#39;,&#39;:&#39;] for symbol in punctuation: text = text.replace(symbol, &#39; {} &#39;.format(symbol)); words = text.split(&#39; &#39;) # Filter out empty words words = [word for word in words if word != &#39;&#39;] wordDict = {} for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = {} if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDict wordDict = buildWordDict(text) #Generate a Markov chain of length 100 length = 100 chain = [&#39;Vietnam&#39;] for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord) print(&#39; &#39;.join(chain)) . Vietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate . ” and . def getFirstSentenceContaining(ngram, text): #print(ngram) sentences = text.upper().split(&quot;. &quot;) for sentence in sentences: if ngram in sentence: return sentence+&#39; n&#39; return &quot;&quot; print(getFirstSentenceContaining(&#39;I&#39;, text)) . CIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS . #text . from urllib.request import urlopen from bs4 import BeautifulSoup import re import string from collections import Counter def cleanSentence(sentence): sentence = sentence.split(&#39; &#39;) sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence] sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == &#39;a&#39; or word.lower() == &#39;i&#39;)] return sentence def cleanInput(content): content = content.upper() content = re.sub(&#39; n&#39;, &#39; &#39;, content) content = bytes(content, &#39;UTF-8&#39;) content = content.decode(&#39;ascii&#39;, &#39;ignore&#39;) sentences = content.split(&#39;. &#39;) return [cleanSentence(sentence) for sentence in sentences] def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return output def getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [&#39; &#39;.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams) content = str(text) ngrams = getNgrams(content, 3) #print(ngrams) . from urllib.request import urlopen from bs4 import BeautifulSoup import re import string from collections import Counter def isCommon(ngram): commonWords = [&#39;THE&#39;, &#39;BE&#39;, &#39;AND&#39;, &#39;OF&#39;, &#39;A&#39;, &#39;IN&#39;, &#39;TO&#39;, &#39;HAVE&#39;, &#39;IT&#39;, &#39;I&#39;, &#39;THAT&#39;, &#39;FOR&#39;, &#39;YOU&#39;, &#39;HE&#39;, &#39;WITH&#39;, &#39;ON&#39;, &#39;DO&#39;, &#39;SAY&#39;, &#39;THIS&#39;, &#39;THEY&#39;, &#39;IS&#39;, &#39;AN&#39;, &#39;AT&#39;, &#39;BUT&#39;, &#39;WE&#39;, &#39;HIS&#39;, &#39;FROM&#39;, &#39;THAT&#39;, &#39;NOT&#39;, &#39;BY&#39;, &#39;SHE&#39;, &#39;OR&#39;, &#39;AS&#39;, &#39;WHAT&#39;, &#39;GO&#39;, &#39;THEIR&#39;, &#39;CAN&#39;, &#39;WHO&#39;, &#39;GET&#39;, &#39;IF&#39;, &#39;WOULD&#39;, &#39;HER&#39;, &#39;ALL&#39;, &#39;MY&#39;, &#39;MAKE&#39;, &#39;ABOUT&#39;, &#39;KNOW&#39;, &#39;WILL&#39;, &#39;AS&#39;, &#39;UP&#39;, &#39;ONE&#39;, &#39;TIME&#39;, &#39;HAS&#39;, &#39;BEEN&#39;, &#39;THERE&#39;, &#39;YEAR&#39;, &#39;SO&#39;, &#39;THINK&#39;, &#39;WHEN&#39;, &#39;WHICH&#39;, &#39;THEM&#39;, &#39;SOME&#39;, &#39;ME&#39;, &#39;PEOPLE&#39;, &#39;TAKE&#39;, &#39;OUT&#39;, &#39;INTO&#39;, &#39;JUST&#39;, &#39;SEE&#39;, &#39;HIM&#39;, &#39;YOUR&#39;, &#39;COME&#39;, &#39;COULD&#39;, &#39;NOW&#39;, &#39;THAN&#39;, &#39;LIKE&#39;, &#39;OTHER&#39;, &#39;HOW&#39;, &#39;THEN&#39;, &#39;ITS&#39;, &#39;OUR&#39;, &#39;TWO&#39;, &#39;MORE&#39;, &#39;THESE&#39;, &#39;WANT&#39;, &#39;WAY&#39;, &#39;LOOK&#39;, &#39;FIRST&#39;, &#39;ALSO&#39;, &#39;NEW&#39;, &#39;BECAUSE&#39;, &#39;DAY&#39;, &#39;MORE&#39;, &#39;USE&#39;, &#39;NO&#39;, &#39;MAN&#39;, &#39;FIND&#39;, &#39;HERE&#39;, &#39;THING&#39;, &#39;GIVE&#39;, &#39;MANY&#39;, &#39;WELL&#39;] for word in ngram: if word in commonWords: return True return False def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): if not isCommon(content[i:i+n]): output.append(content[i:i+n]) return output ngrams = getNgrams(content, 3) #print(ngrams) . def getFirstSentenceContaining(ngram, content): #print(ngram) sentences = content.upper().split(&quot;. &quot;) for sentence in sentences: if ngram in sentence: return sentence+&#39; n&#39; return &quot;&quot; print(getFirstSentenceContaining(&#39;SINO-JAPANESE WAR 1894-1895&#39;, content)) print(getFirstSentenceContaining(&#39;2ND VIETNAM WAR&#39;, content)) print(getFirstSentenceContaining(&#39;COLD WAR ARMY&#39;, content)) print(getFirstSentenceContaining(&#39;WORLD WAR II&#39;, content)) print(getFirstSentenceContaining(&#39;ARMS CONTROL AGREEMENTS&#39;, content)) . IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD. THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE ‘OTHER SIDE,’ ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION COLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTION’S INTELLECTUAL AND HISTORICAL FRAMEWORK OF “PREPAREDNESS” TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR I ARGUE THAT A NORM PROTECTING STATES’ TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS. THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY . from urllib.request import urlopen from random import randint def wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sum def retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return word def buildWordDict(text): # Remove newlines and quotes text = text.replace(&#39; n&#39;, &#39; &#39;); text = text.replace(&#39;&quot;&#39;, &#39;&#39;); # Make sure punctuation marks are treated as their own &quot;words,&quot; # so that they will be included in the Markov chain punctuation = [&#39;,&#39;,&#39;.&#39;,&#39;;&#39;,&#39;:&#39;] for symbol in punctuation: text = text.replace(symbol, &#39; {} &#39;.format(symbol)); words = text.split(&#39; &#39;) # Filter out empty words words = [word for word in words if word != &#39;&#39;] wordDict = {} for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = {} if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDict wordDict = buildWordDict(text) #Generate a Markov chain of length 100 length = 100 chain = [&#39;I&#39;] for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord) #print(&#39; &#39;.join(chain)) . import re def getNgrams(content, n): content = re.sub(&#39; n|[[ d+ ]]&#39;, &#39; &#39;, content) content = bytes(content, &#39;UTF-8&#39;) content = content.decode(&#39;ascii&#39;, &#39;ignore&#39;) content = content.split(&#39; &#39;) content = [word for word in content if word != &#39;&#39;] output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return output . from collections import Counter def getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [&#39; &#39;.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams) . #print(getNgrams(content, 2)) . def isCommon(ngram): commonWords = [&#39;THE&#39;, &#39;BE&#39;, &#39;AND&#39;, &#39;OF&#39;, &#39;A&#39;, &#39;IN&#39;, &#39;TO&#39;, &#39;HAVE&#39;, &#39;IT&#39;, &#39;I&#39;, &#39;THAT&#39;, &#39;FOR&#39;, &#39;YOU&#39;, &#39;HE&#39;, &#39;WITH&#39;, &#39;ON&#39;, &#39;DO&#39;, &#39;SAY&#39;, &#39;THIS&#39;, &#39;THEY&#39;, &#39;IS&#39;, &#39;AN&#39;, &#39;AT&#39;, &#39;BUT&#39;, &#39;WE&#39;, &#39;HIS&#39;, &#39;FROM&#39;, &#39;THAT&#39;, &#39;NOT&#39;, &#39;BY&#39;, &#39;SHE&#39;, &#39;OR&#39;, &#39;AS&#39;, &#39;WHAT&#39;, &#39;GO&#39;, &#39;THEIR&#39;, &#39;CAN&#39;, &#39;WHO&#39;, &#39;GET&#39;, &#39;IF&#39;, &#39;WOULD&#39;, &#39;HER&#39;, &#39;ALL&#39;, &#39;MY&#39;, &#39;MAKE&#39;, &#39;ABOUT&#39;, &#39;KNOW&#39;, &#39;WILL&#39;, &#39;AS&#39;, &#39;UP&#39;, &#39;ONE&#39;, &#39;TIME&#39;, &#39;HAS&#39;, &#39;BEEN&#39;, &#39;THERE&#39;, &#39;YEAR&#39;, &#39;SO&#39;, &#39;THINK&#39;, &#39;WHEN&#39;, &#39;WHICH&#39;, &#39;THEM&#39;, &#39;SOME&#39;, &#39;ME&#39;, &#39;PEOPLE&#39;, &#39;TAKE&#39;, &#39;OUT&#39;, &#39;INTO&#39;, &#39;JUST&#39;, &#39;SEE&#39;, &#39;HIM&#39;, &#39;YOUR&#39;, &#39;COME&#39;, &#39;COULD&#39;, &#39;NOW&#39;, &#39;THAN&#39;, &#39;LIKE&#39;, &#39;OTHER&#39;, &#39;HOW&#39;, &#39;THEN&#39;, &#39;ITS&#39;, &#39;OUR&#39;, &#39;TWO&#39;, &#39;MORE&#39;, &#39;THESE&#39;, &#39;WANT&#39;, &#39;WAY&#39;, &#39;LOOK&#39;, &#39;FIRST&#39;, &#39;ALSO&#39;, &#39;NEW&#39;, &#39;BECAUSE&#39;, &#39;DAY&#39;, &#39;MORE&#39;, &#39;USE&#39;, &#39;NO&#39;, &#39;MAN&#39;, &#39;FIND&#39;, &#39;HERE&#39;, &#39;THING&#39;, &#39;GIVE&#39;, &#39;MANY&#39;, &#39;WELL&#39;] for word in ngram: if word in commonWords: return True return False def getNgramsFromSentence(text, n): output = [] for i in range(len(text)-n+1): if not isCommon(text[i:i+n]): output.append(text[i:i+n]) return output ngrams = getNgrams(text, 3) #print(ngrams) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/02/NLP_Ngram.html",
            "relUrl": "/2020/09/02/NLP_Ngram.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post65": {
            "title": "NLP with Pyspark",
            "content": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. . from pyspark.ml.feature import Tokenizer, RegexTokenizer from pyspark.sql.functions import col, udf from pyspark.sql.types import IntegerType . sentenceDataFrame = spark.createDataFrame([ (0, &quot;Hi I heard about Spark&quot;), (1, &quot;I wish Java could use case classes&quot;), (2, &quot;Logistic,regression,models,are,neat&quot;) ], [&quot;id&quot;, &quot;sentence&quot;]) . sentenceDataFrame.show() . ++--+ id| sentence| ++--+ 0|Hi I heard about ...| 1|I wish Java could...| 2|Logistic,regressi...| ++--+ Using Tokenizer and RegexTokenizer . tokenizer = Tokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;) regexTokenizer = RegexTokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;, pattern=&quot; W&quot;) # alternatively, pattern=&quot; w+&quot;, gaps(False) countTokens = udf(lambda words: len(words), IntegerType()) tokenized = tokenizer.transform(sentenceDataFrame) tokenized.select(&quot;sentence&quot;, &quot;words&quot;) .withColumn(&quot;tokens&quot;, countTokens(col(&quot;words&quot;))).show(truncate=False) regexTokenized = regexTokenizer.transform(sentenceDataFrame) regexTokenized.select(&quot;sentence&quot;, &quot;words&quot;) .withColumn(&quot;tokens&quot;, countTokens(col(&quot;words&quot;))).show(truncate=False) . +--+++ sentence |words |tokens| +--+++ Hi I heard about Spark |[hi, i, heard, about, spark] |5 | I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 | Logistic,regression,models,are,neat|[logistic,regression,models,are,neat] |1 | +--+++ +--+++ sentence |words |tokens| +--+++ Hi I heard about Spark |[hi, i, heard, about, spark] |5 | I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 | Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5 | +--+++ Removing Stop Words . from pyspark.ml.feature import StopWordsRemover sentenceData = spark.createDataFrame([ (0, [&quot;I&quot;, &quot;saw&quot;, &quot;the&quot;, &quot;red&quot;, &quot;balloon&quot;]), (1, [&quot;Mary&quot;, &quot;had&quot;, &quot;a&quot;, &quot;little&quot;, &quot;lamb&quot;]) ], [&quot;id&quot;, &quot;raw&quot;]) remover = StopWordsRemover(inputCol=&quot;raw&quot;, outputCol=&quot;filtered&quot;) remover.transform(sentenceData).show(truncate=False) . ++-+--+ id |raw |filtered | ++-+--+ 0 |[I, saw, the, red, balloon] |[saw, red, balloon] | 1 |[Mary, had, a, little, lamb]|[Mary, little, lamb]| ++-+--+ n-grams . from pyspark.ml.feature import NGram wordDataFrame = spark.createDataFrame([ (0, [&quot;Hi&quot;, &quot;I&quot;, &quot;heard&quot;, &quot;about&quot;, &quot;Spark&quot;]), (1, [&quot;I&quot;, &quot;wish&quot;, &quot;Java&quot;, &quot;could&quot;, &quot;use&quot;, &quot;case&quot;, &quot;classes&quot;]), (2, [&quot;Logistic&quot;, &quot;regression&quot;, &quot;models&quot;, &quot;are&quot;, &quot;neat&quot;]) ], [&quot;id&quot;, &quot;words&quot;]) ngram = NGram(n=2, inputCol=&quot;words&quot;, outputCol=&quot;ngrams&quot;) ngramDataFrame = ngram.transform(wordDataFrame) ngramDataFrame.select(&quot;ngrams&quot;).show(truncate=False) . ++ ngrams | ++ [Hi I, I heard, heard about, about Spark] | [I wish, wish Java, Java could, could use, use case, case classes]| [Logistic regression, regression models, models are, are neat] | ++ from pyspark.ml.feature import HashingTF, IDF, Tokenizer sentenceData = spark.createDataFrame([ (0.0, &quot;Hi I heard about Spark&quot;), (0.0, &quot;I wish Java could use case classes&quot;), (1.0, &quot;Logistic regression models are neat&quot;) ], [&quot;label&quot;, &quot;sentence&quot;]) sentenceData.show() . +--+--+ label| sentence| +--+--+ 0.0|Hi I heard about ...| 0.0|I wish Java could...| 1.0|Logistic regressi...| +--+--+ tokenizer = Tokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;) wordsData = tokenizer.transform(sentenceData) wordsData.show() . +--+--+--+ label| sentence| words| +--+--+--+ 0.0|Hi I heard about ...|[hi, i, heard, ab...| 0.0|I wish Java could...|[i, wish, java, c...| 1.0|Logistic regressi...|[logistic, regres...| +--+--+--+ hashingTF = HashingTF(inputCol=&quot;words&quot;, outputCol=&quot;rawFeatures&quot;, numFeatures=20) featurizedData = hashingTF.transform(wordsData) # alternatively, CountVectorizer can also be used to get term frequency vectors idf = IDF(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;) idfModel = idf.fit(featurizedData) rescaledData = idfModel.transform(featurizedData) rescaledData.select(&quot;label&quot;, &quot;features&quot;).show() . +--+--+ label| features| +--+--+ 0.0|(20,[6,8,13,16],[...| 0.0|(20,[0,2,7,13,15,...| 1.0|(20,[3,4,6,11,19]...| +--+--+ CountVectorizer . from pyspark.ml.feature import CountVectorizer # Input data: Each row is a bag of words with a ID. df = spark.createDataFrame([ (0, &quot;a b c&quot;.split(&quot; &quot;)), (1, &quot;a b b c a&quot;.split(&quot; &quot;)) ], [&quot;id&quot;, &quot;words&quot;]) # fit a CountVectorizerModel from the corpus. cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;features&quot;, vocabSize=3, minDF=2.0) model = cv.fit(df) result = model.transform(df) result.show(truncate=False) . +++-+ id |words |features | +++-+ 0 |[a, b, c] |(3,[0,1,2],[1.0,1.0,1.0])| 1 |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])| +++-+ df = spark.read.load(&quot;/FileStore/tables/SMSSpamCollection&quot;, format=&quot;csv&quot;, sep=&quot; t&quot;, inferSchema=&quot;true&quot;, header=&quot;false&quot;) . df.printSchema() . root -- _c0: string (nullable = true) -- _c1: string (nullable = true) data = df.withColumnRenamed(&#39;_c0&#39;,&#39;class&#39;).withColumnRenamed(&#39;_c1&#39;,&#39;text&#39;) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) Clean and Prepare the Data . from pyspark.sql.functions import length . data = data.withColumn(&#39;length&#39;,length(data[&#39;text&#39;])) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) -- length: integer (nullable = true) # Pretty Clear Difference data.groupby(&#39;class&#39;).mean().show() . +--+--+ class| avg(length)| +--+--+ ham| 71.4545266210897| spam|138.6706827309237| +--+--+ from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer tokenizer = Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;token_text&quot;) stopremove = StopWordsRemover(inputCol=&#39;token_text&#39;,outputCol=&#39;stop_tokens&#39;) count_vec = CountVectorizer(inputCol=&#39;stop_tokens&#39;,outputCol=&#39;c_vec&#39;) idf = IDF(inputCol=&quot;c_vec&quot;, outputCol=&quot;tf_idf&quot;) ham_spam_to_num = StringIndexer(inputCol=&#39;class&#39;,outputCol=&#39;label&#39;) . from pyspark.ml.feature import VectorAssembler from pyspark.ml.linalg import Vector . clean_up = VectorAssembler(inputCols=[&#39;tf_idf&#39;,&#39;length&#39;],outputCol=&#39;features&#39;) . Naive Bayes . from pyspark.ml.classification import NaiveBayes . # Use defaults nb = NaiveBayes() . ### Pipeline . from pyspark.ml import Pipeline . data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up]) . cleaner = data_prep_pipe.fit(data) . clean_data = cleaner.transform(data) . Training and Evaluation . clean_data = clean_data.select([&#39;label&#39;,&#39;features&#39;]) . clean_data.show() . +--+--+ label| features| +--+--+ 0.0|(13424,[7,11,31,6...| 0.0|(13424,[0,24,297,...| 1.0|(13424,[2,13,19,3...| 0.0|(13424,[0,70,80,1...| 0.0|(13424,[36,134,31...| 1.0|(13424,[10,60,139...| 0.0|(13424,[10,53,103...| 0.0|(13424,[125,184,4...| 1.0|(13424,[1,47,118,...| 1.0|(13424,[0,1,13,27...| 0.0|(13424,[18,43,120...| 1.0|(13424,[8,17,37,8...| 1.0|(13424,[13,30,47,...| 0.0|(13424,[39,96,217...| 0.0|(13424,[552,1697,...| 1.0|(13424,[30,109,11...| 0.0|(13424,[82,214,47...| 0.0|(13424,[0,2,49,13...| 0.0|(13424,[0,74,105,...| 1.0|(13424,[4,30,33,5...| +--+--+ only showing top 20 rows (training,testing) = clean_data.randomSplit([0.7,0.3]) . spam_predictor = nb.fit(training) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) -- length: integer (nullable = true) test_results = spam_predictor.transform(testing) . test_results.show() . +--+--+--+--+-+ label| features| rawPrediction| probability|prediction| +--+--+--+--+-+ 0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...| 0.0| 0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...| 0.0| 0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...| 0.0| 0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...| 0.0| 0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...| 0.0| 0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...| 0.0| 0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...| 0.0| 0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...| 0.0| 0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...| 0.0| 0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...| 0.0| 0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...| 1.0| 0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...| 0.0| 0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...| 0.0| 0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...| 0.0| 0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...| 0.0| 0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...| 0.0| 0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...| 0.0| 0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...| 0.0| 0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...| 0.0| 0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...| 0.0| +--+--+--+--+-+ only showing top 20 rows ## Evaluating Model Accuracy . from pyspark.ml.evaluation import MulticlassClassificationEvaluator . acc_eval = MulticlassClassificationEvaluator() acc = acc_eval.evaluate(test_results) print(&quot;Accuracy of model at predicting spam was: {}&quot;.format(acc)) . Accuracy of model at predicting spam was: 0.9204435112848836",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/30/NLP-with-Pyspark.html",
            "relUrl": "/2020/08/30/NLP-with-Pyspark.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post66": {
            "title": "Clustering with Pyspark",
            "content": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler . feat_cols = [&#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;,&#39;fr&#39;] . feat_cols = [&#39;gdp&#39;] . vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol=&#39;features&#39;) . final_df = vec_assembler.transform(df) . Using the StandardScaler . from pyspark.ml.feature import StandardScaler . scaler = StandardScaler(inputCol=&quot;features&quot;, outputCol=&quot;scaledFeatures&quot;, withStd=True, withMean=False) . Fitting the StandardScaler . # Compute summary statistics by fitting the StandardScaler scalerModel = scaler.fit(final_df) . # Normalize each feature to have unit standard deviation. cluster_final_data = scalerModel.transform(final_df) . kmeans3 = KMeans(featuresCol=&#39;scaledFeatures&#39;,k=3) kmeans2 = KMeans(featuresCol=&#39;scaledFeatures&#39;,k=2) . model_k3 = kmeans3.fit(cluster_final_data) model_k2 = kmeans2.fit(cluster_final_data) . model_k3.transform(cluster_final_data).groupBy(&#39;prediction&#39;).count().show() . +-+--+ prediction|count| +-+--+ 1| 15| 2| 86| 0| 259| +-+--+ model_k2.transform(cluster_final_data).groupBy(&#39;prediction&#39;).count().show() . +-+--+ prediction|count| +-+--+ 1| 308| 0| 52| +-+--+",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/29/Clustering-with-Pyspark.html",
            "relUrl": "/2020/08/29/Clustering-with-Pyspark.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post67": {
            "title": "Regression and Classification with Pyspark ML",
            "content": "Linear Regression and Random Forest/GBT Classification with Pyspark . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType from pyspark.sql.functions import * data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows df.groupBy(&#39;province&#39;).count().show() . ++--+ province|count| ++--+ Guangdong| 12| Hunan| 12| Shanxi| 12| Tibet| 12| Hubei| 12| Tianjin| 12| Beijing| 12| Heilongjiang| 12| Liaoning| 12| Henan| 12| Anhui| 12| Xinjiang| 12| Fujian| 12| Jiangxi| 12| Jilin| 12| Chongqing| 12| Shaanxi| 12| Sichuan| 12| Yunnan| 12| Gansu| 12| ++--+ only showing top 20 rows Imputation of mean values to prepare the data . mean_val = df.select(mean(df[&#39;general&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;general&quot;]) . mean_val = df.select(mean(df[&#39;specific&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;specific&quot;]) . mean_val = df.select(mean(df[&#39;rr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;rr&quot;]) . mean_val = df.select(mean(df[&#39;fr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;fr&quot;]) . mean_val = df.select(mean(df[&#39;rnr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;rnr&quot;]) . mean_val = df.select(mean(df[&#39;i&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;i&quot;]) . Creating binary target feature from extant column for classification . from pyspark.sql.functions import * df = df.withColumn(&#39;specific_classification&#39;,when(df.specific &gt;= 583470.7303370787,1).otherwise(0)) . Using StringIndexer for categorical encoding of string type columns . from pyspark.ml.feature import StringIndexer . indexer = StringIndexer(inputCol=&quot;province&quot;, outputCol=&quot;provinceIndex&quot;) df = indexer.fit(df).transform(df) . indexer = StringIndexer(inputCol=&quot;reg&quot;, outputCol=&quot;regionIndex&quot;) df = indexer.fit(df).transform(df) . df.show() . ++--+++-+-+--++-+-+-+--+-+--+-+--+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it|specific_classification|provinceIndex|regionIndex| ++--+++-+-+--++-+-+-+--+-+--+-+--+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 0| 0.0| 0.0| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 0| 0.0| 0.0| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 0| 0.0| 0.0| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131.0|0.0355944252244898|0.05968862|0.08376352|1646891| East China|1227364| 0| 0.0| 0.0| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 0| 0.0| 0.0| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 0| 0.0| 0.0| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 0| 0.0| 0.0| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 1| 0.0| 0.0| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0|2522449| East China|3422176| 1| 0.0| 0.0| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000.0| 0.0| 0.0| 0.3243243|2522449| East China|3874846| 1| 0.0| 0.0| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354.0| 0.0| 0.0| 0.3243243|3434548| East China|5167300| 1| 0.0| 0.0| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892.0| 0.0| 0.0| 0.3243243|4468640| East China|7040099| 1| 0.0| 0.0| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290.0|0.0355944252244898|0.05968862|0.08376352| 634562|North China| 508135| 0| 1.0| 4.0| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 0| 1.0| 4.0| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 0| 1.0| 4.0| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525.0| 0.0| 0.0| 0.53|2522449|North China| 944047| 0| 1.0| 4.0| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 0| 1.0| 4.0| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 0| 1.0| 4.0| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 0| 1.0| 4.0| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126.0| 0.0| 0.7948718| 0.0|2823366|North China|1426600| 1| 1.0| 4.0| ++--+++-+-+--++-+-+-+--+-+--+-+--+ only showing top 20 rows Using VectorAssembler to prepare features for machine learning . from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler . df.columns . Out[375]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;, &#39;specific_classification&#39;, &#39;provinceIndex&#39;, &#39;regionIndex&#39;] assembler = VectorAssembler( inputCols=[ &#39;provinceIndex&#39;, # &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, #&#39;rnr&#39;, #&#39;rr&#39;, #&#39;i&#39;, #&#39;fr&#39;, &#39;regionIndex&#39;, &#39;it&#39; ], outputCol=&quot;features&quot;) . output = assembler.transform(df) . final_data = output.select(&quot;features&quot;, &quot;specific&quot;) . Spliting data into train and test . train_data,test_data = final_data.randomSplit([0.7,0.3]) . Regression with Pyspark ML . from pyspark.ml.regression import LinearRegression lr = LinearRegression(labelCol=&#39;specific&#39;) . Fitting the linear regression model to the training data . lrModel = lr.fit(train_data) . Coefficients and Intercept of the linear regression model . print(&quot;Coefficients: {} Intercept: {}&quot;.format(lrModel.coefficients,lrModel.intercept)) . Coefficients: [-4936.461707001148,0.8007702471080539,-3994.683052325085,-7.5033201950338,0.42095493334994133,50994.51222529955,0.2531915644818595] Intercept: 7695214.561654471 Evaluating trained linear regression model on the test data . test_results = lrModel.evaluate(test_data) . Metrics of trained linear regression model on the test data (RMSE, MSE, R2) . print(&quot;RMSE: {}&quot;.format(test_results.rootMeanSquaredError)) print(&quot;MSE: {}&quot;.format(test_results.meanSquaredError)) print(&quot;R2: {}&quot;.format(test_results.r2)) . RMSE: 292695.0825058327 MSE: 85670411323.0962 R2: 0.7853651103073853 Looking at correlations with corr . from pyspark.sql.functions import corr . df.select(corr(&#39;specific&#39;,&#39;gdp&#39;)).show() . +-+ corr(specific, gdp)| +-+ 0.5141876884991972| +-+ Classification with Pyspark ML . from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier from pyspark.ml import Pipeline . DecisionTreeClassifier, RandomForestClassifier and GBTClassifier . dtc = DecisionTreeClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) rfc = RandomForestClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) gbt = GBTClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) . Selecting features and binary target . final_data = output.select(&quot;features&quot;, &quot;specific_classification&quot;) train_data,test_data = final_data.randomSplit([0.7,0.3]) . Fitting the Classifiers to the Training Data . rfc_model = rfc.fit(train_data) gbt_model = gbt.fit(train_data) dtc_model = dtc.fit(train_data) . Classifier predictions on test data . dtc_predictions = dtc_model.transform(test_data) rfc_predictions = rfc_model.transform(test_data) gbt_predictions = gbt_model.transform(test_data) . Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator . from pyspark.ml.evaluation import MulticlassClassificationEvaluator . Classifier Accuracy . acc_evaluator = MulticlassClassificationEvaluator(labelCol=&quot;specific_classification&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;) . Classifier Accuracy Metrics . dtc_acc = acc_evaluator.evaluate(dtc_predictions) rfc_acc = acc_evaluator.evaluate(rfc_predictions) gbt_acc = acc_evaluator.evaluate(gbt_predictions) . print(&#39;-&#39;*80) print(&#39;Decision tree accuracy: {0:2.2f}%&#39;.format(dtc_acc*100)) print(&#39;-&#39;*80) print(&#39;Random forest ensemble accuracy: {0:2.2f}%&#39;.format(rfc_acc*100)) print(&#39;-&#39;*80) print(&#39;GBT accuracy: {0:2.2f}%&#39;.format(gbt_acc*100)) print(&#39;-&#39;*80) . -- Decision tree accuracy: 81.98% -- Random forest ensemble accuracy: 88.29% -- GBT accuracy: 81.08% -- Classification Correlation with Corr . df.select(corr(&#39;specific_classification&#39;,&#39;fdi&#39;)).show() . +-+ corr(specific_classification, fdi)| +-+ 0.307429849493392| +-+ df.select(corr(&#39;specific_classification&#39;,&#39;gdp&#39;)).show() . +-+ corr(specific_classification, gdp)| +-+ 0.492176921599151| +-+ This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/25/Linear-Regression-and-Random-Forest_GBT-Classification-with-Pyspark.html",
            "relUrl": "/2020/08/25/Linear-Regression-and-Random-Forest_GBT-Classification-with-Pyspark.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post68": {
            "title": "Window functions and Pivot Tables with Pyspark",
            "content": "Resilient Distributed Datasets . Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory. . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType from pyspark.sql.functions import * data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows Using toPandas to look at the data . df.limit(10).toPandas() . _c0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.300049 | 50661.0 | 0.0 | 0.0 | 0.000000 | 1128873.0 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.320068 | 43443.0 | 0.0 | 0.0 | 0.000000 | 1356287.0 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.959961 | 27673.0 | 0.0 | 0.0 | 0.000000 | 1518236.0 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.340088 | 26131.0 | NaN | NaN | NaN | 1646891.0 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.090088 | 31847.0 | 0.0 | 0.0 | 0.000000 | 1601508.0 | East China | 1499110 | . 5 5 | Anhui | 250898.0 | NaN | 2001 | 3246.709961 | 33672.0 | 0.0 | 0.0 | 0.000000 | 1672445.0 | East China | 2165189 | . 6 6 | Anhui | 434149.0 | 66529.0 | 2002 | 3519.719971 | 38375.0 | 0.0 | 0.0 | 0.000000 | 1677840.0 | East China | 2404936 | . 7 7 | Anhui | 619201.0 | 52108.0 | 2003 | 3923.110107 | 36720.0 | 0.0 | 0.0 | 0.000000 | 1896479.0 | East China | 2815820 | . 8 8 | Anhui | 898441.0 | 349699.0 | 2004 | 4759.299805 | 54669.0 | 0.0 | 0.0 | 0.000000 | NaN | East China | 3422176 | . 9 9 | Anhui | 898441.0 | NaN | 2005 | 5350.169922 | 69000.0 | 0.0 | 0.0 | 0.324324 | NaN | East China | 3874846 | . Renaming Columns . df = df.withColumnRenamed(&quot;reg&quot;,&quot;region&quot;) . df.limit(10).toPandas() . _c0 province specific general year gdp fdi rnr rr i fr region it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.300049 | 50661.0 | 0.0 | 0.0 | 0.000000 | 1128873.0 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.320068 | 43443.0 | 0.0 | 0.0 | 0.000000 | 1356287.0 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.959961 | 27673.0 | 0.0 | 0.0 | 0.000000 | 1518236.0 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.340088 | 26131.0 | NaN | NaN | NaN | 1646891.0 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.090088 | 31847.0 | 0.0 | 0.0 | 0.000000 | 1601508.0 | East China | 1499110 | . 5 5 | Anhui | 250898.0 | NaN | 2001 | 3246.709961 | 33672.0 | 0.0 | 0.0 | 0.000000 | 1672445.0 | East China | 2165189 | . 6 6 | Anhui | 434149.0 | 66529.0 | 2002 | 3519.719971 | 38375.0 | 0.0 | 0.0 | 0.000000 | 1677840.0 | East China | 2404936 | . 7 7 | Anhui | 619201.0 | 52108.0 | 2003 | 3923.110107 | 36720.0 | 0.0 | 0.0 | 0.000000 | 1896479.0 | East China | 2815820 | . 8 8 | Anhui | 898441.0 | 349699.0 | 2004 | 4759.299805 | 54669.0 | 0.0 | 0.0 | 0.000000 | NaN | East China | 3422176 | . 9 9 | Anhui | 898441.0 | NaN | 2005 | 5350.169922 | 69000.0 | 0.0 | 0.0 | 0.324324 | NaN | East China | 3874846 | . # df = df.toDF(*[&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;it&#39;, &#39;fr&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;_c0&#39;, &#39;specific_classification&#39;, &#39;provinceIndex&#39;, &#39;regionIndex&#39;]) . Selecting Columns of Interest . df = df.select(&#39;year&#39;,&#39;region&#39;,&#39;province&#39;,&#39;gdp&#39;, &#39;fdi&#39;) . df.sort(&quot;gdp&quot;).show() . +-++--++-+ year| region|province| gdp| fdi| +-++--++-+ 1996|Southwest China| Tibet| 64.98| 679.0| 1997|Southwest China| Tibet| 77.24| 63.0| 1998|Southwest China| Tibet| 91.5| 481.0| 1999|Southwest China| Tibet|105.98| 196.0| 2000|Southwest China| Tibet| 117.8| 2.0| 2001|Southwest China| Tibet|139.16| 106.0| 2002|Southwest China| Tibet|162.04| 293.0| 1996|Northwest China| Qinghai|184.17| 576.0| 2003|Southwest China| Tibet|185.09| 467.0| 1997|Northwest China| Qinghai|202.79| 247.0| 1996|Northwest China| Ningxia| 202.9| 2826.0| 2004|Southwest China| Tibet|220.34| 2699.0| 1998|Northwest China| Qinghai|220.92| 1010.0| 1997|Northwest China| Ningxia|224.59| 671.0| 1999|Northwest China| Qinghai|239.38| 459.0| 1998|Northwest China| Ningxia|245.44| 1856.0| 2005|Southwest China| Tibet| 248.8| 1151.0| 2000|Northwest China| Qinghai|263.68|11020.0| 1999|Northwest China| Ningxia|264.58| 5134.0| 2006|Southwest China| Tibet|290.76| 1522.0| +-++--++-+ only showing top 20 rows Sorting RDDs by Columns . from pyspark.sql import functions as F df.sort(F.desc(&quot;gdp&quot;)).show() . +-+-++--++ year| region| province| gdp| fdi| +-+-++--++ 2007|South Central China|Guangdong|31777.01|1712603.0| 2006|South Central China|Guangdong|26587.76|1451065.0| 2007| East China| Shandong|25776.91|1101159.0| 2005|South Central China|Guangdong|22557.37|1236400.0| 2006| East China| Shandong|21900.19|1000069.0| 2007| East China| Jiangsu|21742.05|1743140.0| 2004|South Central China|Guangdong|18864.62|1001158.0| 2007| East China| Zhejiang|18753.73|1036576.0| 2006| East China| Jiangsu|18598.69|1318339.0| 2005| East China| Shandong|18366.87| 897000.0| 2003|South Central China|Guangdong|15844.64| 782294.0| 2006| East China| Zhejiang|15718.47| 888935.0| 2004| East China| Shandong|15021.84| 870064.0| 2007|South Central China| Henan|15012.46| 306162.0| 2005| East China| Jiangsu| 15003.6|1213800.0| 2007| North China| Hebei|13607.32| 241621.0| 2002|South Central China|Guangdong|13502.42|1133400.0| 2005| East China| Zhejiang|13417.68| 772000.0| 2007| East China| Shanghai|12494.01| 792000.0| 2004| East China| Jiangsu|12442.87|1056365.0| +-+-++--++ only showing top 20 rows Casting Data Types . from pyspark.sql.types import IntegerType, StringType, DoubleType df = df.withColumn(&#39;gdp&#39;, F.col(&#39;gdp&#39;).cast(DoubleType())) . df = df.withColumn(&#39;province&#39;, F.col(&#39;province&#39;).cast(StringType())) . df.filter((df.gdp&gt;10000) &amp; (df.region==&#39;East China&#39;)).show() . +-+-+--+-++ year| region|province| gdp| fdi| +-+-+--+-++ 2003|East China| Jiangsu| 10606.849609375|1018960.0| 2004|East China| Jiangsu|12442.8701171875|1056365.0| 2005|East China| Jiangsu| 15003.599609375|1213800.0| 2006|East China| Jiangsu| 18598.689453125|1318339.0| 2007|East China| Jiangsu| 21742.05078125|1743140.0| 2002|East China|Shandong| 10275.5| 473404.0| 2003|East China|Shandong| 12078.150390625| 601617.0| 2004|East China|Shandong| 15021.83984375| 870064.0| 2005|East China|Shandong| 18366.869140625| 897000.0| 2006|East China|Shandong| 21900.189453125|1000069.0| 2007|East China|Shandong| 25776.91015625|1101159.0| 2006|East China|Shanghai| 10572.240234375| 710700.0| 2007|East China|Shanghai| 12494.009765625| 792000.0| 2004|East China|Zhejiang|11648.7001953125| 668128.0| 2005|East China|Zhejiang| 13417.6796875| 772000.0| 2006|East China|Zhejiang|15718.4697265625| 888935.0| 2007|East China|Zhejiang| 18753.73046875|1036576.0| +-+-+--+-++ Aggregating using groupBy, .agg and sum/max . from pyspark.sql import functions as F df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg(F.sum(&quot;gdp&quot;) ,F.max(&quot;gdp&quot;)).show() . +-++++ region| province| sum(gdp)| max(gdp)| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg(F.sum(&quot;gdp&quot;).alias(&quot;SumGDP&quot;),F.max(&quot;gdp&quot;).alias(&quot;MaxGDP&quot;)).show() . +-++++ region| province| GDP| MaxGDP| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg( F.sum(&quot;gdp&quot;).alias(&quot;SumGDP&quot;), F.max(&quot;gdp&quot;).alias(&quot;MaxGDP&quot;) ).show() . +-++++ region| province| SumGDP| MaxGDP| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.limit(10).toPandas() . year region province gdp fdi . 0 1996 | East China | Anhui | 2093.300049 | 50661.0 | . 1 1997 | East China | Anhui | 2347.320068 | 43443.0 | . 2 1998 | East China | Anhui | 2542.959961 | 27673.0 | . 3 1999 | East China | Anhui | 2712.340088 | 26131.0 | . 4 2000 | East China | Anhui | 2902.090088 | 31847.0 | . 5 2001 | East China | Anhui | 3246.709961 | 33672.0 | . 6 2002 | East China | Anhui | 3519.719971 | 38375.0 | . 7 2003 | East China | Anhui | 3923.110107 | 36720.0 | . 8 2004 | East China | Anhui | 4759.299805 | 54669.0 | . 9 2005 | East China | Anhui | 5350.169922 | 69000.0 | . Exponentials using exp . df = df.withColumn(&quot;Exp_GDP&quot;, F.exp(&quot;gdp&quot;)) df.show() . +-+--+--+--+--+--+ year| region|province| gdp| fdi| Exp_GDP| +-+--+--+--+--+--+ 1996| East China| Anhui|2093.300048828125| 50661.0|Infinity| 1997| East China| Anhui|2347.320068359375| 43443.0|Infinity| 1998| East China| Anhui| 2542.9599609375| 27673.0|Infinity| 1999| East China| Anhui|2712.340087890625| 26131.0|Infinity| 2000| East China| Anhui|2902.090087890625| 31847.0|Infinity| 2001| East China| Anhui| 3246.7099609375| 33672.0|Infinity| 2002| East China| Anhui|3519.719970703125| 38375.0|Infinity| 2003| East China| Anhui|3923.110107421875| 36720.0|Infinity| 2004| East China| Anhui| 4759.2998046875| 54669.0|Infinity| 2005| East China| Anhui| 5350.169921875| 69000.0|Infinity| 2006| East China| Anhui| 6112.5|139354.0|Infinity| 2007| East China| Anhui| 7360.919921875|299892.0|Infinity| 1996|North China| Beijing|1789.199951171875|155290.0|Infinity| 1997|North China| Beijing|2077.090087890625|159286.0|Infinity| 1998|North China| Beijing|2377.179931640625|216800.0|Infinity| 1999|North China| Beijing|2678.820068359375|197525.0|Infinity| 2000|North China| Beijing|3161.659912109375|168368.0|Infinity| 2001|North China| Beijing| 3707.9599609375|176818.0|Infinity| 2002|North China| Beijing| 4315.0|172464.0|Infinity| 2003|North China| Beijing| 5007.2099609375|219126.0|Infinity| +-+--+--+--+--+--+ only showing top 20 rows Window functions . . Note: Window functions . # Window functions from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(F.desc(&#39;gdp&#39;)) df.withColumn(&quot;rank&quot;,F.rank().over(windowSpec)).show() . +-+-++--++--+-+ year| region| province| gdp| fdi| Exp_GDP|rank| +-+-++--++--+-+ 2007|South Central China|Guangdong| 31777.009765625|1712603.0|Infinity| 1| 2006|South Central China|Guangdong| 26587.759765625|1451065.0|Infinity| 2| 2005|South Central China|Guangdong| 22557.369140625|1236400.0|Infinity| 3| 2004|South Central China|Guangdong| 18864.619140625|1001158.0|Infinity| 4| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity| 5| 2002|South Central China|Guangdong| 13502.419921875|1133400.0|Infinity| 6| 2001|South Central China|Guangdong| 12039.25|1193203.0|Infinity| 7| 2000|South Central China|Guangdong| 10741.25|1128091.0|Infinity| 8| 1999|South Central China|Guangdong| 9250.6796875|1165750.0|Infinity| 9| 1998|South Central China|Guangdong| 8530.8798828125|1201994.0|Infinity| 10| 1997|South Central China|Guangdong| 7774.52978515625|1171083.0|Infinity| 11| 1996|South Central China|Guangdong| 6834.97021484375|1162362.0|Infinity| 12| 2007|South Central China| Hunan| 9439.599609375| 327051.0|Infinity| 1| 2006|South Central China| Hunan| 7688.669921875| 259335.0|Infinity| 2| 2005|South Central China| Hunan| 6596.10009765625| 207200.0|Infinity| 3| 2004|South Central China| Hunan| 5641.93994140625| 141800.0|Infinity| 4| 2003|South Central China| Hunan| 4659.990234375| 101835.0|Infinity| 5| 2002|South Central China| Hunan| 4151.5400390625| 90022.0|Infinity| 6| 2001|South Central China| Hunan| 3831.89990234375| 81011.0|Infinity| 7| 2000|South Central China| Hunan|3551.489990234375| 67833.0|Infinity| 8| +-+-++--++--+-+ only showing top 20 rows from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;) . Lagging Variables . dfWithLag = df.withColumn(&quot;lag_7&quot;,F.lag(&quot;gdp&quot;, 7).over(windowSpec)) . df.filter(df.year&gt;&#39;2000&#39;).show() . +-++++--+--+ year| region| province| gdp| fdi| Exp_GDP| +-++++--+--+ 2001| East China| Anhui| 3246.7099609375| 33672.0|Infinity| 2002| East China| Anhui| 3519.719970703125| 38375.0|Infinity| 2003| East China| Anhui| 3923.110107421875| 36720.0|Infinity| 2004| East China| Anhui| 4759.2998046875| 54669.0|Infinity| 2005| East China| Anhui| 5350.169921875| 69000.0|Infinity| 2006| East China| Anhui| 6112.5|139354.0|Infinity| 2007| East China| Anhui| 7360.919921875|299892.0|Infinity| 2001| North China| Beijing| 3707.9599609375|176818.0|Infinity| 2002| North China| Beijing| 4315.0|172464.0|Infinity| 2003| North China| Beijing| 5007.2099609375|219126.0|Infinity| 2004| North China| Beijing| 6033.2099609375|308354.0|Infinity| 2005| North China| Beijing| 6969.52001953125|352638.0|Infinity| 2006| North China| Beijing| 8117.77978515625|455191.0|Infinity| 2007| North China| Beijing| 9846.8095703125|506572.0|Infinity| 2001|Southwest China|Chongqing|1976.8599853515625| 25649.0|Infinity| 2002|Southwest China|Chongqing| 2232.860107421875| 19576.0|Infinity| 2003|Southwest China|Chongqing| 2555.719970703125| 26083.0|Infinity| 2004|Southwest China|Chongqing| 3034.580078125| 40508.0|Infinity| 2005|Southwest China|Chongqing| 3467.719970703125| 51600.0|Infinity| 2006|Southwest China|Chongqing| 3907.22998046875| 69595.0|Infinity| +-++++--+--+ only showing top 20 rows Looking at windows within the data . from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;).rowsBetween(-6,0) . dfWithRoll = df.withColumn(&quot;roll_7_confirmed&quot;,F.mean(&quot;gdp&quot;).over(windowSpec)) . dfWithRoll.filter(dfWithLag.year&gt;&#39;2001&#39;).show() . +-+-++++--++ year| region| province| gdp| fdi| Exp_GDP| roll_7_confirmed| +-+-++++--++ 2002|South Central China|Guangdong| 13502.419921875|1133400.0| Infinity| 9810.56849888393| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0| Infinity|11097.664132254464| 2004|South Central China|Guangdong| 18864.619140625|1001158.0| Infinity|12681.962611607143| 2005|South Central China|Guangdong| 22557.369140625|1236400.0| Infinity|14685.746791294643| 2006|South Central China|Guangdong| 26587.759765625|1451065.0| Infinity|17162.472516741072| 2007|South Central China|Guangdong| 31777.009765625|1712603.0| Infinity| 20167.5810546875| 2002|South Central China| Hunan| 4151.5400390625| 90022.0| Infinity|3309.1999860491073| 2003|South Central China| Hunan| 4659.990234375| 101835.0| Infinity| 3612.037179129464| 2004|South Central China| Hunan| 5641.93994140625| 141800.0| Infinity|4010.9900251116073| 2005|South Central China| Hunan| 6596.10009765625| 207200.0| Infinity| 4521.07146344866| 2006|South Central China| Hunan| 7688.669921875| 259335.0| Infinity| 5160.232875279018| 2007|South Central China| Hunan| 9439.599609375| 327051.0| Infinity| 6001.391392299107| 2002| North China| Shanxi| 2324.800048828125| 21164.0| Infinity|1749.4771379743304| 2003| North China| Shanxi| 2855.22998046875| 21361.0| Infinity| 1972.779994419643| 2004| North China| Shanxi| 3571.3701171875| 62184.0| Infinity| 2272.118582589286| 2005| North China| Shanxi| 4230.52978515625| 27516.0| Infinity| 2646.325701032366| 2006| North China| Shanxi| 4878.60986328125| 47199.0| Infinity|3105.1128278459823| 2007| North China| Shanxi| 6024.4501953125| 134283.0| Infinity| 3702.074288504464| 2002| Southwest China| Tibet| 162.0399932861328| 293.0|2.360885537826244E70|108.38571493966239| 2003| Southwest China| Tibet|185.08999633789062| 467.0|2.418600091901801E80|125.54428536551339| +-+-++++--++ only showing top 20 rows from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;).rowsBetween(Window.unboundedPreceding,Window.currentRow) . dfWithRoll = df.withColumn(&quot;cumulative_gdp&quot;,F.sum(&quot;gdp&quot;).over(windowSpec)) . dfWithRoll.filter(dfWithLag.year&gt;&#39;1999&#39;).show() . +-+-++--++--++ year| region| province| gdp| fdi| Exp_GDP| cumulative_gdp| +-+-++--++--++ 2000|South Central China|Guangdong| 10741.25|1128091.0|Infinity| 43132.3095703125| 2001|South Central China|Guangdong| 12039.25|1193203.0|Infinity| 55171.5595703125| 2002|South Central China|Guangdong| 13502.419921875|1133400.0|Infinity| 68673.9794921875| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity| 84518.619140625| 2004|South Central China|Guangdong| 18864.619140625|1001158.0|Infinity| 103383.23828125| 2005|South Central China|Guangdong| 22557.369140625|1236400.0|Infinity| 125940.607421875| 2006|South Central China|Guangdong| 26587.759765625|1451065.0|Infinity| 152528.3671875| 2007|South Central China|Guangdong| 31777.009765625|1712603.0|Infinity| 184305.376953125| 2000|South Central China| Hunan|3551.489990234375| 67833.0|Infinity| 15180.9599609375| 2001|South Central China| Hunan| 3831.89990234375| 81011.0|Infinity| 19012.85986328125| 2002|South Central China| Hunan| 4151.5400390625| 90022.0|Infinity| 23164.39990234375| 2003|South Central China| Hunan| 4659.990234375| 101835.0|Infinity| 27824.39013671875| 2004|South Central China| Hunan| 5641.93994140625| 141800.0|Infinity| 33466.330078125| 2005|South Central China| Hunan| 6596.10009765625| 207200.0|Infinity| 40062.43017578125| 2006|South Central China| Hunan| 7688.669921875| 259335.0|Infinity| 47751.10009765625| 2007|South Central China| Hunan| 9439.599609375| 327051.0|Infinity| 57190.69970703125| 2000| North China| Shanxi|1845.719970703125| 22472.0|Infinity|7892.0098876953125| 2001| North China| Shanxi|2029.530029296875| 23393.0|Infinity| 9921.539916992188| 2002| North China| Shanxi|2324.800048828125| 21164.0|Infinity|12246.339965820312| 2003| North China| Shanxi| 2855.22998046875| 21361.0|Infinity|15101.569946289062| +-+-++--++--++ only showing top 20 rows Pivot Dataframes . . Note: Pivot Dataframes . pivoted_df = df.groupBy(&#39;year&#39;).pivot(&#39;province&#39;) .agg(F.sum(&#39;gdp&#39;).alias(&#39;gdp&#39;) , F.sum(&#39;fdi&#39;).alias(&#39;fdi&#39;)) pivoted_df.limit(10).toPandas() . year Anhui_gdp Anhui_fdi Beijing_gdp Beijing_fdi Chongqing_gdp Chongqing_fdi Fujian_gdp Fujian_fdi Gansu_gdp Gansu_fdi Guangdong_gdp Guangdong_fdi Guangxi_gdp Guangxi_fdi Guizhou_gdp Guizhou_fdi Hainan_gdp Hainan_fdi Hebei_gdp Hebei_fdi Heilongjiang_gdp Heilongjiang_fdi Henan_gdp Henan_fdi Hubei_gdp Hubei_fdi Hunan_gdp Hunan_fdi Jiangsu_gdp Jiangsu_fdi Jiangxi_gdp Jiangxi_fdi Jilin_gdp Jilin_fdi Liaoning_gdp Liaoning_fdi Ningxia_gdp Ningxia_fdi Qinghai_gdp Qinghai_fdi Shaanxi_gdp Shaanxi_fdi Shandong_gdp Shandong_fdi Shanghai_gdp Shanghai_fdi Shanxi_gdp Shanxi_fdi Sichuan_gdp Sichuan_fdi Tianjin_gdp Tianjin_fdi Tibet_gdp Tibet_fdi Xinjiang_gdp Xinjiang_fdi Yunnan_gdp Yunnan_fdi Zhejiang_gdp Zhejiang_fdi . 0 2003 | 3923.110107 | 36720.0 | 5007.209961 | 219126.0 | 2555.719971 | 26083.0 | 4983.669922 | 259903.0 | 1399.829956 | 2342.0 | 15844.639648 | 782294.0 | 2821.110107 | 41856.0 | 1426.339966 | 4521.0 | 713.960022 | 42125.0 | 6921.290039 | 96405.0 | 4057.399902 | 32180.0 | 6867.700195 | 53903.0 | 4757.450195 | 156886.0 | 4659.990234 | 101835.0 | 10606.849609 | 1018960.0 | 2450.479980 | 108197.0 | 2348.540039 | 24468.0 | 5458.220215 | 341168.0 | 445.359985 | 1743.0 | 390.200012 | 2522.0 | 2587.719971 | 33190.0 | 12078.150391 | 601617.0 | 6694.229980 | 546849.0 | 2855.229980 | 21361.0 | 5333.089844 | 41231.0 | 2578.030029 | 153473.0 | 185.089996 | 467.0 | 1886.349976 | 1534.0 | 2556.020020 | 8384.0 | 9705.019531 | 498055.0 | . 1 2007 | 7360.919922 | 299892.0 | 9846.809570 | 506572.0 | 4676.129883 | 108534.0 | 9248.530273 | 406058.0 | 2703.979980 | 11802.0 | 31777.009766 | 1712603.0 | 5823.410156 | 68396.0 | 2884.110107 | 12651.0 | 1254.170044 | 112001.0 | 13607.320312 | 241621.0 | 7104.000000 | 208508.0 | 15012.459961 | 306162.0 | 9333.400391 | 276622.0 | 9439.599609 | 327051.0 | 21742.050781 | 1743140.0 | 4820.529785 | 280657.0 | 4275.120117 | 76064.0 | 9304.519531 | 598554.0 | 919.109985 | 5047.0 | 797.349976 | 31000.0 | 5757.290039 | 119516.0 | 25776.910156 | 1101159.0 | 12494.009766 | 792000.0 | 6024.450195 | 134283.0 | 10562.389648 | 149322.0 | 5252.759766 | 527776.0 | 341.429993 | 2418.0 | 3523.159912 | 12484.0 | 4772.520020 | 39453.0 | 18753.730469 | 1036576.0 | . 2 2006 | 6112.500000 | 139354.0 | 8117.779785 | 455191.0 | 3907.229980 | 69595.0 | 7583.850098 | 322047.0 | 2277.350098 | 2954.0 | 26587.759766 | 1451065.0 | 4746.160156 | 44740.0 | 2338.979980 | 9384.0 | 1065.670044 | 74878.0 | 11467.599609 | 201434.0 | 6211.799805 | 170801.0 | 12362.790039 | 184526.0 | 7617.470215 | 244853.0 | 7688.669922 | 259335.0 | 18598.689453 | 1318339.0 | 4056.760010 | 242000.0 | 3620.270020 | 66100.0 | 8047.259766 | 359000.0 | 725.900024 | 3718.0 | 648.500000 | 27500.0 | 4743.609863 | 92489.0 | 21900.189453 | 1000069.0 | 10572.240234 | 710700.0 | 4878.609863 | 47199.0 | 8690.240234 | 120819.0 | 4462.740234 | 413077.0 | 290.760010 | 1522.0 | 3045.260010 | 10366.0 | 3988.139893 | 30234.0 | 15718.469727 | 888935.0 | . 3 1997 | 2347.320068 | 43443.0 | 2077.090088 | 159286.0 | 1509.750000 | 38675.0 | 2870.899902 | 419666.0 | 793.570007 | 4144.0 | 7774.529785 | 1171083.0 | 1817.250000 | 87986.0 | 805.789978 | 4977.0 | 411.160004 | 70554.0 | 3953.780029 | 110064.0 | 2667.500000 | 73485.0 | 4041.090088 | 69204.0 | 2856.469971 | 79019.0 | 2849.270020 | 91702.0 | 6004.209961 | 507208.0 | 1409.739990 | 30068.0 | 1346.790039 | 45155.0 | 3157.689941 | 167142.0 | 224.589996 | 671.0 | 202.789993 | 247.0 | 1363.599976 | 62816.0 | 6537.069824 | 249294.0 | 3438.790039 | 422536.0 | 1476.000000 | 26592.0 | 3241.469971 | 24846.0 | 1264.630005 | 251135.0 | 77.239998 | 63.0 | 1039.849976 | 2472.0 | 1676.170044 | 16566.0 | 4686.109863 | 150345.0 | . 4 2004 | 4759.299805 | 54669.0 | 6033.209961 | 308354.0 | 3034.580078 | 40508.0 | 5763.350098 | 474801.0 | 1688.489990 | 3539.0 | 18864.619141 | 1001158.0 | 3433.500000 | 29579.0 | 1677.800049 | 6533.0 | 819.659973 | 64343.0 | 8477.629883 | 162341.0 | 4750.600098 | 123639.0 | 8553.790039 | 87367.0 | 5633.240234 | 207126.0 | 5641.939941 | 141800.0 | 12442.870117 | 1056365.0 | 2807.409912 | 161202.0 | 2662.080078 | 19059.0 | 6002.540039 | 282410.0 | 537.109985 | 6689.0 | 466.100006 | 22500.0 | 3175.580078 | 52664.0 | 15021.839844 | 870064.0 | 8072.830078 | 654100.0 | 3571.370117 | 62184.0 | 6379.629883 | 70129.0 | 3110.969971 | 247243.0 | 220.339996 | 2699.0 | 2209.090088 | 4586.0 | 3081.909912 | 14200.0 | 11648.700195 | 668128.0 | . 5 1996 | 2093.300049 | 50661.0 | 1789.199951 | 155290.0 | 1315.119995 | 21878.0 | 2484.250000 | 407876.0 | 722.520020 | 9002.0 | 6834.970215 | 1162362.0 | 1697.900024 | 66618.0 | 723.179993 | 3138.0 | 389.679993 | 78960.0 | 3452.969971 | 123652.0 | 2370.500000 | 54841.0 | 3634.689941 | 52566.0 | 2499.770020 | 68878.0 | 2540.129883 | 70344.0 | 5155.250000 | 478058.0 | 1169.729980 | 28818.0 | 1137.229980 | 39876.0 | 2793.370117 | 140405.0 | 202.899994 | 2826.0 | 184.169998 | 576.0 | 1215.839966 | 33008.0 | 5883.799805 | 259041.0 | 2957.550049 | 471578.0 | 1292.109985 | 13802.0 | 2871.649902 | 22519.0 | 1121.930054 | 200587.0 | 64.980003 | 679.0 | 900.929993 | 6639.0 | 1517.689941 | 18000.0 | 4188.529785 | 152021.0 | . 6 1998 | 2542.959961 | 27673.0 | 2377.179932 | 216800.0 | 1602.380005 | 43107.0 | 3159.909912 | 421211.0 | 887.669983 | 3864.0 | 8530.879883 | 1201994.0 | 1911.300049 | 88613.0 | 858.390015 | 4535.0 | 442.130005 | 71715.0 | 4256.009766 | 142868.0 | 2774.399902 | 52639.0 | 4308.240234 | 61654.0 | 3114.020020 | 97294.0 | 3025.530029 | 81816.0 | 6680.339844 | 543511.0 | 1605.770020 | 47768.0 | 1464.339966 | 40227.0 | 3582.459961 | 220470.0 | 245.440002 | 1856.0 | 220.919998 | 1010.0 | 1458.400024 | 30010.0 | 7021.350098 | 220274.0 | 3801.090088 | 360150.0 | 1611.079956 | 24451.0 | 3474.090088 | 37248.0 | 1374.599976 | 211361.0 | 91.500000 | 481.0 | 1106.949951 | 2167.0 | 1831.329956 | 14568.0 | 5052.620117 | 131802.0 | . 7 2001 | 3246.709961 | 33672.0 | 3707.959961 | 176818.0 | 1976.859985 | 25649.0 | 4072.850098 | 391804.0 | 1125.369995 | 7439.0 | 12039.250000 | 1193203.0 | 2279.340088 | 38416.0 | 1133.270020 | 2829.0 | 579.169983 | 46691.0 | 5516.759766 | 66989.0 | 3390.100098 | 34114.0 | 5533.009766 | 45729.0 | 3880.530029 | 118860.0 | 3831.899902 | 81011.0 | 8553.690430 | 642550.0 | 2003.069946 | 22724.0 | 1951.510010 | 33701.0 | 4669.060059 | 204446.0 | 337.440002 | 1680.0 | 300.130005 | 3649.0 | 2010.619995 | 35174.0 | 9195.040039 | 352093.0 | 5210.120117 | 429159.0 | 2029.530029 | 23393.0 | 4293.490234 | 58188.0 | 1919.089966 | 213348.0 | 139.160004 | 106.0 | 1491.599976 | 2035.0 | 2138.310059 | 6457.0 | 6898.339844 | 221162.0 | . 8 2005 | 5350.169922 | 69000.0 | 6969.520020 | 352638.0 | 3467.719971 | 51600.0 | 6554.689941 | 260800.0 | 1933.979980 | 2000.0 | 22557.369141 | 1236400.0 | 3984.100098 | 37866.0 | 2005.420044 | 10768.0 | 918.750000 | 68400.0 | 10012.110352 | 191000.0 | 5513.700195 | 145000.0 | 10587.419922 | 123000.0 | 6590.189941 | 218500.0 | 6596.100098 | 207200.0 | 15003.599609 | 1213800.0 | 3456.699951 | 205238.0 | 3122.010010 | 45266.0 | 6672.000000 | 540679.0 | 612.609985 | 14100.0 | 543.320007 | 26600.0 | 3933.719971 | 62800.0 | 18366.869141 | 897000.0 | 9247.660156 | 685000.0 | 4230.529785 | 27516.0 | 7385.100098 | 88686.0 | 3905.639893 | 332885.0 | 248.800003 | 1151.0 | 2604.189941 | 4700.0 | 3462.729980 | 17352.0 | 13417.679688 | 772000.0 | . 9 2000 | 2902.090088 | 31847.0 | 3161.659912 | 168368.0 | 1791.000000 | 24436.0 | 3764.540039 | 343191.0 | 1052.880005 | 6235.0 | 10741.250000 | 1128091.0 | 2080.040039 | 52466.0 | 1029.920044 | 2501.0 | 526.820007 | 43080.0 | 5043.959961 | 67923.0 | 3151.399902 | 30086.0 | 5052.990234 | 56403.0 | 3545.389893 | 94368.0 | 3551.489990 | 67833.0 | 7697.819824 | 607756.0 | 1853.650024 | 32080.0 | 1672.959961 | 30120.0 | 4171.689941 | 106173.0 | 295.019989 | 1741.0 | 263.679993 | 11020.0 | 1804.000000 | 28842.0 | 8337.469727 | 297119.0 | 4771.169922 | 316014.0 | 1845.719971 | 22472.0 | 3928.199951 | 43694.0 | 1701.880005 | 116601.0 | 117.800003 | 2.0 | 1363.560059 | 1911.0 | 2011.189941 | 12812.0 | 6141.029785 | 161266.0 | . pivoted_df.columns . Out[55]: [&#39;year&#39;, &#39;Anhui_gdp&#39;, &#39;Anhui_fdi&#39;, &#39;Beijing_gdp&#39;, &#39;Beijing_fdi&#39;, &#39;Chongqing_gdp&#39;, &#39;Chongqing_fdi&#39;, &#39;Fujian_gdp&#39;, &#39;Fujian_fdi&#39;, &#39;Gansu_gdp&#39;, &#39;Gansu_fdi&#39;, &#39;Guangdong_gdp&#39;, &#39;Guangdong_fdi&#39;, &#39;Guangxi_gdp&#39;, &#39;Guangxi_fdi&#39;, &#39;Guizhou_gdp&#39;, &#39;Guizhou_fdi&#39;, &#39;Hainan_gdp&#39;, &#39;Hainan_fdi&#39;, &#39;Hebei_gdp&#39;, &#39;Hebei_fdi&#39;, &#39;Heilongjiang_gdp&#39;, &#39;Heilongjiang_fdi&#39;, &#39;Henan_gdp&#39;, &#39;Henan_fdi&#39;, &#39;Hubei_gdp&#39;, &#39;Hubei_fdi&#39;, &#39;Hunan_gdp&#39;, &#39;Hunan_fdi&#39;, &#39;Jiangsu_gdp&#39;, &#39;Jiangsu_fdi&#39;, &#39;Jiangxi_gdp&#39;, &#39;Jiangxi_fdi&#39;, &#39;Jilin_gdp&#39;, &#39;Jilin_fdi&#39;, &#39;Liaoning_gdp&#39;, &#39;Liaoning_fdi&#39;, &#39;Ningxia_gdp&#39;, &#39;Ningxia_fdi&#39;, &#39;Qinghai_gdp&#39;, &#39;Qinghai_fdi&#39;, &#39;Shaanxi_gdp&#39;, &#39;Shaanxi_fdi&#39;, &#39;Shandong_gdp&#39;, &#39;Shandong_fdi&#39;, &#39;Shanghai_gdp&#39;, &#39;Shanghai_fdi&#39;, &#39;Shanxi_gdp&#39;, &#39;Shanxi_fdi&#39;, &#39;Sichuan_gdp&#39;, &#39;Sichuan_fdi&#39;, &#39;Tianjin_gdp&#39;, &#39;Tianjin_fdi&#39;, &#39;Tibet_gdp&#39;, &#39;Tibet_fdi&#39;, &#39;Xinjiang_gdp&#39;, &#39;Xinjiang_fdi&#39;, &#39;Yunnan_gdp&#39;, &#39;Yunnan_fdi&#39;, &#39;Zhejiang_gdp&#39;, &#39;Zhejiang_fdi&#39;] newColnames = [x.replace(&quot;-&quot;,&quot;_&quot;) for x in pivoted_df.columns] . pivoted_df = pivoted_df.toDF(*newColnames) . expression = &quot;&quot; cnt=0 for column in pivoted_df.columns: if column!=&#39;year&#39;: cnt +=1 expression += f&quot;&#39;{column}&#39; , {column},&quot; expression = f&quot;stack({cnt}, {expression[:-1]}) as (Type,Value)&quot; . Unpivoting RDDs . unpivoted_df = pivoted_df.select(&#39;year&#39;,F.expr(expression)) unpivoted_df.show() . +-+-++ year| Type| Value| +-+-++ 2003| Anhui_gdp| 3923.110107421875| 2003| Anhui_fdi| 36720.0| 2003| Beijing_gdp| 5007.2099609375| 2003| Beijing_fdi| 219126.0| 2003|Chongqing_gdp| 2555.719970703125| 2003|Chongqing_fdi| 26083.0| 2003| Fujian_gdp| 4983.669921875| 2003| Fujian_fdi| 259903.0| 2003| Gansu_gdp|1399.8299560546875| 2003| Gansu_fdi| 2342.0| 2003|Guangdong_gdp| 15844.6396484375| 2003|Guangdong_fdi| 782294.0| 2003| Guangxi_gdp| 2821.110107421875| 2003| Guangxi_fdi| 41856.0| 2003| Guizhou_gdp|1426.3399658203125| 2003| Guizhou_fdi| 4521.0| 2003| Hainan_gdp| 713.9600219726562| 2003| Hainan_fdi| 42125.0| 2003| Hebei_gdp| 6921.2900390625| 2003| Hebei_fdi| 96405.0| +-+-++ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/22/Window-functions-and-Pivot-Tables-with-Pyspark.html",
            "relUrl": "/2020/08/22/Window-functions-and-Pivot-Tables-with-Pyspark.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post69": {
            "title": "RDDs and Schemas and Data Types with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) df.columns . Out[64]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;] df.describe() . Out[65]: DataFrame[summary: string, _c0: string, province: string, specific: string, general: string, year: string, gdp: string, fdi: string, rnr: string, rr: string, i: string, fr: string, reg: string, it: string] Setting Data Schema and Data Types . from pyspark.sql.types import StructField,StringType,IntegerType,StructType . data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, IntegerType(), True) ,StructField(&quot;general&quot;, IntegerType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, IntegerType(), True) ,StructField(&quot;fdi&quot;, IntegerType(), True) ,StructField(&quot;rnr&quot;, IntegerType(), True) ,StructField(&quot;rr&quot;, IntegerType(), True) ,StructField(&quot;i&quot;, IntegerType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] . final_struc = StructType(fields=data_schema) . Applying the Data Schema/Data Types while reading in a CSV . df = spark.read.format(&quot;CSV&quot;).schema(final_struc).load(file_location) . df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) df.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows df[&#39;fr&#39;] . Out[72]: Column&lt;b&#39;fr&#39;&gt; type(df[&#39;fr&#39;]) . Out[73]: pyspark.sql.column.Column df.select(&#39;fr&#39;) . Out[74]: DataFrame[fr: int] type(df.select(&#39;fr&#39;)) . Out[75]: pyspark.sql.dataframe.DataFrame df.select(&#39;fr&#39;).show() . +-+ fr| +-+ null| 1128873| 1356287| 1518236| 1646891| 1601508| 1672445| 1677840| 1896479| null| null| 3434548| 4468640| 634562| 634562| 938788| null| 1667114| 2093925| 2511249| +-+ only showing top 20 rows df.head(2) . Out[77]: [Row(_c0=None, province=&#39;province&#39;, specific=None, general=None, year=None, gdp=None, fdi=None, rnr=None, rr=None, i=None, fr=None, reg=&#39;reg&#39;, it=None), Row(_c0=0, province=&#39;Anhui&#39;, specific=None, general=None, year=1996, gdp=None, fdi=50661, rnr=None, rr=None, i=None, fr=1128873, reg=&#39;East China&#39;, it=631930)] df.select([&#39;reg&#39;,&#39;fr&#39;]) . Out[78]: DataFrame[reg: string, fr: int] Using select with RDDs . df.select([&#39;reg&#39;,&#39;fr&#39;]).show() . +--+-+ reg| fr| +--+-+ reg| null| East China|1128873| East China|1356287| East China|1518236| East China|1646891| East China|1601508| East China|1672445| East China|1677840| East China|1896479| East China| null| East China| null| East China|3434548| East China|4468640| North China| 634562| North China| 634562| North China| 938788| North China| null| North China|1667114| North China|2093925| North China|2511249| +--+-+ only showing top 20 rows df.withColumn(&#39;fiscal_revenue&#39;,df[&#39;fr&#39;]).show() . +-+--+--+-+-+-++-+-+-+-+--+-+--+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-+--+ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1128873| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 1356287| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 1518236| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 1646891| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 1601508| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 1672445| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 1677840| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 1896479| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 3434548| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 4468640| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 634562| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 634562| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 938788| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 1667114| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 2093925| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 2511249| +-+--+--+-+-+-++-+-+-+-+--+-+--+ only showing top 20 rows df.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows Renaming Columns using withColumnRenamed . df.withColumnRenamed(&#39;fr&#39;,&#39;new_fiscal_revenue&#39;).show() . +-+--+--+-+-+-++-+-+-++--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i|new_fiscal_revenue| reg| it| +-+--+--+-+-+-++-+-+-++--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null| 1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null| 1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null| 1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null| 1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null| 1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null| 1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null| 1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null| 1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null| 3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null| 4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null| 1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null| 2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null| 2511249|North China|1078754| +-+--+--+-+-+-++-+-+-++--+-+ only showing top 20 rows New Columns by Transforming extant Columns using withColumn . df.withColumn(&#39;double_fiscal_revenue&#39;,df[&#39;fr&#39;]*2).show() . +-+--+--+-+-+-++-+-+-+-+--+-++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|double_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-++ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 2257746| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2712574| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3036472| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 3293782| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 3203016| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 3344890| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 3355680| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 3792958| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 6869096| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 8937280| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 1269124| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 1269124| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 1877576| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 3334228| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 4187850| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 5022498| +-+--+--+-+-+-++-+-+-+-+--+-++ only showing top 20 rows df.withColumn(&#39;add_fiscal_revenue&#39;,df[&#39;fr&#39;]+1).show() . +-+--+--+-+-+-++-+-+-+-+--+-++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|add_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-++ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1128874| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 1356288| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 1518237| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 1646892| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 1601509| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 1672446| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 1677841| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 1896480| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 3434549| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 4468641| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 634563| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 634563| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 938789| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 1667115| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 2093926| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 2511250| +-+--+--+-+-+-++-+-+-+-+--+-++ only showing top 20 rows df.withColumn(&#39;half_fiscal_revenue&#39;,df[&#39;fr&#39;]/2).show() . +-+--+--+-+-+-++-+-+-+-+--+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|half_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 564436.5| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 678143.5| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 759118.0| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 823445.5| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 800754.0| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 836222.5| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 838920.0| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 948239.5| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 1717274.0| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 2234320.0| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 317281.0| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 317281.0| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 469394.0| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 833557.0| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 1046962.5| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 1255624.5| +-+--+--+-+-+-++-+-+-+-+--+-+-+ only showing top 20 rows df.withColumn(&#39;half_fr&#39;,df[&#39;fr&#39;]/2) . Out[86]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int, half_fr: double] Spark SQL for SQL functionality using createOrReplaceTempView . df.createOrReplaceTempView(&quot;economic_data&quot;) . sql_results = spark.sql(&quot;SELECT * FROM economic_data&quot;) . sql_results . Out[89]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int] sql_results.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows spark.sql(&quot;SELECT * FROM economic_data WHERE fr=634562&quot;).show() . ++--+--+-+-+-++-+-+-++--++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+-++-+-+-++--++ 12| Beijing| null| null|1996|null|155290|null|null|null|634562|North China|508135| 13| Beijing| null| null|1997|null|159286|null|null|null|634562|North China|569283| ++--+--+-+-+-++-+-+-++--++ This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/21/RDDs-and-Schemas-and-Data-Types-with-Pyspark.html",
            "relUrl": "/2020/08/21/RDDs-and-Schemas-and-Data-Types-with-Pyspark.html",
            "date": " • Aug 21, 2020"
        }
        
    
  
    
        ,"post70": {
            "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.columns . Out[85]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;] df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) # for row in df.head(5): # print(row) # print(&#39; n&#39;) . df.describe().show() . +-++--+--+++--++-+--+-++++ summary| _c0|province| specific| general| year| gdp| fdi| rnr| rr| i| fr| reg| it| +-++--+--+++--++-+--+-++++ count| 360| 360| 356| 169| 360| 360| 360| 294| 296| 287| 295| 360| 360| mean| 179.5| null|583470.7303370787|309127.53846153844| 2001.5|4428.653416666667|196139.38333333333| 0.0355944252244898|0.059688621057432424|0.08376351662369343|2522449.0034013605| null|2165819.2583333333| stddev|104.06728592598157| null|654055.3290782663| 355423.5760674793|3.4568570586927794|4484.668659976412|303043.97011891654|0.16061503029299648| 0.15673351824073453| 0.1838933104683607|3491329.8613106664| null|1769294.2935487411| min| 0| Anhui| 8964.0| 0.0| 1996| 64.98| 2| 0.0| 0.0| 0.0| #REF!| East China| 147897| max| 359|Zhejiang| 3937966.0| 1737800.0| 2007| 31777.01| 1743140| 1.214285714| 0.84| 1.05| 9898522|Southwest China| 10533312| +-++--+--+++--++-+--+-++++ df.describe().printSchema() . root -- summary: string (nullable = true) -- _c0: string (nullable = true) -- province: string (nullable = true) -- specific: string (nullable = true) -- general: string (nullable = true) -- year: string (nullable = true) -- gdp: string (nullable = true) -- fdi: string (nullable = true) -- rnr: string (nullable = true) -- rr: string (nullable = true) -- i: string (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: string (nullable = true) Casting Data Types and Formatting Significant Digits . from pyspark.sql.functions import format_number . result = df.describe() result.select(result[&#39;province&#39;] ,format_number(result[&#39;specific&#39;].cast(&#39;float&#39;),2).alias(&#39;specific&#39;) ,format_number(result[&#39;general&#39;].cast(&#39;float&#39;),2).alias(&#39;general&#39;) ,format_number(result[&#39;year&#39;].cast(&#39;int&#39;),2).alias(&#39;year&#39;),format_number(result[&#39;gdp&#39;].cast(&#39;float&#39;),2).alias(&#39;gdp&#39;) ,format_number(result[&#39;rnr&#39;].cast(&#39;int&#39;),2).alias(&#39;rnr&#39;),format_number(result[&#39;rr&#39;].cast(&#39;float&#39;),2).alias(&#39;rr&#39;) ,format_number(result[&#39;fdi&#39;].cast(&#39;int&#39;),2).alias(&#39;fdi&#39;),format_number(result[&#39;it&#39;].cast(&#39;float&#39;),2).alias(&#39;it&#39;) ,result[&#39;reg&#39;].cast(&#39;string&#39;).alias(&#39;reg&#39;) ).show() . +--+++--+++++-++ province| specific| general| year| gdp| rnr| rr| fdi| it| reg| +--+++--+++++-++ 360| 356.00| 169.00| 360.00| 360.00|294.00|296.00| 360.00| 360.00| 360| null| 583,470.75| 309,127.53|2,001.00| 4,428.65| 0.00| 0.06| 196,139.00| 2,165,819.25| null| null| 654,055.31| 355,423.56| 3.00| 4,484.67| 0.00| 0.16| 303,043.00| 1,769,294.25| null| Anhui| 8,964.00| 0.00|1,996.00| 64.98| 0.00| 0.00| 2.00| 147,897.00| East China| Zhejiang|3,937,966.00|1,737,800.00|2,007.00|31,777.01| 1.00| 0.84|1,743,140.00|10,533,312.00|Southwest China| +--+++--+++++-++ New Columns generated from extant columns using withColumn . df2 = df.withColumn(&quot;specific_gdp_ratio&quot;,df[&quot;specific&quot;]/(df[&quot;gdp&quot;]*100))#.show() . df2.select(&#39;specific_gdp_ratio&#39;).show() . ++ specific_gdp_ratio| ++ 0.7022500358285959| 0.6474660463848132| 0.6878991411583352| 1.0519477646607727| 0.673928100093381| 0.7727761333780966| 1.233475958314866| 1.5783421826051272| 1.8877587040110941| 1.6792756118029895| 2.3850666666666664| 3.0077639751552794| 0.9275486250838364| 0.7989880072601573| 1.0314658544998698| 1.448708759827088| 0.8912058855158366| 1.1918224576316896| 1.2944820393974508| 1.283311464867661| ++ only showing top 20 rows df.orderBy(df[&quot;specific&quot;].asc()).head(1)[0][0] . Out[94]: 24 Finding the Mean, Max, and Min . from pyspark.sql.functions import mean df.select(mean(&quot;specific&quot;)).show() . +--+ avg(specific)| +--+ 583470.7303370787| +--+ from pyspark.sql.functions import max,min . df.select(max(&quot;specific&quot;),min(&quot;specific&quot;)).show() . +-+-+ max(specific)|min(specific)| +-+-+ 3937966.0| 8964.0| +-+-+ df.filter(&quot;specific &lt; 60000&quot;).count() . Out[98]: 23 df.filter(df[&#39;specific&#39;] &lt; 60000).count() . Out[99]: 23 from pyspark.sql.functions import count result = df.filter(df[&#39;specific&#39;] &lt; 60000) result.select(count(&#39;specific&#39;)).show() . ++ count(specific)| ++ 23| ++ (df.filter(df[&quot;gdp&quot;]&gt;8000).count()*1.0/df.count())*100 . Out[101]: 14.444444444444443 from pyspark.sql.functions import corr df.select(corr(&quot;gdp&quot;,&quot;fdi&quot;)).show() . ++ corr(gdp, fdi)| ++ 0.8366328478935896| ++ Finding the max value by Year . from pyspark.sql.functions import year #yeardf = df.withColumn(&quot;Year&quot;,year(df[&quot;year&quot;])) . max_df = df.groupBy(&#39;year&#39;).max() . max_df.select(&#39;year&#39;,&#39;max(gdp)&#39;).show() . +-+--+ year|max(gdp)| +-+--+ 2003|15844.64| 2007|31777.01| 2006|26587.76| 1997| 7774.53| 2004|18864.62| 1996| 6834.97| 1998| 8530.88| 2001|12039.25| 2005|22557.37| 2000|10741.25| 1999| 9250.68| 2002|13502.42| +-+--+ from pyspark.sql.functions import month . #df.select(&quot;year&quot;,&quot;avg(gdp)&quot;).orderBy(&#39;year&#39;).show() . This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/20/Pyspark-Dataframes-Data-Types.html",
            "relUrl": "/2020/08/20/Pyspark-Dataframes-Data-Types.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post71": {
            "title": "Dataframe Filitering and Operations with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . Filtering on values in a column . df.filter(&quot;specific&lt;10000&quot;).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| 269|Shanghai| 9834.0| null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917| ++--+--+-+-+-++++-+-+-+-+ df.filter(&quot;specific&lt;10000&quot;).select(&#39;province&#39;).show() . +--+ province| +--+ Shanghai| Shanghai| +--+ df.filter(&quot;specific&lt;10000&quot;).select([&#39;province&#39;,&#39;year&#39;]).show() . +--+-+ province|year| +--+-+ Shanghai|2000| Shanghai|2001| +--+-+ df.filter(df[&quot;specific&quot;] &lt; 10000).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| 269|Shanghai| 9834.0| null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917| ++--+--+-+-+-++++-+-+-+-+ Filtering on values in 2+ columns . df.filter((df[&quot;specific&quot;] &lt; 55000) &amp; (df[&#39;gdp&#39;] &gt; 200) ).show() . ++--+--+-+-+--++-+-+-+-+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+--++-+-+-+-+-+-+ 98| Hainan| 54462.0| null|1998| 442.13| 71715|null|null|null| 236461|South Central China| 177748| 216| Ningxia| 32088.0| null|1996| 202.9| 2826|null|null|null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671|null|null|null| 102083| Northwest China| 195295| 268|Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269|Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 270|Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271|Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 272|Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 273|Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 274|Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6|211361|null|null|null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464| North China| 547120| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| ++--+--+-+-+--++-+-+-+-+-+-+ df.filter((df[&quot;specific&quot;] &lt; 55000) | (df[&#39;gdp&#39;] &gt; 20000) ).show() . ++++--+-+--+-+--+-+--+--+-+-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++++--+-+--+-+--+-+--+--+-+-+ 69|Guangdong|1491588.0| null|2005|22557.37|1236400|0.027027027000000002| 0.0| 0.0| null|South Central China|4327217| 70|Guangdong|1897575.0|498913.0|2006|26587.76|1451065|0.027027027000000002| 0.0| 0.0|16804703|South Central China|4559252| 71|Guangdong| 859482.0| 0.0|2007|31777.01|1712603|0.027027027000000002| 0.0| 0.0|27858007|South Central China|4947824| 98| Hainan| 54462.0| null|1998| 442.13| 71715| null|null| null| 236461|South Central China| 177748| 179| Jiangsu|1188989.0| 0.0|2007|21742.05|1743140| 0.0| 0.0|0.275862069|22377276| East China|3557071| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null| null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null| null| 102083| Northwest China| 195295| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null| null| 73260| Northwest China| 218361| 262| Shandong|1204547.0|112137.0|2006|21900.19|1000069| 0.0| 0.0| 0.0|11673659| East China|5304833| 263| Shandong|2121243.0|581800.0|2007|25776.91|1101159| 0.0| 0.0| 0.0|16753980| East China|6357869| 268| Shanghai| 8964.0| null|2000| 4771.17| 316014| 0.0| 0.0| 0.44| 2224124| East China|1212473| 269| Shanghai| 9834.0| null|2001| 5210.12| 429159| 0.0| 0.0| 0.44| 2947285| East China|1053917| 270| Shanghai| 19985.0| null|2002| 5741.03| 427229| 0.0| 0.0| 0.44| 3380397| East China|1572208| 271| Shanghai| 23547.0| null|2003| 6694.23| 546849| 0.0|0.53| 0.0| 4461153| East China|2031496| 272| Shanghai| 29943.0| null|2004| 8072.83| 654100| 0.0|0.53| 0.0| null| East China|2703643| 273| Shanghai| 29943.0| null|2005| 9247.66| 685000| 0.0|0.53| 0.0| null| East China|2140461| 274| Shanghai| 42928.0| null|2006|10572.24| 710700| 0.0|0.53| 0.0| 8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6| 211361| null|null| null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95| 176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88| 116601| 0.0| 0.0| 0.0| 757464| North China| 547120| ++++--+-+--+-+--+-+--+--+-+-+ only showing top 20 rows df.filter((df[&quot;specific&quot;] &lt; 55000) &amp; ~(df[&#39;gdp&#39;] &gt; 20000) ).show() . ++--+--+-+-+--++--+-+-+-+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+--++--+-+-+-+-+-+ 98| Hainan| 54462.0| null|1998| 442.13| 71715| null|null|null| 236461|South Central China| 177748| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null|null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null|null| 102083| Northwest China| 195295| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null|null| 73260| Northwest China| 218361| 268|Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269|Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 270|Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271|Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 272|Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 273|Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 274|Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6|211361| null|null|null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464| North China| 547120| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| 312| Tibet| 18829.0| null|1996| 64.98| 679|0.181818182| 0.0| 0.0| 27801| Southwest China| 306114| 313| Tibet| 25185.0| null|1997| 77.24| 63|0.181818182| 0.0| 0.0| 33787| Southwest China| 346368| 314| Tibet| 48197.0| null|1998| 91.5| 481| 0.0|0.24| 0.0| 3810| Southwest China| 415547| ++--+--+-+-+--++--+-+-+-+-+-+ df.filter(df[&quot;specific&quot;] == 8964.0).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| ++--+--+-+-+-++++-+-+-+-+ df.filter(df[&quot;province&quot;] == &quot;Zhejiang&quot;).show() . ++--++--+-+--+-+--+--+--+--+-+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+--+-+--+--+--+--+-+-+ 348|Zhejiang| 273253.0| null|1996| 4188.53| 152021| 0.0| 0.0| 0.0| 1291252|East China| 740327| 349|Zhejiang| 330558.0| null|1997| 4686.11| 150345| 0.0| 0.0| 0.0| 1432453|East China| 814253| 350|Zhejiang| 426756.0| null|1998| 5052.62| 131802| 0.0| 0.0| 0.0| 1761084|East China| 923455| 351|Zhejiang| 586457.0| null|1999| 5443.92| 123262| 0.0| 0.0| 0.0| 2146200|East China|1001703| 352|Zhejiang| 408151.0| null|2000| 6141.03| 161266| 0.0| 0.0| 0.0| 2955508|East China|1135215| 353|Zhejiang| 358714.0| null|2001| 6898.34| 221162| 0.0| 0.0| 0.0| 4436868|East China|1203372| 354|Zhejiang| 365437.0|321686.0|2002| 8003.67| 307610| 0.0| 0.0| 0.0| 4958329|East China|1962633| 355|Zhejiang| 391292.0|260313.0|2003| 9705.02| 498055|1.214285714|0.035714286|0.035714286| 6217715|East China|2261631| 356|Zhejiang| 656175.0|276652.0|2004| 11648.7| 668128|1.214285714|0.035714286|0.035714286| null|East China|3162299| 357|Zhejiang| 656175.0| null|2005|13417.68| 772000|1.214285714|0.035714286|0.035714286| null|East China|2370200| 358|Zhejiang|1017303.0|394795.0|2006|15718.47| 888935|1.214285714|0.035714286|0.035714286|11537149|East China|2553268| 359|Zhejiang| 844647.0| 0.0|2007|18753.73|1036576|0.047619048| 0.0| 0.0|16494981|East China|2939778| ++--++--+-+--+-+--+--+--+--+-+-+ df.filter(df[&quot;specific&quot;] == 8964.0).collect() . Out[15]: [Row(_c0=268, province=&#39;Shanghai&#39;, specific=8964.0, general=None, year=2000, gdp=4771.17, fdi=316014, rnr=0.0, rr=0.0, i=0.44, fr=&#39;2224124&#39;, reg=&#39;East China&#39;, it=1212473)] result = df.filter(df[&quot;specific&quot;] == 8964.0).collect() . type(result[0]) . Out[17]: pyspark.sql.types.Row row = result[0] . row.asDict() . Out[19]: {&#39;_c0&#39;: 268, &#39;province&#39;: &#39;Shanghai&#39;, &#39;specific&#39;: 8964.0, &#39;general&#39;: None, &#39;year&#39;: 2000, &#39;gdp&#39;: 4771.17, &#39;fdi&#39;: 316014, &#39;rnr&#39;: 0.0, &#39;rr&#39;: 0.0, &#39;i&#39;: 0.44, &#39;fr&#39;: &#39;2224124&#39;, &#39;reg&#39;: &#39;East China&#39;, &#39;it&#39;: 1212473} for item in result[0]: print(item) . 268 Shanghai 8964.0 None 2000 4771.17 316014 0.0 0.0 0.44 2224124 East China 1212473 This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/19/Pyspark-Filtering.html",
            "relUrl": "/2020/08/19/Pyspark-Filtering.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post72": {
            "title": "Handling Missing Data with Pyspark",
            "content": "from pyspark.sql import SparkSession from pyspark.sql.functions import countDistinct, avg,stddev # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Dropping Columns without non-null values . # Has to have at least 2 NON-null values df.na.drop(thresh=2).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Dropping any row that contains missing data . df.na.drop().show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| 46| Fujian| 397517.0| 149549.0|2006|7583.85|322047| 0.4| 0.0| 0.0| 4830320| East China|2135224| 47| Fujian| 753552.0| 317700.0|2007|9248.53|406058| 0.4| 0.0| 0.0| 6994577| East China|2649011| 52| Gansu| 223984.0| 58533.0|2000|1052.88| 6235| 0.0|0.153846154| 0.0| 505196|Northwest China|1258100| 54| Gansu| 337894.0| 129791.0|2002|1232.03| 6121| 0.0| 0.13| 0.0| 597159|Northwest China|1898911| 58| Gansu| 833430.0| 516342.0|2006|2277.35| 2954| 0.0| 0.0|0.128205128| 924080|Northwest China|3847158| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(subset=[&quot;general&quot;]).show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 20| Beijing|1009936.0| 309025.0|2004|6033.21|308354| 0.0|0.794871795| 0.0| null| North China|1644601| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 30|Chongqing| 311770.0| 41907.0|2002|2232.86| 19576| null| null| null| 762806|Southwest China|1906968| 31|Chongqing| 335715.0| 18700.0|2003|2555.72| 26083| null| null| null| 929935|Southwest China|1778125| 32|Chongqing| 568835.0| 97500.0|2004|3034.58| 40508| null| null| null| null|Southwest China|2197948| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(how=&#39;any&#39;).show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| 46| Fujian| 397517.0| 149549.0|2006|7583.85|322047| 0.4| 0.0| 0.0| 4830320| East China|2135224| 47| Fujian| 753552.0| 317700.0|2007|9248.53|406058| 0.4| 0.0| 0.0| 6994577| East China|2649011| 52| Gansu| 223984.0| 58533.0|2000|1052.88| 6235| 0.0|0.153846154| 0.0| 505196|Northwest China|1258100| 54| Gansu| 337894.0| 129791.0|2002|1232.03| 6121| 0.0| 0.13| 0.0| 597159|Northwest China|1898911| 58| Gansu| 833430.0| 516342.0|2006|2277.35| 2954| 0.0| 0.0|0.128205128| 924080|Northwest China|3847158| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(how=&#39;all&#39;).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of Null Values . df.na.fill(&#39;example&#39;).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0|example| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324|example| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53|example|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of 0 . df.na.fill(0).show() . ++--++--+-+-+++--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--++--+-+-+++--+--+-+--+-+ 0| Anhui| 147002.0| 0.0|1996| 2093.3| 50661|0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| 0.0|1997|2347.32| 43443|0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| 0.0|1998|2542.96| 27673|0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| 0.0|1999|2712.34| 26131|0.0| 0.0| 0.0|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847|0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| 0.0|2001|3246.71| 33672|0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375|0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720|0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669|0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| 0.0|2005|5350.17| 69000|0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354|0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892|0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| 0.0|1996| 1789.2|155290|0.0| 0.0| 0.0| 634562|North China| 508135| 13| Beijing| 165957.0| 0.0|1997|2077.09|159286|0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| 0.0|1998|2377.18|216800|0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| 0.0|1999|2678.82|197525|0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368|0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| 0.0|2001|3707.96|176818|0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464|0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126|0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-+++--+--+-+--+-+ only showing top 20 rows df.na.fill(&#39;example&#39;,subset=[&#39;fr&#39;]).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0|example| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324|example| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53|example|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows df.na.fill(0,subset=[&#39;general&#39;]).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| 0.0|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| 0.0|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| 0.0|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| 0.0|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| 0.0|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| 0.0|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| 0.0|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| 0.0|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| 0.0|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| 0.0|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| 0.0|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of the Mean . from pyspark.sql.functions import mean mean_val = df.select(mean(df[&#39;general&#39;])).collect() . mean_val[0][0] . Out[19]: 309127.53846153844 mean_gen = mean_val[0][0] . df.na.fill(mean_gen,[&quot;general&quot;]).show() . ++--+++-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+++-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--+++-+-++-+--+--+-+--+-+ only showing top 20 rows df.na.fill(df.select(mean(df[&#39;general&#39;])).collect()[0][0],[&#39;general&#39;]).show() . ++--+++-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+++-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--+++-+-++-+--+--+-+--+-+ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/18/Pyspark-NAs.html",
            "relUrl": "/2020/08/18/Pyspark-NAs.html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post73": {
            "title": "Group By and Aggregation with Pyspark",
            "content": "&quot;Group By and Aggregation with Pyspark&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [pyspark, jupyter] | description: Group By and Aggregation with Pyspark | title: Group By and Aggregation with Pyspark | . Read CSV and inferSchema . from pyspark.sql import SparkSession from pyspark.sql.functions import countDistinct, avg,stddev # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) Using groupBy for Averages and Counts . df.groupBy(&quot;province&quot;) . Out[8]: &lt;pyspark.sql.group.GroupedData at 0x7f939a0aada0&gt; df.groupBy(&quot;province&quot;).mean().show() . ++--++++++--+--+--++ province|avg(_c0)| avg(specific)| avg(general)|avg(year)| avg(gdp)| avg(fdi)| avg(rnr)| avg(rr)| avg(i)| avg(it)| ++--++++++--+--+--++ Guangdong| 65.5|1123328.0833333333| 312308.0| 2001.5|15358.781666666668| 1194950.25|0.011261261250000001| 0.0| 0.0| 3099014.25| Hunan| 161.5| 824676.9166666666| 480788.3333333333| 2001.5| 4765.891666666666| 132110.25| 0.0| 0.07291666666666667| 0.0| 3215128.5| Shanxi| 281.5| 577540.4166666666| 351680.0| 2001.5| 2817.210833333333|38628.833333333336| 0.0| 0.0| 0.0|1983718.3333333333| Tibet| 317.5|189219.91666666666|165365.33333333334| 2001.5|170.42666666666665| 839.75| 0.03030303033333333| 0.15583333333333335| 0.20278090583333333|1174175.5833333333| Hubei| 149.5| 595463.25| 391326.5| 2001.5| 4772.503333333333| 149713.25| 0.045045045| 0.11386386375000002| 0.06230392158333333| 2904659.75| Tianjin| 305.5| 76884.16666666667| 126636.0| 2001.5|2528.6650000000004|250173.33333333334| 0.0| 0.0| 0.0| 831028.4166666666| Beijing| 17.5| 581440.8333333334| 412825.0| 2001.5| 4673.453333333333|257369.33333333334| 0.0| 0.3613053613636364| 0.29545454545454547|1175965.4166666667| Heilongjiang| 125.5|1037878.1666666666| 315925.3333333333| 2001.5| 4041.241666666667| 82719.33333333333| 0.0| 0.0| 0.03931203927272728|3230451.1666666665| Liaoning| 209.5| 1111002.75|185280.83333333334| 2001.5| 5231.135000000001| 285925.3333333333| 0.11469534044444446| 0.0| null|2628358.4166666665| Henan| 137.5| 955407.4166666666| 673392.5| 2001.5| 7208.966666666667| 94426.0| 0.0| 0.04| 0.08602150533333335|3671970.6666666665| Anhui| 5.5| 643984.1666666666|159698.83333333334| 2001.5|3905.8700000000003| 70953.08333333333| 0.0| 0.0| 0.08845208836363637|2649674.4166666665| Xinjiang| 329.5| 345334.3333333333| 412906.0| 2001.5|1828.8966666666665| 4433.083333333333| 0.0| 0.0| 0.0| 2251012.0| Fujian| 41.5|246144.16666666666|140619.33333333334| 2001.5|4864.0233333333335| 374466.4166666667| 0.1366666666666667|0.049999999999999996| 0.09999999999999999| 1274116.75| Jiangxi| 185.5| 592906.3333333334| 458268.6666666667| 2001.5| 2460.7825| 103735.25| 0.0| 0.1491841490909091|0.042727272727272725| 1760613.25| Jilin| 197.5| 711132.25| 348186.0| 2001.5|2274.8541666666665|41226.583333333336| 0.0| 0.0| 0.0|2136634.9166666665| Chongqing| 29.5| 561854.1111111111| 151201.4| 2001.5| 2477.7125|41127.833333333336| 0.09677419400000001| 0.0| 0.0|1636146.4166666667| Shaanxi| 245.5| 387167.1666666667| 386760.5| 2001.5| 2658.034166666667|50892.583333333336|0.002840909090909091| 0.0| 0.07386363636363637|2474031.4166666665| Sichuan| 293.5| 1194640.5| 707032.8333333334| 2001.5| 5377.79|62197.166666666664| 0.00818181818181818| 0.00818181818181818| 0.2|4016479.5833333335| Yunnan| 341.5| 802151.1666666666| 200426.0| 2001.5| 2604.054166666667|17048.333333333332| 0.0| 0.0| 0.0|3165418.9166666665| Gansu| 53.5| 498930.9166666667| 382092.6666666667| 2001.5|1397.8325000000002| 5295.5| 0.11111111120000002| 0.088974359| 0.13038461533333334| 2045347.0| ++--++++++--+--+--++ only showing top 20 rows df.groupBy(&quot;reg&quot;).mean().show() . +-+++++++--+--+--++ reg| avg(_c0)| avg(specific)| avg(general)|avg(year)| avg(gdp)| avg(fdi)| avg(rnr)| avg(rr)| avg(i)| avg(it)| +-+++++++--+--+--++ Southwest China| 214.3| 648086.8070175438| 327627.0| 2001.5|2410.3988333333336|25405.083333333332| 0.01764440930612245|0.053185448081632655| 0.13679739081632653| 2424971.4| East China|183.78571428571428|517524.90476190473|230217.37142857144| 2001.5| 7126.732976190476|414659.03571428574| 0.08284508739240506| 0.05701117448101268| 0.09036240282278483|1949130.4761904762| Northeast China| 177.5| 953337.7222222222|283130.72222222225| 2001.5| 3849.076944444444| 136623.75| 0.03686635942857143| 0.0| 0.02275960168421053|2665148.1666666665| North China| 179.5|506433.57446808513|334689.14285714284| 2001.5| 4239.038541666667|169600.58333333334| 0.0| 0.15428824051724138| 0.11206896551724138|1733718.7291666667| Northwest China| 216.7|324849.06666666665|293066.73333333334| 2001.5|1340.0261666666668|15111.133333333333|0.022847222240000003|0.033887245249999996|0.048179240615384616| 1703537.75| South Central China| 115.5| 690125.8333333334| 382414.8888888889| 2001.5| 5952.826944444445|281785.59722222225|0.014928879322033899| 0.07324349771186443| 0.06797753142372882| 2626299.875| +-+++++++--+--+--++ # Count df.groupBy(&quot;reg&quot;).count().show() . +-+--+ reg|count| +-+--+ Southwest China| 60| East China| 84| Northeast China| 36| North China| 48| Northwest China| 60| South Central China| 72| +-+--+ # Max df.groupBy(&quot;reg&quot;).max().show() . +-+--+-+++--+--++--+-+--+ reg|max(_c0)|max(specific)|max(general)|max(year)|max(gdp)|max(fdi)| max(rnr)| max(rr)| max(i)| max(it)| +-+--+-+++--+--++--+-+--+ Southwest China| 347| 3937966.0| 1725100.0| 2007|10562.39| 149322| 0.181818182| 0.84| 0.75|10384846| East China| 359| 2213991.0| 1272600.0| 2007|25776.91| 1743140| 1.214285714| 0.53| 0.6| 7040099| Northeast China| 215| 3847672.0| 1046700.0| 2007| 9304.52| 598554| 0.516129032| 0.0|0.21621621600000002| 7968319| North China| 311| 2981235.0| 1023453.0| 2007|13607.32| 527776| 0.0|0.794871795| 0.6| 7537692| Northwest China| 335| 2669238.0| 1197400.0| 2007| 5757.29| 119516|0.5555555560000001| 0.5| 1.05| 6308151| South Central China| 167| 3860764.0| 1737800.0| 2007|31777.01| 1712603| 0.27027027| 0.4375| 0.6176470589999999|10533312| +-+--+-+++--+--++--+-+--+ # Min df.groupBy(&quot;reg&quot;).min().show() . +-+--+-+++--+--+--+-++-+ reg|min(_c0)|min(specific)|min(general)|min(year)|min(gdp)|min(fdi)|min(rnr)|min(rr)|min(i)|min(it)| +-+--+-+++--+--+--+-++-+ Southwest China| 24| 18829.0| 18700.0| 1996| 64.98| 2| 0.0| 0.0| 0.0| 176802| East China| 0| 8964.0| 0.0| 1996| 1169.73| 22724| 0.0| 0.0| 0.0| 489132| Northeast China| 120| 80595.0| 19360.0| 1996| 1137.23| 19059| 0.0| 0.0| 0.0| 625471| North China| 12| 35084.0| 32119.0| 1996| 1121.93| 13802| 0.0| 0.0| 0.0| 303992| Northwest China| 48| 32088.0| 2990.0| 1996| 184.17| 247| 0.0| 0.0| 0.0| 178668| South Central China| 60| 54462.0| 0.0| 1996| 389.68| 29579| 0.0| 0.0| 0.0| 147897| +-+--+-+++--+--+--+-++-+ # Sum df.groupBy(&quot;reg&quot;).sum().show() . +-+--+-++++--+++-++ reg|sum(_c0)|sum(specific)|sum(general)|sum(year)| sum(gdp)|sum(fdi)| sum(rnr)| sum(rr)| sum(i)| sum(it)| +-+--+-++++--+++-++ Southwest China| 12858| 3.6940948E7| 9501183.0| 120090|144623.93000000002| 1524305| 0.864576056| 2.606086956| 6.70307215|145498284| East China| 15438| 4.3472092E7| 8057608.0| 168126| 598645.57|34831359| 6.544761904| 4.503882784000002| 7.138629823000002|163726960| Northeast China| 6390| 3.4320158E7| 5096353.0| 72054| 138566.77| 4918455| 1.032258064| 0.0|0.43243243200000003| 95945334| North China| 8616| 2.3802378E7| 7028472.0| 96072| 203473.85| 8140828| 0.0| 4.474358975| 3.25| 83218499| Northwest China| 13002| 1.9490944E7| 8792002.0| 120090| 80401.57| 906668|1.1423611120000001|1.7621367529999998| 2.505320512|102212265| South Central China| 8316| 4.968906E7| 1.3766936E7| 144108|428603.54000000004|20288563| 0.88080388| 4.321366365000001| 4.010674354000001|189093591| +-+--+-++++--+++-++ # Max it across everything df.agg({&#39;specific&#39;:&#39;max&#39;}).show() . +-+ max(specific)| +-+ 3937966.0| +-+ grouped = df.groupBy(&quot;reg&quot;) grouped.agg({&quot;it&quot;:&#39;max&#39;}).show() . +-+--+ reg| max(it)| +-+--+ Southwest China|10384846| East China| 7040099| Northeast China| 7968319| North China| 7537692| Northwest China| 6308151| South Central China|10533312| +-+--+ df.select(countDistinct(&quot;reg&quot;)).show() . +-+ count(DISTINCT reg)| +-+ 6| +-+ df.select(countDistinct(&quot;reg&quot;).alias(&quot;Distinct Region&quot;)).show() . ++ Distinct Region| ++ 6| ++ df.select(avg(&#39;specific&#39;)).show() . +--+ avg(specific)| +--+ 583470.7303370787| +--+ df.select(stddev(&quot;specific&quot;)).show() . ++ stddev_samp(specific)| ++ 654055.3290782663| ++ Choosing Significant Digits with format_number . from pyspark.sql.functions import format_number . specific_std = df.select(stddev(&quot;specific&quot;).alias(&#39;std&#39;)) specific_std.show() . +--+ std| +--+ 654055.3290782663| +--+ specific_std.select(format_number(&#39;std&#39;,0)).show() . ++ format_number(std, 0)| ++ 654,055| ++ Using orderBy . df.orderBy(&quot;specific&quot;).show() . +++--+-+-+--++--+-+-+-++-+ _c0| province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++--+-+-+--++--+-+-+-++-+ 28|Chongqing| null| null|2000| 1791.0| 24436| null|null|null| null|Southwest China|1022148| 109| Hebei| null| null|1997| 3953.78|110064| null|null|null| null| North China| 826734| 24|Chongqing| null| null|1996| 1315.12| 21878| null|null|null| null|Southwest China| 176802| 25|Chongqing| null| null|1997| 1509.75| 38675| null|null|null| null|Southwest China| 383402| 268| Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269| Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 312| Tibet| 18829.0| null|1996| 64.98| 679|0.181818182| 0.0| 0.0| 27801|Southwest China| 306114| 270| Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271| Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 313| Tibet| 25185.0| null|1997| 77.24| 63|0.181818182| 0.0| 0.0| 33787|Southwest China| 346368| 273| Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 272| Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null|null| 90805|Northwest China| 178668| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null|null| 73260|Northwest China| 218361| 302| Tianjin| 39364.0| null|1998| 1374.6|211361| null|null|null| 540178| North China| 361723| 274| Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null|null| 102083|Northwest China| 195295| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 314| Tibet| 48197.0| null|1998| 91.5| 481| 0.0|0.24| 0.0| 3810|Southwest China| 415547| +++--+-+-+--++--+-+-+-++-+ only showing top 20 rows df.orderBy(df[&quot;specific&quot;].desc()).show() . +++++-+--+-+--+--+-+--+-+--+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+--+-+--+--+-+--+-+--+ 299| Sichuan|3937966.0|1725100.0|2007|10562.39| 149322| null| null| null| 8508606| Southwest China|10384846| 143| Henan|3860764.0|1737800.0|2007|15012.46| 306162| 0.0| 0.0| 0.0| 8620804|South Central China|10533312| 131|Heilongjiang|3847672.0|1046700.0|2007| 7104.0| 208508| 0.0| 0.0|0.21621621600000002| 4404689| Northeast China| 7968319| 215| Liaoning|3396397.0| 599600.0|2007| 9304.52| 598554|0.516129032| 0.0| null|10826948| Northeast China| 5502192| 167| Hunan|3156087.0|1329200.0|2007| 9439.6| 327051| 0.0| 0.4375| 0.0| 6065508|South Central China| 8340692| 119| Hebei|2981235.0| 694400.0|2007|13607.32| 241621| 0.0| 0.5| 0.0| 7891198| North China| 7537692| 155| Hubei|2922784.0|1263500.0|2007| 9333.4| 276622| 0.0|0.111111111| 0.0| 5903552|South Central China| 7666512| 251| Shaanxi|2669238.0|1081000.0|2007| 5757.29| 119516| 0.03125| 0.0| 0.8125| 4752398| Northwest China| 6308151| 203| Jilin|2663667.0|1016400.0|2007| 4275.12| 76064| 0.0| 0.0| 0.0| 3206892| Northeast China| 4607955| 347| Yunnan|2482173.0| 564400.0|2007| 4772.52| 39453| 0.0| 0.0| 0.0| 4867146| Southwest China| 6832541| 298| Sichuan|2225220.0|1187958.0|2006| 8690.24| 120819| 0.0| 0.0| 0.55| 4247403| Southwest China| 7646885| 11| Anhui|2213991.0| 178705.0|2007| 7360.92| 299892| 0.0| 0.0| 0.324324324| 4468640| East China| 7040099| 287| Shanxi|2189020.0| 661200.0|2007| 6024.45| 134283| null| null| null| 5978870| North China| 5070166| 263| Shandong|2121243.0| 581800.0|2007|25776.91|1101159| 0.0| 0.0| 0.0|16753980| East China| 6357869| 191| Jiangxi|2045869.0|1272600.0|2007| 4820.53| 280657| 0.0| 0.41025641| 0.0| 3898510| East China| 4229821| 83| Guangxi|2022957.0|1214100.0|2007| 5823.41| 68396|0.205128205| 0.0|0.23076923100000002| 4188265|South Central China| 6185600| 142| Henan|2018158.0|1131615.0|2006|12362.79| 184526| 0.0| 0.0| 0.0| 6212824|South Central China| 7601825| 59| Gansu|2010553.0|1039400.0|2007| 2703.98| 11802| null| 0.0| 1.05| 1909107| Northwest China| 5111059| 95| Guizhou|1956261.0|1239200.0|2007| 2884.11| 12651| 0.0| 0.0| 0.7105263159999999| 2851375| Southwest China| 5639838| 214| Liaoning|1947031.0| 179893.0|2006| 8047.26| 359000|0.516129032| 0.0| null| 6530236| Northeast China| 4605917| +++++-+--+-+--+--+-+--+-+--+ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/17/Pyspark-Group-By.html",
            "relUrl": "/2020/08/17/Pyspark-Group-By.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post74": {
            "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
            "content": "# Import required packages import numpy as np import pandas as pd %matplotlib inline import seaborn as sns import statsmodels.formula.api as smf from matplotlib import pyplot as plt from matplotlib.lines import Line2D %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . np.random.seed(42) import matplotlib as mpl mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) . Read required datasets . df = pd.read_csv(&#39;ttb_county_clean.csv&#39;) df1 = pd.read_csv(&#39;df_panel_fix.csv&#39;) . Figure 1 . df.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=df[&quot;specific&quot;]/100, label=&quot;Specific Purpose Transfers&quot;, figsize=(12,8), c=&quot;nightlights&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) #save_fig(&quot;cn-spt-county-heat&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f0a4e16b4e0&gt; . Panel regression framework with year and province fixed effects . lin_reg = smf.ols(&#39;np.log(specific) ~ np.log(gdp) + np.log(fdi) + i + rnr + rr + C(province) + C(year)&#39;, data=df1).fit() . #lin_reg.summary() . Figure 2 . coef_df = pd.read_csv(&#39;coef.csv&#39;) fig, ax = plt.subplots(figsize=(16, 10)) coef_df.plot(x=&#39;varname&#39;, y=&#39;coef&#39;, kind=&#39;bar&#39;, ax=ax, color=&#39;none&#39;, yerr=&#39;err&#39;, legend=False) ax.set_ylabel(&#39;Specific Purpose Transfers (ln)&#39;) ax.set_xlabel(&#39;Independant Variables&#39;) ax.scatter(x=pd.np.arange(coef_df.shape[0]), marker=&#39;s&#39;, s=120, y=coef_df[&#39;coef&#39;], color=&#39;black&#39;) ax.axhline(y=0, linestyle=&#39;--&#39;, color=&#39;blue&#39;, linewidth=4) ax.xaxis.set_ticks_position(&#39;none&#39;) _ = ax.set_xticklabels([&#39;GDP&#39;, &#39;FDI&#39;, &#39;Incumbent&#39;, &#39;Non Relevant Rival&#39;, &#39;Relevant Rival&#39;], rotation=0, fontsize=20) fs = 16 ax.annotate(&#39;Controls&#39;, xy=(0.2, -0.2), xytext=(0.2, -0.3), xycoords=&#39;axes fraction&#39;, textcoords=&#39;axes fraction&#39;, fontsize=fs, ha=&#39;center&#39;, va=&#39;bottom&#39;, bbox=dict(boxstyle=&#39;square&#39;, fc=&#39;white&#39;, ec=&#39;blue&#39;), arrowprops=dict(arrowstyle=&#39;-[, widthB=5.5, lengthB=1.2&#39;, lw=2.0, color=&#39;blue&#39;)) _ = ax.annotate(&#39;Connections&#39;, xy=(0.7, -0.2), xytext=(0.7, -0.3), xycoords=&#39;axes fraction&#39;, textcoords=&#39;axes fraction&#39;, fontsize=fs, ha=&#39;center&#39;, va=&#39;bottom&#39;, bbox=dict(boxstyle=&#39;square&#39;, fc=&#39;white&#39;, ec=&#39;red&#39;), arrowprops=dict(arrowstyle=&#39;-[, widthB=10.5, lengthB=1.2&#39;, lw=2.0, color=&#39;red&#39;)) #save_fig(&quot;i-coef-plot&quot;) . Figure 3 . import numpy as np from plotly import __version__ from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot print (__version__) # requires version &gt;= 1.9.0 #Always run this the command before at the start of notebook init_notebook_mode(connected=True) import plotly.graph_objs as go trace1 = go.Bar( x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007], y=[188870900000.0, 185182900000.0, 237697500000.0, 347187900000.0, 296716700000.0, 397833100000.0, 440204800000.0, 514254300000.0, 686016600000.0, 677746300000.0, 940057900000.0, 1881304000000], name=&#39;All Other Province Leaders&#39;, marker=dict( color=&#39;rgb(55, 83, 109)&#39; ) ) trace2 = go.Bar( x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007], y=[260376000000.0, 264934700000.0, 367350200000.0, 463861200000.0, 199068500000.0, 216582600000.0, 298631800000.0, 409759300000.0, 830363200000.0, 878158000000.0, 1143745000000.0, 2125891000000.0], name=&#39;Incumbent Connected Province Leaders&#39;, marker=dict( color=&#39;rgb(26, 118, 255)&#39; ) ) data = [trace1, trace2] layout = go.Layout( title=&#39;Specific Purpose Transfers&#39;, xaxis=dict( tickfont=dict( size=14, color=&#39;rgb(107, 107, 107)&#39; ) ), yaxis=dict( title=&#39;RMB&#39;, titlefont=dict( size=16, color=&#39;rgb(107, 107, 107)&#39; ), tickfont=dict( size=14, color=&#39;rgb(107, 107, 107)&#39; ) ), legend=dict( x=0, y=1.0, bgcolor=&#39;rgba(255, 255, 255, 0)&#39;, bordercolor=&#39;rgba(255, 255, 255, 0)&#39; ), barmode=&#39;group&#39;, bargap=0.15, bargroupgap=0.1 ) fig = go.Figure(data=data, layout=layout) #iplot(fig, filename=&#39;style-bar&#39;) iplot(fig, image=&#39;png&#39;,filename=&#39;spt-i-bar&#39;) . 4.1.1 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/16/pandas-stats-fiscal.html",
            "relUrl": "/2020/08/16/pandas-stats-fiscal.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post75": {
            "title": "Pyspark Regression with Fiscal Data",
            "content": "&quot;A minimal example of using Pyspark for Linear Regression&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [pyspark, jupyter] | description: A minimal example of using Pyspark for Linear Regression | title: Pyspark Regression with Fiscal Data | . Bring in needed imports . from pyspark.sql.functions import col from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType from pyspark.sql.functions import * . Load data from CSV . #collapse-hide # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.createOrReplaceTempView(&quot;fiscal_stats&quot;) sums = spark.sql(&quot;&quot;&quot; select year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr from fiscal_stats group by 1 order by year asc &quot;&quot;&quot;) sums.show() . +-+++ year|total_yearly_it|total_yearly_fr| +-+++ 1996| 19825341| 2.9579215E7| 1997| 21391321| 2.9110765E7| 1998| 25511453| 3.8154711E7| 1999| 31922107| 4.2128627E7| 2000| 38721293| 4.8288092E7| 2001| 50754944| 5.8910649E7| 2002| 62375881| 6.2071474E7| 2003| 69316709| 7.2479293E7| 2004| 88626786| null| 2005| 98263665| null| 2006| 119517822| 1.3349148E8| 2007| 153467611| 2.27385701E8| +-+++ Describing the Data . df.describe().toPandas().transpose() . 0 1 2 3 4 . summary count | mean | stddev | min | max | . _c0 360 | 179.5 | 104.06728592598157 | 0 | 359 | . province 360 | None | None | Anhui | Zhejiang | . specific 356 | 583470.7303370787 | 654055.3290782663 | 8964.0 | 3937966.0 | . general 169 | 309127.53846153844 | 355423.5760674793 | 0.0 | 1737800.0 | . year 360 | 2001.5 | 3.4568570586927794 | 1996 | 2007 | . gdp 360 | 4428.653416666667 | 4484.668659976412 | 64.98 | 31777.01 | . fdi 360 | 196139.38333333333 | 303043.97011891654 | 2 | 1743140 | . rnr 294 | 0.0355944252244898 | 0.16061503029299648 | 0.0 | 1.214285714 | . rr 296 | 0.059688621057432424 | 0.15673351824073453 | 0.0 | 0.84 | . i 287 | 0.08376351662369343 | 0.1838933104683607 | 0.0 | 1.05 | . fr 295 | 2522449.0034013605 | 3491329.8613106664 | #REF! | 9898522 | . reg 360 | None | None | East China | Southwest China | . it 360 | 2165819.2583333333 | 1769294.2935487411 | 147897 | 10533312 | . Cast Data Type . df2 = df.withColumn(&quot;gdp&quot;,col(&quot;gdp&quot;).cast(IntegerType())) .withColumn(&quot;specific&quot;,col(&quot;specific&quot;).cast(IntegerType())) .withColumn(&quot;general&quot;,col(&quot;general&quot;).cast(IntegerType())) .withColumn(&quot;year&quot;,col(&quot;year&quot;).cast(IntegerType())) .withColumn(&quot;fdi&quot;,col(&quot;fdi&quot;).cast(IntegerType())) .withColumn(&quot;rnr&quot;,col(&quot;rnr&quot;).cast(IntegerType())) .withColumn(&quot;rr&quot;,col(&quot;rr&quot;).cast(IntegerType())) .withColumn(&quot;i&quot;,col(&quot;i&quot;).cast(IntegerType())) .withColumn(&quot;fr&quot;,col(&quot;fr&quot;).cast(IntegerType())) . printSchema . df2.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import LinearRegression assembler = VectorAssembler(inputCols=[&#39;gdp&#39;, &#39;fdi&#39;], outputCol=&quot;features&quot;) train_df = assembler.transform(df2) . train_df.select(&quot;specific&quot;, &quot;year&quot;).show() . +--+-+ specific|year| +--+-+ 147002|1996| 151981|1997| 174930|1998| 285324|1999| 195580|2000| 250898|2001| 434149|2002| 619201|2003| 898441|2004| 898441|2005| 1457872|2006| 2213991|2007| 165957|1996| 165957|1997| 245198|1998| 388083|1999| 281769|2000| 441923|2001| 558569|2002| 642581|2003| +--+-+ only showing top 20 rows Linear Regression in Pyspark . lr = LinearRegression(featuresCol = &#39;features&#39;, labelCol=&#39;it&#39;) lr_model = lr.fit(train_df) trainingSummary = lr_model.summary print(&quot;Coefficients: &quot; + str(lr_model.coefficients)) print(&quot;RMSE: %f&quot; % trainingSummary.rootMeanSquaredError) print(&quot;R2: %f&quot; % trainingSummary.r2) . Coefficients: [495.05888709337756,-4.968141828763066] RMSE: 1234228.673087 R2: 0.512023 lr_predictions = lr_model.transform(train_df) lr_predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show(5) from pyspark.ml.evaluation import RegressionEvaluator lr_evaluator = RegressionEvaluator(predictionCol=&quot;prediction&quot;, labelCol=&quot;it&quot;,metricName=&quot;r2&quot;) . ++-+-+ prediction| it| features| ++-+-+ 1732528.7382477913| 631930|[2093.0,50661.0]| 1894133.7432895212| 657860|[2347.0,43443.0]| 2069017.8229123235| 889463|[2542.0,27673.0]| 2160838.7084181504|1227364|[2712.0,26131.0]| 2226501.9982726825|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows print(&quot;R Squared (R2) on test data = %g&quot; % lr_evaluator.evaluate(lr_predictions)) . R Squared (R2) on test data = 0.512023 print(&quot;numIterations: %d&quot; % trainingSummary.totalIterations) print(&quot;objectiveHistory: %s&quot; % str(trainingSummary.objectiveHistory)) trainingSummary.residuals.show() . numIterations: 1 objectiveHistory: [0.0] +-+ residuals| +-+ -1100598.7382477913| -1236273.7432895212| -1179554.8229123235| -933474.7084181504| -727391.9982726825| -222546.39659531135| -94585.30175113119| 108072.63313654158| 389732.58121094666| 621021.2194867637| 1885768.997742407| 3938310.059555837| -554084.125169754| -615660.3899049093| -352195.3468934437| -348450.00565795833| -918476.5594253046| -710059.9133252408| -1148661.0062004486| -911572.322055324| +-+ only showing top 20 rows predictions = lr_model.transform(test_df) predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show() . ++-++ prediction| it| features| ++-++ 976371.9212205639| 306114| [64.0,679.0]| 990722.2032541803| 415547| [91.0,481.0]| 1016348.0830204486| 983251| [139.0,106.0]| 1036290.7062801318| 218361| [184.0,576.0]| 1034023.4471330958| 178668| [202.0,2826.0]| 1060130.0768520113| 274994| [245.0,1856.0]| 1023513.0851009073| 546541|[263.0,11020.0]| 1053250.6267921| 361358| [264.0,5134.0]| 1123768.8091592425| 866691| [377.0,2200.0]| 1128604.8330225947| 948521| [390.0,2522.0]| 810587.2575938476| 177748|[442.0,71715.0]| 1159703.254297337| 736165| [445.0,1743.0]| 1066975.770986663|1260633|[466.0,22500.0]| 1288507.6625716756|1423771| [725.0,3718.0]| 1320055.238474972| 573905| [793.0,4144.0]| 1188611.0570700848|2347862|[797.0,31000.0]| 1321857.482976733| 582711| [805.0,4977.0]| 1033849.5995896922| 746784|[819.0,64343.0]| 1445051.792853667|1216605|[1029.0,2501.0]| 1437887.1056682135|1258100|[1052.0,6235.0]| ++-++ only showing top 20 rows from pyspark.ml.regression import DecisionTreeRegressor dt = DecisionTreeRegressor(featuresCol =&#39;features&#39;, labelCol = &#39;it&#39;) dt_model = dt.fit(train_df) dt_predictions = dt_model.transform(train_df) dt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = dt_evaluator.evaluate(dt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . Root Mean Squared Error (RMSE) on test data = 1.01114e+06 from pyspark.ml.regression import GBTRegressor gbt = GBTRegressor(featuresCol = &#39;features&#39;, labelCol = &#39;it&#39;, maxIter=10) gbt_model = gbt.fit(train_df) gbt_predictions = gbt_model.transform(train_df) gbt_predictions.select(&#39;prediction&#39;, &#39;it&#39;, &#39;features&#39;).show(5) gbt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = gbt_evaluator.evaluate(gbt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . ++-+-+ prediction| it| features| ++-+-+ 1388898.308543053| 631930|[2093.0,50661.0]| 1388898.308543053| 657860|[2347.0,43443.0]| 1649083.6277172007| 889463|[2542.0,27673.0]| 1649083.6277172007|1227364|[2712.0,26131.0]| 1649083.6277172007|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows Root Mean Squared Error (RMSE) on test data = 778728 This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "relUrl": "/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post76": {
            "title": "Pyspark for Data Science",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. . # Prints &#39;2&#39; print(1+1) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/markdown/2020/06/28/2st-markdown-post.html",
            "relUrl": "/markdown/2020/06/28/2st-markdown-post.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post77": {
            "title": "Data Science, Big Data and Healthcare Research",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. . # Prints &#39;2&#39; print(1+1) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/markdown/2020/06/08/1st-markdown-post.html",
            "relUrl": "/markdown/2020/06/08/1st-markdown-post.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post78": {
            "title": "A timer for ML functions",
            "content": "&quot;A timer for ML functions&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [timer, jupyter] | description: A timer for ML functions | title: A timer for ML functions | . #collapse-hide from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper . . @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . from collections import Counter import math, random # # data splitting # def split_data(data, prob): &quot;&quot;&quot;split data into fractions [prob, 1 - prob]&quot;&quot;&quot; results = [], [] for row in data: results[0 if random.random() &lt; prob else 1].append(row) return results def train_test_split(x, y, test_pct): data = list(zip(x, y)) # pair corresponding values train, test = split_data(data, 1 - test_pct) # split the dataset of pairs x_train, y_train = list(zip(*train)) # magical un-zip trick x_test, y_test = list(zip(*test)) return x_train, x_test, y_train, y_test # # correctness # def accuracy(tp, fp, fn, tn): correct = tp + tn total = tp + fp + fn + tn return correct / total def precision(tp, fp, fn, tn): return tp / (tp + fp) def recall(tp, fp, fn, tn): return tp / (tp + fn) def f1_score(tp, fp, fn, tn): p = precision(tp, fp, fn, tn) r = recall(tp, fp, fn, tn) return 2 * p * r / (p + r) if __name__ == &quot;__main__&quot;: print(&quot;accuracy(70, 4930, 13930, 981070)&quot;, accuracy(70, 4930, 13930, 981070)) print(&quot;precision(70, 4930, 13930, 981070)&quot;, precision(70, 4930, 13930, 981070)) print(&quot;recall(70, 4930, 13930, 981070)&quot;, recall(70, 4930, 13930, 981070)) print(&quot;f1_score(70, 4930, 13930, 981070)&quot;, f1_score(70, 4930, 13930, 981070)) . favorite_number = 7 def add(a, b): return a + b def sub(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): return a / b def count_vowels(word): count = 0 for letter in word.lower(): count += letter in &#39;aeiou&#39; return count . # import example_module as sm # print(sm.favorite_number) # # add two numbers together # print(sm.add(3, 8)) # # count the number of vowels in a string # print(sm.count_vowels(&#39;Testing&#39;)) . import pandas as pd from alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler config_handler.set_global(theme=&#39;ascii&#39;, spinner=&#39;notes&#39;, bar=&#39;solid&#39;) with alive_bar(3) as bar: df = pd.read_csv(&#39;https://gist.githubusercontent.com/davidrkearney/bb461ba351da484336a19bd00a2612e2/raw/18dd90b57fec46a247248d161ffd8085de2a00db/china_province_economicdata_1996_2007.csv&#39;) bar(&#39;file read, printing file&#39;) print(df.head) bar(&#39;data printed ok, printing methods of data&#39;) print(dir(df)) bar(&#39;process complete&#39;) . from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . This post includes code adapted from Data Science from Scratch .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/06/07/kwargs-decorators.html",
            "relUrl": "/2020/06/07/kwargs-decorators.html",
            "date": " • Jun 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Senior Data Scientist at CVS Health. | Ph.D. from Duke University | .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  

  

  

  

  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}