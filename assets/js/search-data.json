{
  
    
        "post0": {
            "title": "Xgboost for Health Data (Pipeline Step 3)",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 0 30669 | Male | 3.0 | 0 | 0 | No | children | Rural | 95.12 | 18.0 | NaN | 0 | . 1 30468 | Male | 58.0 | 1 | 0 | Yes | Private | Urban | 87.96 | 39.2 | never smoked | 0 | . 2 16523 | Female | 8.0 | 0 | 0 | No | Private | Urban | 110.89 | 17.6 | NaN | 0 | . 3 56543 | Female | 70.0 | 0 | 0 | Yes | Private | Rural | 69.04 | 35.9 | formerly smoked | 0 | . 4 46136 | Male | 14.0 | 0 | 0 | No | Never_worked | Rural | 161.28 | 19.1 | NaN | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 43395 56196 | Female | 10.0 | 0 | 0 | No | children | Urban | 58.64 | 20.4 | never smoked | 0 | . 43396 5450 | Female | 56.0 | 0 | 0 | Yes | Govt_job | Urban | 213.61 | 55.4 | formerly smoked | 0 | . 43397 28375 | Female | 82.0 | 1 | 0 | Yes | Private | Urban | 91.94 | 28.9 | formerly smoked | 0 | . 43398 27973 | Male | 40.0 | 0 | 0 | Yes | Private | Urban | 99.16 | 33.2 | never smoked | 0 | . 43399 36271 | Female | 82.0 | 0 | 0 | Yes | Private | Urban | 79.48 | 20.6 | never smoked | 0 | . 43400 rows × 12 columns . df = df.drop(columns = [&#39;id&#39;]) . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import preprocessing . target = df.stroke.values features = df.drop(columns =[&quot;stroke&quot;]) . X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0) . df.dtypes . gender object age float64 hypertension int64 heart_disease int64 ever_married object work_type object Residence_type object avg_glucose_level float64 bmi float64 smoking_status object stroke int64 dtype: object . # Label Encoding for f in df.columns: if X_train[f].dtype==&#39;object&#39; or X_test[f].dtype==&#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(X_train[f].values) + list(X_test[f].values)) X_train[f] = lbl.transform(list(X_train[f].values)) X_test[f] = lbl.transform(list(X_test[f].values)) . for f in df.columns: print(f) . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . # Label Encoding for f in df.columns: if df[f].dtype==&#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(df[f].values)) df[f] = lbl.transform(list(df[f].values)) . df . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 0 1 | 3.0 | 0 | 0 | 0 | 4 | 0 | 95.12 | 18.0 | 1 | 0 | . 1 1 | 58.0 | 1 | 0 | 1 | 2 | 1 | 87.96 | 39.2 | 2 | 0 | . 2 0 | 8.0 | 0 | 0 | 0 | 2 | 1 | 110.89 | 17.6 | 1 | 0 | . 3 0 | 70.0 | 0 | 0 | 1 | 2 | 0 | 69.04 | 35.9 | 0 | 0 | . 4 1 | 14.0 | 0 | 0 | 0 | 1 | 0 | 161.28 | 19.1 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 43395 0 | 10.0 | 0 | 0 | 0 | 4 | 1 | 58.64 | 20.4 | 2 | 0 | . 43396 0 | 56.0 | 0 | 0 | 1 | 0 | 1 | 213.61 | 55.4 | 0 | 0 | . 43397 0 | 82.0 | 1 | 0 | 1 | 2 | 1 | 91.94 | 28.9 | 0 | 0 | . 43398 1 | 40.0 | 0 | 0 | 1 | 2 | 1 | 99.16 | 33.2 | 2 | 0 | . 43399 0 | 82.0 | 0 | 0 | 1 | 2 | 1 | 79.48 | 20.6 | 2 | 0 | . 43400 rows × 11 columns . # Label Encoding for f in X_train.columns: if X_train[f].dtype==&#39;object&#39; or X_test[f].dtype==&#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(X_train[f].values) + list(X_test[f].values)) X_train[f] = lbl.transform(list(X_train[f].values)) X_test[f] = lbl.transform(list(X_test[f].values)) . from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate . import xgboost as xgb . clf = xgb.XGBClassifier( n_estimators=500, max_depth=9, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9, missing=-999, random_state=2021, tree_method=&#39;auto&#39; # tree_method=&#39;hist&#39; # tree_method=&#39;gpu_hist&#39; ) . ## K Fold Cross Validation (5 Folds) . kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42) . # Define Parameter grid for hyperparameter search process . param_grid = { &#39;colsample_bytree&#39;:[.75,1], &#39;learning_rate&#39;:[0.01,0.05,0.1,0.3,0.5], &#39;max_depth&#39;:[1,2,3,5], &#39;subsample&#39;:[.75,1], &#39;n_estimators&#39;: list(range(50, 400, 50)) } . ## Run the GridSearch,optimizing on ROC . grid_search = GridSearchCV(estimator=clf, scoring=&#39;roc_auc&#39;, param_grid=param_grid, n_jobs=-1, cv=kfold) . %%time grid_result = grid_search.fit(X_train, y_train) . /home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [09:05:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. CPU times: user 27 s, sys: 996 ms, total: 28 s Wall time: 20min 15s . print(f&#39;Best: {grid_result.best_score_} using {grid_result.best_params_}&#39;,&#39; n&#39;) . Best: 0.8617766137590029 using {&#39;colsample_bytree&#39;: 0.75, &#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 300, &#39;subsample&#39;: 0.75} . #Set our final hyperparameters to the tuned values xgbcl = xgb.XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1.0, gamma=0.0, max_delta_step=0.0, min_child_weight=1.0, missing=None, n_jobs=-1, objective=&#39;binary:logistic&#39;, random_state=42, reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0, tree_method=&#39;auto&#39;, colsample_bytree = grid_result.best_params_[&#39;colsample_bytree&#39;], learning_rate = grid_result.best_params_[&#39;learning_rate&#39;], max_depth = grid_result.best_params_[&#39;max_depth&#39;], subsample = grid_result.best_params_[&#39;subsample&#39;], n_estimators = grid_result.best_params_[&#39;n_estimators&#39;]) kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) #refit the model on k-folds to get stable avg error metrics scores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, scoring=[&#39;accuracy&#39;, &#39;roc_auc&#39;, &#39;precision&#39;, &#39;recall&#39;, &#39;f1&#39;]) print(&#39;Training 5-fold Cross Validation Results: n&#39;) print(&#39;AUC: &#39;, scores[&#39;test_roc_auc&#39;].mean()) print(&#39;Accuracy: &#39;, scores[&#39;test_accuracy&#39;].mean()) print(&#39;Precision: &#39;, scores[&#39;test_precision&#39;].mean()) print(&#39;Recall: &#39;, scores[&#39;test_recall&#39;].mean()) print(&#39;F1: &#39;, scores[&#39;test_f1&#39;].mean(), &#39; n&#39;) . Training 5-fold Cross Validation Results: AUC: 0.8632093427558646 Accuracy: 0.981278801843318 Precision: 0.0 Recall: 0.0 F1: 0.0 . import sklearn.metrics as metrics . #Fit the final model xgbcl.fit(X_train, y_train) #Generate predictions against our training and test data pred_train = xgbcl.predict(X_train) proba_train = xgbcl.predict_proba(X_train) pred_test = xgbcl.predict(X_test) proba_test = xgbcl.predict_proba(X_test) # Print model report print(&quot;Classification report (Test): n&quot;) print(metrics.classification_report(y_test, pred_test)) print(&quot;Confusion matrix (Test): n&quot;) print(metrics.confusion_matrix(y_test, pred_test)/len(y_test)) print (&#39; nTrain Accuracy:&#39;, metrics.accuracy_score(y_train, pred_train)) print (&#39;Test Accuracy:&#39;, metrics.accuracy_score(y_test, pred_test)) print (&#39; nTrain AUC:&#39;, metrics.roc_auc_score(y_train, proba_train[:,1])) print (&#39;Test AUC:&#39;, metrics.roc_auc_score(y_test, proba_test[:,1])) # calculate the fpr and tpr for all thresholds of the classification train_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1]) test_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1]) train_roc_auc = metrics.auc(train_fpr, train_tpr) test_roc_auc = metrics.auc(test_fpr, test_tpr) import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=[7,5]) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(train_fpr, train_tpr, &#39;b&#39;, label = &#39;Train AUC = %0.2f&#39; % train_roc_auc) plt.plot(test_fpr, test_tpr, &#39;g&#39;, label = &#39;Test AUC = %0.2f&#39; % test_roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() # plot feature importance xgb.plot_importance(xgbcl, importance_type=&#39;gain&#39;); . [20:30:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. Classification report (Test): precision recall f1-score support 0 0.98 1.00 0.99 8547 1 0.00 0.00 0.00 133 accuracy 0.98 8680 macro avg 0.49 0.50 0.50 8680 weighted avg 0.97 0.98 0.98 8680 Confusion matrix (Test): [[0.98467742 0. ] [0.01532258 0. ]] Train Accuracy: 0.981278801843318 Test Accuracy: 0.9846774193548387 Train AUC: 0.8907315481700571 Test AUC: 0.8661175578468812 . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . #take a random row of data X_rand = features.sample(1, random_state = 5) display(df.iloc[X_rand.index]) . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 33658 0 | 60.0 | 0 | 0 | 1 | 2 | 0 | 108.13 | 28.6 | 2 | 0 | . X = features . #Set our final hyperparameters to the tuned values xgbcl = xgb.XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1.0, gamma=0.0, max_delta_step=0.0, min_child_weight=1.0, missing=None, n_jobs=-1, objective=&#39;binary:logistic&#39;, random_state=42, reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0, tree_method=&#39;auto&#39;, colsample_bytree = grid_result.best_params_[&#39;colsample_bytree&#39;], learning_rate = grid_result.best_params_[&#39;learning_rate&#39;], max_depth = grid_result.best_params_[&#39;max_depth&#39;], subsample = grid_result.best_params_[&#39;subsample&#39;], n_estimators = grid_result.best_params_[&#39;n_estimators&#39;]) kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) #refit the model on k-folds to get stable avg error metrics scores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, scoring=[&#39;accuracy&#39;, &#39;roc_auc&#39;, &#39;precision&#39;, &#39;recall&#39;, &#39;f1&#39;]) print(&#39;Training 5-fold Cross Validation Results: n&#39;) print(&#39;AUC: &#39;, scores[&#39;test_roc_auc&#39;].mean()) print(&#39;Accuracy: &#39;, scores[&#39;test_accuracy&#39;].mean()) print(&#39;Precision: &#39;, scores[&#39;test_precision&#39;].mean()) print(&#39;Recall: &#39;, scores[&#39;test_recall&#39;].mean()) print(&#39;F1: &#39;, scores[&#39;test_f1&#39;].mean(), &#39; n&#39;) . NameError Traceback (most recent call last) &lt;ipython-input-16-36f8657b8499&gt; in &lt;module&gt; 4 missing=None, n_jobs=-1, objective=&#39;binary:logistic&#39;, random_state=42, reg_alpha=0.0, 5 reg_lambda=1.0, scale_pos_weight=1.0, tree_method=&#39;auto&#39;, -&gt; 6 colsample_bytree = grid_result.best_params_[&#39;colsample_bytree&#39;], 7 learning_rate = grid_result.best_params_[&#39;learning_rate&#39;], 8 max_depth = grid_result.best_params_[&#39;max_depth&#39;], NameError: name &#39;grid_result&#39; is not defined . #Generate predictions against our training and test data pred_train = clf.predict(X_train) proba_train = clf.predict_proba(X_train) pred_test = clf.predict(X_test) proba_test = clf.predict_proba(X_test) . import sklearn.metrics as metrics # Print model report print(&quot;Classification report (Test): n&quot;) print(metrics.classification_report(y_test, pred_test)) print(&quot;Confusion matrix (Test): n&quot;) print(metrics.confusion_matrix(y_test, pred_test)/len(y_test)) print (&#39; nTrain Accuracy:&#39;, metrics.accuracy_score(y_train, pred_train)) print (&#39;Test Accuracy:&#39;, metrics.accuracy_score(y_test, pred_test)) print (&#39; nTrain AUC:&#39;, metrics.roc_auc_score(y_train, proba_train[:,1])) print (&#39;Test AUC:&#39;, metrics.roc_auc_score(y_test, proba_test[:,1])) . Classification report (Test): precision recall f1-score support 0 0.98 1.00 0.99 8547 1 0.20 0.02 0.03 133 accuracy 0.98 8680 macro avg 0.59 0.51 0.51 8680 weighted avg 0.97 0.98 0.98 8680 Confusion matrix (Test): [[9.83755760e-01 9.21658986e-04] [1.50921659e-02 2.30414747e-04]] Train Accuracy: 0.9959389400921659 Test Accuracy: 0.9839861751152074 Train AUC: 0.9999601273396401 Test AUC: 0.835284948066903 . # calculate the fpr and tpr for all thresholds of the classification train_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1]) test_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1]) train_roc_auc = metrics.auc(train_fpr, train_tpr) test_roc_auc = metrics.auc(test_fpr, test_tpr) . import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=[7,5]) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(train_fpr, train_tpr, &#39;b&#39;, label = &#39;Train AUC = %0.2f&#39; % train_roc_auc) plt.plot(test_fpr, test_tpr, &#39;g&#39;, label = &#39;Test AUC = %0.2f&#39; % test_roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() # plot feature importance xgb.plot_importance(clf, importance_type=&#39;gain&#39;); . #take a random row of data X_rand = features.sample(1, random_state = 5) display(df.iloc[X_rand.index]) . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 33658 Female | 60.0 | 0 | 0 | Yes | Private | Rural | 108.13 | 28.6 | never smoked | 0 | . import shap . ## kernel shap sends data as numpy array which has no column names, so we fix it def xgb_predict(data_asarray): data_asframe = pd.DataFrame(data_asarray, columns=feature_names) return estimator.predict(data_asframe) . #### Kernel SHAP X_summary = shap.kmeans(X_train, 10) . #shap_kernel_explainer = shap.KernelExplainer(xgbcl, X_summary) . def xgb_predict(df): data_asframe = pd.DataFrame(df, columns=feature_names) return estimator.predict(data_asframe) . #### Tree SHAP shap_tree_explainer = shap.TreeExplainer(xgbcl, feature_perturbation = &quot;interventional&quot;) . shap.initjs() ## shapely values with kernel SHAP shap.force_plot(shap_tree_explainer.expected_value, shap_values_single, X_test.iloc[[5]]) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() ## shapely values with Tree SHAP shap_values_single = shap_tree_explainer.shap_values(X_test.iloc[[10]]) shap.force_plot(shap_tree_explainer.expected_value, shap_values_single, X_test.iloc[[5]]) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap_values = shap_tree_explainer.shap_values(X_train) shap.summary_plot(shap_values, features) . AssertionError Traceback (most recent call last) &lt;ipython-input-71-0722fac4ad3c&gt; in &lt;module&gt; 1 shap_values = shap_tree_explainer.shap_values(X_train) -&gt; 2 shap.summary_plot(shap_values, features) ~/anaconda3/lib/python3.8/site-packages/shap/plots/_beeswarm.py in summary_legacy(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, auto_size_plot, use_log_scale) 635 vmin = vmax 636 --&gt; 637 assert features.shape[0] == len(shaps), &#34;Feature and SHAP matrices must have the same number of rows!&#34; 638 639 # plot the nan values in the interaction feature as grey AssertionError: Feature and SHAP matrices must have the same number of rows! . # #Display all features and SHAP values # display(pd.DataFrame(data=shap_values, columns=X_train.columns, index=[126]).transpose().sort_values(by=126, ascending=True)) . X_train.columns . Index([&#39;gender&#39;, &#39;age&#39;, &#39;hypertension&#39;, &#39;heart_disease&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;avg_glucose_level&#39;, &#39;bmi&#39;, &#39;smoking_status&#39;], dtype=&#39;object&#39;) . shap.dependence_plot(&#39;age&#39;, shap_values, X_train, interaction_index=&#39;bmi&#39;) shap.dependence_plot(&#39;bmi&#39;, shap_values, X_train) #when we don&#39;t specify an interaction_index, the strongest one is automatically chosen for us shap.dependence_plot(&#39;heart_disease&#39;, shap_values, X_train, interaction_index=&#39;age&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/19/_Xgboost_Health_data.html",
            "relUrl": "/2021/01/19/_Xgboost_Health_data.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Modeling Health Care Data App",
            "content": "import sys import os from scipy import stats from datetime import datetime, date import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from sklearn import metrics from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.neighbors import KNeighborsClassifier from sklearn.pipeline import make_pipeline import xgboost as xgb %matplotlib inline plt.style.use(&quot;fivethirtyeight&quot;) sns.set_context(&quot;notebook&quot;) from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import SMOTE . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import preprocessing . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import warnings import pickle from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict,cross_validate from xgboost import XGBClassifier from sklearn.metrics import accuracy_score, confusion_matrix . Import DF . url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 43400 entries, 0 to 43399 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 id 43400 non-null int64 1 gender 43400 non-null object 2 age 43400 non-null float64 3 hypertension 43400 non-null int64 4 heart_disease 43400 non-null int64 5 ever_married 43400 non-null object 6 work_type 43400 non-null object 7 Residence_type 43400 non-null object 8 avg_glucose_level 43400 non-null float64 9 bmi 41938 non-null float64 10 smoking_status 30108 non-null object 11 stroke 43400 non-null int64 dtypes: float64(3), int64(4), object(5) memory usage: 4.0+ MB . Data Prep . df = df.drop(columns = [&#39;id&#39;]) . # Label Encoding for f in df.columns: if df[f].dtype==&#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(df[f].values)) df[f] = lbl.transform(list(df[f].values)) . pct_list = [] for col in df.columns: pct_missing = np.mean(df[col].isnull()) if round(pct_missing*100) &gt;0: pct_list.append([col, round(pct_missing*100)]) print(&#39;{} - {}%&#39;.format(col, round(pct_missing*100))) . gender - 0% age - 0% hypertension - 0% heart_disease - 0% ever_married - 0% work_type - 0% Residence_type - 0% avg_glucose_level - 0% bmi - 3% smoking_status - 0% stroke - 0% . df = df.fillna(df.mean()) . df=df.dropna() df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 43400 entries, 0 to 43399 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 gender 43400 non-null int64 1 age 43400 non-null float64 2 hypertension 43400 non-null int64 3 heart_disease 43400 non-null int64 4 ever_married 43400 non-null int64 5 work_type 43400 non-null int64 6 Residence_type 43400 non-null int64 7 avg_glucose_level 43400 non-null float64 8 bmi 43400 non-null float64 9 smoking_status 43400 non-null int64 10 stroke 43400 non-null int64 dtypes: float64(3), int64(8) memory usage: 4.0 MB . Features = [&#39;age&#39;,&#39;heart_disease&#39;,&#39;ever_married&#39;] x = df[Features] y = df[&quot;stroke&quot;] . # Train Test split X_train, X_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=2) . #### Data Preprocessing from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) . from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate import xgboost as xgb . clf = xgb.XGBClassifier( n_estimators=500, max_depth=9, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9, missing=-999, random_state=2021, tree_method=&#39;auto&#39; # tree_method=&#39;hist&#39; # tree_method=&#39;gpu_hist&#39; ) . kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42) . param_grid = { &#39;colsample_bytree&#39;:[.75,1], &#39;learning_rate&#39;:[0.01,0.05,0.1,0.3,0.5], &#39;max_depth&#39;:[1,2,3,5], &#39;subsample&#39;:[.75,1], &#39;n_estimators&#39;: list(range(50, 400, 50)) } . grid_search = GridSearchCV(estimator=clf, scoring=&#39;roc_auc&#39;, param_grid=param_grid, n_jobs=-1, cv=kfold) . %%time grid_result = grid_search.fit(X_train, y_train) . /home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [12:38:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. CPU times: user 9.02 s, sys: 860 ms, total: 9.88 s Wall time: 12min 43s . print(f&#39;Best: {grid_result.best_score_} using {grid_result.best_params_}&#39;,&#39; n&#39;) . Best: 0.8410524780191915 using {&#39;colsample_bytree&#39;: 0.75, &#39;learning_rate&#39;: 0.05, &#39;max_depth&#39;: 1, &#39;n_estimators&#39;: 200, &#39;subsample&#39;: 0.75} . #Set our final hyperparameters to the tuned values xgbcl = xgb.XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1.0, gamma=0.0, max_delta_step=0.0, min_child_weight=1.0, missing=None, n_jobs=-1, objective=&#39;binary:logistic&#39;, random_state=42, reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0, tree_method=&#39;auto&#39;, colsample_bytree = grid_result.best_params_[&#39;colsample_bytree&#39;], learning_rate = grid_result.best_params_[&#39;learning_rate&#39;], max_depth = grid_result.best_params_[&#39;max_depth&#39;], subsample = grid_result.best_params_[&#39;subsample&#39;], n_estimators = grid_result.best_params_[&#39;n_estimators&#39;]) kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) #refit the model on k-folds to get stable avg error metrics scores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, scoring=[&#39;accuracy&#39;, &#39;roc_auc&#39;, &#39;precision&#39;, &#39;recall&#39;, &#39;f1&#39;]) print(&#39;Training 5-fold Cross Validation Results: n&#39;) print(&#39;AUC: &#39;, scores[&#39;test_roc_auc&#39;].mean()) print(&#39;Accuracy: &#39;, scores[&#39;test_accuracy&#39;].mean()) print(&#39;Precision: &#39;, scores[&#39;test_precision&#39;].mean()) print(&#39;Recall: &#39;, scores[&#39;test_recall&#39;].mean()) print(&#39;F1: &#39;, scores[&#39;test_f1&#39;].mean(), &#39; n&#39;) . Training 5-fold Cross Validation Results: AUC: 0.8429184127269972 Accuracy: 0.9820852534562212 Precision: 0.0 Recall: 0.0 F1: 0.0 . import sklearn.metrics as metrics . #Fit the final model xgbcl.fit(X_train, y_train) #Generate predictions against our training and test data pred_train = xgbcl.predict(X_train) proba_train = xgbcl.predict_proba(X_train) pred_test = xgbcl.predict(X_test) proba_test = xgbcl.predict_proba(X_test) # Print model report print(&quot;Classification report (Test): n&quot;) print(metrics.classification_report(y_test, pred_test)) print(&quot;Confusion matrix (Test): n&quot;) print(metrics.confusion_matrix(y_test, pred_test)/len(y_test)) print (&#39; nTrain Accuracy:&#39;, metrics.accuracy_score(y_train, pred_train)) print (&#39;Test Accuracy:&#39;, metrics.accuracy_score(y_test, pred_test)) print (&#39; nTrain AUC:&#39;, metrics.roc_auc_score(y_train, proba_train[:,1])) print (&#39;Test AUC:&#39;, metrics.roc_auc_score(y_test, proba_test[:,1])) # calculate the fpr and tpr for all thresholds of the classification train_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1]) test_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1]) train_roc_auc = metrics.auc(train_fpr, train_tpr) test_roc_auc = metrics.auc(test_fpr, test_tpr) import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=[7,5]) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(train_fpr, train_tpr, &#39;b&#39;, label = &#39;Train AUC = %0.2f&#39; % train_roc_auc) plt.plot(test_fpr, test_tpr, &#39;g&#39;, label = &#39;Test AUC = %0.2f&#39; % test_roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() # plot feature importance xgb.plot_importance(xgbcl, importance_type=&#39;gain&#39;); . [12:38:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . /home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . Classification report (Test): precision recall f1-score support 0 0.98 1.00 0.99 8519 1 0.00 0.00 0.00 161 accuracy 0.98 8680 macro avg 0.49 0.50 0.50 8680 weighted avg 0.96 0.98 0.97 8680 Confusion matrix (Test): [[0.98145161 0. ] [0.01854839 0. ]] Train Accuracy: 0.9820852534562212 Test Accuracy: 0.9814516129032258 Train AUC: 0.8465318377764562 Test AUC: 0.8563193417126058 . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . pickle.dump(xgbcl, open(&#39;stroke_xgboost_model.pkl&#39;, &#39;wb&#39;)) pickle.dump(scaler, open(&#39;scaler.pkl&#39;, &#39;wb&#39;)) model = pickle.load(open(&#39;stroke_xgboost_model.pkl&#39;, &#39;rb&#39;)) print(model) . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1.0, colsample_bynode=1, colsample_bytree=0.75, gamma=0.0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.05, max_delta_step=0.0, max_depth=1, min_child_weight=1.0, missing=None, monotone_constraints=&#39;()&#39;, n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=42, reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0, subsample=0.75, tree_method=&#39;auto&#39;, validate_parameters=1, verbosity=None) . Random Forest Classifier . Feature and Target Selection . # Select feature and target variables: X = df.drop([&#39;stroke&#39;], axis=1) y = df[[&#39;stroke&#39;]] . #One-hot encode the data using pandas get_dummies X = pd.get_dummies(X) . #rus = RandomUnderSampler(random_state=0, replacement=True) #X_resampled, y_resampled = rus.fit_resample(X, y) #print(np.vstack(np.unique([tuple(row) for row in X_resampled], axis=0)).shape) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) columns = X_train.columns sm = SMOTE(random_state=1) X_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train) . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train_SMOTE, y_train_SMOTE) y_pred = model.predict(X_test) . &lt;ipython-input-172-58a1a4a55e81&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train_SMOTE, y_train_SMOTE) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.9s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 2.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . from sklearn.metrics import roc_auc_score # Calculate roc auc roc_value = roc_auc_score(y_test, y_pred) roc_value . 0.5381645695594856 . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9531490015360983 Precision: 0.05491990846681922 Recall: 0.1085972850678733 . y_pred_proba = model.predict_proba(X_test)[::,1] fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba) auc = metrics.roc_auc_score(y_test, y_pred_proba) plt.plot(fpr,tpr,label=&quot;data 1, auc=&quot;+str(auc)) plt.legend(loc=4) plt.show() . [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished . from sklearn.metrics import precision_recall_curve import matplotlib.pyplot as plt from inspect import signature precision, recall, _ = precision_recall_curve(y_test, y_pred) plt.plot(precision,recall) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) . Text(0, 0.5, &#39;Precision&#39;) . # Import numpy and matplotlib import numpy as np import matplotlib.pyplot as plt # Construct the histogram with a flattened 3d array and a range of bins plt.hist(y_pred_proba.ravel()) # Add a title to the plot plt.title(&#39;Predicted Probability of Stroke&#39;) # Show the plot plt.show() . len(y_pred_proba) . 13020 . y_pred . array([1, 0, 0, ..., 0, 0, 1]) . Get feature importances for interpretability . # Get numerical feature importances importances = list(model.feature_importances_) # List of tuples with variable and importance feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X, importances)] # Sort the feature importances by most important first feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True) # Print out the features and importances [print(&#39;Variable: {:20} Importance: {}&#39;.format(*pair)) for pair in feature_importances]; . Variable: age Importance: 0.44 Variable: avg_glucose_level Importance: 0.19 Variable: bmi Importance: 0.15 Variable: work_type Importance: 0.06 Variable: smoking_status Importance: 0.05 Variable: gender Importance: 0.03 Variable: Residence_type Importance: 0.03 Variable: hypertension Importance: 0.02 Variable: ever_married Importance: 0.02 Variable: heart_disease Importance: 0.01 . plt.figure(1) plt.title(&#39;Feature Importance&#39;) x_values = list(range(len(importances))) plt.barh(x_values, importances, align=&#39;center&#39;) plt.yticks(x_values, X) plt.xlabel(&#39;Relative Importance&#39;) plt.tight_layout() . import pandas as pd feature_importances = pd.DataFrame(model.feature_importances_, index = X_train.columns, columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) . importances . [0.030891482100094805, 0.4448331026265109, 0.017695413344245573, 0.009520184938617332, 0.01937533663501595, 0.06499114062861666, 0.026702090192497516, 0.18974364950033454, 0.14563102038830425, 0.050616579645762515] . Confusion Matrix . from sklearn.metrics import confusion_matrix cnf_matrix = metrics.confusion_matrix(y_test, y_pred) cnf_matrix . array([[12386, 413], [ 197, 24]]) . sns.set(font_scale=5.0) conf_mat = confusion_matrix(y_test, y_pred) cm_normalized = conf_mat.astype(&#39;float&#39;) / conf_mat.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(30,30), dpi = 100) sns.heatmap(cm_normalized, annot=True, cmap=&quot;Blues&quot;) sns.set(font_scale=1) plt.ylabel(&#39;Actual&#39;) plt.xlabel(&#39;Predicted&#39;) #fig.savefig(&#39;cm_augmented.png&#39;, dpi=fig.dpi, transparent=True) plt.show() . cm_normalized . array([[0.96773185, 0.03226815], [0.89140271, 0.10859729]]) . fig, ax = plt.subplots() # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=&quot;Blues&quot; ,fmt=&#39;g&#39;) ax.xaxis.set_label_position(&quot;top&quot;) plt.tight_layout() plt.title(&#39;Confusion matrix&#39;, y=1.5) plt.ylabel(&#39;Actual label&#39;) plt.xlabel(&#39;Predicted label&#39;) plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=10, labelbottom = False, bottom=False, top = True, labeltop=True) . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train, y_train) y_pred = model.predict(X_test) . &lt;ipython-input-188-533b6710c3e5&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train, y_train) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.6s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train_SMOTE, y_train_SMOTE) y_pred = model.predict(X_test) . &lt;ipython-input-189-58a1a4a55e81&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train_SMOTE, y_train_SMOTE) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.8s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 1.9s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9541474654377881 Precision: 0.05660377358490566 Recall: 0.1085972850678733 . y_pred = model.predict_proba(X_test)[:,1] train_proba = pd.DataFrame({&#39;predicted_probability&#39;: y_pred}) train_proba.info() . [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13020 entries, 0 to 13019 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 predicted_probability 13020 non-null float64 dtypes: float64(1) memory usage: 101.8 KB . [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished . ##check whether y_train indexes are the same as X_train indexes same_index = y_test.index == X_test.index same_index.all() . True . ## get them into the same pandas frame table = pd.concat([y_test.reset_index(drop=True), train_proba.reset_index(drop=True)], axis=1) table . stroke predicted_probability . 0 0 | 0.63 | . 1 0 | 0.25 | . 2 0 | 0.00 | . 3 0 | 0.00 | . 4 0 | 0.00 | . ... ... | ... | . 13015 0 | 0.00 | . 13016 0 | 0.00 | . 13017 0 | 0.00 | . 13018 0 | 0.00 | . 13019 1 | 0.70 | . 13020 rows × 2 columns . table.stroke.value_counts() . 0 12799 1 221 Name: stroke, dtype: int64 . table.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13020 entries, 0 to 13019 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 stroke 13020 non-null int64 1 predicted_probability 13020 non-null float64 dtypes: float64(1), int64(1) memory usage: 203.6 KB . table.to_csv(&#39;../processed_csvs/healthcare_table.csv&#39;) . Cross-Validation Precision . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) . #cross validation predictions for test set y_test_pred = cross_val_predict(forest_clf, X_test, y_test, cv=5) print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_test_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_test_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_test_pred)) . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) . Accuracy: 0.9827188940092166 Precision: 0.0 Recall: 0.0 . #cross validation predictions for full dataset y_pred = cross_val_predict(forest_clf, X, y, cv=5) . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y, y_pred)) . Accuracy: 0.9817741935483871 Precision: 0.0 Recall: 0.0 . test_proba = pd.DataFrame({&#39;predicted_probability&#39;: y_pred}) test_proba.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 43400 entries, 0 to 43399 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 predicted_probability 43400 non-null int64 dtypes: int64(1) memory usage: 339.2 KB . ##check whether y_test indexes are the same as X_test indexes same_index = y.index == X.index same_index.all() . True . ## get them into the same pandas frame table = pd.concat([y.reset_index(drop=True), test_proba.reset_index(drop=True)], axis=1) table . stroke predicted_probability . 0 0 | 0 | . 1 0 | 0 | . 2 0 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 43395 0 | 0 | . 43396 0 | 0 | . 43397 0 | 0 | . 43398 0 | 0 | . 43399 0 | 0 | . 43400 rows × 2 columns . table.stroke.value_counts() . 0 42617 1 783 Name: stroke, dtype: int64 . table.to_csv(&#39;../processed_csvs/final_model_table.csv&#39;) . 5-Fold Cross Validation . from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.model_selection import cross_val_score models = [ LogisticRegression(solver=&quot;liblinear&quot;, random_state=42), RandomForestClassifier(n_estimators=10, random_state=42), KNeighborsClassifier(n_neighbors = 5, metric = &#39;minkowski&#39;, p = 2), GaussianNB(), ] CV = 5 cv_df = pd.DataFrame(index=range(CV * len(models))) entries = [] for model in models: model_name = model.__class__.__name__ accuracies = cross_val_score(model, X, y, scoring=&#39;precision&#39;, cv=CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df = pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;precision&#39;]) sns.boxplot(x=&#39;model_name&#39;, y=&#39;precision&#39;, data=cv_df) sns.stripplot(x=&#39;model_name&#39;, y=&#39;precision&#39;, data=cv_df, size=8, jitter=True, edgecolor=&quot;gray&quot;, linewidth=2) plt.xticks(rotation=45) plt.tight_layout() plt.show() . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/19/_Healthcare_Modeling_app.html",
            "relUrl": "/2021/01/19/_Healthcare_Modeling_app.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Modeling Health Care Data App",
            "content": "import sys import os from scipy import stats from datetime import datetime, date import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from sklearn import metrics from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.neighbors import KNeighborsClassifier from sklearn.pipeline import make_pipeline import xgboost as xgb %matplotlib inline plt.style.use(&quot;fivethirtyeight&quot;) sns.set_context(&quot;notebook&quot;) from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import SMOTE from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import preprocessing import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import warnings import pickle from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict,cross_validate from xgboost import XGBClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Import DF url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df.info() ## Data Prep df = df.drop(columns = [&#39;id&#39;]) # Label Encoding for f in df.columns: if df[f].dtype==&#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(df[f].values)) df[f] = lbl.transform(list(df[f].values)) pct_list = [] for col in df.columns: pct_missing = np.mean(df[col].isnull()) if round(pct_missing*100) &gt;0: pct_list.append([col, round(pct_missing*100)]) print(&#39;{} - {}%&#39;.format(col, round(pct_missing*100))) df = df.fillna(df.mean()) df=df.dropna() df.info() Features = [&#39;age&#39;,&#39;heart_disease&#39;,&#39;ever_married&#39;] x = df[Features] y = df[&quot;stroke&quot;] # Train Test split X_train, X_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=2) #### Data Preprocessing from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate import xgboost as xgb clf = xgb.XGBClassifier( n_estimators=500, max_depth=9, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9, missing=-999, random_state=2021, tree_method=&#39;auto&#39; # tree_method=&#39;hist&#39; # tree_method=&#39;gpu_hist&#39; ) kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42) param_grid = { &#39;colsample_bytree&#39;:[.75,1], &#39;learning_rate&#39;:[0.01,0.05,0.1,0.3,0.5], &#39;max_depth&#39;:[1,2,3,5], &#39;subsample&#39;:[.75,1], &#39;n_estimators&#39;: list(range(50, 400, 50)) } grid_search = GridSearchCV(estimator=clf, scoring=&#39;roc_auc&#39;, param_grid=param_grid, n_jobs=-1, cv=kfold) %%time grid_result = grid_search.fit(X_train, y_train) print(f&#39;Best: {grid_result.best_score_} using {grid_result.best_params_}&#39;,&#39; n&#39;) #Set our final hyperparameters to the tuned values xgbcl = xgb.XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1.0, gamma=0.0, max_delta_step=0.0, min_child_weight=1.0, missing=None, n_jobs=-1, objective=&#39;binary:logistic&#39;, random_state=42, reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0, tree_method=&#39;auto&#39;, colsample_bytree = grid_result.best_params_[&#39;colsample_bytree&#39;], learning_rate = grid_result.best_params_[&#39;learning_rate&#39;], max_depth = grid_result.best_params_[&#39;max_depth&#39;], subsample = grid_result.best_params_[&#39;subsample&#39;], n_estimators = grid_result.best_params_[&#39;n_estimators&#39;]) kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) #refit the model on k-folds to get stable avg error metrics scores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, scoring=[&#39;accuracy&#39;, &#39;roc_auc&#39;, &#39;precision&#39;, &#39;recall&#39;, &#39;f1&#39;]) print(&#39;Training 5-fold Cross Validation Results: n&#39;) print(&#39;AUC: &#39;, scores[&#39;test_roc_auc&#39;].mean()) print(&#39;Accuracy: &#39;, scores[&#39;test_accuracy&#39;].mean()) print(&#39;Precision: &#39;, scores[&#39;test_precision&#39;].mean()) print(&#39;Recall: &#39;, scores[&#39;test_recall&#39;].mean()) print(&#39;F1: &#39;, scores[&#39;test_f1&#39;].mean(), &#39; n&#39;) import sklearn.metrics as metrics #Fit the final model xgbcl.fit(X_train, y_train) #Generate predictions against our training and test data pred_train = xgbcl.predict(X_train) proba_train = xgbcl.predict_proba(X_train) pred_test = xgbcl.predict(X_test) proba_test = xgbcl.predict_proba(X_test) # Print model report print(&quot;Classification report (Test): n&quot;) print(metrics.classification_report(y_test, pred_test)) print(&quot;Confusion matrix (Test): n&quot;) print(metrics.confusion_matrix(y_test, pred_test)/len(y_test)) print (&#39; nTrain Accuracy:&#39;, metrics.accuracy_score(y_train, pred_train)) print (&#39;Test Accuracy:&#39;, metrics.accuracy_score(y_test, pred_test)) print (&#39; nTrain AUC:&#39;, metrics.roc_auc_score(y_train, proba_train[:,1])) print (&#39;Test AUC:&#39;, metrics.roc_auc_score(y_test, proba_test[:,1])) # calculate the fpr and tpr for all thresholds of the classification train_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1]) test_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1]) train_roc_auc = metrics.auc(train_fpr, train_tpr) test_roc_auc = metrics.auc(test_fpr, test_tpr) import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=[7,5]) plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(train_fpr, train_tpr, &#39;b&#39;, label = &#39;Train AUC = %0.2f&#39; % train_roc_auc) plt.plot(test_fpr, test_tpr, &#39;g&#39;, label = &#39;Test AUC = %0.2f&#39; % test_roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() # plot feature importance xgb.plot_importance(xgbcl, importance_type=&#39;gain&#39;); pickle.dump(xgbcl, open(&#39;stroke_xgboost_model.pkl&#39;, &#39;wb&#39;)) pickle.dump(scaler, open(&#39;scaler.pkl&#39;, &#39;wb&#39;)) model = pickle.load(open(&#39;stroke_xgboost_model.pkl&#39;, &#39;rb&#39;)) print(model) . Random Forest Classifier . Feature and Target Selection . # Select feature and target variables: X = df.drop([&#39;stroke&#39;], axis=1) y = df[[&#39;stroke&#39;]] . #One-hot encode the data using pandas get_dummies X = pd.get_dummies(X) . #rus = RandomUnderSampler(random_state=0, replacement=True) #X_resampled, y_resampled = rus.fit_resample(X, y) #print(np.vstack(np.unique([tuple(row) for row in X_resampled], axis=0)).shape) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) columns = X_train.columns sm = SMOTE(random_state=1) X_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train) . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train_SMOTE, y_train_SMOTE) y_pred = model.predict(X_test) . &lt;ipython-input-172-58a1a4a55e81&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train_SMOTE, y_train_SMOTE) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.9s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 2.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . from sklearn.metrics import roc_auc_score # Calculate roc auc roc_value = roc_auc_score(y_test, y_pred) roc_value . 0.5381645695594856 . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9531490015360983 Precision: 0.05491990846681922 Recall: 0.1085972850678733 . y_pred_proba = model.predict_proba(X_test)[::,1] fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba) auc = metrics.roc_auc_score(y_test, y_pred_proba) plt.plot(fpr,tpr,label=&quot;data 1, auc=&quot;+str(auc)) plt.legend(loc=4) plt.show() . [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished . from sklearn.metrics import precision_recall_curve import matplotlib.pyplot as plt from inspect import signature precision, recall, _ = precision_recall_curve(y_test, y_pred) plt.plot(precision,recall) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) . Text(0, 0.5, &#39;Precision&#39;) . # Import numpy and matplotlib import numpy as np import matplotlib.pyplot as plt # Construct the histogram with a flattened 3d array and a range of bins plt.hist(y_pred_proba.ravel()) # Add a title to the plot plt.title(&#39;Predicted Probability of Stroke&#39;) # Show the plot plt.show() . len(y_pred_proba) . 13020 . y_pred . array([1, 0, 0, ..., 0, 0, 1]) . Get feature importances for interpretability . # Get numerical feature importances importances = list(model.feature_importances_) # List of tuples with variable and importance feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X, importances)] # Sort the feature importances by most important first feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True) # Print out the features and importances [print(&#39;Variable: {:20} Importance: {}&#39;.format(*pair)) for pair in feature_importances]; . Variable: age Importance: 0.44 Variable: avg_glucose_level Importance: 0.19 Variable: bmi Importance: 0.15 Variable: work_type Importance: 0.06 Variable: smoking_status Importance: 0.05 Variable: gender Importance: 0.03 Variable: Residence_type Importance: 0.03 Variable: hypertension Importance: 0.02 Variable: ever_married Importance: 0.02 Variable: heart_disease Importance: 0.01 . plt.figure(1) plt.title(&#39;Feature Importance&#39;) x_values = list(range(len(importances))) plt.barh(x_values, importances, align=&#39;center&#39;) plt.yticks(x_values, X) plt.xlabel(&#39;Relative Importance&#39;) plt.tight_layout() . import pandas as pd feature_importances = pd.DataFrame(model.feature_importances_, index = X_train.columns, columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) . importances . [0.030891482100094805, 0.4448331026265109, 0.017695413344245573, 0.009520184938617332, 0.01937533663501595, 0.06499114062861666, 0.026702090192497516, 0.18974364950033454, 0.14563102038830425, 0.050616579645762515] . Confusion Matrix . from sklearn.metrics import confusion_matrix cnf_matrix = metrics.confusion_matrix(y_test, y_pred) cnf_matrix . array([[12386, 413], [ 197, 24]]) . sns.set(font_scale=5.0) conf_mat = confusion_matrix(y_test, y_pred) cm_normalized = conf_mat.astype(&#39;float&#39;) / conf_mat.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(30,30), dpi = 100) sns.heatmap(cm_normalized, annot=True, cmap=&quot;Blues&quot;) sns.set(font_scale=1) plt.ylabel(&#39;Actual&#39;) plt.xlabel(&#39;Predicted&#39;) #fig.savefig(&#39;cm_augmented.png&#39;, dpi=fig.dpi, transparent=True) plt.show() . cm_normalized . array([[0.96773185, 0.03226815], [0.89140271, 0.10859729]]) . fig, ax = plt.subplots() # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=&quot;Blues&quot; ,fmt=&#39;g&#39;) ax.xaxis.set_label_position(&quot;top&quot;) plt.tight_layout() plt.title(&#39;Confusion matrix&#39;, y=1.5) plt.ylabel(&#39;Actual label&#39;) plt.xlabel(&#39;Predicted label&#39;) plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=10, labelbottom = False, bottom=False, top = True, labeltop=True) . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train, y_train) y_pred = model.predict(X_test) . &lt;ipython-input-188-533b6710c3e5&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train, y_train) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.6s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train_SMOTE, y_train_SMOTE) y_pred = model.predict(X_test) . &lt;ipython-input-189-58a1a4a55e81&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train_SMOTE, y_train_SMOTE) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.8s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 1.9s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9541474654377881 Precision: 0.05660377358490566 Recall: 0.1085972850678733 . y_pred = model.predict_proba(X_test)[:,1] train_proba = pd.DataFrame({&#39;predicted_probability&#39;: y_pred}) train_proba.info() . [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13020 entries, 0 to 13019 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 predicted_probability 13020 non-null float64 dtypes: float64(1) memory usage: 101.8 KB . [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished . ##check whether y_train indexes are the same as X_train indexes same_index = y_test.index == X_test.index same_index.all() . True . ## get them into the same pandas frame table = pd.concat([y_test.reset_index(drop=True), train_proba.reset_index(drop=True)], axis=1) table . stroke predicted_probability . 0 0 | 0.63 | . 1 0 | 0.25 | . 2 0 | 0.00 | . 3 0 | 0.00 | . 4 0 | 0.00 | . ... ... | ... | . 13015 0 | 0.00 | . 13016 0 | 0.00 | . 13017 0 | 0.00 | . 13018 0 | 0.00 | . 13019 1 | 0.70 | . 13020 rows × 2 columns . table.stroke.value_counts() . 0 12799 1 221 Name: stroke, dtype: int64 . table.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13020 entries, 0 to 13019 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 stroke 13020 non-null int64 1 predicted_probability 13020 non-null float64 dtypes: float64(1), int64(1) memory usage: 203.6 KB . table.to_csv(&#39;../processed_csvs/healthcare_table.csv&#39;) . Cross-Validation Precision . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) . #cross validation predictions for test set y_test_pred = cross_val_predict(forest_clf, X_test, y_test, cv=5) print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_test_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_test_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_test_pred)) . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) . Accuracy: 0.9827188940092166 Precision: 0.0 Recall: 0.0 . #cross validation predictions for full dataset y_pred = cross_val_predict(forest_clf, X, y, cv=5) . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y, y_pred)) . Accuracy: 0.9817741935483871 Precision: 0.0 Recall: 0.0 . test_proba = pd.DataFrame({&#39;predicted_probability&#39;: y_pred}) test_proba.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 43400 entries, 0 to 43399 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 predicted_probability 43400 non-null int64 dtypes: int64(1) memory usage: 339.2 KB . ##check whether y_test indexes are the same as X_test indexes same_index = y.index == X.index same_index.all() . True . ## get them into the same pandas frame table = pd.concat([y.reset_index(drop=True), test_proba.reset_index(drop=True)], axis=1) table . stroke predicted_probability . 0 0 | 0 | . 1 0 | 0 | . 2 0 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 43395 0 | 0 | . 43396 0 | 0 | . 43397 0 | 0 | . 43398 0 | 0 | . 43399 0 | 0 | . 43400 rows × 2 columns . table.stroke.value_counts() . 0 42617 1 783 Name: stroke, dtype: int64 . table.to_csv(&#39;../processed_csvs/final_model_table.csv&#39;) . 5-Fold Cross Validation . from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.model_selection import cross_val_score models = [ LogisticRegression(solver=&quot;liblinear&quot;, random_state=42), RandomForestClassifier(n_estimators=10, random_state=42), KNeighborsClassifier(n_neighbors = 5, metric = &#39;minkowski&#39;, p = 2), GaussianNB(), ] CV = 5 cv_df = pd.DataFrame(index=range(CV * len(models))) entries = [] for model in models: model_name = model.__class__.__name__ accuracies = cross_val_score(model, X, y, scoring=&#39;precision&#39;, cv=CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df = pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;precision&#39;]) sns.boxplot(x=&#39;model_name&#39;, y=&#39;precision&#39;, data=cv_df) sns.stripplot(x=&#39;model_name&#39;, y=&#39;precision&#39;, data=cv_df, size=8, jitter=True, edgecolor=&quot;gray&quot;, linewidth=2) plt.xticks(rotation=45) plt.tight_layout() plt.show() . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/19/_Healthcare_Modeling_app-Copy1.html",
            "relUrl": "/2021/01/19/_Healthcare_Modeling_app-Copy1.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Modeling Health Care Data",
            "content": "import sys import os from scipy import stats from datetime import datetime, date import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score from sklearn import metrics from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.neighbors import KNeighborsClassifier from sklearn.pipeline import make_pipeline import xgboost as xgb %matplotlib inline plt.style.use(&quot;fivethirtyeight&quot;) sns.set_context(&quot;notebook&quot;) from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import SMOTE . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import preprocessing . Import DF . url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 43400 entries, 0 to 43399 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 id 43400 non-null int64 1 gender 43400 non-null object 2 age 43400 non-null float64 3 hypertension 43400 non-null int64 4 heart_disease 43400 non-null int64 5 ever_married 43400 non-null object 6 work_type 43400 non-null object 7 Residence_type 43400 non-null object 8 avg_glucose_level 43400 non-null float64 9 bmi 41938 non-null float64 10 smoking_status 30108 non-null object 11 stroke 43400 non-null int64 dtypes: float64(3), int64(4), object(5) memory usage: 4.0+ MB . Data Prep . df = df.drop(columns = [&#39;id&#39;]) . # Label Encoding for f in df.columns: if df[f].dtype==&#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(df[f].values)) df[f] = lbl.transform(list(df[f].values)) . pct_list = [] for col in df.columns: pct_missing = np.mean(df[col].isnull()) if round(pct_missing*100) &gt;0: pct_list.append([col, round(pct_missing*100)]) print(&#39;{} - {}%&#39;.format(col, round(pct_missing*100))) . gender - 0% age - 0% hypertension - 0% heart_disease - 0% ever_married - 0% work_type - 0% Residence_type - 0% avg_glucose_level - 0% bmi - 3% smoking_status - 0% stroke - 0% . df = df.fillna(df.mean()) . df=df.dropna() df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 43400 entries, 0 to 43399 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 gender 43400 non-null int64 1 age 43400 non-null float64 2 hypertension 43400 non-null int64 3 heart_disease 43400 non-null int64 4 ever_married 43400 non-null int64 5 work_type 43400 non-null int64 6 Residence_type 43400 non-null int64 7 avg_glucose_level 43400 non-null float64 8 bmi 43400 non-null float64 9 smoking_status 43400 non-null int64 10 stroke 43400 non-null int64 dtypes: float64(3), int64(8) memory usage: 4.0 MB . Random Forest Classifier . Feature and Target Selection . # Select feature and target variables: X = df.drop([&#39;stroke&#39;], axis=1) y = df[[&#39;stroke&#39;]] . #One-hot encode the data using pandas get_dummies X = pd.get_dummies(X) . #rus = RandomUnderSampler(random_state=0, replacement=True) #X_resampled, y_resampled = rus.fit_resample(X, y) #print(np.vstack(np.unique([tuple(row) for row in X_resampled], axis=0)).shape) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) columns = X_train.columns sm = SMOTE(random_state=1) X_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train) . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train_SMOTE, y_train_SMOTE) y_pred = model.predict(X_test) . &lt;ipython-input-172-58a1a4a55e81&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train_SMOTE, y_train_SMOTE) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.9s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 2.0s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . from sklearn.metrics import roc_auc_score # Calculate roc auc roc_value = roc_auc_score(y_test, y_pred) roc_value . 0.5381645695594856 . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9531490015360983 Precision: 0.05491990846681922 Recall: 0.1085972850678733 . y_pred_proba = model.predict_proba(X_test)[::,1] fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba) auc = metrics.roc_auc_score(y_test, y_pred_proba) plt.plot(fpr,tpr,label=&quot;data 1, auc=&quot;+str(auc)) plt.legend(loc=4) plt.show() . [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished . from sklearn.metrics import precision_recall_curve import matplotlib.pyplot as plt from inspect import signature precision, recall, _ = precision_recall_curve(y_test, y_pred) plt.plot(precision,recall) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) . Text(0, 0.5, &#39;Precision&#39;) . # Import numpy and matplotlib import numpy as np import matplotlib.pyplot as plt # Construct the histogram with a flattened 3d array and a range of bins plt.hist(y_pred_proba.ravel()) # Add a title to the plot plt.title(&#39;Predicted Probability of Stroke&#39;) # Show the plot plt.show() . len(y_pred_proba) . 13020 . y_pred . array([1, 0, 0, ..., 0, 0, 1]) . Get feature importances for interpretability . # Get numerical feature importances importances = list(model.feature_importances_) # List of tuples with variable and importance feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X, importances)] # Sort the feature importances by most important first feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True) # Print out the features and importances [print(&#39;Variable: {:20} Importance: {}&#39;.format(*pair)) for pair in feature_importances]; . Variable: age Importance: 0.44 Variable: avg_glucose_level Importance: 0.19 Variable: bmi Importance: 0.15 Variable: work_type Importance: 0.06 Variable: smoking_status Importance: 0.05 Variable: gender Importance: 0.03 Variable: Residence_type Importance: 0.03 Variable: hypertension Importance: 0.02 Variable: ever_married Importance: 0.02 Variable: heart_disease Importance: 0.01 . plt.figure(1) plt.title(&#39;Feature Importance&#39;) x_values = list(range(len(importances))) plt.barh(x_values, importances, align=&#39;center&#39;) plt.yticks(x_values, X) plt.xlabel(&#39;Relative Importance&#39;) plt.tight_layout() . import pandas as pd feature_importances = pd.DataFrame(model.feature_importances_, index = X_train.columns, columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) . importances . [0.030891482100094805, 0.4448331026265109, 0.017695413344245573, 0.009520184938617332, 0.01937533663501595, 0.06499114062861666, 0.026702090192497516, 0.18974364950033454, 0.14563102038830425, 0.050616579645762515] . Confusion Matrix . from sklearn.metrics import confusion_matrix cnf_matrix = metrics.confusion_matrix(y_test, y_pred) cnf_matrix . array([[12386, 413], [ 197, 24]]) . sns.set(font_scale=5.0) conf_mat = confusion_matrix(y_test, y_pred) cm_normalized = conf_mat.astype(&#39;float&#39;) / conf_mat.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(30,30), dpi = 100) sns.heatmap(cm_normalized, annot=True, cmap=&quot;Blues&quot;) sns.set(font_scale=1) plt.ylabel(&#39;Actual&#39;) plt.xlabel(&#39;Predicted&#39;) #fig.savefig(&#39;cm_augmented.png&#39;, dpi=fig.dpi, transparent=True) plt.show() . cm_normalized . array([[0.96773185, 0.03226815], [0.89140271, 0.10859729]]) . fig, ax = plt.subplots() # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=&quot;Blues&quot; ,fmt=&#39;g&#39;) ax.xaxis.set_label_position(&quot;top&quot;) plt.tight_layout() plt.title(&#39;Confusion matrix&#39;, y=1.5) plt.ylabel(&#39;Actual label&#39;) plt.xlabel(&#39;Predicted label&#39;) plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=10, labelbottom = False, bottom=False, top = True, labeltop=True) . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train, y_train) y_pred = model.predict(X_test) . &lt;ipython-input-188-533b6710c3e5&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train, y_train) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.3s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.6s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . from sklearn.ensemble import RandomForestClassifier from sklearn import metrics model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=&#39;sqrt&#39;, n_jobs=3, verbose=1, class_weight=&quot;balanced&quot;) model.fit(X_train_SMOTE, y_train_SMOTE) y_pred = model.predict(X_test) . &lt;ipython-input-189-58a1a4a55e81&gt;:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). model.fit(X_train_SMOTE, y_train_SMOTE) [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.8s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 1.9s finished [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.0s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.1s finished . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9541474654377881 Precision: 0.05660377358490566 Recall: 0.1085972850678733 . y_pred = model.predict_proba(X_test)[:,1] train_proba = pd.DataFrame({&#39;predicted_probability&#39;: y_pred}) train_proba.info() . [Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers. . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13020 entries, 0 to 13019 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 predicted_probability 13020 non-null float64 dtypes: float64(1) memory usage: 101.8 KB . [Parallel(n_jobs=3)]: Done 44 tasks | elapsed: 0.1s [Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed: 0.2s finished . ##check whether y_train indexes are the same as X_train indexes same_index = y_test.index == X_test.index same_index.all() . True . ## get them into the same pandas frame table = pd.concat([y_test.reset_index(drop=True), train_proba.reset_index(drop=True)], axis=1) table . stroke predicted_probability . 0 0 | 0.63 | . 1 0 | 0.25 | . 2 0 | 0.00 | . 3 0 | 0.00 | . 4 0 | 0.00 | . ... ... | ... | . 13015 0 | 0.00 | . 13016 0 | 0.00 | . 13017 0 | 0.00 | . 13018 0 | 0.00 | . 13019 1 | 0.70 | . 13020 rows × 2 columns . table.stroke.value_counts() . 0 12799 1 221 Name: stroke, dtype: int64 . table.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13020 entries, 0 to 13019 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 stroke 13020 non-null int64 1 predicted_probability 13020 non-null float64 dtypes: float64(1), int64(1) memory usage: 203.6 KB . table.to_csv(&#39;../processed_csvs/healthcare_table.csv&#39;) . Cross-Validation Precision . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=100, random_state=42) . #cross validation predictions for test set y_test_pred = cross_val_predict(forest_clf, X_test, y_test, cv=5) print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_test_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_test_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_test_pred)) . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) . Accuracy: 0.9827188940092166 Precision: 0.0 Recall: 0.0 . #cross validation predictions for full dataset y_pred = cross_val_predict(forest_clf, X, y, cv=5) . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y, y_pred)) . Accuracy: 0.9817741935483871 Precision: 0.0 Recall: 0.0 . test_proba = pd.DataFrame({&#39;predicted_probability&#39;: y_pred}) test_proba.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 43400 entries, 0 to 43399 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 predicted_probability 43400 non-null int64 dtypes: int64(1) memory usage: 339.2 KB . ##check whether y_test indexes are the same as X_test indexes same_index = y.index == X.index same_index.all() . True . ## get them into the same pandas frame table = pd.concat([y.reset_index(drop=True), test_proba.reset_index(drop=True)], axis=1) table . stroke predicted_probability . 0 0 | 0 | . 1 0 | 0 | . 2 0 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 43395 0 | 0 | . 43396 0 | 0 | . 43397 0 | 0 | . 43398 0 | 0 | . 43399 0 | 0 | . 43400 rows × 2 columns . table.stroke.value_counts() . 0 42617 1 783 Name: stroke, dtype: int64 . table.to_csv(&#39;../processed_csvs/final_model_table.csv&#39;) . 5-Fold Cross Validation . from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.model_selection import cross_val_score models = [ LogisticRegression(solver=&quot;liblinear&quot;, random_state=42), RandomForestClassifier(n_estimators=10, random_state=42), KNeighborsClassifier(n_neighbors = 5, metric = &#39;minkowski&#39;, p = 2), GaussianNB(), ] CV = 5 cv_df = pd.DataFrame(index=range(CV * len(models))) entries = [] for model in models: model_name = model.__class__.__name__ accuracies = cross_val_score(model, X, y, scoring=&#39;precision&#39;, cv=CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df = pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;precision&#39;]) sns.boxplot(x=&#39;model_name&#39;, y=&#39;precision&#39;, data=cv_df) sns.stripplot(x=&#39;model_name&#39;, y=&#39;precision&#39;, data=cv_df, size=8, jitter=True, edgecolor=&quot;gray&quot;, linewidth=2) plt.xticks(rotation=45) plt.tight_layout() plt.show() . /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). estimator.fit(X_train, y_train, **fit_params) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) /home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(**kwargs) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/19/_Healthcare_Modeling-Copy1.html",
            "relUrl": "/2021/01/19/_Healthcare_Modeling-Copy1.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "EDA for Health Data (Pipeline Step 2)",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 0 30669 | Male | 3.0 | 0 | 0 | No | children | Rural | 95.12 | 18.0 | NaN | 0 | . 1 30468 | Male | 58.0 | 1 | 0 | Yes | Private | Urban | 87.96 | 39.2 | never smoked | 0 | . 2 16523 | Female | 8.0 | 0 | 0 | No | Private | Urban | 110.89 | 17.6 | NaN | 0 | . 3 56543 | Female | 70.0 | 0 | 0 | Yes | Private | Rural | 69.04 | 35.9 | formerly smoked | 0 | . 4 46136 | Male | 14.0 | 0 | 0 | No | Never_worked | Rural | 161.28 | 19.1 | NaN | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 43395 56196 | Female | 10.0 | 0 | 0 | No | children | Urban | 58.64 | 20.4 | never smoked | 0 | . 43396 5450 | Female | 56.0 | 0 | 0 | Yes | Govt_job | Urban | 213.61 | 55.4 | formerly smoked | 0 | . 43397 28375 | Female | 82.0 | 1 | 0 | Yes | Private | Urban | 91.94 | 28.9 | formerly smoked | 0 | . 43398 27973 | Male | 40.0 | 0 | 0 | Yes | Private | Urban | 99.16 | 33.2 | never smoked | 0 | . 43399 36271 | Female | 82.0 | 0 | 0 | Yes | Private | Urban | 79.48 | 20.6 | never smoked | 0 | . 43400 rows × 12 columns . #df = df.drop(columns = [&#39;id&#39;]) . import matplotlib.pyplot as plt from pandas_profiling import ProfileReport profile = ProfileReport(df, title=&#39;Pandas Profiling Report&#39;) . profile.to_widgets() . . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 0 30669 | Male | 3.0 | 0 | 0 | No | children | Rural | 95.12 | 18.0 | NaN | 0 | . 1 30468 | Male | 58.0 | 1 | 0 | Yes | Private | Urban | 87.96 | 39.2 | never smoked | 0 | . 2 16523 | Female | 8.0 | 0 | 0 | No | Private | Urban | 110.89 | 17.6 | NaN | 0 | . 3 56543 | Female | 70.0 | 0 | 0 | Yes | Private | Rural | 69.04 | 35.9 | formerly smoked | 0 | . 4 46136 | Male | 14.0 | 0 | 0 | No | Never_worked | Rural | 161.28 | 19.1 | NaN | 0 | . 5 32257 | Female | 47.0 | 0 | 0 | Yes | Private | Urban | 210.95 | 50.1 | NaN | 0 | . 6 52800 | Female | 52.0 | 0 | 0 | Yes | Private | Urban | 77.59 | 17.7 | formerly smoked | 0 | . 7 41413 | Female | 75.0 | 0 | 1 | Yes | Self-employed | Rural | 243.53 | 27.0 | never smoked | 0 | . 8 15266 | Female | 32.0 | 0 | 0 | Yes | Private | Rural | 77.67 | 32.3 | smokes | 0 | . 9 28674 | Female | 74.0 | 1 | 0 | Yes | Self-employed | Urban | 205.84 | 54.6 | never smoked | 0 | . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 43390 10096 | Female | 69.0 | 0 | 0 | Yes | Self-employed | Urban | 229.85 | 31.2 | never smoked | 0 | . 43391 30077 | Male | 6.0 | 0 | 0 | No | children | Urban | 77.48 | 19.1 | NaN | 0 | . 43392 45266 | Female | 18.0 | 0 | 0 | No | Private | Urban | 131.96 | 22.8 | NaN | 0 | . 43393 69344 | Male | 39.0 | 0 | 0 | Yes | Private | Rural | 132.22 | 31.6 | never smoked | 0 | . 43394 52380 | Male | 47.0 | 0 | 0 | No | Govt_job | Urban | 68.52 | 25.2 | formerly smoked | 0 | . 43395 56196 | Female | 10.0 | 0 | 0 | No | children | Urban | 58.64 | 20.4 | never smoked | 0 | . 43396 5450 | Female | 56.0 | 0 | 0 | Yes | Govt_job | Urban | 213.61 | 55.4 | formerly smoked | 0 | . 43397 28375 | Female | 82.0 | 1 | 0 | Yes | Private | Urban | 91.94 | 28.9 | formerly smoked | 0 | . 43398 27973 | Male | 40.0 | 0 | 0 | Yes | Private | Urban | 99.16 | 33.2 | never smoked | 0 | . 43399 36271 | Female | 82.0 | 0 | 0 | Yes | Private | Urban | 79.48 | 20.6 | never smoked | 0 | . profile.to_notebook_iframe() . . profile.to_file(output_file=&quot;pandas_profiling.html&quot;) . . Find and plot missingness in the data . pct_list = [] for col in df.columns: pct_missing = np.mean(df[col].isnull()) if round(pct_missing*100) &gt;0: pct_list.append([col, round(pct_missing*100)]) print(&#39;{} - {}%&#39;.format(col, round(pct_missing*100))) . id - 0% gender - 0% age - 0% hypertension - 0% heart_disease - 0% ever_married - 0% work_type - 0% Residence_type - 0% avg_glucose_level - 0% bmi - 3% smoking_status - 31% stroke - 0% . cols = df.columns colours = [&#39;darkblue&#39;, &#39;red&#39;] sns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours)) . &lt;AxesSubplot:&gt; . df.groupby([&quot;age&quot;, &#39;heart_disease&#39;])[&#39;stroke&#39;].agg([&#39;sum&#39;]).round(0) . sum . age heart_disease . 0.08 0 0 | . 0.16 0 0 | . 0.24 0 0 | . 0.32 0 0 | . 0.40 0 0 | . ... ... ... | . 80.00 1 13 | . 81.00 0 32 | . 1 11 | . 82.00 0 24 | . 1 12 | . 163 rows × 1 columns . df.dtypes . id int64 gender object age float64 hypertension int64 heart_disease int64 ever_married object work_type object Residence_type object avg_glucose_level float64 bmi float64 smoking_status object stroke int64 dtype: object . # Discretize with respective equal-width bin df[&#39;age_binned&#39;] = pd.cut(df[&#39;age&#39;], np.arange(0, 91, 5)) df[&#39;avg_glucose_level_binned&#39;] = pd.cut(df[&#39;avg_glucose_level&#39;], np.arange(0, 301, 10)) df[&#39;bmi_binned&#39;] = pd.cut(df[&#39;bmi&#39;], np.arange(0, 101, 5)) . import seaborn as sns . # Create the correlation heatmap heatmap = sns.heatmap(df[[&#39;age&#39;, &#39;avg_glucose_level&#39;, &#39;bmi&#39;]].corr(), vmin=-1, vmax=1, annot=True) # Create the title heatmap.set_title(&#39;Correlation Heatmap&#39;); . def get_stacked_bar_chart(column): # Get the count of records by column and stroke df_pct = df.groupby([column, &#39;stroke&#39;])[&#39;age&#39;].count() # Create proper DataFrame&#39;s format df_pct = df_pct.unstack() return df_pct.plot.bar(stacked=True, figsize=(6,6), width=1); . def get_100_percent_stacked_bar_chart(column, width = 0.5): # Get the count of records by column and stroke df_breakdown = df.groupby([column, &#39;stroke&#39;])[&#39;age&#39;].count() # Get the count of records by gender df_total = df.groupby([column])[&#39;age&#39;].count() # Get the percentage for 100% stacked bar chart df_pct = df_breakdown / df_total * 100 # Create proper DataFrame&#39;s format df_pct = df_pct.unstack() return df_pct.plot.bar(stacked=True, figsize=(6,6), width=width); . # Age related to risk get_stacked_bar_chart(&#39;age_binned&#39;) . &lt;AxesSubplot:xlabel=&#39;age_binned&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;age_binned&#39;, width = 0.9) . &lt;AxesSubplot:xlabel=&#39;age_binned&#39;&gt; . get_stacked_bar_chart(&#39;bmi_binned&#39;) get_100_percent_stacked_bar_chart(&#39;bmi_binned&#39;, width = 0.9) . &lt;AxesSubplot:xlabel=&#39;bmi_binned&#39;&gt; . get_stacked_bar_chart(&#39;avg_glucose_level_binned&#39;) get_100_percent_stacked_bar_chart(&#39;avg_glucose_level_binned&#39;, width = 0.9) . &lt;AxesSubplot:xlabel=&#39;avg_glucose_level_binned&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;hypertension&#39;) get_100_percent_stacked_bar_chart(&#39;heart_disease&#39;) . &lt;AxesSubplot:xlabel=&#39;heart_disease&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;gender&#39;) get_100_percent_stacked_bar_chart(&#39;Residence_type&#39;) . &lt;AxesSubplot:xlabel=&#39;Residence_type&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;work_type&#39;) df.groupby([&#39;work_type&#39;])[[&#39;age&#39;]].agg([&#39;count&#39;, &#39;mean&#39;]) . age . count mean . work_type . Govt_job 5440 | 49.097610 | . Never_worked 177 | 17.757062 | . Private 24834 | 45.015060 | . Self-employed 6793 | 59.307817 | . children 6156 | 6.699253 | . get_100_percent_stacked_bar_chart(&#39;ever_married&#39;) df.groupby([&#39;ever_married&#39;])[[&#39;age&#39;]].agg([&#39;count&#39;, &#39;mean&#39;]) . age . count mean . ever_married . No 15462 | 21.238236 | . Yes 27938 | 53.828871 | . g = sns.catplot(x=&quot;Residence_type&quot;, hue=&quot;smoking_status&quot;, col=&quot;work_type&quot;, data=df, kind=&quot;count&quot;, height=4, aspect=.7) . import missingno missingno.matrix(df, figsize = (30,5)) . &lt;AxesSubplot:&gt; . fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,7)) fig.suptitle(&quot;Countplot for the dataset&quot;, fontsize=35) sns.countplot(x=&quot;gender&quot;, data=df,ax=ax1) sns.countplot(x=&quot;stroke&quot;, data=df,ax=ax2) sns.countplot(x=&quot;ever_married&quot;, data=df,ax=ax3) sns.countplot(x=&quot;hypertension&quot;, data=df,ax=ax4) . &lt;AxesSubplot:xlabel=&#39;hypertension&#39;, ylabel=&#39;count&#39;&gt; . sns.displot(x=&quot;age&quot;, data=df, kind=&quot;kde&quot;, hue=&quot;gender&quot;, col=&quot;smoking_status&quot;, row=&quot;Residence_type&quot;) . /home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) /home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) /home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1c3d08f70&gt; . fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,7)) fig.suptitle(&quot;Boxplot for Dataset&quot;, fontsize=35) sns.boxplot(x=&quot;stroke&quot;, y=&quot;avg_glucose_level&quot;, data=df,ax=ax1) sns.boxplot(x=&quot;stroke&quot;, y=&quot;bmi&quot;, data=df,ax=ax2) sns.boxplot(x=&quot;stroke&quot;, y=&quot;age&quot;, data=df,ax=ax3) . &lt;AxesSubplot:xlabel=&#39;stroke&#39;, ylabel=&#39;age&#39;&gt; . # Compute a correlation matrix and convert to long-form corr_mat = df.corr(&quot;kendall&quot;).stack().reset_index(name=&quot;correlation&quot;) # Draw each cell as a scatter point with varying size and color g = sns.relplot( data=corr_mat, x=&quot;level_0&quot;, y=&quot;level_1&quot;, hue=&quot;correlation&quot;, size=&quot;correlation&quot;, palette=&quot;vlag&quot;, hue_norm=(-1, 1), edgecolor=&quot;.7&quot;, height=5, sizes=(50, 250), size_norm=(-.2, .8), ) # Tweak the figure to finalize g.set(xlabel=&quot;&quot;, ylabel=&quot;&quot;, aspect=&quot;equal&quot;) g.despine(left=True, bottom=True) g.ax.margins(0.25) for label in g.ax.get_xticklabels(): label.set_rotation(90) for artist in g.legend.legendHandles: artist.set_edgecolor(&quot;.1&quot;) . strokes_temp_df=df strokes_temp_df[[&#39;stroke&#39;,&#39;hypertension&#39;]] = df[[&#39;stroke&#39;,&#39;hypertension&#39;]].astype(&#39;int&#39;) corr = strokes_temp_df.corr() corr.style.background_gradient() corr.style.background_gradient().set_precision(2) . id age hypertension heart_disease avg_glucose_level bmi stroke . id 1.00 | 0.01 | 0.01 | 0.01 | 0.02 | 0.02 | 0.00 | . age 0.01 | 1.00 | 0.27 | 0.25 | 0.24 | 0.36 | 0.16 | . hypertension 0.01 | 0.27 | 1.00 | 0.12 | 0.16 | 0.16 | 0.08 | . heart_disease 0.01 | 0.25 | 0.12 | 1.00 | 0.15 | 0.06 | 0.11 | . avg_glucose_level 0.02 | 0.24 | 0.16 | 0.15 | 1.00 | 0.19 | 0.08 | . bmi 0.02 | 0.36 | 0.16 | 0.06 | 0.19 | 1.00 | 0.02 | . stroke 0.00 | 0.16 | 0.08 | 0.11 | 0.08 | 0.02 | 1.00 | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/19/_EDA_Health_Data.html",
            "relUrl": "/2021/01/19/_EDA_Health_Data.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Reading CSV from Datasette SQL Database",
            "content": "import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt %matplotlib inline . df = pd.read_csv(&#39;https://stocks-snp-500.herokuapp.com/stocks/stocks_table.csv?_size=max&#39;) . df . rowid Date MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . 0 1 | 2020-01-02 00:00:00.000000 | 158.779999 | 68.800003 | NaN | 112.980003 | . 1 2 | 2020-01-03 00:00:00.000000 | 158.320007 | 67.620003 | NaN | 112.190002 | . 2 3 | 2020-01-06 00:00:00.000000 | 157.080002 | 66.629997 | NaN | 112.589996 | . 3 4 | 2020-01-07 00:00:00.000000 | 159.320007 | 70.290001 | NaN | 112.290001 | . 4 5 | 2020-01-08 00:00:00.000000 | 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | ... | ... | . 258 259 | 2021-01-11 00:00:00.000000 | 218.470001 | 344.980011 | 295.000000 | 131.750000 | . 259 260 | 2021-01-12 00:00:00.000000 | 216.500000 | 333.200012 | 298.000000 | 131.800003 | . 260 261 | 2021-01-13 00:00:00.000000 | 214.020004 | 360.000000 | 295.000000 | 132.100006 | . 261 262 | 2021-01-14 00:00:00.000000 | 215.910004 | 371.000000 | 305.000000 | 131.619995 | . 262 263 | 2021-01-15 00:00:00.000000 | 213.520004 | 397.709991 | 306.820007 | 130.679993 | . 263 rows × 6 columns . df[&#39;FXAIX_stock&#39;].plot() . &lt;AxesSubplot:&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/18/datasette-csv-reading.html",
            "relUrl": "/2021/01/18/datasette-csv-reading.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Maps and Folium",
            "content": "import folium from IPython.core.display import display, HTML import tempfile def folium_ipython_show(m): tmp_output_filename = tempfile.NamedTemporaryFile(suffix=&#39;.html&#39;).name m.save(tmp_output_filename) f = open(tmp_output_filename, &quot;r&quot;) data = f.read() data_fixed_width = data.replace(&#39;width: 100%;height: 100%&#39;, &#39;width: 100%&#39;).replace(&#39;height: 100.0%;&#39;, &#39;height: 609px;&#39;) display(HTML(data_fixed_width)) . folium_ipython_show(folium.Map(location=[42.079391, -87.815622], zoom_start=13)) . &lt;!DOCTYPE html&gt; . m3 = folium.Map(location=[42.079391, -87.815622], zoom_start=12, tiles=&quot;Stamen Terrain&quot;) folium_ipython_show(m3) . &lt;!DOCTYPE html&gt; . m4 = folium.Map(location=[42.079391, -87.815622], zoom_start=12, tiles=&quot;Stamen Terrain&quot;) folium_ipython_show(m4) . &lt;!DOCTYPE html&gt; . m5 = folium.Map(location=[42.079391, -87.815622], tiles=&quot;Stamen Toner&quot;, zoom_start=13) folium_ipython_show(m5) . &lt;!DOCTYPE html&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/15/Folium-Maps.html",
            "relUrl": "/2021/01/15/Folium-Maps.html",
            "date": " • Jan 15, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
            "content": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader import datetime import pandas_datareader.data as web import statsmodels.api as sm import quandl . start = datetime.datetime(2019, 1, 1) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2019-01-01 2607.39 | . 2019-02-01 2754.86 | . 2019-03-01 2803.98 | . 2019-04-01 2903.80 | . 2019-05-01 2854.71 | . 2019-05-31 2752.08 | . 2019-06-01 2890.17 | . 2019-07-01 2996.11 | . 2019-08-01 2897.50 | . 2019-09-01 2982.16 | . 2019-10-01 2977.68 | . 2019-11-01 3104.90 | . 2019-12-01 3176.75 | . 2019-12-31 3230.58 | . 2020-01-01 3278.20 | . 2020-01-31 3225.04 | . 2020-02-01 3277.31 | . 2020-02-28 2954.81 | . 2020-03-01 2652.39 | . 2020-03-31 2584.59 | . 2020-04-01 2761.98 | . 2020-04-30 2912.43 | . 2020-05-01 2919.61 | . 2020-06-01 3104.66 | . 2020-06-30 3100.29 | . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3365.52 | . 2020-09-30 3363.00 | . 2020-10-01 3418.70 | . 2020-11-01 3429.33 | . 2020-11-30 3621.63 | . 2020-12-01 3662.45 | . SP500 . Value . Date . 2019-01-01 2607.39 | . 2019-02-01 2754.86 | . 2019-03-01 2803.98 | . 2019-04-01 2903.80 | . 2019-05-01 2854.71 | . 2019-05-31 2752.08 | . 2019-06-01 2890.17 | . 2019-07-01 2996.11 | . 2019-08-01 2897.50 | . 2019-09-01 2982.16 | . 2019-10-01 2977.68 | . 2019-11-01 3104.90 | . 2019-12-01 3176.75 | . 2019-12-31 3230.58 | . 2020-01-01 3278.20 | . 2020-01-31 3225.04 | . 2020-02-01 3277.31 | . 2020-02-28 2954.81 | . 2020-03-01 2652.39 | . 2020-03-31 2584.59 | . 2020-04-01 2761.98 | . 2020-04-30 2912.43 | . 2020-05-01 2919.61 | . 2020-06-01 3104.66 | . 2020-06-30 3100.29 | . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3365.52 | . 2020-09-30 3363.00 | . 2020-10-01 3418.70 | . 2020-11-01 3429.33 | . 2020-11-30 3621.63 | . 2020-12-01 3662.45 | . SP500.dropna(inplace=True) SP500.index = pd.to_datetime(SP500.index) . SP500.head() . Value . Date . 2019-01-01 2607.39 | . 2019-02-01 2754.86 | . 2019-03-01 2803.98 | . 2019-04-01 2903.80 | . 2019-05-01 2854.71 | . Simple Moving Averages . SP500[&#39;6-month-SMA&#39;]=SP500[&#39;Value&#39;].rolling(window=6).mean() SP500[&#39;12-month-SMA&#39;]=SP500[&#39;Value&#39;].rolling(window=12).mean() . SP500.head() . Value 6-month-SMA 12-month-SMA . Date . 2019-01-01 2607.39 | NaN | NaN | . 2019-02-01 2754.86 | NaN | NaN | . 2019-03-01 2803.98 | NaN | NaN | . 2019-04-01 2903.80 | NaN | NaN | . 2019-05-01 2854.71 | NaN | NaN | . SP500.plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . Exponentially-weighted moving averages . SP500[&#39;EWMA12&#39;] = SP500[&#39;Value&#39;].ewm(span=12).mean() . SP500[[&#39;Value&#39;,&#39;EWMA12&#39;]].plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/02/SNP-SMA-EWMA.html",
            "relUrl": "/2021/01/02/SNP-SMA-EWMA.html",
            "date": " • Jan 2, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Stock Market Analysis of the S&P 500 Index",
            "content": "This post includes code and notes adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader import datetime import pandas_datareader.data as web . import statsmodels.api as sm import quandl . start = datetime.datetime(2019, 1, 1) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2019-01-01 2607.39 | . 2019-02-01 2754.86 | . 2019-03-01 2803.98 | . 2019-04-01 2903.80 | . 2019-05-01 2854.71 | . 2019-05-31 2752.08 | . 2019-06-01 2890.17 | . 2019-07-01 2996.11 | . 2019-08-01 2897.50 | . 2019-09-01 2982.16 | . 2019-10-01 2977.68 | . 2019-11-01 3104.90 | . 2019-12-01 3176.75 | . 2019-12-31 3230.58 | . 2020-01-01 3278.20 | . 2020-01-31 3225.04 | . 2020-02-01 3277.31 | . 2020-02-28 2954.81 | . 2020-03-01 2652.39 | . 2020-03-31 2584.59 | . 2020-04-01 2761.98 | . 2020-04-30 2912.43 | . 2020-05-01 2919.61 | . 2020-06-01 3104.66 | . 2020-06-30 3100.29 | . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3365.52 | . 2020-09-30 3363.00 | . 2020-10-01 3418.70 | . 2020-11-01 3429.33 | . 2020-11-30 3621.63 | . 2020-12-01 3662.45 | . SP500.plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . SP500.index . DatetimeIndex([&#39;2019-01-01&#39;, &#39;2019-02-01&#39;, &#39;2019-03-01&#39;, &#39;2019-04-01&#39;, &#39;2019-05-01&#39;, &#39;2019-05-31&#39;, &#39;2019-06-01&#39;, &#39;2019-07-01&#39;, &#39;2019-08-01&#39;, &#39;2019-09-01&#39;, &#39;2019-10-01&#39;, &#39;2019-11-01&#39;, &#39;2019-12-01&#39;, &#39;2019-12-31&#39;, &#39;2020-01-01&#39;, &#39;2020-01-31&#39;, &#39;2020-02-01&#39;, &#39;2020-02-28&#39;, &#39;2020-03-01&#39;, &#39;2020-03-31&#39;, &#39;2020-04-01&#39;, &#39;2020-04-30&#39;, &#39;2020-05-01&#39;, &#39;2020-06-01&#39;, &#39;2020-06-30&#39;, &#39;2020-07-01&#39;, &#39;2020-07-31&#39;, &#39;2020-08-01&#39;, &#39;2020-08-31&#39;, &#39;2020-09-01&#39;, &#39;2020-09-30&#39;, &#39;2020-10-01&#39;, &#39;2020-11-01&#39;, &#39;2020-11-30&#39;, &#39;2020-12-01&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=None) . SP500.head() . Value . Date . 2019-01-01 2607.39 | . 2019-02-01 2754.86 | . 2019-03-01 2803.98 | . 2019-04-01 2903.80 | . 2019-05-01 2854.71 | . SP500[&#39;Value&#39;].plot() plt.ylabel(&quot;SP500 Value&quot;) . Text(0, 0.5, &#39;SP500 Value&#39;) . Getting at the trend by removing the cyclical elements of the S&amp;P 500 . # Tuple unpacking SP500_cycle, SP500_trend = sm.tsa.filters.hpfilter(SP500.Value) . SP500_cycle . Date 2019-01-01 -159.626002 2019-02-01 -32.234117 2019-03-01 -3.092466 2019-04-01 76.968864 2019-05-01 8.461718 2019-05-31 -113.170164 2019-06-01 6.401669 2019-07-01 94.446400 2019-08-01 -21.300789 2019-09-01 47.056253 2019-10-01 27.196993 2019-11-01 140.021486 2019-12-01 198.502790 2019-12-31 239.946447 2020-01-01 275.993938 2020-01-31 211.756776 2020-02-01 252.953978 2020-02-28 -81.237788 2020-03-01 -396.749949 2020-03-31 -479.773159 2020-04-01 -320.220102 2020-04-30 -190.403605 2020-05-01 -206.636358 2020-06-01 -47.642048 2020-06-30 -80.445214 2020-07-01 -3.630618 2020-07-31 27.617253 2020-08-01 114.566186 2020-08-31 188.466703 2020-09-01 18.177725 2020-09-30 -20.499622 2020-10-01 -1.485569 2020-11-01 -27.927537 2020-11-30 127.057981 2020-12-01 130.481948 Name: Value_cycle, dtype: float64 . type(SP500_cycle) . pandas.core.series.Series . SP500[&quot;trend&quot;] = SP500_trend . SP500[[&#39;trend&#39;,&#39;Value&#39;]].plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . SP500[[&#39;trend&#39;,&#39;Value&#39;]][&quot;2000-03-31&quot;:].plot(figsize=(12,8)) . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2021/01/01/SNP-trend-minus-cycle.html",
            "relUrl": "/2021/01/01/SNP-trend-minus-cycle.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Text summarizer in Python, A Tale of Two Cities",
            "content": "Credit: code from https://github.com/louisteo9/personal-text-summarizer . # Natural Language Tool Kit (NLTK) import nltk nltk.download(&#39;stopwords&#39;) # Regular Expression for text preprocessing import re # Heap (priority) queue algorithm to get the top sentences import heapq # NumPy for numerical computing import numpy as np # pandas for creating DataFrames import pandas as pd # matplotlib for plot from matplotlib import pyplot as plt %matplotlib inline . [nltk_data] Downloading package stopwords to /home/david/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. . Load text data . import requests import re r = requests.get(&quot;https://www.gutenberg.org/files/98/98-0.txt&quot;) raw_text = r.text . print(raw_text[0:1000]) . ï»¿The Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you&#39;ll have to check the laws of the country where you are located before using this ebook. Title: A Tale of Two Cities A Story of the French Revolution Author: Charles Dickens Release Date: January, 1994 [EBook #98] [Most recently updated: December 20, 2020] Language: English Character set encoding: UTF-8 *** START OF THIS PROJECT GUTENBERG EBOOK A TALE OF TWO CITIES *** Produced by Judith Boss, and David Widger A TALE OF TWO CITIES A STORY OF THE FRENCH REVOLUTION By Charles Dickens CONTENTS . # # load text file # with open(&#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt&#39;, &#39;r&#39;) as f: # file_data = f.read() . # text_file = open(&quot;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt&quot;, &quot;r&quot;) # lines = raw_text.readlines() . # lines = raw_text.readlines() . # text_file.close() . # df = pd.read_txt(&#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt&#39;) . # # view text data # print(lines) . Preprocess text . text = raw_text text = re.sub(r&#39; [[0-9]* ]&#39;,&#39; &#39;,text) # replace reference number i.e. [1], [10], [20] with empty space, if any.. text = re.sub(r&#39; s+&#39;,&#39; &#39;,text) # replace one or more spaces with single space #print(text) . Next, we form a clean text with lower case (without special characters, digits and extra spaces) and split it into individual word, for word score computation and formation of the word histogram. . The reason to form a clean text is so that the algorithm won&#39;t treat, i.e. &quot;understanding&quot; and understanding, as two different words. . # generate clean text clean_text = text.lower() # convert all uppercase characters into lowercase characters clean_text = re.sub(r&#39; W&#39;,&#39; &#39;,clean_text) # replace character other than [a-zA-Z0-9] with empty space clean_text = re.sub(r&#39; d&#39;,&#39; &#39;,clean_text) # replace digit with empty space clean_text = re.sub(r&#39; s+&#39;,&#39; &#39;,clean_text) # replace one or more spaces with a single space #print(clean_text) . Split text into sentences . We split (tokenize) the text into sentences using NLTK sent_tokenize() method. We will evaluate the importance of each of sentences, then decide if we should each include in our summary. . # split (tokenize) the sentences sentences = nltk.sent_tokenize(text) #print(sentences) . import nltk nltk.download(&#39;punkt&#39;) from nltk import word_tokenize,sent_tokenize . [nltk_data] Downloading package punkt to /home/david/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. . Remove stop words . Stop words are English words which do not add much meaning to a sentence. They can be safely ignored without sacrificing the meaning of the sentence. We already downloaded a file with English stop words in the first section of the notebook. . Here, we will get the list of stop words and store them in stop_word variable. . # get stop words list stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) print(stop_words) . [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &#34;you&#39;re&#34;, &#34;you&#39;ve&#34;, &#34;you&#39;ll&#34;, &#34;you&#39;d&#34;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;, &#39;himself&#39;, &#39;she&#39;, &#34;she&#39;s&#34;, &#39;her&#39;, &#39;hers&#39;, &#39;herself&#39;, &#39;it&#39;, &#34;it&#39;s&#34;, &#39;its&#39;, &#39;itself&#39;, &#39;they&#39;, &#39;them&#39;, &#39;their&#39;, &#39;theirs&#39;, &#39;themselves&#39;, &#39;what&#39;, &#39;which&#39;, &#39;who&#39;, &#39;whom&#39;, &#39;this&#39;, &#39;that&#39;, &#34;that&#39;ll&#34;, &#39;these&#39;, &#39;those&#39;, &#39;am&#39;, &#39;is&#39;, &#39;are&#39;, &#39;was&#39;, &#39;were&#39;, &#39;be&#39;, &#39;been&#39;, &#39;being&#39;, &#39;have&#39;, &#39;has&#39;, &#39;had&#39;, &#39;having&#39;, &#39;do&#39;, &#39;does&#39;, &#39;did&#39;, &#39;doing&#39;, &#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;and&#39;, &#39;but&#39;, &#39;if&#39;, &#39;or&#39;, &#39;because&#39;, &#39;as&#39;, &#39;until&#39;, &#39;while&#39;, &#39;of&#39;, &#39;at&#39;, &#39;by&#39;, &#39;for&#39;, &#39;with&#39;, &#39;about&#39;, &#39;against&#39;, &#39;between&#39;, &#39;into&#39;, &#39;through&#39;, &#39;during&#39;, &#39;before&#39;, &#39;after&#39;, &#39;above&#39;, &#39;below&#39;, &#39;to&#39;, &#39;from&#39;, &#39;up&#39;, &#39;down&#39;, &#39;in&#39;, &#39;out&#39;, &#39;on&#39;, &#39;off&#39;, &#39;over&#39;, &#39;under&#39;, &#39;again&#39;, &#39;further&#39;, &#39;then&#39;, &#39;once&#39;, &#39;here&#39;, &#39;there&#39;, &#39;when&#39;, &#39;where&#39;, &#39;why&#39;, &#39;how&#39;, &#39;all&#39;, &#39;any&#39;, &#39;both&#39;, &#39;each&#39;, &#39;few&#39;, &#39;more&#39;, &#39;most&#39;, &#39;other&#39;, &#39;some&#39;, &#39;such&#39;, &#39;no&#39;, &#39;nor&#39;, &#39;not&#39;, &#39;only&#39;, &#39;own&#39;, &#39;same&#39;, &#39;so&#39;, &#39;than&#39;, &#39;too&#39;, &#39;very&#39;, &#39;s&#39;, &#39;t&#39;, &#39;can&#39;, &#39;will&#39;, &#39;just&#39;, &#39;don&#39;, &#34;don&#39;t&#34;, &#39;should&#39;, &#34;should&#39;ve&#34;, &#39;now&#39;, &#39;d&#39;, &#39;ll&#39;, &#39;m&#39;, &#39;o&#39;, &#39;re&#39;, &#39;ve&#39;, &#39;y&#39;, &#39;ain&#39;, &#39;aren&#39;, &#34;aren&#39;t&#34;, &#39;couldn&#39;, &#34;couldn&#39;t&#34;, &#39;didn&#39;, &#34;didn&#39;t&#34;, &#39;doesn&#39;, &#34;doesn&#39;t&#34;, &#39;hadn&#39;, &#34;hadn&#39;t&#34;, &#39;hasn&#39;, &#34;hasn&#39;t&#34;, &#39;haven&#39;, &#34;haven&#39;t&#34;, &#39;isn&#39;, &#34;isn&#39;t&#34;, &#39;ma&#39;, &#39;mightn&#39;, &#34;mightn&#39;t&#34;, &#39;mustn&#39;, &#34;mustn&#39;t&#34;, &#39;needn&#39;, &#34;needn&#39;t&#34;, &#39;shan&#39;, &#34;shan&#39;t&#34;, &#39;shouldn&#39;, &#34;shouldn&#39;t&#34;, &#39;wasn&#39;, &#34;wasn&#39;t&#34;, &#39;weren&#39;, &#34;weren&#39;t&#34;, &#39;won&#39;, &#34;won&#39;t&#34;, &#39;wouldn&#39;, &#34;wouldn&#39;t&#34;] . Build word histogram . # create an empty dictionary to house the word count word_count = {} # loop through tokenized words, remove stop words and save word count to dictionary for word in nltk.word_tokenize(clean_text): # remove stop words if word not in stop_words: # save word count to dictionary if word not in word_count.keys(): word_count[word] = 1 else: word_count[word] += 1 . Let&#39;s plot the word histogram and see the results. . # plt.figure(figsize=(16,10)) # plt.xticks(rotation = 90) # plt.bar(word_count.keys(), word_count.values()) # plt.show() . def plot_top_words(word_count_dict, show_top_n=20): word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = &#39;index&#39;).rename(columns={0: &#39;score&#39;}) word_count_table.sort_values(by=&#39;score&#39;).tail(show_top_n).plot(kind=&#39;barh&#39;, figsize=(10,10)) plt.show() . plot_top_words(word_count, 20) . sentence_score = {} # loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score for sentence in sentences: # check if word in sentence is in word_count dictionary for word in nltk.word_tokenize(sentence.lower()): if word in word_count.keys(): # only take sentence that has less than 30 words if len(sentence.split(&#39; &#39;)) &lt; 30: # add word score to sentence score if sentence not in sentence_score.keys(): sentence_score[sentence] = word_count[word] else: sentence_score[sentence] += word_count[word] . df_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = &#39;index&#39;).rename(columns={0: &#39;score&#39;}) df_sentence_score.sort_values(by=&#39;score&#39;, ascending = False) . score . I know this is a confidence,â she modestly said, after a little hesitation, and in earnest tears, âI know you would say this to no one else. 2445 | . Thus engaged, with her right elbow supported by her left hand, Madame Defarge said nothing when her lord came in, but coughed just one grain of cough. 2373 | . Vengeance and retribution require a long time; it is the rule.â âIt does not take a long time to strike a man with Lightning,â said Defarge. 2312 | . âWe have borne this a long time,â said Madame Defarge, turning her eyes again upon Lucie. 2208 | . âYou had better, Lucie,â said Mr. Lorry, doing all he could to propitiate, by tone and manner, âhave the dear child here, and our good Pross. 2197 | . ... ... | . What are you hooroaring at? 1 | . âHim and his hooroars! 1 | . âCome on at a footpace! 1 | . But he was not persuaded. 1 | . âDonât be ungrateful. 1 | . 3832 rows × 1 columns . best_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get) . print(&#39;SUMMARY&#39;) print(&#39;&#39;) for sentence in sentences: if sentence in best_sentences: print (sentence) . SUMMARY Thus engaged, with her right elbow supported by her left hand, Madame Defarge said nothing when her lord came in, but coughed just one grain of cough. I know this is a confidence,â she modestly said, after a little hesitation, and in earnest tears, âI know you would say this to no one else. Vengeance and retribution require a long time; it is the rule.â âIt does not take a long time to strike a man with Lightning,â said Defarge. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/12/31/TextSummarizer-Dickens.html",
            "relUrl": "/2020/12/31/TextSummarizer-Dickens.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Text summarizer in Python, Tieng Viet",
            "content": "Credit: code from https://github.com/louisteo9/personal-text-summarizer . # Natural Language Tool Kit (NLTK) import nltk nltk.download(&#39;stopwords&#39;) # Regular Expression for text preprocessing import re # Heap (priority) queue algorithm to get the top sentences import heapq # NumPy for numerical computing import numpy as np # pandas for creating DataFrames import pandas as pd # matplotlib for plot from matplotlib import pyplot as plt %matplotlib inline . [nltk_data] Downloading package stopwords to /home/david/nltk_data... [nltk_data] Package stopwords is already up-to-date! . Load text data . # load text file with open(&#39;viet_article1.txt&#39;, &#39;r&#39;) as f: file_data = f.read() . # view text data print(file_data) . Thêm 9 người nhập cảnh nhiễm nCoV Ngày 1/1, Bộ Y tế ghi nhận ca dương tính nCoV, đều là người nhập cảnh được cách ly ngay tại Khánh Hòa, Bình Dương và Long An. Tổng ca nhiễm lên 1.474. &#34;Bệnh nhân 1466&#34;, nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên. &#34;Bệnh nhân 1467&#34; nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. &#34;Bệnh nhân 1468&#34;, nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An. Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa. Kết quả xét nghiệm lần một ngày 25/12 âm tính; lấy mẫu lần hai ngày 30/12 kết quả xét nghiệm tại Trung tâm Kiểm soát Bệnh tật tỉnh Khánh Hòa dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Bệnh nhiệt đới tỉnh Khánh Hòa. Trước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV. &#34;Bệnh nhân 1469&#34;, nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng. &#34;Bệnh nhân 1470&#34;, nữ, 40 tuổi, ở huyện Ba Vì, và &#34;bệnh nhân 1471&#34;, nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội. &#34;Bệnh nhân 1472&#34;, nam, 29 tuổi, ở huyện Kim Thành, và &#34;bệnh nhân 1473&#34;, nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương. Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương. &#34;Bệnh nhân 1474&#34;, nam, 50 tuổi, ở Quận 7, TP HCM. Ông từ Canada quá cảnh sân bay Incheon (Hàn Quốc), sau đó nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN405 ngày 29/12, được cách ly tại tỉnh Long An. Lấy mẫu ngày 30/12, kết quả xét nghiệm tại Trung tâm Kiểm soát bệnh tật tỉnh Long An và Viện Pasteur TP HCM đều dương tính với nCoV, bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Long An. Như vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325. Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính. Các bệnh nhân còn lại đa số sức khỏe ổn định, trong đó 7 người xét nghiệm âm tính nCoV lần một, 11 người âm tính lần hai và 10 người âm tính lần ba. Tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000. Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú. Việt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng. Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước. Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép. Bộ Y tế khuyến cáo người dân tiếp tục thực hiện tốt &#34;Thông điệp 5K&#34;, nhất là đeo khẩu trang và rửa tay bằng xà phòng, dung dịch sát khuẩn. Thế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm. Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil. . text = file_data text = re.sub(r&#39; [[0-9]* ]&#39;,&#39; &#39;,text) # replace reference number i.e. [1], [10], [20] with empty space, if any.. text = re.sub(r&#39; s+&#39;,&#39; &#39;,text) # replace one or more spaces with single space print(text) . Thêm 9 người nhập cảnh nhiễm nCoV Ngày 1/1, Bộ Y tế ghi nhận ca dương tính nCoV, đều là người nhập cảnh được cách ly ngay tại Khánh Hòa, Bình Dương và Long An. Tổng ca nhiễm lên 1.474. &#34;Bệnh nhân 1466&#34;, nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên. &#34;Bệnh nhân 1467&#34; nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. &#34;Bệnh nhân 1468&#34;, nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An. Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa. Kết quả xét nghiệm lần một ngày 25/12 âm tính; lấy mẫu lần hai ngày 30/12 kết quả xét nghiệm tại Trung tâm Kiểm soát Bệnh tật tỉnh Khánh Hòa dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Bệnh nhiệt đới tỉnh Khánh Hòa. Trước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV. &#34;Bệnh nhân 1469&#34;, nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng. &#34;Bệnh nhân 1470&#34;, nữ, 40 tuổi, ở huyện Ba Vì, và &#34;bệnh nhân 1471&#34;, nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội. &#34;Bệnh nhân 1472&#34;, nam, 29 tuổi, ở huyện Kim Thành, và &#34;bệnh nhân 1473&#34;, nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương. Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương. &#34;Bệnh nhân 1474&#34;, nam, 50 tuổi, ở Quận 7, TP HCM. Ông từ Canada quá cảnh sân bay Incheon (Hàn Quốc), sau đó nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN405 ngày 29/12, được cách ly tại tỉnh Long An. Lấy mẫu ngày 30/12, kết quả xét nghiệm tại Trung tâm Kiểm soát bệnh tật tỉnh Long An và Viện Pasteur TP HCM đều dương tính với nCoV, bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Long An. Như vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325. Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính. Các bệnh nhân còn lại đa số sức khỏe ổn định, trong đó 7 người xét nghiệm âm tính nCoV lần một, 11 người âm tính lần hai và 10 người âm tính lần ba. Tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000. Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú. Việt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng. Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước. Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép. Bộ Y tế khuyến cáo người dân tiếp tục thực hiện tốt &#34;Thông điệp 5K&#34;, nhất là đeo khẩu trang và rửa tay bằng xà phòng, dung dịch sát khuẩn. Thế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm. Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil. . # generate clean text clean_text = text.lower() # convert all uppercase characters into lowercase characters # replace characters other than [a-zA-Z0-9], digits &amp; one or more spaces with single space regex_patterns = [r&#39; W&#39;,r&#39; d&#39;,r&#39; s+&#39;] for regex in regex_patterns: clean_text = re.sub(regex,&#39; &#39;,clean_text) print(clean_text) . thêm người nhập cảnh nhiễm ncov ngày bộ y tế ghi nhận ca dương tính ncov đều là người nhập cảnh được cách ly ngay tại khánh hòa bình dương và long an tổng ca nhiễm lên bệnh nhân nam tuổi ở tp hưng yên tỉnh hưng yên bệnh nhân nam tuổi ở huyện thanh hà tỉnh hải dương bệnh nhân nam tuổi ở huyện thanh chương tỉnh nghệ an ba người này từ nga nhập cảnh sân bay cam ranh trên chuyến bay vn ngày cách ly tại tỉnh khánh hòa kết quả xét nghiệm lần một ngày âm tính lấy mẫu lần hai ngày kết quả xét nghiệm tại trung tâm kiểm soát bệnh tật tỉnh khánh hòa dương tính với ncov các bệnh nhân điều trị tại bệnh viện bệnh nhiệt đới tỉnh khánh hòa trước đó trên chuyến bay này đã ghi nhận trường hợp dương tính với ncov bệnh nhân nam tuổi ở quận hải châu tp đà nẵng bệnh nhân nữ tuổi ở huyện ba vì và bệnh nhân nam tuổi ở huyện chương mỹ tp hà nội bệnh nhân nam tuổi ở huyện kim thành và bệnh nhân nam tuổi ở huyện thanh hà tỉnh hải dương họ từ đức nhập cảnh sân bay tân sơn nhất trên chuyến bay vn ngày được cách ly tại tỉnh bình dương kết quả xét nghiệm ngày tại viện pasteur tp hcm dương tính với ncov các bệnh nhân điều trị tại bệnh viện đa khoa tỉnh bình dương bệnh nhân nam tuổi ở quận tp hcm ông từ canada quá cảnh sân bay incheon hàn quốc sau đó nhập cảnh sân bay tân sơn nhất trên chuyến bay vn ngày được cách ly tại tỉnh long an lấy mẫu ngày kết quả xét nghiệm tại trung tâm kiểm soát bệnh tật tỉnh long an và viện pasteur tp hcm đều dương tính với ncov bệnh nhân điều trị tại bệnh viện đa khoa tỉnh long an như vậy tổng ca nhiễm lên tổng số khỏi số người tử vong do covid là bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính các bệnh nhân còn lại đa số sức khỏe ổn định trong đó người xét nghiệm âm tính ncov lần một người âm tính lần hai và người âm tính lần ba tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn trong đó cách ly tập trung tại bệnh viện cách ly tập trung hơn còn lại ở nhà hoặc nơi lưu trú việt nam đã ngày không ghi nhận ca nhiễm mới trong cộng đồng song nguy cơ dịch xâm nhập luôn thường trực đặc biệt khi tăng số lượng chuyến bay đưa công dân việt nam chuyên gia về nước nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép bộ y tế khuyến cáo người dân tiếp tục thực hiện tốt thông điệp k nhất là đeo khẩu trang và rửa tay bằng xà phòng dung dịch sát khuẩn thế giới ghi nhận hơn triệu người chết vì ncov trong hơn triệu người nhiễm mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch tiếp theo là ấn độ và brazil . # split (tokenize) the sentences sentences = nltk.sent_tokenize(text) print(sentences) . [&#39;Thêm 9 người nhập cảnh nhiễm nCoV Ngày 1/1, Bộ Y tế ghi nhận ca dương tính nCoV, đều là người nhập cảnh được cách ly ngay tại Khánh Hòa, Bình Dương và Long An.&#39;, &#39;Tổng ca nhiễm lên 1.474.&#39;, &#39;&#34;Bệnh nhân 1466&#34;, nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên.&#39;, &#39;&#34;Bệnh nhân 1467&#34; nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.&#39;, &#39;&#34;Bệnh nhân 1468&#34;, nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An.&#39;, &#39;Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa.&#39;, &#39;Kết quả xét nghiệm lần một ngày 25/12 âm tính; lấy mẫu lần hai ngày 30/12 kết quả xét nghiệm tại Trung tâm Kiểm soát Bệnh tật tỉnh Khánh Hòa dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Bệnh nhiệt đới tỉnh Khánh Hòa.&#39;, &#39;Trước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV.&#39;, &#39;&#34;Bệnh nhân 1469&#34;, nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng.&#39;, &#39;&#34;Bệnh nhân 1470&#34;, nữ, 40 tuổi, ở huyện Ba Vì, và &#34;bệnh nhân 1471&#34;, nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội.&#39;, &#39;&#34;Bệnh nhân 1472&#34;, nam, 29 tuổi, ở huyện Kim Thành, và &#34;bệnh nhân 1473&#34;, nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.&#39;, &#39;Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương.&#39;, &#39;Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương.&#39;, &#39;&#34;Bệnh nhân 1474&#34;, nam, 50 tuổi, ở Quận 7, TP HCM.&#39;, &#39;Ông từ Canada quá cảnh sân bay Incheon (Hàn Quốc), sau đó nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN405 ngày 29/12, được cách ly tại tỉnh Long An.&#39;, &#39;Lấy mẫu ngày 30/12, kết quả xét nghiệm tại Trung tâm Kiểm soát bệnh tật tỉnh Long An và Viện Pasteur TP HCM đều dương tính với nCoV, bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Long An.&#39;, &#39;Như vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325.&#39;, &#39;Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính.&#39;, &#39;Các bệnh nhân còn lại đa số sức khỏe ổn định, trong đó 7 người xét nghiệm âm tính nCoV lần một, 11 người âm tính lần hai và 10 người âm tính lần ba.&#39;, &#39;Tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000.&#39;, &#39;Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú.&#39;, &#39;Việt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng.&#39;, &#39;Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước.&#39;, &#39;Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép.&#39;, &#39;Bộ Y tế khuyến cáo người dân tiếp tục thực hiện tốt &#34;Thông điệp 5K&#34;, nhất là đeo khẩu trang và rửa tay bằng xà phòng, dung dịch sát khuẩn.&#39;, &#39;Thế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm.&#39;, &#39;Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil.&#39;] . # # get stop words list # stop_words = nltk.corpus.stopwords.words(&#39;Vietnamese&#39;) # print(stop_words) . # create an empty dictionary to house the word count word_count = {} # loop through tokenized words, remove stop words and save word count to dictionary for word in nltk.word_tokenize(clean_text): # save word count to dictionary if word not in word_count.keys(): word_count[word] = 1 else: word_count[word] += 1 . plt.figure(figsize=(16,10)) plt.xticks(rotation = 90) plt.bar(word_count.keys(), word_count.values()) plt.show() . # helper function for plotting the top words. def plot_top_words(word_count_dict, show_top_n=20): word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = &#39;index&#39;).rename(columns={0: &#39;score&#39;}) word_count_table.sort_values(by=&#39;score&#39;).tail(show_top_n).plot(kind=&#39;barh&#39;, figsize=(10,10)) plt.show() . plot_top_words(word_count, 20) . # create empty dictionary to house sentence score sentence_score = {} # loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score for sentence in sentences: # check if word in sentence is in word_count dictionary for word in nltk.word_tokenize(sentence.lower()): if word in word_count.keys(): # only take sentence that has less than 30 words if len(sentence.split(&#39; &#39;)) &lt; 30: # add word score to sentence score if sentence not in sentence_score.keys(): sentence_score[sentence] = word_count[word] else: sentence_score[sentence] += word_count[word] . df_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = &#39;index&#39;).rename(columns={0: &#39;score&#39;}) df_sentence_score.sort_values(by=&#39;score&#39;, ascending = False) . score . Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương. 198 | . &quot;Bệnh nhân 1472&quot;, nam, 29 tuổi, ở huyện Kim Thành, và &quot;bệnh nhân 1473&quot;, nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. 177 | . &quot;Bệnh nhân 1470&quot;, nữ, 40 tuổi, ở huyện Ba Vì, và &quot;bệnh nhân 1471&quot;, nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội. 155 | . Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa. 130 | . Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương. 127 | . Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú. 109 | . &quot;Bệnh nhân 1467&quot; nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. 99 | . &quot;Bệnh nhân 1468&quot;, nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An. 91 | . Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính. 89 | . &quot;Bệnh nhân 1466&quot;, nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên. 88 | . Tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000. 87 | . &quot;Bệnh nhân 1469&quot;, nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng. 76 | . Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép. 74 | . &quot;Bệnh nhân 1474&quot;, nam, 50 tuổi, ở Quận 7, TP HCM. 73 | . Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước. 70 | . Trước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV. 69 | . Thế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm. 68 | . Việt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng. 48 | . Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil. 46 | . Như vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325. 27 | . Tổng ca nhiễm lên 1.474. 15 | . # get the best 3 sentences for summary best_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get) . print(&#39;SUMMARY&#39;) print(&#39;&#39;) # display top sentences based on their sentence sequence in the original text for sentence in sentences: if sentence in best_sentences: print (sentence) . SUMMARY &#34;Bệnh nhân 1470&#34;, nữ, 40 tuổi, ở huyện Ba Vì, và &#34;bệnh nhân 1471&#34;, nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội. &#34;Bệnh nhân 1472&#34;, nam, 29 tuổi, ở huyện Kim Thành, và &#34;bệnh nhân 1473&#34;, nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương. . import wordcloud # Wordcloud of training set cloud = np.array(file_data).flatten() plt.figure(figsize=(20,10)) word_cloud = wordcloud.WordCloud( max_words=200,background_color =&quot;black&quot;, width=2000,height=1000,mode=&quot;RGB&quot; ).generate(str(cloud)) plt.axis(&quot;off&quot;) plt.imshow(word_cloud) . &lt;matplotlib.image.AxesImage at 0x7f3cc9e372b0&gt; . import wordcloud # Wordcloud of training set cloud = np.array(best_sentences).flatten() plt.figure(figsize=(20,10)) word_cloud = wordcloud.WordCloud( max_words=200,background_color =&quot;black&quot;, width=2000,height=1000,mode=&quot;RGB&quot; ).generate(str(cloud)) plt.axis(&quot;off&quot;) plt.imshow(word_cloud) . &lt;matplotlib.image.AxesImage at 0x7f3d09c0b190&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/12/30/TextSummarizer-tieng-viet.html",
            "relUrl": "/2020/12/30/TextSummarizer-tieng-viet.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "EDA on Healthcare Data",
            "content": "Credit: code from https://towardsdatascience.com/step-by-step-exploratory-data-analysis-on-stroke-dataset-840aefea8739 and https://www.kaggle.com/lirilkumaramal/heart-stroke . import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 0 30669 | Male | 3.0 | 0 | 0 | No | children | Rural | 95.12 | 18.0 | NaN | 0 | . 1 30468 | Male | 58.0 | 1 | 0 | Yes | Private | Urban | 87.96 | 39.2 | never smoked | 0 | . 2 16523 | Female | 8.0 | 0 | 0 | No | Private | Urban | 110.89 | 17.6 | NaN | 0 | . 3 56543 | Female | 70.0 | 0 | 0 | Yes | Private | Rural | 69.04 | 35.9 | formerly smoked | 0 | . 4 46136 | Male | 14.0 | 0 | 0 | No | Never_worked | Rural | 161.28 | 19.1 | NaN | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 43395 56196 | Female | 10.0 | 0 | 0 | No | children | Urban | 58.64 | 20.4 | never smoked | 0 | . 43396 5450 | Female | 56.0 | 0 | 0 | Yes | Govt_job | Urban | 213.61 | 55.4 | formerly smoked | 0 | . 43397 28375 | Female | 82.0 | 1 | 0 | Yes | Private | Urban | 91.94 | 28.9 | formerly smoked | 0 | . 43398 27973 | Male | 40.0 | 0 | 0 | Yes | Private | Urban | 99.16 | 33.2 | never smoked | 0 | . 43399 36271 | Female | 82.0 | 0 | 0 | Yes | Private | Urban | 79.48 | 20.6 | never smoked | 0 | . 43400 rows × 12 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 43400 entries, 0 to 43399 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 id 43400 non-null int64 1 gender 43400 non-null object 2 age 43400 non-null float64 3 hypertension 43400 non-null int64 4 heart_disease 43400 non-null int64 5 ever_married 43400 non-null object 6 work_type 43400 non-null object 7 Residence_type 43400 non-null object 8 avg_glucose_level 43400 non-null float64 9 bmi 41938 non-null float64 10 smoking_status 30108 non-null object 11 stroke 43400 non-null int64 dtypes: float64(3), int64(4), object(5) memory usage: 4.0+ MB . df[&#39;stroke&#39;].value_counts() . 0 42617 1 783 Name: stroke, dtype: int64 . # labeled target is unbalanced . # Drop the id column df.drop(columns=[&#39;id&#39;], inplace=True) . # Showing records where patient suffered from stroke but had missing value in bmi attribute. df[df[&#39;bmi&#39;].isna() &amp; df[&#39;stroke&#39;] == 1] . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 81 Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | NaN | never smoked | 1 | . 407 Female | 59.0 | 0 | 0 | Yes | Private | Rural | 76.15 | NaN | NaN | 1 | . 747 Male | 78.0 | 0 | 1 | Yes | Private | Urban | 219.84 | NaN | NaN | 1 | . 1139 Male | 57.0 | 0 | 1 | No | Govt_job | Urban | 217.08 | NaN | NaN | 1 | . 1613 Male | 58.0 | 0 | 0 | Yes | Private | Rural | 189.84 | NaN | NaN | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 42530 Male | 66.0 | 0 | 0 | Yes | Self-employed | Urban | 182.89 | NaN | never smoked | 1 | . 42839 Female | 67.0 | 1 | 0 | Yes | Govt_job | Urban | 234.43 | NaN | never smoked | 1 | . 43007 Female | 69.0 | 0 | 1 | Yes | Self-employed | Rural | 89.19 | NaN | smokes | 1 | . 43100 Male | 67.0 | 0 | 0 | Yes | Self-employed | Urban | 136.79 | NaN | smokes | 1 | . 43339 Female | 76.0 | 0 | 0 | No | Private | Rural | 100.55 | NaN | never smoked | 1 | . 140 rows × 11 columns . # Replace the missing values with mean of bmi attribute df[&#39;bmi&#39;].fillna(np.round(df[&#39;bmi&#39;].mean(), 1), inplace = True) . # Create a new category named &#39;not known&#39; df[&#39;smoking_status&#39;].fillna(&#39;not known&#39;, inplace=True) print(df[&#39;smoking_status&#39;].value_counts()) . never smoked 16053 not known 13292 formerly smoked 7493 smokes 6562 Name: smoking_status, dtype: int64 . # Discretize with respective equal-width bin df[&#39;age_binned&#39;] = pd.cut(df[&#39;age&#39;], np.arange(0, 91, 5)) df[&#39;avg_glucose_level_binned&#39;] = pd.cut(df[&#39;avg_glucose_level&#39;], np.arange(0, 301, 10)) df[&#39;bmi_binned&#39;] = pd.cut(df[&#39;bmi&#39;], np.arange(0, 101, 5)) . &lt;ipython-input-23-b4c4cb89b1bd&gt;:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df[&#39;age_binned&#39;] = pd.cut(df[&#39;age&#39;], np.arange(0, 91, 5)) &lt;ipython-input-23-b4c4cb89b1bd&gt;:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df[&#39;avg_glucose_level_binned&#39;] = pd.cut(df[&#39;avg_glucose_level&#39;], np.arange(0, 301, 10)) &lt;ipython-input-23-b4c4cb89b1bd&gt;:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df[&#39;bmi_binned&#39;] = pd.cut(df[&#39;bmi&#39;], np.arange(0, 101, 5)) . # Create the correlation heatmap heatmap = sns.heatmap(df[[&#39;age_norm&#39;, &#39;avg_glucose_level_norm&#39;, &#39;bmi_norm&#39;]].corr(), vmin=-1, vmax=1, annot=True) # Create the title heatmap.set_title(&#39;Correlation Heatmap&#39;); . def get_stacked_bar_chart(column): # Get the count of records by column and stroke df_pct = df.groupby([column, &#39;stroke&#39;])[&#39;age&#39;].count() # Create proper DataFrame&#39;s format df_pct = df_pct.unstack() return df_pct.plot.bar(stacked=True, figsize=(6,6), width=1); . def get_100_percent_stacked_bar_chart(column, width = 0.5): # Get the count of records by column and stroke df_breakdown = df.groupby([column, &#39;stroke&#39;])[&#39;age&#39;].count() # Get the count of records by gender df_total = df.groupby([column])[&#39;age&#39;].count() # Get the percentage for 100% stacked bar chart df_pct = df_breakdown / df_total * 100 # Create proper DataFrame&#39;s format df_pct = df_pct.unstack() return df_pct.plot.bar(stacked=True, figsize=(6,6), width=width); . # Age related to risk get_stacked_bar_chart(&#39;age_binned&#39;) . &lt;AxesSubplot:xlabel=&#39;age_binned&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;age_binned&#39;, width = 0.9) . &lt;AxesSubplot:xlabel=&#39;age_binned&#39;&gt; . get_stacked_bar_chart(&#39;bmi_binned&#39;) get_100_percent_stacked_bar_chart(&#39;bmi_binned&#39;, width = 0.9) . &lt;AxesSubplot:xlabel=&#39;bmi_binned&#39;&gt; . get_stacked_bar_chart(&#39;avg_glucose_level_binned&#39;) get_100_percent_stacked_bar_chart(&#39;avg_glucose_level_binned&#39;, width = 0.9) . &lt;AxesSubplot:xlabel=&#39;avg_glucose_level_binned&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;hypertension&#39;) get_100_percent_stacked_bar_chart(&#39;heart_disease&#39;) . &lt;AxesSubplot:xlabel=&#39;heart_disease&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;gender&#39;) get_100_percent_stacked_bar_chart(&#39;Residence_type&#39;) . &lt;AxesSubplot:xlabel=&#39;Residence_type&#39;&gt; . get_100_percent_stacked_bar_chart(&#39;work_type&#39;) df.groupby([&#39;work_type&#39;])[[&#39;age&#39;]].agg([&#39;count&#39;, &#39;mean&#39;]) . age . count mean . work_type . Govt_job 5438 | 49.098750 | . Never_worked 177 | 17.757062 | . Private 24827 | 45.016837 | . Self-employed 6793 | 59.307817 | . children 6154 | 6.698018 | . get_100_percent_stacked_bar_chart(&#39;ever_married&#39;) df.groupby([&#39;ever_married&#39;])[[&#39;age&#39;]].agg([&#39;count&#39;, &#39;mean&#39;]) . age . count mean . ever_married . No 15456 | 21.237487 | . Yes 27933 | 53.829735 | . g = sns.catplot(x=&quot;Residence_type&quot;, hue=&quot;smoking_status&quot;, col=&quot;work_type&quot;, data=df, kind=&quot;count&quot;, height=4, aspect=.7) . missingno.matrix(df, figsize = (30,5)) . NameError Traceback (most recent call last) &lt;ipython-input-38-4ca028d03d7b&gt; in &lt;module&gt; -&gt; 1 missingno.matrix(df, figsize = (30,5)) NameError: name &#39;missingno&#39; is not defined . fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,7)) fig.suptitle(&quot;Countplot for the dataset&quot;, fontsize=35) sns.countplot(x=&quot;gender&quot;, data=df,ax=ax1) sns.countplot(x=&quot;stroke&quot;, data=df,ax=ax2) sns.countplot(x=&quot;ever_married&quot;, data=df,ax=ax3) sns.countplot(x=&quot;hypertension&quot;, data=df,ax=ax4) . &lt;AxesSubplot:xlabel=&#39;hypertension&#39;, ylabel=&#39;count&#39;&gt; . sns.displot(x=&quot;age&quot;, data=df, kind=&quot;kde&quot;, hue=&quot;gender&quot;, col=&quot;smoking_status&quot;, row=&quot;Residence_type&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fe05aa233d0&gt; . fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,7)) fig.suptitle(&quot;Boxplot for Dataset&quot;, fontsize=35) sns.boxplot(x=&quot;stroke&quot;, y=&quot;avg_glucose_level&quot;, data=df,ax=ax1) sns.boxplot(x=&quot;stroke&quot;, y=&quot;bmi&quot;, data=df,ax=ax2) sns.boxplot(x=&quot;stroke&quot;, y=&quot;age&quot;, data=df,ax=ax3) . &lt;AxesSubplot:xlabel=&#39;stroke&#39;, ylabel=&#39;age&#39;&gt; . # Compute a correlation matrix and convert to long-form corr_mat = df.corr(&quot;kendall&quot;).stack().reset_index(name=&quot;correlation&quot;) # Draw each cell as a scatter point with varying size and color g = sns.relplot( data=corr_mat, x=&quot;level_0&quot;, y=&quot;level_1&quot;, hue=&quot;correlation&quot;, size=&quot;correlation&quot;, palette=&quot;vlag&quot;, hue_norm=(-1, 1), edgecolor=&quot;.7&quot;, height=5, sizes=(50, 250), size_norm=(-.2, .8), ) # Tweak the figure to finalize g.set(xlabel=&quot;&quot;, ylabel=&quot;&quot;, aspect=&quot;equal&quot;) g.despine(left=True, bottom=True) g.ax.margins(0.25) for label in g.ax.get_xticklabels(): label.set_rotation(90) for artist in g.legend.legendHandles: artist.set_edgecolor(&quot;.1&quot;) . strokes_temp_df=df strokes_temp_df[[&#39;stroke&#39;,&#39;hypertension&#39;]] = df[[&#39;stroke&#39;,&#39;hypertension&#39;]].astype(&#39;int&#39;) corr = strokes_temp_df.corr() corr.style.background_gradient() corr.style.background_gradient().set_precision(2) . age hypertension heart_disease avg_glucose_level bmi stroke age_norm avg_glucose_level_norm bmi_norm . age 1.00 | 0.27 | 0.25 | 0.24 | 0.35 | 0.16 | 1.00 | 0.24 | 0.35 | . hypertension 0.27 | 1.00 | 0.12 | 0.16 | 0.15 | 0.08 | 0.27 | 0.16 | 0.15 | . heart_disease 0.25 | 0.12 | 1.00 | 0.15 | 0.05 | 0.11 | 0.25 | 0.15 | 0.05 | . avg_glucose_level 0.24 | 0.16 | 0.15 | 1.00 | 0.18 | 0.08 | 0.24 | 1.00 | 0.18 | . bmi 0.35 | 0.15 | 0.05 | 0.18 | 1.00 | 0.02 | 0.35 | 0.18 | 1.00 | . stroke 0.16 | 0.08 | 0.11 | 0.08 | 0.02 | 1.00 | 0.16 | 0.08 | 0.02 | . age_norm 1.00 | 0.27 | 0.25 | 0.24 | 0.35 | 0.16 | 1.00 | 0.24 | 0.35 | . avg_glucose_level_norm 0.24 | 0.16 | 0.15 | 1.00 | 0.18 | 0.08 | 0.24 | 1.00 | 0.18 | . bmi_norm 0.35 | 0.15 | 0.05 | 0.18 | 1.00 | 0.02 | 0.35 | 0.18 | 1.00 | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/12/09/Health-Data-EDA.html",
            "relUrl": "/2020/12/09/Health-Data-EDA.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Data Science Research Areas",
            "content": "Research Areas: . Big Data . ‘Big data refers to data that would typically be too expensive to store, manage, and analyze using traditional (relational and/or monolithic) database systems. Usually, such systems are cost-inefficient because of their inflexibility for storing unstructured data (such as images, text, and video), accommodating “high-velocity” (real-time) data, or scaling to support very large (petabyte-scale) data volumes.’ . ‘Data analytics only returns more value when you have access to more data, so organizations across multiple industries have found big data to be a rich resource for uncovering profound business insights. And, because machine-learning models get more efficient as they are “trained” with more data, machine learning and big data are highly complementary.’ https://cloud.google.com/what-is-big-data . four challenges have to be considered: Volume (the sheer amount of data — the year 2025 will feature eight times more data than in 2017[2]) Velocity (the speed with which data is generated and processed — e.g. streaming, IOT, social media) Variety (structured and increasingly unstructured data) Veracity (lack of data quality and missing know-how for evaluation) Typical characteristics of Big Data (Storage) Technologies are: Distributed Storage Data Replication Local Data Processing High Availability Data Partitioning Denormalized Data Working with Structured and unstructured data Big Data: when the data amount reaches terabytes/petabytes or traditional systems are no longer powerful enough and are also significantly more expensive when working with this kind of data amounts. Even for analytics: if there is any single guarantee, it’s that your data will grow over time--probably, exponentially. For ML products: Another point is that the emerging field of deep/machine learning becomes more and more efficient by training with more data. Therefore, the area is a perfect addition to Big Data https://towardsdatascience.com/what-big-data-actually-means-d4b00e8ae00 . RCTs and Statistical Evaluation .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20research%20areas/2020/11/22/research-areas.html",
            "relUrl": "/data%20science%20research%20areas/2020/11/22/research-areas.html",
            "date": " • Nov 22, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Dask for Predicting Onset/Diagnosis of Chronic Conditions, Diabetes",
            "content": "categories: [Big Data] | . Credit: http://matthewrocklin.com/blog/work/2017/03/28/dask-xgboost, https://examples.dask.org/applications/forecasting-with-prophet.html . import dask from dask.distributed import Client, progress . from dask.distributed import Client client = Client(n_workers=4) client . Client . Scheduler: tcp://127.0.0.1:41309 | Dashboard: http://127.0.0.1:8787/status | . | Cluster . Workers: 4 | Cores: 8 | Memory: 16.57 GB | . | . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 763 10 | 101 | 76 | 48 | 180 | 32.9 | 0.171 | 63 | 0 | . 764 2 | 122 | 70 | 27 | 0 | 36.8 | 0.340 | 27 | 0 | . 765 5 | 121 | 72 | 23 | 112 | 26.2 | 0.245 | 30 | 0 | . 766 1 | 126 | 60 | 0 | 0 | 30.1 | 0.349 | 47 | 1 | . 767 1 | 93 | 70 | 31 | 0 | 30.4 | 0.315 | 23 | 0 | . 768 rows × 9 columns . from dask import dataframe as dd ddf = dd.from_pandas(df, npartitions=5) ddf . Dask DataFrame Structure: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . npartitions=5 . 0 int64 | int64 | int64 | int64 | int64 | float64 | float64 | int64 | int64 | . 154 ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 616 ... | ... | ... | ... | ... | ... | ... | ... | ... | . 767 ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks import dask.dataframe as dd # Subset of the columns to use cols = [&#39;Pregnancies&#39;, &#39;Glucose&#39;, &#39;BloodPressure&#39;, &#39;SkinThickness&#39;, &#39;Insulin&#39;, &#39;BMI&#39;, &#39;DiabetesPedigreeFunction&#39;, &#39;Age&#39;] . ddf2 = ddf.sample(frac=0.2) # XGBoost requires a bit of RAM, we need a larger cluster . ddf2 . Dask DataFrame Structure: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . npartitions=5 . 0 int64 | int64 | int64 | int64 | int64 | float64 | float64 | int64 | int64 | . 154 ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 616 ... | ... | ... | ... | ... | ... | ... | ... | ... | . 767 ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: sample, 10 tasks diab_diag = (ddf.Outcome) # column of labels . del ddf[&#39;Outcome&#39;] # Remove delay information from training dataframe . ddf, diab_diag = dask.persist(ddf, diab_diag) # start work in the background . diab_diag.head() . 0 1 1 0 2 1 3 0 4 1 Name: Outcome, dtype: int64 . df2 = dd.get_dummies(ddf.categorize()).persist() . len(df2.columns) . 8 . data_train, data_test = df2.random_split([0.9, 0.1], random_state=1234) labels_train, labels_test = diab_diag.random_split([0.9, 0.1], random_state=1234) . %%time import dask_xgboost as dxgb params = {&#39;objective&#39;: &#39;binary:logistic&#39;, &#39;nround&#39;: 1000, &#39;max_depth&#39;: 16, &#39;eta&#39;: 0.01, &#39;subsample&#39;: 0.5, &#39;min_child_weight&#39;: 1, &#39;tree_method&#39;: &#39;hist&#39;, &#39;grow_policy&#39;: &#39;lossguide&#39;} bst = dxgb.train(client, params, data_train, labels_train) . CPU times: user 1.23 s, sys: 607 ms, total: 1.84 s Wall time: 3.56 s . bst . &lt;xgboost.core.Booster at 0x7fde80f05f98&gt; . import xgboost as xgb pandas_df = data_test.head() dtest = xgb.DMatrix(pandas_df) . bst.predict(dtest) . array([0.52612805, 0.51560616, 0.47321838, 0.5084377 , 0.45707062], dtype=float32) . predictions = dxgb.predict(client, bst, data_test).persist() . predictions . | Array Chunk . Bytes unknown | unknown | . Shape (nan,) | (nan,) | . Count 5 Tasks | 5 Chunks | . Type float32 | numpy.ndarray | . | | . from sklearn.metrics import roc_auc_score, roc_curve . print(roc_auc_score(labels_test.compute(), predictions.compute())) . 0.7775157232704403 . import matplotlib.pyplot as plt fpr, tpr, _ = roc_curve(labels_test.compute(), predictions.compute()) # Taken from # http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py plt.figure(figsize=(8, 8)) lw = 2 plt.plot(fpr, tpr, color=&#39;darkorange&#39;, lw=lw, label=&#39;ROC curve&#39;) plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;Receiver operating characteristic example&#39;) plt.legend(loc=&quot;lower right&quot;) plt.show() . import dask import xgboost import dask_xgboost . %matplotlib inline import matplotlib.pyplot as plt ax = xgboost.plot_importance(bst, height=0.8, max_num_features=9) ax.grid(False, axis=&quot;y&quot;) ax.set_title(&#39;Estimated feature importance&#39;) plt.show() . y_hat = dask_xgboost.predict(client, bst, data_test).persist() y_hat . | Array Chunk . Bytes unknown | unknown | . Shape (nan,) | (nan,) | . Count 5 Tasks | 5 Chunks | . Type float32 | numpy.ndarray | . | | . from sklearn.metrics import roc_curve labels_test, y_hat = dask.compute(labels_test, y_hat) fpr, tpr, _ = roc_curve(labels_test, y_hat) . from sklearn.metrics import auc fig, ax = plt.subplots(figsize=(5, 5)) ax.plot(fpr, tpr, lw=3, label=&#39;ROC Curve (area = {:.2f})&#39;.format(auc(fpr, tpr))) ax.plot([0, 1], [0, 1], &#39;k--&#39;, lw=2) ax.set( xlim=(0, 1), ylim=(0, 1), title=&quot;ROC Curve&quot;, xlabel=&quot;False Positive Rate&quot;, ylabel=&quot;True Positive Rate&quot;, ) ax.legend(); plt.show() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/20/Dask-Diab.html",
            "relUrl": "/2020/11/20/Dask-Diab.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Text summarizer in Python, Notes from underground",
            "content": "Credit: code from https://github.com/louisteo9/personal-text-summarizer . # Natural Language Tool Kit (NLTK) import nltk nltk.download(&#39;stopwords&#39;) # Regular Expression for text preprocessing import re # Heap (priority) queue algorithm to get the top sentences import heapq # NumPy for numerical computing import numpy as np # pandas for creating DataFrames import pandas as pd # matplotlib for plot from matplotlib import pyplot as plt %matplotlib inline . [nltk_data] Downloading package stopwords to /home/gao/nltk_data... [nltk_data] Package stopwords is already up-to-date! . Load text data . import requests import re r = requests.get(&quot;https://www.gutenberg.org/cache/epub/600/pg600.txt&quot;) raw_text = r.text #print(raw_text[0:1000]) . # # load text file # with open(&#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt&#39;, &#39;r&#39;) as f: # file_data = f.read() . # text_file = open(&quot;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt&quot;, &quot;r&quot;) # lines = raw_text.readlines() . # lines = raw_text.readlines() . # text_file.close() . # df = pd.read_txt(&#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt&#39;) . Let&#39;s take a look at the text. . # view text data print(lines) . [&#39;In an attempt to scale up its AI portfolio, Apple has acquired Spain-based AI video startup Vilynx for approximately $50 million. n&#39;, &#39; n&#39;, &#39;Reported by Bloomberg, the AI startup Vilynx is headquartered in Barcelona, which is known to build software using computer vision to analyse a video�s visual, text, and audio content with the goal of �understanding� what�s in the video. This helps it categorising and tagging metadata to the videos, as well as generate automated video previews, and recommend related content to users, according to the company website. n&#39;, &#39; n&#39;, &#39;Apple told the media that the company typically acquires smaller technology companies from time to time, and with the recent buy, the company could potentially use Vilynx�s technology to help improve a variety of apps. According to the media, Siri, search, Photos, and other apps that rely on Apple are possible candidates as are Apple TV, Music, News, to name a few that are going to be revolutionised with Vilynx�s technology. n&#39;, &#39; n&#39;, &#39;With CEO Tim Cooks vision of the potential of augmented reality, the company could also make use of AI-based tools like Vilynx. n&#39;, &#39; n&#39;, &#39;The purchase will also advance Apples AI expertise, adding up to 50 engineers and data scientists joining from Vilynx, and the startup is going to become one of Apple�s key AI research hubs in Europe, according to the news. n&#39;, &#39; n&#39;, &#39;Apple has made significant progress in the space of artificial intelligence over the past few months, with this purchase of UK-based Spectral Edge last December, Seattle-based Xnor.ai for $200 million and Voysis and Inductiv to help it improve Siri. With its habit of quietly purchasing smaller companies, Apple is making a mark in the AI space. In 2018, CEO Tim Cook said in an interview that the company had bought 20 companies over six months, while only six were public knowledge.&#39;] . Preprocess text . We use regular expression to do text preprocessing. We will: . replace reference number with empty space, if any... | replace one or more spaces with single space. | text = raw_text text = re.sub(r&#39; [[0-9]* ]&#39;,&#39; &#39;,text) # replace reference number i.e. [1], [10], [20] with empty space, if any.. text = re.sub(r&#39; s+&#39;,&#39; &#39;,text) # replace one or more spaces with single space print(text) . ﻿Project Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: Notes from the Underground Author: Feodor Dostoevsky Posting Date: September 13, 2008 [EBook #600] Release Date: July, 1996 Language: English *** START OF THIS PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Produced by Judith Boss. HTML version by Al Haines. Notes from the Underground FYODOR DOSTOYEVSKY PART I Underground* *The author of the diary and the diary itself are, of course, imaginary. Nevertheless it is clear that such persons as the writer of these notes not only may, but positively must, exist in our society, when we consider the circumstances in the midst of which our society is formed. I have tried to expose to the view of the public more distinctly than is commonly done, one of the characters of the recent past. He is one of the representatives of a generation still living. In this fragment, entitled &#34;Underground,&#34; this person introduces himself and his views, and, as it were, tries to explain the causes owing to which he has made his appearance and was bound to make his appearance in our midst. In the second fragment there are added the actual notes of this person concerning certain events in his life.--AUTHOR&#39;S NOTE. I I am a sick man.... I am a spiteful man. I am an unattractive man. I believe my liver is diseased. However, I know nothing at all about my disease, and do not know for certain what ails me. I don&#39;t consult a doctor for it, and never have, though I have a respect for medicine and doctors. Besides, I am extremely superstitious, sufficiently so to respect medicine, anyway (I am well-educated enough not to be superstitious, but I am superstitious). No, I refuse to consult a doctor from spite. That you probably will not understand. Well, I understand it, though. Of course, I can&#39;t explain who it is precisely that I am mortifying in this case by my spite: I am perfectly well aware that I cannot &#34;pay out&#34; the doctors by not consulting them; I know better than anyone that by all this I am only injuring myself and no one else. But still, if I don&#39;t consult a doctor it is from spite. My liver is bad, well--let it get worse! I have been going on like that for a long time--twenty years. Now I am forty. I used to be in the government service, but am no longer. I was a spiteful official. I was rude and took pleasure in being so. I did not take bribes, you see, so I was bound to find a recompense in that, at least. (A poor jest, but I will not scratch it out. I wrote it thinking it would sound very witty; but now that I have seen myself that I only wanted to show off in a despicable way, I will not scratch it out on purpose!) When petitioners used to come for information to the table at which I sat, I used to grind my teeth at them, and felt intense enjoyment when I succeeded in making anybody unhappy. I almost did succeed. For the most part they were all timid people--of course, they were petitioners. But of the uppish ones there was one officer in particular I could not endure. He simply would not be humble, and clanked his sword in a disgusting way. I carried on a feud with him for eighteen months over that sword. At last I got the better of him. He left off clanking it. That happened in my youth, though. But do you know, gentlemen, what was the chief point about my spite? Why, the whole point, the real sting of it lay in the fact that continually, even in the moment of the acutest spleen, I was inwardly conscious with shame that I was not only not a spiteful but not even an embittered man, that I was simply scaring sparrows at random and amusing myself by it. I might foam at the mouth, but bring me a doll to play with, give me a cup of tea with sugar in it, and maybe I should be appeased. I might even be genuinely touched, though probably I should grind my teeth at myself afterwards and lie awake at night with shame for months after. That was my way. I was lying when I said just now that I was a spiteful official. I was lying from spite. I was simply amusing myself with the petitioners and with the officer, and in reality I never could become spiteful. I was conscious every moment in myself of many, very many elements absolutely opposite to that. I felt them positively swarming in me, these opposite elements. I knew that they had been swarming in me all my life and craving some outlet from me, but I would not let them, would not let them, purposely would not let them come out. They tormented me till I was ashamed: they drove me to convulsions and--sickened me, at last, how they sickened me! Now, are not you fancying, gentlemen, that I am expressing remorse for something now, that I am asking your forgiveness for something? I am sure you are fancying that ... However, I assure you I do not care if you are.... It was not only that I could not become spiteful, I did not know how to become anything; neither spiteful nor kind, neither a rascal nor an honest man, neither a hero nor an insect. Now, I am living out my life in my corner, taunting myself with the spiteful and useless consolation that an intelligent man cannot become anything seriously, and it is only the fool who becomes anything. Yes, a man in the nineteenth century must and morally ought to be pre-eminently a characterless creature; a man of character, an active man is pre-eminently a limited creature. That is my conviction of forty years. I am forty years old now, and you know forty years is a whole lifetime; you know it is extreme old age. To live longer than forty years is bad manners, is vulgar, immoral. Who does live beyond forty? Answer that, sincerely and honestly I will tell you who do: fools and worthless fellows. I tell all old men that to their face, all these venerable old men, all these silver-haired and reverend seniors! I tell the whole world that to its face! I have a right to say so, for I shall go on living to sixty myself. To seventy! To eighty! ... Stay, let me take breath ... You imagine no doubt, gentlemen, that I want to amuse you. You are mistaken in that, too. I am by no means such a mirthful person as you imagine, or as you may imagine; however, irritated by all this babble (and I feel that you are irritated) you think fit to ask me who I am--then my answer is, I am a collegiate assessor. I was in the service that I might have something to eat (and solely for that reason), and when last year a distant relation left me six thousand roubles in his will I immediately retired from the service and settled down in my corner. I used to live in this corner before, but now I have settled down in it. My room is a wretched, horrid one in the outskirts of the town. My servant is an old country-woman, ill-natured from stupidity, and, moreover, there is always a nasty smell about her. I am told that the Petersburg climate is bad for me, and that with my small means it is very expensive to live in Petersburg. I know all that better than all these sage and experienced counsellors and monitors.... But I am remaining in Petersburg; I am not going away from Petersburg! I am not going away because ... ech! Why, it is absolutely no matter whether I am going away or not going away. But what can a decent man speak of with most pleasure? Answer: Of himself. Well, so I will talk about myself. II I want now to tell you, gentlemen, whether you care to hear it or not, why I could not even become an insect. I tell you solemnly, that I have many times tried to become an insect. But I was not equal even to that. I swear, gentlemen, that to be too conscious is an illness--a real thorough-going illness. For man&#39;s everyday needs, it would have been quite enough to have the ordinary human consciousness, that is, half or a quarter of the amount which falls to the lot of a cultivated man of our unhappy nineteenth century, especially one who has the fatal ill-luck to inhabit Petersburg, the most theoretical and intentional town on the whole terrestrial globe. (There are intentional and unintentional towns.) It would have been quite enough, for instance, to have the consciousness by which all so-called direct persons and men of action live. I bet you think I am writing all this from affectation, to be witty at the expense of men of action; and what is more, that from ill-bred affectation, I am clanking a sword like my officer. But, gentlemen, whoever can pride himself on his diseases and even swagger over them? Though, after all, everyone does do that; people do pride themselves on their diseases, and I do, may be, more than anyone. We will not dispute it; my contention was absurd. But yet I am firmly persuaded that a great deal of consciousness, every sort of consciousness, in fact, is a disease. I stick to that. Let us leave that, too, for a minute. Tell me this: why does it happen that at the very, yes, at the very moments when I am most capable of feeling every refinement of all that is &#34;sublime and beautiful,&#34; as they used to say at one time, it would, as though of design, happen to me not only to feel but to do such ugly things, such that ... Well, in short, actions that all, perhaps, commit; but which, as though purposely, occurred to me at the very time when I was most conscious that they ought not to be committed. The more conscious I was of goodness and of all that was &#34;sublime and beautiful,&#34; the more deeply I sank into my mire and the more ready I was to sink in it altogether. But the chief point was that all this was, as it were, not accidental in me, but as though it were bound to be so. It was as though it were my most normal condition, and not in the least disease or depravity, so that at last all desire in me to struggle against this depravity passed. It ended by my almost believing (perhaps actually believing) that this was perhaps my normal condition. But at first, in the beginning, what agonies I endured in that struggle! I did not believe it was the same with other people, and all my life I hid this fact about myself as a secret. I was ashamed (even now, perhaps, I am ashamed): I got to the point of feeling a sort of secret abnormal, despicable enjoyment in returning home to my corner on some disgusting Petersburg night, acutely conscious that that day I had committed a loathsome action again, that what was done could never be undone, and secretly, inwardly gnawing, gnawing at myself for it, tearing and consuming myself till at last the bitterness turned into a sort of shameful accursed sweetness, and at last--into positive real enjoyment! Yes, into enjoyment, into enjoyment! I insist upon that. I have spoken of this because I keep wanting to know for a fact whether other people feel such enjoyment? I will explain; the enjoyment was just from the too intense consciousness of one&#39;s own degradation; it was from feeling oneself that one had reached the last barrier, that it was horrible, but that it could not be otherwise; that there was no escape for you; that you never could become a different man; that even if time and faith were still left you to change into something different you would most likely not wish to change; or if you did wish to, even then you would do nothing; because perhaps in reality there was nothing for you to change into. And the worst of it was, and the root of it all, that it was all in accord with the normal fundamental laws of over-acute consciousness, and with the inertia that was the direct result of those laws, and that consequently one was not only unable to change but could do absolutely nothing. Thus it would follow, as the result of acute consciousness, that one is not to blame in being a scoundrel; as though that were any consolation to the scoundrel once he has come to realise that he actually is a scoundrel. But enough.... Ech, I have talked a lot of nonsense, but what have I explained? How is enjoyment in this to be explained? But I will explain it. I will get to the bottom of it! That is why I have taken up my pen.... I, for instance, have a great deal of AMOUR PROPRE. I am as suspicious and prone to take offence as a humpback or a dwarf. But upon my word I sometimes have had moments when if I had happened to be slapped in the face I should, perhaps, have been positively glad of it. I say, in earnest, that I should probably have been able to discover even in that a peculiar sort of enjoyment--the enjoyment, of course, of despair; but in despair there are the most intense enjoyments, especially when one is very acutely conscious of the hopelessness of one&#39;s position. And when one is slapped in the face--why then the consciousness of being rubbed into a pulp would positively overwhelm one. The worst of it is, look at it which way one will, it still turns out that I was always the most to blame in everything. And what is most humiliating of all, to blame for no fault of my own but, so to say, through the laws of nature. In the first place, to blame because I am cleverer than any of the people surrounding me. (I have always considered myself cleverer than any of the people surrounding me, and sometimes, would you believe it, have been positively ashamed of it. At any rate, I have all my life, as it were, turned my eyes away and never could look people straight in the face.) To blame, finally, because even if I had had magnanimity, I should only have had more suffering from the sense of its uselessness. I should certainly have never been able to do anything from being magnanimous--neither to forgive, for my assailant would perhaps have slapped me from the laws of nature, and one cannot forgive the laws of nature; nor to forget, for even if it were owing to the laws of nature, it is insulting all the same. Finally, even if I had wanted to be anything but magnanimous, had desired on the contrary to revenge myself on my assailant, I could not have revenged myself on any one for anything because I should certainly never have made up my mind to do anything, even if I had been able to. Why should I not have made up my mind? About that in particular I want to say a few words. III With people who know how to revenge themselves and to stand up for themselves in general, how is it done? Why, when they are possessed, let us suppose, by the feeling of revenge, then for the time there is nothing else but that feeling left in their whole being. Such a gentleman simply dashes straight for his object like an infuriated bull with its horns down, and nothing but a wall will stop him. (By the way: facing the wall, such gentlemen--that is, the &#34;direct&#34; persons and men of action--are genuinely nonplussed. For them a wall is not an evasion, as for us people who think and consequently do nothing; it is not an excuse for turning aside, an excuse for which we are always very glad, though we scarcely believe in it ourselves, as a rule. No, they are nonplussed in all sincerity. The wall has for them something tranquillising, morally soothing, final--maybe even something mysterious ... but of the wall later.) Well, such a direct person I regard as the real normal man, as his tender mother nature wished to see him when she graciously brought him into being on the earth. I envy such a man till I am green in the face. He is stupid. I am not disputing that, but perhaps the normal man should be stupid, how do you know? Perhaps it is very beautiful, in fact. And I am the more persuaded of that suspicion, if one can call it so, by the fact that if you take, for instance, the antithesis of the normal man, that is, the man of acute consciousness, who has come, of course, not out of the lap of nature but out of a retort (this is almost mysticism, gentlemen, but I suspect this, too), this retort-made man is sometimes so nonplussed in the presence of his antithesis that with all his exaggerated consciousness he genuinely thinks of himself as a mouse and not a man. It may be an acutely conscious mouse, yet it is a mouse, while the other is a man, and therefore, et caetera, et caetera. And the worst of it is, he himself, his very own self, looks on himself as a mouse; no one asks him to do so; and that is an important point. Now let us look at this mouse in action. Let us suppose, for instance, that it feels insulted, too (and it almost always does feel insulted), and wants to revenge itself, too. There may even be a greater accumulation of spite in it than in L&#39;HOMME DE LA NATURE ET DE LA VERITE. The base and nasty desire to vent that spite on its assailant rankles perhaps even more nastily in it than in L&#39;HOMME DE LA NATURE ET DE LA VERITE. For through his innate stupidity the latter looks upon his revenge as justice pure and simple; while in consequence of his acute consciousness the mouse does not believe in the justice of it. To come at last to the deed itself, to the very act of revenge. Apart from the one fundamental nastiness the luckless mouse succeeds in creating around it so many other nastinesses in the form of doubts and questions, adds to the one question so many unsettled questions that there inevitably works up around it a sort of fatal brew, a stinking mess, made up of its doubts, emotions, and of the contempt spat upon it by the direct men of action who stand solemnly about it as judges and arbitrators, laughing at it till their healthy sides ache. Of course the only thing left for it is to dismiss all that with a wave of its paw, and, with a smile of assumed contempt in which it does not even itself believe, creep ignominiously into its mouse-hole. There in its nasty, stinking, underground home our insulted, crushed and ridiculed mouse promptly becomes absorbed in cold, malignant and, above all, everlasting spite. For forty years together it will remember its injury down to the smallest, most ignominious details, and every time will add, of itself, details still more ignominious, spitefully teasing and tormenting itself with its own imagination. It will itself be ashamed of its imaginings, but yet it will recall it all, it will go over and over every detail, it will invent unheard of things against itself, pretending that those things might happen, and will forgive nothing. Maybe it will begin to revenge itself, too, but, as it were, piecemeal, in trivial ways, from behind the stove, incognito, without believing either in its own right to vengeance, or in the success of its revenge, knowing that from all its efforts at revenge it will suffer a hundred times more than he on whom it revenges itself, while he, I daresay, will not even scratch himself. On its deathbed it will recall it all over again, with interest accumulated over all the years and ... But it is just in that cold, abominable half despair, half belief, in that conscious burying oneself alive for grief in the underworld for forty years, in that acutely recognised and yet partly doubtful hopelessness of one&#39;s position, in that hell of unsatisfied desires turned inward, in that fever of oscillations, of resolutions determined for ever and repented of again a minute later--that the savour of that strange enjoyment of which I have spoken lies. It is so subtle, so difficult of analysis, that persons who are a little limited, or even simply persons of strong nerves, will not understand a single atom of it. &#34;Possibly,&#34; you will add on your own account with a grin, &#34;people will not understand it either who have never received a slap in the face,&#34; and in that way you will politely hint to me that I, too, perhaps, have had the experience of a slap in the face in my life, and so I speak as one who knows. I bet that you are thinking that. But set your minds at rest, gentlemen, I have not received a slap in the face, though it is absolutely a matter of indifference to me what you may think about it. Possibly, I even regret, myself, that I have given so few slaps in the face during my life. But enough ... not another word on that subject of such extreme interest to you. I will continue calmly concerning persons with strong nerves who do not understand a certain refinement of enjoyment. Though in certain circumstances these gentlemen bellow their loudest like bulls, though this, let us suppose, does them the greatest credit, yet, as I have said already, confronted with the impossible they subside at once. The impossible means the stone wall! What stone wall? Why, of course, the laws of nature, the deductions of natural science, mathematics. As soon as they prove to you, for instance, that you are descended from a monkey, then it is no use scowling, accept it for a fact. When they prove to you that in reality one drop of your own fat must be dearer to you than a hundred thousand of your fellow-creatures, and that this conclusion is the final solution of all so-called virtues and duties and all such prejudices and fancies, then you have just to accept it, there is no help for it, for twice two is a law of mathematics. Just try refuting it. &#34;Upon my word, they will shout at you, it is no use protesting: it is a case of twice two makes four! Nature does not ask your permission, she has nothing to do with your wishes, and whether you like her laws or dislike them, you are bound to accept her as she is, and consequently all her conclusions. A wall, you see, is a wall ... and so on, and so on.&#34; Merciful Heavens! but what do I care for the laws of nature and arithmetic, when, for some reason I dislike those laws and the fact that twice two makes four? Of course I cannot break through the wall by battering my head against it if I really have not the strength to knock it down, but I am not going to be reconciled to it simply because it is a stone wall and I have not the strength. As though such a stone wall really were a consolation, and really did contain some word of conciliation, simply because it is as true as twice two makes four. Oh, absurdity of absurdities! How much better it is to understand it all, to recognise it all, all the impossibilities and the stone wall; not to be reconciled to one of those impossibilities and stone walls if it disgusts you to be reconciled to it; by the way of the most inevitable, logical combinations to reach the most revolting conclusions on the everlasting theme, that even for the stone wall you are yourself somehow to blame, though again it is as clear as day you are not to blame in the least, and therefore grinding your teeth in silent impotence to sink into luxurious inertia, brooding on the fact that there is no one even for you to feel vindictive against, that you have not, and perhaps never will have, an object for your spite, that it is a sleight of hand, a bit of juggling, a card-sharper&#39;s trick, that it is simply a mess, no knowing what and no knowing who, but in spite of all these uncertainties and jugglings, still there is an ache in you, and the more you do not know, the worse the ache. IV &#34;Ha, ha, ha! You will be finding enjoyment in toothache next,&#34; you cry, with a laugh. &#34;Well, even in toothache there is enjoyment,&#34; I answer. I had toothache for a whole month and I know there is. In that case, of course, people are not spiteful in silence, but moan; but they are not candid moans, they are malignant moans, and the malignancy is the whole point. The enjoyment of the sufferer finds expression in those moans; if he did not feel enjoyment in them he would not moan. It is a good example, gentlemen, and I will develop it. Those moans express in the first place all the aimlessness of your pain, which is so humiliating to your consciousness; the whole legal system of nature on which you spit disdainfully, of course, but from which you suffer all the same while she does not. They express the consciousness that you have no enemy to punish, but that you have pain; the consciousness that in spite of all possible Wagenheims you are in complete slavery to your teeth; that if someone wishes it, your teeth will leave off aching, and if he does not, they will go on aching another three months; and that finally if you are still contumacious and still protest, all that is left you for your own gratification is to thrash yourself or beat your wall with your fist as hard as you can, and absolutely nothing more. Well, these mortal insults, these jeers on the part of someone unknown, end at last in an enjoyment which sometimes reaches the highest degree of voluptuousness. I ask you, gentlemen, listen sometimes to the moans of an educated man of the nineteenth century suffering from toothache, on the second or third day of the attack, when he is beginning to moan, not as he moaned on the first day, that is, not simply because he has toothache, not just as any coarse peasant, but as a man affected by progress and European civilisation, a man who is &#34;divorced from the soil and the national elements,&#34; as they express it now-a-days. His moans become nasty, disgustingly malignant, and go on for whole days and nights. And of course he knows himself that he is doing himself no sort of good with his moans; he knows better than anyone that he is only lacerating and harassing himself and others for nothing; he knows that even the audience before whom he is making his efforts, and his whole family, listen to him with loathing, do not put a ha&#39;porth of faith in him, and inwardly understand that he might moan differently, more simply, without trills and flourishes, and that he is only amusing himself like that from ill-humour, from malignancy. Well, in all these recognitions and disgraces it is that there lies a voluptuous pleasure. As though he would say: &#34;I am worrying you, I am lacerating your hearts, I am keeping everyone in the house awake. Well, stay awake then, you, too, feel every minute that I have toothache. I am not a hero to you now, as I tried to seem before, but simply a nasty person, an impostor. Well, so be it, then! I am very glad that you see through me. It is nasty for you to hear my despicable moans: well, let it be nasty; here I will let you have a nastier flourish in a minute....&#34; You do not understand even now, gentlemen? No, it seems our development and our consciousness must go further to understand all the intricacies of this pleasure. You laugh? Delighted. My jests, gentlemen, are of course in bad taste, jerky, involved, lacking self-confidence. But of course that is because I do not respect myself. Can a man of perception respect himself at all? V Come, can a man who attempts to find enjoyment in the very feeling of his own degradation possibly have a spark of respect for himself? I am not saying this now from any mawkish kind of remorse. And, indeed, I could never endure saying, &#34;Forgive me, Papa, I won&#39;t do it again,&#34; not because I am incapable of saying that--on the contrary, perhaps just because I have been too capable of it, and in what a way, too. As though of design I used to get into trouble in cases when I was not to blame in any way. That was the nastiest part of it. At the same time I was genuinely touched and penitent, I used to shed tears and, of course, deceived myself, though I was not acting in the least and there was a sick feeling in my heart at the time.... For that one could not blame even the laws of nature, though the laws of nature have continually all my life offended me more than anything. It is loathsome to remember it all, but it was loathsome even then. Of course, a minute or so later I would realise wrathfully that it was all a lie, a revolting lie, an affected lie, that is, all this penitence, this emotion, these vows of reform. You will ask why did I worry myself with such antics: answer, because it was very dull to sit with one&#39;s hands folded, and so one began cutting capers. That is really it. Observe yourselves more carefully, gentlemen, then you will understand that it is so. I invented adventures for myself and made up a life, so as at least to live in some way. How many times it has happened to me--well, for instance, to take offence simply on purpose, for nothing; and one knows oneself, of course, that one is offended at nothing; that one is putting it on, but yet one brings oneself at last to the point of being really offended. All my life I have had an impulse to play such pranks, so that in the end I could not control it in myself. Another time, twice, in fact, I tried hard to be in love. I suffered, too, gentlemen, I assure you. In the depth of my heart there was no faith in my suffering, only a faint stir of mockery, but yet I did suffer, and in the real, orthodox way; I was jealous, beside myself ... and it was all from ENNUI, gentlemen, all from ENNUI; inertia overcame me. You know the direct, legitimate fruit of consciousness is inertia, that is, conscious sitting-with-the-hands-folded. I have referred to this already. I repeat, I repeat with emphasis: all &#34;direct&#34; persons and men of action are active just because they are stupid and limited. How explain that? I will tell you: in consequence of their limitation they take immediate and secondary causes for primary ones, and in that way persuade themselves more quickly and easily than other people do that they have found an infallible foundation for their activity, and their minds are at ease and you know that is the chief thing. To begin to act, you know, you must first have your mind completely at ease and no trace of doubt left in it. Why, how am I, for example, to set my mind at rest? Where are the primary causes on which I am to build? Where are my foundations? Where am I to get them from? I exercise myself in reflection, and consequently with me every primary cause at once draws after itself another still more primary, and so on to infinity. That is just the essence of every sort of consciousness and reflection. It must be a case of the laws of nature again. What is the result of it in the end? Why, just the same. Remember I spoke just now of vengeance. (I am sure you did not take it in.) I said that a man revenges himself because he sees justice in it. Therefore he has found a primary cause, that is, justice. And so he is at rest on all sides, and consequently he carries out his revenge calmly and successfully, being persuaded that he is doing a just and honest thing. But I see no justice in it, I find no sort of virtue in it either, and consequently if I attempt to revenge myself, it is only out of spite. Spite, of course, might overcome everything, all my doubts, and so might serve quite successfully in place of a primary cause, precisely because it is not a cause. But what is to be done if I have not even spite (I began with that just now, you know). In consequence again of those accursed laws of consciousness, anger in me is subject to chemical disintegration. You look into it, the object flies off into air, your reasons evaporate, the criminal is not to be found, the wrong becomes not a wrong but a phantom, something like the toothache, for which no one is to blame, and consequently there is only the same outlet left again--that is, to beat the wall as hard as you can. So you give it up with a wave of the hand because you have not found a fundamental cause. And try letting yourself be carried away by your feelings, blindly, without reflection, without a primary cause, repelling consciousness at least for a time; hate or love, if only not to sit with your hands folded. The day after tomorrow, at the latest, you will begin despising yourself for having knowingly deceived yourself. Result: a soap-bubble and inertia. Oh, gentlemen, do you know, perhaps I consider myself an intelligent man, only because all my life I have been able neither to begin nor to finish anything. Granted I am a babbler, a harmless vexatious babbler, like all of us. But what is to be done if the direct and sole vocation of every intelligent man is babble, that is, the intentional pouring of water through a sieve? VI Oh, if I had done nothing simply from laziness! Heavens, how I should have respected myself, then. I should have respected myself because I should at least have been capable of being lazy; there would at least have been one quality, as it were, positive in me, in which I could have believed myself. Question: What is he? Answer: A sluggard; how very pleasant it would have been to hear that of oneself! It would mean that I was positively defined, it would mean that there was something to say about me. &#34;Sluggard&#34;--why, it is a calling and vocation, it is a career. Do not jest, it is so. I should then be a member of the best club by right, and should find my occupation in continually respecting myself. I knew a gentleman who prided himself all his life on being a connoisseur of Lafitte. He considered this as his positive virtue, and never doubted himself. He died, not simply with a tranquil, but with a triumphant conscience, and he was quite right, too. Then I should have chosen a career for myself, I should have been a sluggard and a glutton, not a simple one, but, for instance, one with sympathies for everything sublime and beautiful. How do you like that? I have long had visions of it. That &#34;sublime and beautiful&#34; weighs heavily on my mind at forty But that is at forty; then--oh, then it would have been different! I should have found for myself a form of activity in keeping with it, to be precise, drinking to the health of everything &#34;sublime and beautiful.&#34; I should have snatched at every opportunity to drop a tear into my glass and then to drain it to all that is &#34;sublime and beautiful.&#34; I should then have turned everything into the sublime and the beautiful; in the nastiest, unquestionable trash, I should have sought out the sublime and the beautiful. I should have exuded tears like a wet sponge. An artist, for instance, paints a picture worthy of Gay. At once I drink to the health of the artist who painted the picture worthy of Gay, because I love all that is &#34;sublime and beautiful.&#34; An author has written AS YOU WILL: at once I drink to the health of &#34;anyone you will&#34; because I love all that is &#34;sublime and beautiful.&#34; I should claim respect for doing so. I should persecute anyone who would not show me respect. I should live at ease, I should die with dignity, why, it is charming, perfectly charming! And what a good round belly I should have grown, what a treble chin I should have established, what a ruby nose I should have coloured for myself, so that everyone would have said, looking at me: &#34;Here is an asset! Here is something real and solid!&#34; And, say what you like, it is very agreeable to hear such remarks about oneself in this negative age. VII But these are all golden dreams. Oh, tell me, who was it first announced, who was it first proclaimed, that man only does nasty things because he does not know his own interests; and that if he were enlightened, if his eyes were opened to his real normal interests, man would at once cease to do nasty things, would at once become good and noble because, being enlightened and understanding his real advantage, he would see his own advantage in the good and nothing else, and we all know that not one man can, consciously, act against his own interests, consequently, so to say, through necessity, he would begin doing good? Oh, the babe! Oh, the pure, innocent child! Why, in the first place, when in all these thousands of years has there been a time when man has acted only from his own interest? What is to be done with the millions of facts that bear witness that men, CONSCIOUSLY, that is fully understanding their real interests, have left them in the background and have rushed headlong on another path, to meet peril and danger, compelled to this course by nobody and by nothing, but, as it were, simply disliking the beaten track, and have obstinately, wilfully, struck out another difficult, absurd way, seeking it almost in the darkness. So, I suppose, this obstinacy and perversity were pleasanter to them than any advantage.... Advantage! What is advantage? And will you take it upon yourself to define with perfect accuracy in what the advantage of man consists? And what if it so happens that a man&#39;s advantage, SOMETIMES, not only may, but even must, consist in his desiring in certain cases what is harmful to himself and not advantageous. And if so, if there can be such a case, the whole principle falls into dust. What do you think--are there such cases? You laugh; laugh away, gentlemen, but only answer me: have man&#39;s advantages been reckoned up with perfect certainty? Are there not some which not only have not been included but cannot possibly be included under any classification? You see, you gentlemen have, to the best of my knowledge, taken your whole register of human advantages from the averages of statistical figures and politico-economical formulas. Your advantages are prosperity, wealth, freedom, peace--and so on, and so on. So that the man who should, for instance, go openly and knowingly in opposition to all that list would to your thinking, and indeed mine, too, of course, be an obscurantist or an absolute madman: would not he? But, you know, this is what is surprising: why does it so happen that all these statisticians, sages and lovers of humanity, when they reckon up human advantages invariably leave out one? They don&#39;t even take it into their reckoning in the form in which it should be taken, and the whole reckoning depends upon that. It would be no greater matter, they would simply have to take it, this advantage, and add it to the list. But the trouble is, that this strange advantage does not fall under any classification and is not in place in any list. I have a friend for instance ... Ech! gentlemen, but of course he is your friend, too; and indeed there is no one, no one to whom he is not a friend! When he prepares for any undertaking this gentleman immediately explains to you, elegantly and clearly, exactly how he must act in accordance with the laws of reason and truth. What is more, he will talk to you with excitement and passion of the true normal interests of man; with irony he will upbraid the short-sighted fools who do not understand their own interests, nor the true significance of virtue; and, within a quarter of an hour, without any sudden outside provocation, but simply through something inside him which is stronger than all his interests, he will go off on quite a different tack--that is, act in direct opposition to what he has just been saying about himself, in opposition to the laws of reason, in opposition to his own advantage, in fact in opposition to everything ... I warn you that my friend is a compound personality and therefore it is difficult to blame him as an individual. The fact is, gentlemen, it seems there must really exist something that is dearer to almost every man than his greatest advantages, or (not to be illogical) there is a most advantageous advantage (the very one omitted of which we spoke just now) which is more important and more advantageous than all other advantages, for the sake of which a man if necessary is ready to act in opposition to all laws; that is, in opposition to reason, honour, peace, prosperity--in fact, in opposition to all those excellent and useful things if only he can attain that fundamental, most advantageous advantage which is dearer to him than all. &#34;Yes, but it&#39;s advantage all the same,&#34; you will retort. But excuse me, I&#39;ll make the point clear, and it is not a case of playing upon words. What matters is, that this advantage is remarkable from the very fact that it breaks down all our classifications, and continually shatters every system constructed by lovers of mankind for the benefit of mankind. In fact, it upsets everything. But before I mention this advantage to you, I want to compromise myself personally, and therefore I boldly declare that all these fine systems, all these theories for explaining to mankind their real normal interests, in order that inevitably striving to pursue these interests they may at once become good and noble--are, in my opinion, so far, mere logical exercises! Yes, logical exercises. Why, to maintain this theory of the regeneration of mankind by means of the pursuit of his own advantage is to my mind almost the same thing ... as to affirm, for instance, following Buckle, that through civilisation mankind becomes softer, and consequently less bloodthirsty and less fitted for warfare. Logically it does seem to follow from his arguments. But man has such a predilection for systems and abstract deductions that he is ready to distort the truth intentionally, he is ready to deny the evidence of his senses only to justify his logic. I take this example because it is the most glaring instance of it. Only look about you: blood is being spilt in streams, and in the merriest way, as though it were champagne. Take the whole of the nineteenth century in which Buckle lived. Take Napoleon--the Great and also the present one. Take North America--the eternal union. Take the farce of Schleswig-Holstein.... And what is it that civilisation softens in us? The only gain of civilisation for mankind is the greater capacity for variety of sensations--and absolutely nothing more. And through the development of this many-sidedness man may come to finding enjoyment in bloodshed. In fact, this has already happened to him. Have you noticed that it is the most civilised gentlemen who have been the subtlest slaughterers, to whom the Attilas and Stenka Razins could not hold a candle, and if they are not so conspicuous as the Attilas and Stenka Razins it is simply because they are so often met with, are so ordinary and have become so familiar to us. In any case civilisation has made mankind if not more bloodthirsty, at least more vilely, more loathsomely bloodthirsty. In old days he saw justice in bloodshed and with his conscience at peace exterminated those he thought proper. Now we do think bloodshed abominable and yet we engage in this abomination, and with more energy than ever. Which is worse? Decide that for yourselves. They say that Cleopatra (excuse an instance from Roman history) was fond of sticking gold pins into her slave-girls&#39; breasts and derived gratification from their screams and writhings. You will say that that was in the comparatively barbarous times; that these are barbarous times too, because also, comparatively speaking, pins are stuck in even now; that though man has now learned to see more clearly than in barbarous ages, he is still far from having learnt to act as reason and science would dictate. But yet you are fully convinced that he will be sure to learn when he gets rid of certain old bad habits, and when common sense and science have completely re-educated human nature and turned it in a normal direction. You are confident that then man will cease from INTENTIONAL error and will, so to say, be compelled not to want to set his will against his normal interests. That is not all; then, you say, science itself will teach man (though to my mind it&#39;s a superfluous luxury) that he never has really had any caprice or will of his own, and that he himself is something of the nature of a piano-key or the stop of an organ, and that there are, besides, things called the laws of nature; so that everything he does is not done by his willing it, but is done of itself, by the laws of nature. Consequently we have only to discover these laws of nature, and man will no longer have to answer for his actions and life will become exceedingly easy for him. All human actions will then, of course, be tabulated according to these laws, mathematically, like tables of logarithms up to 108,000, and entered in an index; or, better still, there would be published certain edifying works of the nature of encyclopaedic lexicons, in which everything will be so clearly calculated and explained that there will be no more incidents or adventures in the world. Then--this is all what you say--new economic relations will be established, all ready-made and worked out with mathematical exactitude, so that every possible question will vanish in the twinkling of an eye, simply because every possible answer to it will be provided. Then the &#34;Palace of Crystal&#34; will be built. Then ... In fact, those will be halcyon days. Of course there is no guaranteeing (this is my comment) that it will not be, for instance, frightfully dull then (for what will one have to do when everything will be calculated and tabulated), but on the other hand everything will be extraordinarily rational. Of course boredom may lead you to anything. It is boredom sets one sticking golden pins into people, but all that would not matter. What is bad (this is my comment again) is that I dare say people will be thankful for the gold pins then. Man is stupid, you know, phenomenally stupid; or rather he is not at all stupid, but he is so ungrateful that you could not find another like him in all creation. I, for instance, would not be in the least surprised if all of a sudden, A PROPOS of nothing, in the midst of general prosperity a gentleman with an ignoble, or rather with a reactionary and ironical, countenance were to arise and, putting his arms akimbo, say to us all: &#34;I say, gentleman, hadn&#39;t we better kick over the whole show and scatter rationalism to the winds, simply to send these logarithms to the devil, and to enable us to live once more at our own sweet foolish will!&#34; That again would not matter, but what is annoying is that he would be sure to find followers--such is the nature of man. And all that for the most foolish reason, which, one would think, was hardly worth mentioning: that is, that man everywhere and at all times, whoever he may be, has preferred to act as he chose and not in the least as his reason and advantage dictated. And one may choose what is contrary to one&#39;s own interests, and sometimes one POSITIVELY OUGHT (that is my idea). One&#39;s own free unfettered choice, one&#39;s own caprice, however wild it may be, one&#39;s own fancy worked up at times to frenzy--is that very &#34;most advantageous advantage&#34; which we have overlooked, which comes under no classification and against which all systems and theories are continually being shattered to atoms. And how do these wiseacres know that man wants a normal, a virtuous choice? What has made them conceive that man must want a rationally advantageous choice? What man wants is simply INDEPENDENT choice, whatever that independence may cost and wherever it may lead. And choice, of course, the devil only knows what choice. VIII &#34;Ha! ha! ha! But you know there is no such thing as choice in reality, say what you like,&#34; you will interpose with a chuckle. &#34;Science has succeeded in so far analysing man that we know already that choice and what is called freedom of will is nothing else than--&#34; Stay, gentlemen, I meant to begin with that myself I confess, I was rather frightened. I was just going to say that the devil only knows what choice depends on, and that perhaps that was a very good thing, but I remembered the teaching of science ... and pulled myself up. And here you have begun upon it. Indeed, if there really is some day discovered a formula for all our desires and caprices--that is, an explanation of what they depend upon, by what laws they arise, how they develop, what they are aiming at in one case and in another and so on, that is a real mathematical formula--then, most likely, man will at once cease to feel desire, indeed, he will be certain to. For who would want to choose by rule? Besides, he will at once be transformed from a human being into an organ-stop or something of the sort; for what is a man without desires, without free will and without choice, if not a stop in an organ? What do you think? Let us reckon the chances--can such a thing happen or not? &#34;H&#39;m!&#34; you decide. &#34;Our choice is usually mistaken from a false view of our advantage. We sometimes choose absolute nonsense because in our foolishness we see in that nonsense the easiest means for attaining a supposed advantage. But when all that is explained and worked out on paper (which is perfectly possible, for it is contemptible and senseless to suppose that some laws of nature man will never understand), then certainly so-called desires will no longer exist. For if a desire should come into conflict with reason we shall then reason and not desire, because it will be impossible retaining our reason to be SENSELESS in our desires, and in that way knowingly act against reason and desire to injure ourselves. And as all choice and reasoning can be really calculated--because there will some day be discovered the laws of our so-called free will--so, joking apart, there may one day be something like a table constructed of them, so that we really shall choose in accordance with it. If, for instance, some day they calculate and prove to me that I made a long nose at someone because I could not help making a long nose at him and that I had to do it in that particular way, what FREEDOM is left me, especially if I am a learned man and have taken my degree somewhere? Then I should be able to calculate my whole life for thirty years beforehand. In short, if this could be arranged there would be nothing left for us to do; anyway, we should have to understand that. And, in fact, we ought unwearyingly to repeat to ourselves that at such and such a time and in such and such circumstances nature does not ask our leave; that we have got to take her as she is and not fashion her to suit our fancy, and if we really aspire to formulas and tables of rules, and well, even ... to the chemical retort, there&#39;s no help for it, we must accept the retort too, or else it will be accepted without our consent....&#34; Yes, but here I come to a stop! Gentlemen, you must excuse me for being over-philosophical; it&#39;s the result of forty years underground! Allow me to indulge my fancy. You see, gentlemen, reason is an excellent thing, there&#39;s no disputing that, but reason is nothing but reason and satisfies only the rational side of man&#39;s nature, while will is a manifestation of the whole life, that is, of the whole human life including reason and all the impulses. And although our life, in this manifestation of it, is often worthless, yet it is life and not simply extracting square roots. Here I, for instance, quite naturally want to live, in order to satisfy all my capacities for life, and not simply my capacity for reasoning, that is, not simply one twentieth of my capacity for life. What does reason know? Reason only knows what it has succeeded in learning (some things, perhaps, it will never learn; this is a poor comfort, but why not say so frankly?) and human nature acts as a whole, with everything that is in it, consciously or unconsciously, and, even if it goes wrong, it lives. I suspect, gentlemen, that you are looking at me with compassion; you tell me again that an enlightened and developed man, such, in short, as the future man will be, cannot consciously desire anything disadvantageous to himself, that that can be proved mathematically. I thoroughly agree, it can--by mathematics. But I repeat for the hundredth time, there is one case, one only, when man may consciously, purposely, desire what is injurious to himself, what is stupid, very stupid--simply in order to have the right to desire for himself even what is very stupid and not to be bound by an obligation to desire only what is sensible. Of course, this very stupid thing, this caprice of ours, may be in reality, gentlemen, more advantageous for us than anything else on earth, especially in certain cases. And in particular it may be more advantageous than any advantage even when it does us obvious harm, and contradicts the soundest conclusions of our reason concerning our advantage--for in any circumstances it preserves for us what is most precious and most important--that is, our personality, our individuality. Some, you see, maintain that this really is the most precious thing for mankind; choice can, of course, if it chooses, be in agreement with reason; and especially if this be not abused but kept within bounds. It is profitable and sometimes even praiseworthy. But very often, and even most often, choice is utterly and stubbornly opposed to reason ... and ... and ... do you know that that, too, is profitable, sometimes even praiseworthy? Gentlemen, let us suppose that man is not stupid. (Indeed one cannot refuse to suppose that, if only from the one consideration, that, if man is stupid, then who is wise?) But if he is not stupid, he is monstrously ungrateful! Phenomenally ungrateful. In fact, I believe that the best definition of man is the ungrateful biped. But that is not all, that is not his worst defect; his worst defect is his perpetual moral obliquity, perpetual--from the days of the Flood to the Schleswig-Holstein period. Moral obliquity and consequently lack of good sense; for it has long been accepted that lack of good sense is due to no other cause than moral obliquity. Put it to the test and cast your eyes upon the history of mankind. What will you see? Is it a grand spectacle? Grand, if you like. Take the Colossus of Rhodes, for instance, that&#39;s worth something. With good reason Mr. Anaevsky testifies of it that some say that it is the work of man&#39;s hands, while others maintain that it has been created by nature herself. Is it many-coloured? May be it is many-coloured, too: if one takes the dress uniforms, military and civilian, of all peoples in all ages--that alone is worth something, and if you take the undress uniforms you will never get to the end of it; no historian would be equal to the job. Is it monotonous? May be it&#39;s monotonous too: it&#39;s fighting and fighting; they are fighting now, they fought first and they fought last--you will admit, that it is almost too monotonous. In short, one may say anything about the history of the world--anything that might enter the most disordered imagination. The only thing one can&#39;t say is that it&#39;s rational. The very word sticks in one&#39;s throat. And, indeed, this is the odd thing that is continually happening: there are continually turning up in life moral and rational persons, sages and lovers of humanity who make it their object to live all their lives as morally and rationally as possible, to be, so to speak, a light to their neighbours simply in order to show them that it is possible to live morally and rationally in this world. And yet we all know that those very people sooner or later have been false to themselves, playing some queer trick, often a most unseemly one. Now I ask you: what can be expected of man since he is a being endowed with strange qualities? Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick. He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element. It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself--as though that were so necessary--that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar. And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point. And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point! He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object--that is, convince himself that he is a man and not a piano-key! If you say that all this, too, can be calculated and tabulated--chaos and darkness and curses, so that the mere possibility of calculating it all beforehand would stop it all, and reason would reassert itself, then man would purposely go mad in order to be rid of reason and gain his point! I believe in it, I answer for it, for the whole work of man really seems to consist in nothing but proving to himself every minute that he is a man and not a piano-key! It may be at the cost of his skin, it may be by cannibalism! And this being so, can one help being tempted to rejoice that it has not yet come off, and that desire still depends on something we don&#39;t know? You will scream at me (that is, if you condescend to do so) that no one is touching my free will, that all they are concerned with is that my will should of itself, of its own free will, coincide with my own normal interests, with the laws of nature and arithmetic. Good heavens, gentlemen, what sort of free will is left when we come to tabulation and arithmetic, when it will all be a case of twice two make four? Twice two makes four without my will. As if free will meant that! IX Gentlemen, I am joking, and I know myself that my jokes are not brilliant, but you know one can take everything as a joke. I am, perhaps, jesting against the grain. Gentlemen, I am tormented by questions; answer them for me. You, for instance, want to cure men of their old habits and reform their will in accordance with science and good sense. But how do you know, not only that it is possible, but also that it is DESIRABLE to reform man in that way? And what leads you to the conclusion that man&#39;s inclinations NEED reforming? In short, how do you know that such a reformation will be a benefit to man? And to go to the root of the matter, why are you so positively convinced that not to act against his real normal interests guaranteed by the conclusions of reason and arithmetic is certainly always advantageous for man and must always be a law for mankind? So far, you know, this is only your supposition. It may be the law of logic, but not the law of humanity. You think, gentlemen, perhaps that I am mad? Allow me to defend myself. I agree that man is pre-eminently a creative animal, predestined to strive consciously for an object and to engage in engineering--that is, incessantly and eternally to make new roads, WHEREVER THEY MAY LEAD. But the reason why he wants sometimes to go off at a tangent may just be that he is PREDESTINED to make the road, and perhaps, too, that however stupid the &#34;direct&#34; practical man may be, the thought sometimes will occur to him that the road almost always does lead SOMEWHERE, and that the destination it leads to is less important than the process of making it, and that the chief thing is to save the well-conducted child from despising engineering, and so giving way to the fatal idleness, which, as we all know, is the mother of all the vices. Man likes to make roads and to create, that is a fact beyond dispute. But why has he such a passionate love for destruction and chaos also? Tell me that! But on that point I want to say a couple of words myself. May it not be that he loves chaos and destruction (there can be no disputing that he does sometimes love it) because he is instinctively afraid of attaining his object and completing the edifice he is constructing? Who knows, perhaps he only loves that edifice from a distance, and is by no means in love with it at close quarters; perhaps he only loves building it and does not want to live in it, but will leave it, when completed, for the use of LES ANIMAUX DOMESTIQUES--such as the ants, the sheep, and so on. Now the ants have quite a different taste. They have a marvellous edifice of that pattern which endures for ever--the ant-heap. With the ant-heap the respectable race of ants began and with the ant-heap they will probably end, which does the greatest credit to their perseverance and good sense. But man is a frivolous and incongruous creature, and perhaps, like a chess player, loves the process of the game, not the end of it. And who knows (there is no saying with certainty), perhaps the only goal on earth to which mankind is striving lies in this incessant process of attaining, in other words, in life itself, and not in the thing to be attained, which must always be expressed as a formula, as positive as twice two makes four, and such positiveness is not life, gentlemen, but is the beginning of death. Anyway, man has always been afraid of this mathematical certainty, and I am afraid of it now. Granted that man does nothing but seek that mathematical certainty, he traverses oceans, sacrifices his life in the quest, but to succeed, really to find it, dreads, I assure you. He feels that when he has found it there will be nothing for him to look for. When workmen have finished their work they do at least receive their pay, they go to the tavern, then they are taken to the police-station--and there is occupation for a week. But where can man go? Anyway, one can observe a certain awkwardness about him when he has attained such objects. He loves the process of attaining, but does not quite like to have attained, and that, of course, is very absurd. In fact, man is a comical creature; there seems to be a kind of jest in it all. But yet mathematical certainty is after all, something insufferable. Twice two makes four seems to me simply a piece of insolence. Twice two makes four is a pert coxcomb who stands with arms akimbo barring your path and spitting. I admit that twice two makes four is an excellent thing, but if we are to give everything its due, twice two makes five is sometimes a very charming thing too. And why are you so firmly, so triumphantly, convinced that only the normal and the positive--in other words, only what is conducive to welfare--is for the advantage of man? Is not reason in error as regards advantage? Does not man, perhaps, love something besides well-being? Perhaps he is just as fond of suffering? Perhaps suffering is just as great a benefit to him as well-being? Man is sometimes extraordinarily, passionately, in love with suffering, and that is a fact. There is no need to appeal to universal history to prove that; only ask yourself, if you are a man and have lived at all. As far as my personal opinion is concerned, to care only for well-being seems to me positively ill-bred. Whether it&#39;s good or bad, it is sometimes very pleasant, too, to smash things. I hold no brief for suffering nor for well-being either. I am standing for ... my caprice, and for its being guaranteed to me when necessary. Suffering would be out of place in vaudevilles, for instance; I know that. In the &#34;Palace of Crystal&#34; it is unthinkable; suffering means doubt, negation, and what would be the good of a &#34;palace of crystal&#34; if there could be any doubt about it? And yet I think man will never renounce real suffering, that is, destruction and chaos. Why, suffering is the sole origin of consciousness. Though I did lay it down at the beginning that consciousness is the greatest misfortune for man, yet I know man prizes it and would not give it up for any satisfaction. Consciousness, for instance, is infinitely superior to twice two makes four. Once you have mathematical certainty there is nothing left to do or to understand. There will be nothing left but to bottle up your five senses and plunge into contemplation. While if you stick to consciousness, even though the same result is attained, you can at least flog yourself at times, and that will, at any rate, liven you up. Reactionary as it is, corporal punishment is better than nothing. X You believe in a palace of crystal that can never be destroyed--a palace at which one will not be able to put out one&#39;s tongue or make a long nose on the sly. And perhaps that is just why I am afraid of this edifice, that it is of crystal and can never be destroyed and that one cannot put one&#39;s tongue out at it even on the sly. You see, if it were not a palace, but a hen-house, I might creep into it to avoid getting wet, and yet I would not call the hen-house a palace out of gratitude to it for keeping me dry. You laugh and say that in such circumstances a hen-house is as good as a mansion. Yes, I answer, if one had to live simply to keep out of the rain. But what is to be done if I have taken it into my head that that is not the only object in life, and that if one must live one had better live in a mansion? That is my choice, my desire. You will only eradicate it when you have changed my preference. Well, do change it, allure me with something else, give me another ideal. But meanwhile I will not take a hen-house for a mansion. The palace of crystal may be an idle dream, it may be that it is inconsistent with the laws of nature and that I have invented it only through my own stupidity, through the old-fashioned irrational habits of my generation. But what does it matter to me that it is inconsistent? That makes no difference since it exists in my desires, or rather exists as long as my desires exist. Perhaps you are laughing again? Laugh away; I will put up with any mockery rather than pretend that I am satisfied when I am hungry. I know, anyway, that I will not be put off with a compromise, with a recurring zero, simply because it is consistent with the laws of nature and actually exists. I will not accept as the crown of my desires a block of buildings with tenements for the poor on a lease of a thousand years, and perhaps with a sign-board of a dentist hanging out. Destroy my desires, eradicate my ideals, show me something better, and I will follow you. You will say, perhaps, that it is not worth your trouble; but in that case I can give you the same answer. We are discussing things seriously; but if you won&#39;t deign to give me your attention, I will drop your acquaintance. I can retreat into my underground hole. But while I am alive and have desires I would rather my hand were withered off than bring one brick to such a building! Don&#39;t remind me that I have just rejected the palace of crystal for the sole reason that one cannot put out one&#39;s tongue at it. I did not say because I am so fond of putting my tongue out. Perhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one&#39;s tongue. On the contrary, I would let my tongue be cut off out of gratitude if things could be so arranged that I should lose all desire to put it out. It is not my fault that things cannot be so arranged, and that one must be satisfied with model flats. Then why am I made with such desires? Can I have been constructed simply in order to come to the conclusion that all my construction is a cheat? Can this be my whole purpose? I do not believe it. But do you know what: I am convinced that we underground folk ought to be kept on a curb. Though we may sit forty years underground without speaking, when we do come out into the light of day and break out we talk and talk and talk.... XI The long and the short of it is, gentlemen, that it is better to do nothing! Better conscious inertia! And so hurrah for underground! Though I have said that I envy the normal man to the last drop of my bile, yet I should not care to be in his place such as he is now (though I shall not cease envying him). No, no; anyway the underground life is more advantageous. There, at any rate, one can ... Oh, but even now I am lying! I am lying because I know myself that it is not underground that is better, but something different, quite different, for which I am thirsting, but which I cannot find! Damn underground! I will tell you another thing that would be better, and that is, if I myself believed in anything of what I have just written. I swear to you, gentlemen, there is not one thing, not one word of what I have written that I really believe. That is, I believe it, perhaps, but at the same time I feel and suspect that I am lying like a cobbler. &#34;Then why have you written all this?&#34; you will say to me. &#34;I ought to put you underground for forty years without anything to do and then come to you in your cellar, to find out what stage you have reached! How can a man be left with nothing to do for forty years?&#34; &#34;Isn&#39;t that shameful, isn&#39;t that humiliating?&#34; you will say, perhaps, wagging your heads contemptuously. &#34;You thirst for life and try to settle the problems of life by a logical tangle. And how persistent, how insolent are your sallies, and at the same time what a scare you are in! You talk nonsense and are pleased with it; you say impudent things and are in continual alarm and apologising for them. You declare that you are afraid of nothing and at the same time try to ingratiate yourself in our good opinion. You declare that you are gnashing your teeth and at the same time you try to be witty so as to amuse us. You know that your witticisms are not witty, but you are evidently well satisfied with their literary value. You may, perhaps, have really suffered, but you have no respect for your own suffering. You may have sincerity, but you have no modesty; out of the pettiest vanity you expose your sincerity to publicity and ignominy. You doubtlessly mean to say something, but hide your last word through fear, because you have not the resolution to utter it, and only have a cowardly impudence. You boast of consciousness, but you are not sure of your ground, for though your mind works, yet your heart is darkened and corrupt, and you cannot have a full, genuine consciousness without a pure heart. And how intrusive you are, how you insist and grimace! Lies, lies, lies!&#34; Of course I have myself made up all the things you say. That, too, is from underground. I have been for forty years listening to you through a crack under the floor. I have invented them myself, there was nothing else I could invent. It is no wonder that I have learned it by heart and it has taken a literary form.... But can you really be so credulous as to think that I will print all this and give it to you to read too? And another problem: why do I call you &#34;gentlemen,&#34; why do I address you as though you really were my readers? Such confessions as I intend to make are never printed nor given to other people to read. Anyway, I am not strong-minded enough for that, and I don&#39;t see why I should be. But you see a fancy has occurred to me and I want to realise it at all costs. Let me explain. Every man has reminiscences which he would not tell to everyone, but only to his friends. He has other matters in his mind which he would not reveal even to his friends, but only to himself, and that in secret. But there are other things which a man is afraid to tell even to himself, and every decent man has a number of such things stored away in his mind. The more decent he is, the greater the number of such things in his mind. Anyway, I have only lately determined to remember some of my early adventures. Till now I have always avoided them, even with a certain uneasiness. Now, when I am not only recalling them, but have actually decided to write an account of them, I want to try the experiment whether one can, even with oneself, be perfectly open and not take fright at the whole truth. I will observe, in parenthesis, that Heine says that a true autobiography is almost an impossibility, and that man is bound to lie about himself. He considers that Rousseau certainly told lies about himself in his confessions, and even intentionally lied, out of vanity. I am convinced that Heine is right; I quite understand how sometimes one may, out of sheer vanity, attribute regular crimes to oneself, and indeed I can very well conceive that kind of vanity. But Heine judged of people who made their confessions to the public. I write only for myself, and I wish to declare once and for all that if I write as though I were addressing readers, that is simply because it is easier for me to write in that form. It is a form, an empty form--I shall never have readers. I have made this plain already ... I don&#39;t wish to be hampered by any restrictions in the compilation of my notes. I shall not attempt any system or method. I will jot things down as I remember them. But here, perhaps, someone will catch at the word and ask me: if you really don&#39;t reckon on readers, why do you make such compacts with yourself--and on paper too--that is, that you won&#39;t attempt any system or method, that you jot things down as you remember them, and so on, and so on? Why are you explaining? Why do you apologise? Well, there it is, I answer. There is a whole psychology in all this, though. Perhaps it is simply that I am a coward. And perhaps that I purposely imagine an audience before me in order that I may be more dignified while I write. There are perhaps thousands of reasons. Again, what is my object precisely in writing? If it is not for the benefit of the public why should I not simply recall these incidents in my own mind without putting them on paper? Quite so; but yet it is more imposing on paper. There is something more impressive in it; I shall be better able to criticise myself and improve my style. Besides, I shall perhaps obtain actual relief from writing. Today, for instance, I am particularly oppressed by one memory of a distant past. It came back vividly to my mind a few days ago, and has remained haunting me like an annoying tune that one cannot get rid of. And yet I must get rid of it somehow. I have hundreds of such reminiscences; but at times some one stands out from the hundred and oppresses me. For some reason I believe that if I write it down I should get rid of it. Why not try? Besides, I am bored, and I never have anything to do. Writing will be a sort of work. They say work makes man kind-hearted and honest. Well, here is a chance for me, anyway. Snow is falling today, yellow and dingy. It fell yesterday, too, and a few days ago. I fancy it is the wet snow that has reminded me of that incident which I cannot shake off now. And so let it be a story A PROPOS of the falling snow. PART II A Propos of the Wet Snow When from dark error&#39;s subjugation My words of passionate exhortation Had wrenched thy fainting spirit free; And writhing prone in thine affliction Thou didst recall with malediction The vice that had encompassed thee: And when thy slumbering conscience, fretting By recollection&#39;s torturing flame, Thou didst reveal the hideous setting Of thy life&#39;s current ere I came: When suddenly I saw thee sicken, And weeping, hide thine anguished face, Revolted, maddened, horror-stricken, At memories of foul disgrace. NEKRASSOV (translated by Juliet Soskice). I AT THAT TIME I was only twenty-four. My life was even then gloomy, ill-regulated, and as solitary as that of a savage. I made friends with no one and positively avoided talking, and buried myself more and more in my hole. At work in the office I never looked at anyone, and was perfectly well aware that my companions looked upon me, not only as a queer fellow, but even looked upon me--I always fancied this--with a sort of loathing. I sometimes wondered why it was that nobody except me fancied that he was looked upon with aversion? One of the clerks had a most repulsive, pock-marked face, which looked positively villainous. I believe I should not have dared to look at anyone with such an unsightly countenance. Another had such a very dirty old uniform that there was an unpleasant odour in his proximity. Yet not one of these gentlemen showed the slightest self-consciousness--either about their clothes or their countenance or their character in any way. Neither of them ever imagined that they were looked at with repulsion; if they had imagined it they would not have minded--so long as their superiors did not look at them in that way. It is clear to me now that, owing to my unbounded vanity and to the high standard I set for myself, I often looked at myself with furious discontent, which verged on loathing, and so I inwardly attributed the same feeling to everyone. I hated my face, for instance: I thought it disgusting, and even suspected that there was something base in my expression, and so every day when I turned up at the office I tried to behave as independently as possible, and to assume a lofty expression, so that I might not be suspected of being abject. &#34;My face may be ugly,&#34; I thought, &#34;but let it be lofty, expressive, and, above all, EXTREMELY intelligent.&#34; But I was positively and painfully certain that it was impossible for my countenance ever to express those qualities. And what was worst of all, I thought it actually stupid looking, and I would have been quite satisfied if I could have looked intelligent. In fact, I would even have put up with looking base if, at the same time, my face could have been thought strikingly intelligent. Of course, I hated my fellow clerks one and all, and I despised them all, yet at the same time I was, as it were, afraid of them. In fact, it happened at times that I thought more highly of them than of myself. It somehow happened quite suddenly that I alternated between despising them and thinking them superior to myself. A cultivated and decent man cannot be vain without setting a fearfully high standard for himself, and without despising and almost hating himself at certain moments. But whether I despised them or thought them superior I dropped my eyes almost every time I met anyone. I even made experiments whether I could face so and so&#39;s looking at me, and I was always the first to drop my eyes. This worried me to distraction. I had a sickly dread, too, of being ridiculous, and so had a slavish passion for the conventional in everything external. I loved to fall into the common rut, and had a whole-hearted terror of any kind of eccentricity in myself. But how could I live up to it? I was morbidly sensitive as a man of our age should be. They were all stupid, and as like one another as so many sheep. Perhaps I was the only one in the office who fancied that I was a coward and a slave, and I fancied it just because I was more highly developed. But it was not only that I fancied it, it really was so. I was a coward and a slave. I say this without the slightest embarrassment. Every decent man of our age must be a coward and a slave. That is his normal condition. Of that I am firmly persuaded. He is made and constructed to that very end. And not only at the present time owing to some casual circumstances, but always, at all times, a decent man is bound to be a coward and a slave. It is the law of nature for all decent people all over the earth. If anyone of them happens to be valiant about something, he need not be comforted nor carried away by that; he would show the white feather just the same before something else. That is how it invariably and inevitably ends. Only donkeys and mules are valiant, and they only till they are pushed up to the wall. It is not worth while to pay attention to them for they really are of no consequence. Another circumstance, too, worried me in those days: that there was no one like me and I was unlike anyone else. &#34;I am alone and they are EVERYONE,&#34; I thought--and pondered. From that it is evident that I was still a youngster. The very opposite sometimes happened. It was loathsome sometimes to go to the office; things reached such a point that I often came home ill. But all at once, A PROPOS of nothing, there would come a phase of scepticism and indifference (everything happened in phases to me), and I would laugh myself at my intolerance and fastidiousness, I would reproach myself with being ROMANTIC. At one time I was unwilling to speak to anyone, while at other times I would not only talk, but go to the length of contemplating making friends with them. All my fastidiousness would suddenly, for no rhyme or reason, vanish. Who knows, perhaps I never had really had it, and it had simply been affected, and got out of books. I have not decided that question even now. Once I quite made friends with them, visited their homes, played preference, drank vodka, talked of promotions.... But here let me make a digression. We Russians, speaking generally, have never had those foolish transcendental &#34;romantics&#34;--German, and still more French--on whom nothing produces any effect; if there were an earthquake, if all France perished at the barricades, they would still be the same, they would not even have the decency to affect a change, but would still go on singing their transcendental songs to the hour of their death, because they are fools. We, in Russia, have no fools; that is well known. That is what distinguishes us from foreign lands. Consequently these transcendental natures are not found amongst us in their pure form. The idea that they are is due to our &#34;realistic&#34; journalists and critics of that day, always on the look out for Kostanzhoglos and Uncle Pyotr Ivanitchs and foolishly accepting them as our ideal; they have slandered our romantics, taking them for the same transcendental sort as in Germany or France. On the contrary, the characteristics of our &#34;romantics&#34; are absolutely and directly opposed to the transcendental European type, and no European standard can be applied to them. (Allow me to make use of this word &#34;romantic&#34;--an old-fashioned and much respected word which has done good service and is familiar to all.) The characteristics of our romantic are to understand everything, TO SEE EVERYTHING AND TO SEE IT OFTEN INCOMPARABLY MORE CLEARLY THAN OUR MOST REALISTIC MINDS SEE IT; to refuse to accept anyone or anything, but at the same time not to despise anything; to give way, to yield, from policy; never to lose sight of a useful practical object (such as rent-free quarters at the government expense, pensions, decorations), to keep their eye on that object through all the enthusiasms and volumes of lyrical poems, and at the same time to preserve &#34;the sublime and the beautiful&#34; inviolate within them to the hour of their death, and to preserve themselves also, incidentally, like some precious jewel wrapped in cotton wool if only for the benefit of &#34;the sublime and the beautiful.&#34; Our &#34;romantic&#34; is a man of great breadth and the greatest rogue of all our rogues, I assure you.... I can assure you from experience, indeed. Of course, that is, if he is intelligent. But what am I saying! The romantic is always intelligent, and I only meant to observe that although we have had foolish romantics they don&#39;t count, and they were only so because in the flower of their youth they degenerated into Germans, and to preserve their precious jewel more comfortably, settled somewhere out there--by preference in Weimar or the Black Forest. I, for instance, genuinely despised my official work and did not openly abuse it simply because I was in it myself and got a salary for it. Anyway, take note, I did not openly abuse it. Our romantic would rather go out of his mind--a thing, however, which very rarely happens--than take to open abuse, unless he had some other career in view; and he is never kicked out. At most, they would take him to the lunatic asylum as &#34;the King of Spain&#34; if he should go very mad. But it is only the thin, fair people who go out of their minds in Russia. Innumerable &#34;romantics&#34; attain later in life to considerable rank in the service. Their many-sidedness is remarkable! And what a faculty they have for the most contradictory sensations! I was comforted by this thought even in those days, and I am of the same opinion now. That is why there are so many &#34;broad natures&#34; among us who never lose their ideal even in the depths of degradation; and though they never stir a finger for their ideal, though they are arrant thieves and knaves, yet they tearfully cherish their first ideal and are extraordinarily honest at heart. Yes, it is only among us that the most incorrigible rogue can be absolutely and loftily honest at heart without in the least ceasing to be a rogue. I repeat, our romantics, frequently, become such accomplished rascals (I use the term &#34;rascals&#34; affectionately), suddenly display such a sense of reality and practical knowledge that their bewildered superiors and the public generally can only ejaculate in amazement. Their many-sidedness is really amazing, and goodness knows what it may develop into later on, and what the future has in store for us. It is not a poor material! I do not say this from any foolish or boastful patriotism. But I feel sure that you are again imagining that I am joking. Or perhaps it&#39;s just the contrary and you are convinced that I really think so. Anyway, gentlemen, I shall welcome both views as an honour and a special favour. And do forgive my digression. I did not, of course, maintain friendly relations with my comrades and soon was at loggerheads with them, and in my youth and inexperience I even gave up bowing to them, as though I had cut off all relations. That, however, only happened to me once. As a rule, I was always alone. In the first place I spent most of my time at home, reading. I tried to stifle all that was continually seething within me by means of external impressions. And the only external means I had was reading. Reading, of course, was a great help--exciting me, giving me pleasure and pain. But at times it bored me fearfully. One longed for movement in spite of everything, and I plunged all at once into dark, underground, loathsome vice of the pettiest kind. My wretched passions were acute, smarting, from my continual, sickly irritability I had hysterical impulses, with tears and convulsions. I had no resource except reading, that is, there was nothing in my surroundings which I could respect and which attracted me. I was overwhelmed with depression, too; I had an hysterical craving for incongruity and for contrast, and so I took to vice. I have not said all this to justify myself.... But, no! I am lying. I did want to justify myself. I make that little observation for my own benefit, gentlemen. I don&#39;t want to lie. I vowed to myself I would not. And so, furtively, timidly, in solitude, at night, I indulged in filthy vice, with a feeling of shame which never deserted me, even at the most loathsome moments, and which at such moments nearly made me curse. Already even then I had my underground world in my soul. I was fearfully afraid of being seen, of being met, of being recognised. I visited various obscure haunts. One night as I was passing a tavern I saw through a lighted window some gentlemen fighting with billiard cues, and saw one of them thrown out of the window. At other times I should have felt very much disgusted, but I was in such a mood at the time, that I actually envied the gentleman thrown out of the window--and I envied him so much that I even went into the tavern and into the billiard-room. &#34;Perhaps,&#34; I thought, &#34;I&#39;ll have a fight, too, and they&#39;ll throw me out of the window.&#34; I was not drunk--but what is one to do--depression will drive a man to such a pitch of hysteria? But nothing happened. It seemed that I was not even equal to being thrown out of the window and I went away without having my fight. An officer put me in my place from the first moment. I was standing by the billiard-table and in my ignorance blocking up the way, and he wanted to pass; he took me by the shoulders and without a word--without a warning or explanation--moved me from where I was standing to another spot and passed by as though he had not noticed me. I could have forgiven blows, but I could not forgive his having moved me without noticing me. Devil knows what I would have given for a real regular quarrel--a more decent, a more LITERARY one, so to speak. I had been treated like a fly. This officer was over six foot, while I was a spindly little fellow. But the quarrel was in my hands. I had only to protest and I certainly would have been thrown out of the window. But I changed my mind and preferred to beat a resentful retreat. I went out of the tavern straight home, confused and troubled, and the next night I went out again with the same lewd intentions, still more furtively, abjectly and miserably than before, as it were, with tears in my eyes--but still I did go out again. Don&#39;t imagine, though, it was cowardice made me slink away from the officer; I never have been a coward at heart, though I have always been a coward in action. Don&#39;t be in a hurry to laugh--I assure you I can explain it all. Oh, if only that officer had been one of the sort who would consent to fight a duel! But no, he was one of those gentlemen (alas, long extinct!) who preferred fighting with cues or, like Gogol&#39;s Lieutenant Pirogov, appealing to the police. They did not fight duels and would have thought a duel with a civilian like me an utterly unseemly procedure in any case--and they looked upon the duel altogether as something impossible, something free-thinking and French. But they were quite ready to bully, especially when they were over six foot. I did not slink away through cowardice, but through an unbounded vanity. I was afraid not of his six foot, not of getting a sound thrashing and being thrown out of the window; I should have had physical courage enough, I assure you; but I had not the moral courage. What I was afraid of was that everyone present, from the insolent marker down to the lowest little stinking, pimply clerk in a greasy collar, would jeer at me and fail to understand when I began to protest and to address them in literary language. For of the point of honour--not of honour, but of the point of honour (POINT D&#39;HONNEUR)--one cannot speak among us except in literary language. You can&#39;t allude to the &#34;point of honour&#34; in ordinary language. I was fully convinced (the sense of reality, in spite of all my romanticism!) that they would all simply split their sides with laughter, and that the officer would not simply beat me, that is, without insulting me, but would certainly prod me in the back with his knee, kick me round the billiard-table, and only then perhaps have pity and drop me out of the window. Of course, this trivial incident could not with me end in that. I often met that officer afterwards in the street and noticed him very carefully. I am not quite sure whether he recognised me, I imagine not; I judge from certain signs. But I--I stared at him with spite and hatred and so it went on ... for several years! My resentment grew even deeper with years. At first I began making stealthy inquiries about this officer. It was difficult for me to do so, for I knew no one. But one day I heard someone shout his surname in the street as I was following him at a distance, as though I were tied to him--and so I learnt his surname. Another time I followed him to his flat, and for ten kopecks learned from the porter where he lived, on which storey, whether he lived alone or with others, and so on--in fact, everything one could learn from a porter. One morning, though I had never tried my hand with the pen, it suddenly occurred to me to write a satire on this officer in the form of a novel which would unmask his villainy. I wrote the novel with relish. I did unmask his villainy, I even exaggerated it; at first I so altered his surname that it could easily be recognised, but on second thoughts I changed it, and sent the story to the OTETCHESTVENNIYA ZAPISKI. But at that time such attacks were not the fashion and my story was not printed. That was a great vexation to me. Sometimes I was positively choked with resentment. At last I determined to challenge my enemy to a duel. I composed a splendid, charming letter to him, imploring him to apologise to me, and hinting rather plainly at a duel in case of refusal. The letter was so composed that if the officer had had the least understanding of the sublime and the beautiful he would certainly have flung himself on my neck and have offered me his friendship. And how fine that would have been! How we should have got on together! &#34;He could have shielded me with his higher rank, while I could have improved his mind with my culture, and, well ... my ideas, and all sorts of things might have happened.&#34; Only fancy, this was two years after his insult to me, and my challenge would have been a ridiculous anachronism, in spite of all the ingenuity of my letter in disguising and explaining away the anachronism. But, thank God (to this day I thank the Almighty with tears in my eyes) I did not send the letter to him. Cold shivers run down my back when I think of what might have happened if I had sent it. And all at once I revenged myself in the simplest way, by a stroke of genius! A brilliant thought suddenly dawned upon me. Sometimes on holidays I used to stroll along the sunny side of the Nevsky about four o&#39;clock in the afternoon. Though it was hardly a stroll so much as a series of innumerable miseries, humiliations and resentments; but no doubt that was just what I wanted. I used to wriggle along in a most unseemly fashion, like an eel, continually moving aside to make way for generals, for officers of the guards and the hussars, or for ladies. At such minutes there used to be a convulsive twinge at my heart, and I used to feel hot all down my back at the mere thought of the wretchedness of my attire, of the wretchedness and abjectness of my little scurrying figure. This was a regular martyrdom, a continual, intolerable humiliation at the thought, which passed into an incessant and direct sensation, that I was a mere fly in the eyes of all this world, a nasty, disgusting fly--more intelligent, more highly developed, more refined in feeling than any of them, of course--but a fly that was continually making way for everyone, insulted and injured by everyone. Why I inflicted this torture upon myself, why I went to the Nevsky, I don&#39;t know. I felt simply drawn there at every possible opportunity. Already then I began to experience a rush of the enjoyment of which I spoke in the first chapter. After my affair with the officer I felt even more drawn there than before: it was on the Nevsky that I met him most frequently, there I could admire him. He, too, went there chiefly on holidays, He, too, turned out of his path for generals and persons of high rank, and he too, wriggled between them like an eel; but people, like me, or even better dressed than me, he simply walked over; he made straight for them as though there was nothing but empty space before him, and never, under any circumstances, turned aside. I gloated over my resentment watching him and ... always resentfully made way for him. It exasperated me that even in the street I could not be on an even footing with him. &#34;Why must you invariably be the first to move aside?&#34; I kept asking myself in hysterical rage, waking up sometimes at three o&#39;clock in the morning. &#34;Why is it you and not he? There&#39;s no regulation about it; there&#39;s no written law. Let the making way be equal as it usually is when refined people meet; he moves half-way and you move half-way; you pass with mutual respect.&#34; But that never happened, and I always moved aside, while he did not even notice my making way for him. And lo and behold a bright idea dawned upon me! &#34;What,&#34; I thought, &#34;if I meet him and don&#39;t move on one side? What if I don&#39;t move aside on purpose, even if I knock up against him? How would that be?&#34; This audacious idea took such a hold on me that it gave me no peace. I was dreaming of it continually, horribly, and I purposely went more frequently to the Nevsky in order to picture more vividly how I should do it when I did do it. I was delighted. This intention seemed to me more and more practical and possible. &#34;Of course I shall not really push him,&#34; I thought, already more good-natured in my joy. &#34;I will simply not turn aside, will run up against him, not very violently, but just shouldering each other--just as much as decency permits. I will push against him just as much as he pushes against me.&#34; At last I made up my mind completely. But my preparations took a great deal of time. To begin with, when I carried out my plan I should need to be looking rather more decent, and so I had to think of my get-up. &#34;In case of emergency, if, for instance, there were any sort of public scandal (and the public there is of the most RECHERCHE: the Countess walks there; Prince D. walks there; all the literary world is there), I must be well dressed; that inspires respect and of itself puts us on an equal footing in the eyes of the society.&#34; With this object I asked for some of my salary in advance, and bought at Tchurkin&#39;s a pair of black gloves and a decent hat. Black gloves seemed to me both more dignified and BON TON than the lemon-coloured ones which I had contemplated at first. &#34;The colour is too gaudy, it looks as though one were trying to be conspicuous,&#34; and I did not take the lemon-coloured ones. I had got ready long beforehand a good shirt, with white bone studs; my overcoat was the only thing that held me back. The coat in itself was a very good one, it kept me warm; but it was wadded and it had a raccoon collar which was the height of vulgarity. I had to change the collar at any sacrifice, and to have a beaver one like an officer&#39;s. For this purpose I began visiting the Gostiny Dvor and after several attempts I pitched upon a piece of cheap German beaver. Though these German beavers soon grow shabby and look wretched, yet at first they look exceedingly well, and I only needed it for the occasion. I asked the price; even so, it was too expensive. After thinking it over thoroughly I decided to sell my raccoon collar. The rest of the money--a considerable sum for me, I decided to borrow from Anton Antonitch Syetotchkin, my immediate superior, an unassuming person, though grave and judicious. He never lent money to anyone, but I had, on entering the service, been specially recommended to him by an important personage who had got me my berth. I was horribly worried. To borrow from Anton Antonitch seemed to me monstrous and shameful. I did not sleep for two or three nights. Indeed, I did not sleep well at that time, I was in a fever; I had a vague sinking at my heart or else a sudden throbbing, throbbing, throbbing! Anton Antonitch was surprised at first, then he frowned, then he reflected, and did after all lend me the money, receiving from me a written authorisation to take from my salary a fortnight later the sum that he had lent me. In this way everything was at last ready. The handsome beaver replaced the mean-looking raccoon, and I began by degrees to get to work. It would never have done to act offhand, at random; the plan had to be carried out skilfully, by degrees. But I must confess that after many efforts I began to despair: we simply could not run into each other. I made every preparation, I was quite determined--it seemed as though we should run into one another directly--and before I knew what I was doing I had stepped aside for him again and he had passed without noticing me. I even prayed as I approached him that God would grant me determination. One time I had made up my mind thoroughly, but it ended in my stumbling and falling at his feet because at the very last instant when I was six inches from him my courage failed me. He very calmly stepped over me, while I flew on one side like a ball. That night I was ill again, feverish and delirious. And suddenly it ended most happily. The night before I had made up my mind not to carry out my fatal plan and to abandon it all, and with that object I went to the Nevsky for the last time, just to see how I would abandon it all. Suddenly, three paces from my enemy, I unexpectedly made up my mind--I closed my eyes, and we ran full tilt, shoulder to shoulder, against one another! I did not budge an inch and passed him on a perfectly equal footing! He did not even look round and pretended not to notice it; but he was only pretending, I am convinced of that. I am convinced of that to this day! Of course, I got the worst of it--he was stronger, but that was not the point. The point was that I had attained my object, I had kept up my dignity, I had not yielded a step, and had put myself publicly on an equal social footing with him. I returned home feeling that I was fully avenged for everything. I was delighted. I was triumphant and sang Italian arias. Of course, I will not describe to you what happened to me three days later; if you have read my first chapter you can guess for yourself. The officer was afterwards transferred; I have not seen him now for fourteen years. What is the dear fellow doing now? Whom is he walking over? II But the period of my dissipation would end and I always felt very sick afterwards. It was followed by remorse--I tried to drive it away; I felt too sick. By degrees, however, I grew used to that too. I grew used to everything, or rather I voluntarily resigned myself to enduring it. But I had a means of escape that reconciled everything--that was to find refuge in &#34;the sublime and the beautiful,&#34; in dreams, of course. I was a terrible dreamer, I would dream for three months on end, tucked away in my corner, and you may believe me that at those moments I had no resemblance to the gentleman who, in the perturbation of his chicken heart, put a collar of German beaver on his great-coat. I suddenly became a hero. I would not have admitted my six-foot lieutenant even if he had called on me. I could not even picture him before me then. What were my dreams and how I could satisfy myself with them--it is hard to say now, but at the time I was satisfied with them. Though, indeed, even now, I am to some extent satisfied with them. Dreams were particularly sweet and vivid after a spell of dissipation; they came with remorse and with tears, with curses and transports. There were moments of such positive intoxication, of such happiness, that there was not the faintest trace of irony within me, on my honour. I had faith, hope, love. I believed blindly at such times that by some miracle, by some external circumstance, all this would suddenly open out, expand; that suddenly a vista of suitable activity--beneficent, good, and, above all, READY MADE (what sort of activity I had no idea, but the great thing was that it should be all ready for me)--would rise up before me--and I should come out into the light of day, almost riding a white horse and crowned with laurel. Anything but the foremost place I could not conceive for myself, and for that very reason I quite contentedly occupied the lowest in reality. Either to be a hero or to grovel in the mud--there was nothing between. That was my ruin, for when I was in the mud I comforted myself with the thought that at other times I was a hero, and the hero was a cloak for the mud: for an ordinary man it was shameful to defile himself, but a hero was too lofty to be utterly defiled, and so he might defile himself. It is worth noting that these attacks of the &#34;sublime and the beautiful&#34; visited me even during the period of dissipation and just at the times when I was touching the bottom. They came in separate spurts, as though reminding me of themselves, but did not banish the dissipation by their appearance. On the contrary, they seemed to add a zest to it by contrast, and were only sufficiently present to serve as an appetising sauce. That sauce was made up of contradictions and sufferings, of agonising inward analysis, and all these pangs and pin-pricks gave a certain piquancy, even a significance to my dissipation--in fact, completely answered the purpose of an appetising sauce. There was a certain depth of meaning in it. And I could hardly have resigned myself to the simple, vulgar, direct debauchery of a clerk and have endured all the filthiness of it. What could have allured me about it then and have drawn me at night into the street? No, I had a lofty way of getting out of it all. And what loving-kindness, oh Lord, what loving-kindness I felt at times in those dreams of mine! in those &#34;flights into the sublime and the beautiful&#34;; though it was fantastic love, though it was never applied to anything human in reality, yet there was so much of this love that one did not feel afterwards even the impulse to apply it in reality; that would have been superfluous. Everything, however, passed satisfactorily by a lazy and fascinating transition into the sphere of art, that is, into the beautiful forms of life, lying ready, largely stolen from the poets and novelists and adapted to all sorts of needs and uses. I, for instance, was triumphant over everyone; everyone, of course, was in dust and ashes, and was forced spontaneously to recognise my superiority, and I forgave them all. I was a poet and a grand gentleman, I fell in love; I came in for countless millions and immediately devoted them to humanity, and at the same time I confessed before all the people my shameful deeds, which, of course, were not merely shameful, but had in them much that was &#34;sublime and beautiful&#34; something in the Manfred style. Everyone would kiss me and weep (what idiots they would be if they did not), while I should go barefoot and hungry preaching new ideas and fighting a victorious Austerlitz against the obscurantists. Then the band would play a march, an amnesty would be declared, the Pope would agree to retire from Rome to Brazil; then there would be a ball for the whole of Italy at the Villa Borghese on the shores of Lake Como, Lake Como being for that purpose transferred to the neighbourhood of Rome; then would come a scene in the bushes, and so on, and so on--as though you did not know all about it? You will say that it is vulgar and contemptible to drag all this into public after all the tears and transports which I have myself confessed. But why is it contemptible? Can you imagine that I am ashamed of it all, and that it was stupider than anything in your life, gentlemen? And I can assure you that some of these fancies were by no means badly composed.... It did not all happen on the shores of Lake Como. And yet you are right--it really is vulgar and contemptible. And most contemptible of all it is that now I am attempting to justify myself to you. And even more contemptible than that is my making this remark now. But that&#39;s enough, or there will be no end to it; each step will be more contemptible than the last.... I could never stand more than three months of dreaming at a time without feeling an irresistible desire to plunge into society. To plunge into society meant to visit my superior at the office, Anton Antonitch Syetotchkin. He was the only permanent acquaintance I have had in my life, and I wonder at the fact myself now. But I only went to see him when that phase came over me, and when my dreams had reached such a point of bliss that it became essential at once to embrace my fellows and all mankind; and for that purpose I needed, at least, one human being, actually existing. I had to call on Anton Antonitch, however, on Tuesday--his at-home day; so I had always to time my passionate desire to embrace humanity so that it might fall on a Tuesday. This Anton Antonitch lived on the fourth storey in a house in Five Corners, in four low-pitched rooms, one smaller than the other, of a particularly frugal and sallow appearance. He had two daughters and their aunt, who used to pour out the tea. Of the daughters one was thirteen and another fourteen, they both had snub noses, and I was awfully shy of them because they were always whispering and giggling together. The master of the house usually sat in his study on a leather couch in front of the table with some grey-headed gentleman, usually a colleague from our office or some other department. I never saw more than two or three visitors there, always the same. They talked about the excise duty; about business in the senate, about salaries, about promotions, about His Excellency, and the best means of pleasing him, and so on. I had the patience to sit like a fool beside these people for four hours at a stretch, listening to them without knowing what to say to them or venturing to say a word. I became stupefied, several times I felt myself perspiring, I was overcome by a sort of paralysis; but this was pleasant and good for me. On returning home I deferred for a time my desire to embrace all mankind. I had however one other acquaintance of a sort, Simonov, who was an old schoolfellow. I had a number of schoolfellows, indeed, in Petersburg, but I did not associate with them and had even given up nodding to them in the street. I believe I had transferred into the department I was in simply to avoid their company and to cut off all connection with my hateful childhood. Curses on that school and all those terrible years of penal servitude! In short, I parted from my schoolfellows as soon as I got out into the world. There were two or three left to whom I nodded in the street. One of them was Simonov, who had in no way been distinguished at school, was of a quiet and equable disposition; but I discovered in him a certain independence of character and even honesty I don&#39;t even suppose that he was particularly stupid. I had at one time spent some rather soulful moments with him, but these had not lasted long and had somehow been suddenly clouded over. He was evidently uncomfortable at these reminiscences, and was, I fancy, always afraid that I might take up the same tone again. I suspected that he had an aversion for me, but still I went on going to see him, not being quite certain of it. And so on one occasion, unable to endure my solitude and knowing that as it was Thursday Anton Antonitch&#39;s door would be closed, I thought of Simonov. Climbing up to his fourth storey I was thinking that the man disliked me and that it was a mistake to go and see him. But as it always happened that such reflections impelled me, as though purposely, to put myself into a false position, I went in. It was almost a year since I had last seen Simonov. III I found two of my old schoolfellows with him. They seemed to be discussing an important matter. All of them took scarcely any notice of my entrance, which was strange, for I had not met them for years. Evidently they looked upon me as something on the level of a common fly. I had not been treated like that even at school, though they all hated me. I knew, of course, that they must despise me now for my lack of success in the service, and for my having let myself sink so low, going about badly dressed and so on--which seemed to them a sign of my incapacity and insignificance. But I had not expected such contempt. Simonov was positively surprised at my turning up. Even in old days he had always seemed surprised at my coming. All this disconcerted me: I sat down, feeling rather miserable, and began listening to what they were saying. They were engaged in warm and earnest conversation about a farewell dinner which they wanted to arrange for the next day to a comrade of theirs called Zverkov, an officer in the army, who was going away to a distant province. This Zverkov had been all the time at school with me too. I had begun to hate him particularly in the upper forms. In the lower forms he had simply been a pretty, playful boy whom everybody liked. I had hated him, however, even in the lower forms, just because he was a pretty and playful boy. He was always bad at his lessons and got worse and worse as he went on; however, he left with a good certificate, as he had powerful interests. During his last year at school he came in for an estate of two hundred serfs, and as almost all of us were poor he took up a swaggering tone among us. He was vulgar in the extreme, but at the same time he was a good-natured fellow, even in his swaggering. In spite of superficial, fantastic and sham notions of honour and dignity, all but very few of us positively grovelled before Zverkov, and the more so the more he swaggered. And it was not from any interested motive that they grovelled, but simply because he had been favoured by the gifts of nature. Moreover, it was, as it were, an accepted idea among us that Zverkov was a specialist in regard to tact and the social graces. This last fact particularly infuriated me. I hated the abrupt self-confident tone of his voice, his admiration of his own witticisms, which were often frightfully stupid, though he was bold in his language; I hated his handsome, but stupid face (for which I would, however, have gladly exchanged my intelligent one), and the free-and-easy military manners in fashion in the &#34;&#39;forties.&#34; I hated the way in which he used to talk of his future conquests of women (he did not venture to begin his attack upon women until he had the epaulettes of an officer, and was looking forward to them with impatience), and boasted of the duels he would constantly be fighting. I remember how I, invariably so taciturn, suddenly fastened upon Zverkov, when one day talking at a leisure moment with his schoolfellows of his future relations with the fair sex, and growing as sportive as a puppy in the sun, he all at once declared that he would not leave a single village girl on his estate unnoticed, that that was his DROIT DE SEIGNEUR, and that if the peasants dared to protest he would have them all flogged and double the tax on them, the bearded rascals. Our servile rabble applauded, but I attacked him, not from compassion for the girls and their fathers, but simply because they were applauding such an insect. I got the better of him on that occasion, but though Zverkov was stupid he was lively and impudent, and so laughed it off, and in such a way that my victory was not really complete; the laugh was on his side. He got the better of me on several occasions afterwards, but without malice, jestingly, casually. I remained angrily and contemptuously silent and would not answer him. When we left school he made advances to me; I did not rebuff them, for I was flattered, but we soon parted and quite naturally. Afterwards I heard of his barrack-room success as a lieutenant, and of the fast life he was leading. Then there came other rumours--of his successes in the service. By then he had taken to cutting me in the street, and I suspected that he was afraid of compromising himself by greeting a personage as insignificant as me. I saw him once in the theatre, in the third tier of boxes. By then he was wearing shoulder-straps. He was twisting and twirling about, ingratiating himself with the daughters of an ancient General. In three years he had gone off considerably, though he was still rather handsome and adroit. One could see that by the time he was thirty he would be corpulent. So it was to this Zverkov that my schoolfellows were going to give a dinner on his departure. They had kept up with him for those three years, though privately they did not consider themselves on an equal footing with him, I am convinced of that. Of Simonov&#39;s two visitors, one was Ferfitchkin, a Russianised German--a little fellow with the face of a monkey, a blockhead who was always deriding everyone, a very bitter enemy of mine from our days in the lower forms--a vulgar, impudent, swaggering fellow, who affected a most sensitive feeling of personal honour, though, of course, he was a wretched little coward at heart. He was one of those worshippers of Zverkov who made up to the latter from interested motives, and often borrowed money from him. Simonov&#39;s other visitor, Trudolyubov, was a person in no way remarkable--a tall young fellow, in the army, with a cold face, fairly honest, though he worshipped success of every sort, and was only capable of thinking of promotion. He was some sort of distant relation of Zverkov&#39;s, and this, foolish as it seems, gave him a certain importance among us. He always thought me of no consequence whatever; his behaviour to me, though not quite courteous, was tolerable. &#34;Well, with seven roubles each,&#34; said Trudolyubov, &#34;twenty-one roubles between the three of us, we ought to be able to get a good dinner. Zverkov, of course, won&#39;t pay.&#34; &#34;Of course not, since we are inviting him,&#34; Simonov decided. &#34;Can you imagine,&#34; Ferfitchkin interrupted hotly and conceitedly, like some insolent flunkey boasting of his master the General&#39;s decorations, &#34;can you imagine that Zverkov will let us pay alone? He will accept from delicacy, but he will order half a dozen bottles of champagne.&#34; &#34;Do we want half a dozen for the four of us?&#34; observed Trudolyubov, taking notice only of the half dozen. &#34;So the three of us, with Zverkov for the fourth, twenty-one roubles, at the Hotel de Paris at five o&#39;clock tomorrow,&#34; Simonov, who had been asked to make the arrangements, concluded finally. &#34;How twenty-one roubles?&#34; I asked in some agitation, with a show of being offended; &#34;if you count me it will not be twenty-one, but twenty-eight roubles.&#34; It seemed to me that to invite myself so suddenly and unexpectedly would be positively graceful, and that they would all be conquered at once and would look at me with respect. &#34;Do you want to join, too?&#34; Simonov observed, with no appearance of pleasure, seeming to avoid looking at me. He knew me through and through. It infuriated me that he knew me so thoroughly. &#34;Why not? I am an old schoolfellow of his, too, I believe, and I must own I feel hurt that you have left me out,&#34; I said, boiling over again. &#34;And where were we to find you?&#34; Ferfitchkin put in roughly. &#34;You never were on good terms with Zverkov,&#34; Trudolyubov added, frowning. But I had already clutched at the idea and would not give it up. &#34;It seems to me that no one has a right to form an opinion upon that,&#34; I retorted in a shaking voice, as though something tremendous had happened. &#34;Perhaps that is just my reason for wishing it now, that I have not always been on good terms with him.&#34; &#34;Oh, there&#39;s no making you out ... with these refinements,&#34; Trudolyubov jeered. &#34;We&#39;ll put your name down,&#34; Simonov decided, addressing me. &#34;Tomorrow at five-o&#39;clock at the Hotel de Paris.&#34; &#34;What about the money?&#34; Ferfitchkin began in an undertone, indicating me to Simonov, but he broke off, for even Simonov was embarrassed. &#34;That will do,&#34; said Trudolyubov, getting up. &#34;If he wants to come so much, let him.&#34; &#34;But it&#39;s a private thing, between us friends,&#34; Ferfitchkin said crossly, as he, too, picked up his hat. &#34;It&#39;s not an official gathering.&#34; &#34;We do not want at all, perhaps ...&#34; They went away. Ferfitchkin did not greet me in any way as he went out, Trudolyubov barely nodded. Simonov, with whom I was left TETE-A-TETE, was in a state of vexation and perplexity, and looked at me queerly. He did not sit down and did not ask me to. &#34;H&#39;m ... yes ... tomorrow, then. Will you pay your subscription now? I just ask so as to know,&#34; he muttered in embarrassment. I flushed crimson, as I did so I remembered that I had owed Simonov fifteen roubles for ages--which I had, indeed, never forgotten, though I had not paid it. &#34;You will understand, Simonov, that I could have no idea when I came here.... I am very much vexed that I have forgotten....&#34; &#34;All right, all right, that doesn&#39;t matter. You can pay tomorrow after the dinner. I simply wanted to know.... Please don&#39;t...&#34; He broke off and began pacing the room still more vexed. As he walked he began to stamp with his heels. &#34;Am I keeping you?&#34; I asked, after two minutes of silence. &#34;Oh!&#34; he said, starting, &#34;that is--to be truthful--yes. I have to go and see someone ... not far from here,&#34; he added in an apologetic voice, somewhat abashed. &#34;My goodness, why didn&#39;t you say so?&#34; I cried, seizing my cap, with an astonishingly free-and-easy air, which was the last thing I should have expected of myself. &#34;It&#39;s close by ... not two paces away,&#34; Simonov repeated, accompanying me to the front door with a fussy air which did not suit him at all. &#34;So five o&#39;clock, punctually, tomorrow,&#34; he called down the stairs after me. He was very glad to get rid of me. I was in a fury. &#34;What possessed me, what possessed me to force myself upon them?&#34; I wondered, grinding my teeth as I strode along the street, &#34;for a scoundrel, a pig like that Zverkov! Of course I had better not go; of course, I must just snap my fingers at them. I am not bound in any way. I&#39;ll send Simonov a note by tomorrow&#39;s post....&#34; But what made me furious was that I knew for certain that I should go, that I should make a point of going; and the more tactless, the more unseemly my going would be, the more certainly I would go. And there was a positive obstacle to my going: I had no money. All I had was nine roubles, I had to give seven of that to my servant, Apollon, for his monthly wages. That was all I paid him--he had to keep himself. Not to pay him was impossible, considering his character. But I will talk about that fellow, about that plague of mine, another time. However, I knew I should go and should not pay him his wages. That night I had the most hideous dreams. No wonder; all the evening I had been oppressed by memories of my miserable days at school, and I could not shake them off. I was sent to the school by distant relations, upon whom I was dependent and of whom I have heard nothing since--they sent me there a forlorn, silent boy, already crushed by their reproaches, already troubled by doubt, and looking with savage distrust at everyone. My schoolfellows met me with spiteful and merciless jibes because I was not like any of them. But I could not endure their taunts; I could not give in to them with the ignoble readiness with which they gave in to one another. I hated them from the first, and shut myself away from everyone in timid, wounded and disproportionate pride. Their coarseness revolted me. They laughed cynically at my face, at my clumsy figure; and yet what stupid faces they had themselves. In our school the boys&#39; faces seemed in a special way to degenerate and grow stupider. How many fine-looking boys came to us! In a few years they became repulsive. Even at sixteen I wondered at them morosely; even then I was struck by the pettiness of their thoughts, the stupidity of their pursuits, their games, their conversations. They had no understanding of such essential things, they took no interest in such striking, impressive subjects, that I could not help considering them inferior to myself. It was not wounded vanity that drove me to it, and for God&#39;s sake do not thrust upon me your hackneyed remarks, repeated to nausea, that &#34;I was only a dreamer,&#34; while they even then had an understanding of life. They understood nothing, they had no idea of real life, and I swear that that was what made me most indignant with them. On the contrary, the most obvious, striking reality they accepted with fantastic stupidity and even at that time were accustomed to respect success. Everything that was just, but oppressed and looked down upon, they laughed at heartlessly and shamefully. They took rank for intelligence; even at sixteen they were already talking about a snug berth. Of course, a great deal of it was due to their stupidity, to the bad examples with which they had always been surrounded in their childhood and boyhood. They were monstrously depraved. Of course a great deal of that, too, was superficial and an assumption of cynicism; of course there were glimpses of youth and freshness even in their depravity; but even that freshness was not attractive, and showed itself in a certain rakishness. I hated them horribly, though perhaps I was worse than any of them. They repaid me in the same way, and did not conceal their aversion for me. But by then I did not desire their affection: on the contrary, I continually longed for their humiliation. To escape from their derision I purposely began to make all the progress I could with my studies and forced my way to the very top. This impressed them. Moreover, they all began by degrees to grasp that I had already read books none of them could read, and understood things (not forming part of our school curriculum) of which they had not even heard. They took a savage and sarcastic view of it, but were morally impressed, especially as the teachers began to notice me on those grounds. The mockery ceased, but the hostility remained, and cold and strained relations became permanent between us. In the end I could not put up with it: with years a craving for society, for friends, developed in me. I attempted to get on friendly terms with some of my schoolfellows; but somehow or other my intimacy with them was always strained and soon ended of itself. Once, indeed, I did have a friend. But I was already a tyrant at heart; I wanted to exercise unbounded sway over him; I tried to instil into him a contempt for his surroundings; I required of him a disdainful and complete break with those surroundings. I frightened him with my passionate affection; I reduced him to tears, to hysterics. He was a simple and devoted soul; but when he devoted himself to me entirely I began to hate him immediately and repulsed him--as though all I needed him for was to win a victory over him, to subjugate him and nothing else. But I could not subjugate all of them; my friend was not at all like them either, he was, in fact, a rare exception. The first thing I did on leaving school was to give up the special job for which I had been destined so as to break all ties, to curse my past and shake the dust from off my feet.... And goodness knows why, after all that, I should go trudging off to Simonov&#39;s! Early next morning I roused myself and jumped out of bed with excitement, as though it were all about to happen at once. But I believed that some radical change in my life was coming, and would inevitably come that day. Owing to its rarity, perhaps, any external event, however trivial, always made me feel as though some radical change in my life were at hand. I went to the office, however, as usual, but sneaked away home two hours earlier to get ready. The great thing, I thought, is not to be the first to arrive, or they will think I am overjoyed at coming. But there were thousands of such great points to consider, and they all agitated and overwhelmed me. I polished my boots a second time with my own hands; nothing in the world would have induced Apollon to clean them twice a day, as he considered that it was more than his duties required of him. I stole the brushes to clean them from the passage, being careful he should not detect it, for fear of his contempt. Then I minutely examined my clothes and thought that everything looked old, worn and threadbare. I had let myself get too slovenly. My uniform, perhaps, was tidy, but I could not go out to dinner in my uniform. The worst of it was that on the knee of my trousers was a big yellow stain. I had a foreboding that that stain would deprive me of nine-tenths of my personal dignity. I knew, too, that it was very poor to think so. &#34;But this is no time for thinking: now I am in for the real thing,&#34; I thought, and my heart sank. I knew, too, perfectly well even then, that I was monstrously exaggerating the facts. But how could I help it? I could not control myself and was already shaking with fever. With despair I pictured to myself how coldly and disdainfully that &#34;scoundrel&#34; Zverkov would meet me; with what dull-witted, invincible contempt the blockhead Trudolyubov would look at me; with what impudent rudeness the insect Ferfitchkin would snigger at me in order to curry favour with Zverkov; how completely Simonov would take it all in, and how he would despise me for the abjectness of my vanity and lack of spirit--and, worst of all, how paltry, UNLITERARY, commonplace it would all be. Of course, the best thing would be not to go at all. But that was most impossible of all: if I feel impelled to do anything, I seem to be pitchforked into it. I should have jeered at myself ever afterwards: &#34;So you funked it, you funked it, you funked the REAL THING!&#34; On the contrary, I passionately longed to show all that &#34;rabble&#34; that I was by no means such a spiritless creature as I seemed to myself. What is more, even in the acutest paroxysm of this cowardly fever, I dreamed of getting the upper hand, of dominating them, carrying them away, making them like me--if only for my &#34;elevation of thought and unmistakable wit.&#34; They would abandon Zverkov, he would sit on one side, silent and ashamed, while I should crush him. Then, perhaps, we would be reconciled and drink to our everlasting friendship; but what was most bitter and humiliating for me was that I knew even then, knew fully and for certain, that I needed nothing of all this really, that I did not really want to crush, to subdue, to attract them, and that I did not care a straw really for the result, even if I did achieve it. Oh, how I prayed for the day to pass quickly! In unutterable anguish I went to the window, opened the movable pane and looked out into the troubled darkness of the thickly falling wet snow. At last my wretched little clock hissed out five. I seized my hat and, trying not to look at Apollon, who had been all day expecting his month&#39;s wages, but in his foolishness was unwilling to be the first to speak about it, I slipped between him and the door and, jumping into a high-class sledge, on which I spent my last half rouble, I drove up in grand style to the Hotel de Paris. IV I had been certain the day before that I should be the first to arrive. But it was not a question of being the first to arrive. Not only were they not there, but I had difficulty in finding our room. The table was not laid even. What did it mean? After a good many questions I elicited from the waiters that the dinner had been ordered not for five, but for six o&#39;clock. This was confirmed at the buffet too. I felt really ashamed to go on questioning them. It was only twenty-five minutes past five. If they changed the dinner hour they ought at least to have let me know--that is what the post is for, and not to have put me in an absurd position in my own eyes and ... and even before the waiters. I sat down; the servant began laying the table; I felt even more humiliated when he was present. Towards six o&#39;clock they brought in candles, though there were lamps burning in the room. It had not occurred to the waiter, however, to bring them in at once when I arrived. In the next room two gloomy, angry-looking persons were eating their dinners in silence at two different tables. There was a great deal of noise, even shouting, in a room further away; one could hear the laughter of a crowd of people, and nasty little shrieks in French: there were ladies at the dinner. It was sickening, in fact. I rarely passed more unpleasant moments, so much so that when they did arrive all together punctually at six I was overjoyed to see them, as though they were my deliverers, and even forgot that it was incumbent upon me to show resentment. Zverkov walked in at the head of them; evidently he was the leading spirit. He and all of them were laughing; but, seeing me, Zverkov drew himself up a little, walked up to me deliberately with a slight, rather jaunty bend from the waist. He shook hands with me in a friendly, but not over-friendly, fashion, with a sort of circumspect courtesy like that of a General, as though in giving me his hand he were warding off something. I had imagined, on the contrary, that on coming in he would at once break into his habitual thin, shrill laugh and fall to making his insipid jokes and witticisms. I had been preparing for them ever since the previous day, but I had not expected such condescension, such high-official courtesy. So, then, he felt himself ineffably superior to me in every respect! If he only meant to insult me by that high-official tone, it would not matter, I thought--I could pay him back for it one way or another. But what if, in reality, without the least desire to be offensive, that sheepshead had a notion in earnest that he was superior to me and could only look at me in a patronising way? The very supposition made me gasp. &#34;I was surprised to hear of your desire to join us,&#34; he began, lisping and drawling, which was something new. &#34;You and I seem to have seen nothing of one another. You fight shy of us. You shouldn&#39;t. We are not such terrible people as you think. Well, anyway, I am glad to renew our acquaintance.&#34; And he turned carelessly to put down his hat on the window. &#34;Have you been waiting long?&#34; Trudolyubov inquired. &#34;I arrived at five o&#39;clock as you told me yesterday,&#34; I answered aloud, with an irritability that threatened an explosion. &#34;Didn&#39;t you let him know that we had changed the hour?&#34; said Trudolyubov to Simonov. &#34;No, I didn&#39;t. I forgot,&#34; the latter replied, with no sign of regret, and without even apologising to me he went off to order the HORS D&#39;OEUVRE. &#34;So you&#39;ve been here a whole hour? Oh, poor fellow!&#34; Zverkov cried ironically, for to his notions this was bound to be extremely funny. That rascal Ferfitchkin followed with his nasty little snigger like a puppy yapping. My position struck him, too, as exquisitely ludicrous and embarrassing. &#34;It isn&#39;t funny at all!&#34; I cried to Ferfitchkin, more and more irritated. &#34;It wasn&#39;t my fault, but other people&#39;s. They neglected to let me know. It was ... it was ... it was simply absurd.&#34; &#34;It&#39;s not only absurd, but something else as well,&#34; muttered Trudolyubov, naively taking my part. &#34;You are not hard enough upon it. It was simply rudeness--unintentional, of course. And how could Simonov ... h&#39;m!&#34; &#34;If a trick like that had been played on me,&#34; observed Ferfitchkin, &#34;I should ...&#34; &#34;But you should have ordered something for yourself,&#34; Zverkov interrupted, &#34;or simply asked for dinner without waiting for us.&#34; &#34;You will allow that I might have done that without your permission,&#34; I rapped out. &#34;If I waited, it was ...&#34; &#34;Let us sit down, gentlemen,&#34; cried Simonov, coming in. &#34;Everything is ready; I can answer for the champagne; it is capitally frozen.... You see, I did not know your address, where was I to look for you?&#34; he suddenly turned to me, but again he seemed to avoid looking at me. Evidently he had something against me. It must have been what happened yesterday. All sat down; I did the same. It was a round table. Trudolyubov was on my left, Simonov on my right, Zverkov was sitting opposite, Ferfitchkin next to him, between him and Trudolyubov. &#34;Tell me, are you ... in a government office?&#34; Zverkov went on attending to me. Seeing that I was embarrassed he seriously thought that he ought to be friendly to me, and, so to speak, cheer me up. &#34;Does he want me to throw a bottle at his head?&#34; I thought, in a fury. In my novel surroundings I was unnaturally ready to be irritated. &#34;In the N- office,&#34; I answered jerkily, with my eyes on my plate. &#34;And ha-ave you a go-od berth? I say, what ma-a-de you leave your original job?&#34; &#34;What ma-a-de me was that I wanted to leave my original job,&#34; I drawled more than he, hardly able to control myself. Ferfitchkin went off into a guffaw. Simonov looked at me ironically. Trudolyubov left off eating and began looking at me with curiosity. Zverkov winced, but he tried not to notice it. &#34;And the remuneration?&#34; &#34;What remuneration?&#34; &#34;I mean, your sa-a-lary?&#34; &#34;Why are you cross-examining me?&#34; However, I told him at once what my salary was. I turned horribly red. &#34;It is not very handsome,&#34; Zverkov observed majestically. &#34;Yes, you can&#39;t afford to dine at cafes on that,&#34; Ferfitchkin added insolently. &#34;To my thinking it&#39;s very poor,&#34; Trudolyubov observed gravely. &#34;And how thin you have grown! How you have changed!&#34; added Zverkov, with a shade of venom in his voice, scanning me and my attire with a sort of insolent compassion. &#34;Oh, spare his blushes,&#34; cried Ferfitchkin, sniggering. &#34;My dear sir, allow me to tell you I am not blushing,&#34; I broke out at last; &#34;do you hear? I am dining here, at this cafe, at my own expense, not at other people&#39;s--note that, Mr. Ferfitchkin.&#34; &#34;Wha-at? Isn&#39;t every one here dining at his own expense? You would seem to be ...&#34; Ferfitchkin flew out at me, turning as red as a lobster, and looking me in the face with fury. &#34;Tha-at,&#34; I answered, feeling I had gone too far, &#34;and I imagine it would be better to talk of something more intelligent.&#34; &#34;You intend to show off your intelligence, I suppose?&#34; &#34;Don&#39;t disturb yourself, that would be quite out of place here.&#34; &#34;Why are you clacking away like that, my good sir, eh? Have you gone out of your wits in your office?&#34; &#34;Enough, gentlemen, enough!&#34; Zverkov cried, authoritatively. &#34;How stupid it is!&#34; muttered Simonov. &#34;It really is stupid. We have met here, a company of friends, for a farewell dinner to a comrade and you carry on an altercation,&#34; said Trudolyubov, rudely addressing himself to me alone. &#34;You invited yourself to join us, so don&#39;t disturb the general harmony.&#34; &#34;Enough, enough!&#34; cried Zverkov. &#34;Give over, gentlemen, it&#39;s out of place. Better let me tell you how I nearly got married the day before yesterday....&#34; And then followed a burlesque narrative of how this gentleman had almost been married two days before. There was not a word about the marriage, however, but the story was adorned with generals, colonels and kammer-junkers, while Zverkov almost took the lead among them. It was greeted with approving laughter; Ferfitchkin positively squealed. No one paid any attention to me, and I sat crushed and humiliated. &#34;Good Heavens, these are not the people for me!&#34; I thought. &#34;And what a fool I have made of myself before them! I let Ferfitchkin go too far, though. The brutes imagine they are doing me an honour in letting me sit down with them. They don&#39;t understand that it&#39;s an honour to them and not to me! I&#39;ve grown thinner! My clothes! Oh, damn my trousers! Zverkov noticed the yellow stain on the knee as soon as he came in.... But what&#39;s the use! I must get up at once, this very minute, take my hat and simply go without a word ... with contempt! And tomorrow I can send a challenge. The scoundrels! As though I cared about the seven roubles. They may think.... Damn it! I don&#39;t care about the seven roubles. I&#39;ll go this minute!&#34; Of course I remained. I drank sherry and Lafitte by the glassful in my discomfiture. Being unaccustomed to it, I was quickly affected. My annoyance increased as the wine went to my head. I longed all at once to insult them all in a most flagrant manner and then go away. To seize the moment and show what I could do, so that they would say, &#34;He&#39;s clever, though he is absurd,&#34; and ... and ... in fact, damn them all! I scanned them all insolently with my drowsy eyes. But they seemed to have forgotten me altogether. They were noisy, vociferous, cheerful. Zverkov was talking all the time. I began listening. Zverkov was talking of some exuberant lady whom he had at last led on to declaring her love (of course, he was lying like a horse), and how he had been helped in this affair by an intimate friend of his, a Prince Kolya, an officer in the hussars, who had three thousand serfs. &#34;And yet this Kolya, who has three thousand serfs, has not put in an appearance here tonight to see you off,&#34; I cut in suddenly. For one minute every one was silent. &#34;You are drunk already.&#34; Trudolyubov deigned to notice me at last, glancing contemptuously in my direction. Zverkov, without a word, examined me as though I were an insect. I dropped my eyes. Simonov made haste to fill up the glasses with champagne. Trudolyubov raised his glass, as did everyone else but me. &#34;Your health and good luck on the journey!&#34; he cried to Zverkov. &#34;To old times, to our future, hurrah!&#34; They all tossed off their glasses, and crowded round Zverkov to kiss him. I did not move; my full glass stood untouched before me. &#34;Why, aren&#39;t you going to drink it?&#34; roared Trudolyubov, losing patience and turning menacingly to me. &#34;I want to make a speech separately, on my own account ... and then I&#39;ll drink it, Mr. Trudolyubov.&#34; &#34;Spiteful brute!&#34; muttered Simonov. I drew myself up in my chair and feverishly seized my glass, prepared for something extraordinary, though I did not know myself precisely what I was going to say. &#34;SILENCE!&#34; cried Ferfitchkin. &#34;Now for a display of wit!&#34; Zverkov waited very gravely, knowing what was coming. &#34;Mr. Lieutenant Zverkov,&#34; I began, &#34;let me tell you that I hate phrases, phrasemongers and men in corsets ... that&#39;s the first point, and there is a second one to follow it.&#34; There was a general stir. &#34;The second point is: I hate ribaldry and ribald talkers. Especially ribald talkers! The third point: I love justice, truth and honesty.&#34; I went on almost mechanically, for I was beginning to shiver with horror myself and had no idea how I came to be talking like this. &#34;I love thought, Monsieur Zverkov; I love true comradeship, on an equal footing and not ... H&#39;m ... I love ... But, however, why not? I will drink your health, too, Mr. Zverkov. Seduce the Circassian girls, shoot the enemies of the fatherland and ... and ... to your health, Monsieur Zverkov!&#34; Zverkov got up from his seat, bowed to me and said: &#34;I am very much obliged to you.&#34; He was frightfully offended and turned pale. &#34;Damn the fellow!&#34; roared Trudolyubov, bringing his fist down on the table. &#34;Well, he wants a punch in the face for that,&#34; squealed Ferfitchkin. &#34;We ought to turn him out,&#34; muttered Simonov. &#34;Not a word, gentlemen, not a movement!&#34; cried Zverkov solemnly, checking the general indignation. &#34;I thank you all, but I can show him for myself how much value I attach to his words.&#34; &#34;Mr. Ferfitchkin, you will give me satisfaction tomorrow for your words just now!&#34; I said aloud, turning with dignity to Ferfitchkin. &#34;A duel, you mean? Certainly,&#34; he answered. But probably I was so ridiculous as I challenged him and it was so out of keeping with my appearance that everyone including Ferfitchkin was prostrate with laughter. &#34;Yes, let him alone, of course! He is quite drunk,&#34; Trudolyubov said with disgust. &#34;I shall never forgive myself for letting him join us,&#34; Simonov muttered again. &#34;Now is the time to throw a bottle at their heads,&#34; I thought to myself. I picked up the bottle ... and filled my glass.... &#34;No, I&#39;d better sit on to the end,&#34; I went on thinking; &#34;you would be pleased, my friends, if I went away. Nothing will induce me to go. I&#39;ll go on sitting here and drinking to the end, on purpose, as a sign that I don&#39;t think you of the slightest consequence. I will go on sitting and drinking, because this is a public-house and I paid my entrance money. I&#39;ll sit here and drink, for I look upon you as so many pawns, as inanimate pawns. I&#39;ll sit here and drink ... and sing if I want to, yes, sing, for I have the right to ... to sing ... H&#39;m!&#34; But I did not sing. I simply tried not to look at any of them. I assumed most unconcerned attitudes and waited with impatience for them to speak FIRST. But alas, they did not address me! And oh, how I wished, how I wished at that moment to be reconciled to them! It struck eight, at last nine. They moved from the table to the sofa. Zverkov stretched himself on a lounge and put one foot on a round table. Wine was brought there. He did, as a fact, order three bottles on his own account. I, of course, was not invited to join them. They all sat round him on the sofa. They listened to him, almost with reverence. It was evident that they were fond of him. &#34;What for? What for?&#34; I wondered. From time to time they were moved to drunken enthusiasm and kissed each other. They talked of the Caucasus, of the nature of true passion, of snug berths in the service, of the income of an hussar called Podharzhevsky, whom none of them knew personally, and rejoiced in the largeness of it, of the extraordinary grace and beauty of a Princess D., whom none of them had ever seen; then it came to Shakespeare&#39;s being immortal. I smiled contemptuously and walked up and down the other side of the room, opposite the sofa, from the table to the stove and back again. I tried my very utmost to show them that I could do without them, and yet I purposely made a noise with my boots, thumping with my heels. But it was all in vain. They paid no attention. I had the patience to walk up and down in front of them from eight o&#39;clock till eleven, in the same place, from the table to the stove and back again. &#34;I walk up and down to please myself and no one can prevent me.&#34; The waiter who came into the room stopped, from time to time, to look at me. I was somewhat giddy from turning round so often; at moments it seemed to me that I was in delirium. During those three hours I was three times soaked with sweat and dry again. At times, with an intense, acute pang I was stabbed to the heart by the thought that ten years, twenty years, forty years would pass, and that even in forty years I would remember with loathing and humiliation those filthiest, most ludicrous, and most awful moments of my life. No one could have gone out of his way to degrade himself more shamelessly, and I fully realised it, fully, and yet I went on pacing up and down from the table to the stove. &#34;Oh, if you only knew what thoughts and feelings I am capable of, how cultured I am!&#34; I thought at moments, mentally addressing the sofa on which my enemies were sitting. But my enemies behaved as though I were not in the room. Once--only once--they turned towards me, just when Zverkov was talking about Shakespeare, and I suddenly gave a contemptuous laugh. I laughed in such an affected and disgusting way that they all at once broke off their conversation, and silently and gravely for two minutes watched me walking up and down from the table to the stove, TAKING NO NOTICE OF THEM. But nothing came of it: they said nothing, and two minutes later they ceased to notice me again. It struck eleven. &#34;Friends,&#34; cried Zverkov getting up from the sofa, &#34;let us all be off now, THERE!&#34; &#34;Of course, of course,&#34; the others assented. I turned sharply to Zverkov. I was so harassed, so exhausted, that I would have cut my throat to put an end to it. I was in a fever; my hair, soaked with perspiration, stuck to my forehead and temples. &#34;Zverkov, I beg your pardon,&#34; I said abruptly and resolutely. &#34;Ferfitchkin, yours too, and everyone&#39;s, everyone&#39;s: I have insulted you all!&#34; &#34;Aha! A duel is not in your line, old man,&#34; Ferfitchkin hissed venomously. It sent a sharp pang to my heart. &#34;No, it&#39;s not the duel I am afraid of, Ferfitchkin! I am ready to fight you tomorrow, after we are reconciled. I insist upon it, in fact, and you cannot refuse. I want to show you that I am not afraid of a duel. You shall fire first and I shall fire into the air.&#34; &#34;He is comforting himself,&#34; said Simonov. &#34;He&#39;s simply raving,&#34; said Trudolyubov. &#34;But let us pass. Why are you barring our way? What do you want?&#34; Zverkov answered disdainfully. They were all flushed, their eyes were bright: they had been drinking heavily. &#34;I ask for your friendship, Zverkov; I insulted you, but ...&#34; &#34;Insulted? YOU insulted ME? Understand, sir, that you never, under any circumstances, could possibly insult ME.&#34; &#34;And that&#39;s enough for you. Out of the way!&#34; concluded Trudolyubov. &#34;Olympia is mine, friends, that&#39;s agreed!&#34; cried Zverkov. &#34;We won&#39;t dispute your right, we won&#39;t dispute your right,&#34; the others answered, laughing. I stood as though spat upon. The party went noisily out of the room. Trudolyubov struck up some stupid song. Simonov remained behind for a moment to tip the waiters. I suddenly went up to him. &#34;Simonov! give me six roubles!&#34; I said, with desperate resolution. He looked at me in extreme amazement, with vacant eyes. He, too, was drunk. &#34;You don&#39;t mean you are coming with us?&#34; &#34;Yes.&#34; &#34;I&#39;ve no money,&#34; he snapped out, and with a scornful laugh he went out of the room. I clutched at his overcoat. It was a nightmare. &#34;Simonov, I saw you had money. Why do you refuse me? Am I a scoundrel? Beware of refusing me: if you knew, if you knew why I am asking! My whole future, my whole plans depend upon it!&#34; Simonov pulled out the money and almost flung it at me. &#34;Take it, if you have no sense of shame!&#34; he pronounced pitilessly, and ran to overtake them. I was left for a moment alone. Disorder, the remains of dinner, a broken wine-glass on the floor, spilt wine, cigarette ends, fumes of drink and delirium in my brain, an agonising misery in my heart and finally the waiter, who had seen and heard all and was looking inquisitively into my face. &#34;I am going there!&#34; I cried. &#34;Either they shall all go down on their knees to beg for my friendship, or I will give Zverkov a slap in the face!&#34; V &#34;So this is it, this is it at last--contact with real life,&#34; I muttered as I ran headlong downstairs. &#34;This is very different from the Pope&#39;s leaving Rome and going to Brazil, very different from the ball on Lake Como!&#34; &#34;You are a scoundrel,&#34; a thought flashed through my mind, &#34;if you laugh at this now.&#34; &#34;No matter!&#34; I cried, answering myself. &#34;Now everything is lost!&#34; There was no trace to be seen of them, but that made no difference--I knew where they had gone. At the steps was standing a solitary night sledge-driver in a rough peasant coat, powdered over with the still falling, wet, and as it were warm, snow. It was hot and steamy. The little shaggy piebald horse was also covered with snow and coughing, I remember that very well. I made a rush for the roughly made sledge; but as soon as I raised my foot to get into it, the recollection of how Simonov had just given me six roubles seemed to double me up and I tumbled into the sledge like a sack. &#34;No, I must do a great deal to make up for all that,&#34; I cried. &#34;But I will make up for it or perish on the spot this very night. Start!&#34; We set off. There was a perfect whirl in my head. &#34;They won&#39;t go down on their knees to beg for my friendship. That is a mirage, cheap mirage, revolting, romantic and fantastical--that&#39;s another ball on Lake Como. And so I am bound to slap Zverkov&#39;s face! It is my duty to. And so it is settled; I am flying to give him a slap in the face. Hurry up!&#34; The driver tugged at the reins. &#34;As soon as I go in I&#39;ll give it him. Ought I before giving him the slap to say a few words by way of preface? No. I&#39;ll simply go in and give it him. They will all be sitting in the drawing-room, and he with Olympia on the sofa. That damned Olympia! She laughed at my looks on one occasion and refused me. I&#39;ll pull Olympia&#39;s hair, pull Zverkov&#39;s ears! No, better one ear, and pull him by it round the room. Maybe they will all begin beating me and will kick me out. That&#39;s most likely, indeed. No matter! Anyway, I shall first slap him; the initiative will be mine; and by the laws of honour that is everything: he will be branded and cannot wipe off the slap by any blows, by nothing but a duel. He will be forced to fight. And let them beat me now. Let them, the ungrateful wretches! Trudolyubov will beat me hardest, he is so strong; Ferfitchkin will be sure to catch hold sideways and tug at my hair. But no matter, no matter! That&#39;s what I am going for. The blockheads will be forced at last to see the tragedy of it all! When they drag me to the door I shall call out to them that in reality they are not worth my little finger. Get on, driver, get on!&#34; I cried to the driver. He started and flicked his whip, I shouted so savagely. &#34;We shall fight at daybreak, that&#39;s a settled thing. I&#39;ve done with the office. Ferfitchkin made a joke about it just now. But where can I get pistols? Nonsense! I&#39;ll get my salary in advance and buy them. And powder, and bullets? That&#39;s the second&#39;s business. And how can it all be done by daybreak? and where am I to get a second? I have no friends. Nonsense!&#34; I cried, lashing myself up more and more. &#34;It&#39;s of no consequence! The first person I meet in the street is bound to be my second, just as he would be bound to pull a drowning man out of water. The most eccentric things may happen. Even if I were to ask the director himself to be my second tomorrow, he would be bound to consent, if only from a feeling of chivalry, and to keep the secret! Anton Antonitch....&#34; The fact is, that at that very minute the disgusting absurdity of my plan and the other side of the question was clearer and more vivid to my imagination than it could be to anyone on earth. But .... &#34;Get on, driver, get on, you rascal, get on!&#34; &#34;Ugh, sir!&#34; said the son of toil. Cold shivers suddenly ran down me. Wouldn&#39;t it be better ... to go straight home? My God, my God! Why did I invite myself to this dinner yesterday? But no, it&#39;s impossible. And my walking up and down for three hours from the table to the stove? No, they, they and no one else must pay for my walking up and down! They must wipe out this dishonour! Drive on! And what if they give me into custody? They won&#39;t dare! They&#39;ll be afraid of the scandal. And what if Zverkov is so contemptuous that he refuses to fight a duel? He is sure to; but in that case I&#39;ll show them ... I will turn up at the posting station when he&#39;s setting off tomorrow, I&#39;ll catch him by the leg, I&#39;ll pull off his coat when he gets into the carriage. I&#39;ll get my teeth into his hand, I&#39;ll bite him. &#34;See what lengths you can drive a desperate man to!&#34; He may hit me on the head and they may belabour me from behind. I will shout to the assembled multitude: &#34;Look at this young puppy who is driving off to captivate the Circassian girls after letting me spit in his face!&#34; Of course, after that everything will be over! The office will have vanished off the face of the earth. I shall be arrested, I shall be tried, I shall be dismissed from the service, thrown in prison, sent to Siberia. Never mind! In fifteen years when they let me out of prison I will trudge off to him, a beggar, in rags. I shall find him in some provincial town. He will be married and happy. He will have a grown-up daughter.... I shall say to him: &#34;Look, monster, at my hollow cheeks and my rags! I&#39;ve lost everything--my career, my happiness, art, science, THE WOMAN I LOVED, and all through you. Here are pistols. I have come to discharge my pistol and ... and I ... forgive you. Then I shall fire into the air and he will hear nothing more of me....&#34; I was actually on the point of tears, though I knew perfectly well at that moment that all this was out of Pushkin&#39;s SILVIO and Lermontov&#39;s MASQUERADE. And all at once I felt horribly ashamed, so ashamed that I stopped the horse, got out of the sledge, and stood still in the snow in the middle of the street. The driver gazed at me, sighing and astonished. What was I to do? I could not go on there--it was evidently stupid, and I could not leave things as they were, because that would seem as though ... Heavens, how could I leave things! And after such insults! &#34;No!&#34; I cried, throwing myself into the sledge again. &#34;It is ordained! It is fate! Drive on, drive on!&#34; And in my impatience I punched the sledge-driver on the back of the neck. &#34;What are you up to? What are you hitting me for?&#34; the peasant shouted, but he whipped up his nag so that it began kicking. The wet snow was falling in big flakes; I unbuttoned myself, regardless of it. I forgot everything else, for I had finally decided on the slap, and felt with horror that it was going to happen NOW, AT ONCE, and that NO FORCE COULD STOP IT. The deserted street lamps gleamed sullenly in the snowy darkness like torches at a funeral. The snow drifted under my great-coat, under my coat, under my cravat, and melted there. I did not wrap myself up--all was lost, anyway. At last we arrived. I jumped out, almost unconscious, ran up the steps and began knocking and kicking at the door. I felt fearfully weak, particularly in my legs and knees. The door was opened quickly as though they knew I was coming. As a fact, Simonov had warned them that perhaps another gentleman would arrive, and this was a place in which one had to give notice and to observe certain precautions. It was one of those &#34;millinery establishments&#34; which were abolished by the police a good time ago. By day it really was a shop; but at night, if one had an introduction, one might visit it for other purposes. I walked rapidly through the dark shop into the familiar drawing-room, where there was only one candle burning, and stood still in amazement: there was no one there. &#34;Where are they?&#34; I asked somebody. But by now, of course, they had separated. Before me was standing a person with a stupid smile, the &#34;madam&#34; herself, who had seen me before. A minute later a door opened and another person came in. Taking no notice of anything I strode about the room, and, I believe, I talked to myself. I felt as though I had been saved from death and was conscious of this, joyfully, all over: I should have given that slap, I should certainly, certainly have given it! But now they were not here and ... everything had vanished and changed! I looked round. I could not realise my condition yet. I looked mechanically at the girl who had come in: and had a glimpse of a fresh, young, rather pale face, with straight, dark eyebrows, and with grave, as it were wondering, eyes that attracted me at once; I should have hated her if she had been smiling. I began looking at her more intently and, as it were, with effort. I had not fully collected my thoughts. There was something simple and good-natured in her face, but something strangely grave. I am sure that this stood in her way here, and no one of those fools had noticed her. She could not, however, have been called a beauty, though she was tall, strong-looking, and well built. She was very simply dressed. Something loathsome stirred within me. I went straight up to her. I chanced to look into the glass. My harassed face struck me as revolting in the extreme, pale, angry, abject, with dishevelled hair. &#34;No matter, I am glad of it,&#34; I thought; &#34;I am glad that I shall seem repulsive to her; I like that.&#34; VI ... Somewhere behind a screen a clock began wheezing, as though oppressed by something, as though someone were strangling it. After an unnaturally prolonged wheezing there followed a shrill, nasty, and as it were unexpectedly rapid, chime--as though someone were suddenly jumping forward. It struck two. I woke up, though I had indeed not been asleep but lying half-conscious. It was almost completely dark in the narrow, cramped, low-pitched room, cumbered up with an enormous wardrobe and piles of cardboard boxes and all sorts of frippery and litter. The candle end that had been burning on the table was going out and gave a faint flicker from time to time. In a few minutes there would be complete darkness. I was not long in coming to myself; everything came back to my mind at once, without an effort, as though it had been in ambush to pounce upon me again. And, indeed, even while I was unconscious a point seemed continually to remain in my memory unforgotten, and round it my dreams moved drearily. But strange to say, everything that had happened to me in that day seemed to me now, on waking, to be in the far, far away past, as though I had long, long ago lived all that down. My head was full of fumes. Something seemed to be hovering over me, rousing me, exciting me, and making me restless. Misery and spite seemed surging up in me again and seeking an outlet. Suddenly I saw beside me two wide open eyes scrutinising me curiously and persistently. The look in those eyes was coldly detached, sullen, as it were utterly remote; it weighed upon me. A grim idea came into my brain and passed all over my body, as a horrible sensation, such as one feels when one goes into a damp and mouldy cellar. There was something unnatural in those two eyes, beginning to look at me only now. I recalled, too, that during those two hours I had not said a single word to this creature, and had, in fact, considered it utterly superfluous; in fact, the silence had for some reason gratified me. Now I suddenly realised vividly the hideous idea--revolting as a spider--of vice, which, without love, grossly and shamelessly begins with that in which true love finds its consummation. For a long time we gazed at each other like that, but she did not drop her eyes before mine and her expression did not change, so that at last I felt uncomfortable. &#34;What is your name?&#34; I asked abruptly, to put an end to it. &#34;Liza,&#34; she answered almost in a whisper, but somehow far from graciously, and she turned her eyes away. I was silent. &#34;What weather! The snow ... it&#39;s disgusting!&#34; I said, almost to myself, putting my arm under my head despondently, and gazing at the ceiling. She made no answer. This was horrible. &#34;Have you always lived in Petersburg?&#34; I asked a minute later, almost angrily, turning my head slightly towards her. &#34;No.&#34; &#34;Where do you come from?&#34; &#34;From Riga,&#34; she answered reluctantly. &#34;Are you a German?&#34; &#34;No, Russian.&#34; &#34;Have you been here long?&#34; &#34;Where?&#34; &#34;In this house?&#34; &#34;A fortnight.&#34; She spoke more and more jerkily. The candle went out; I could no longer distinguish her face. &#34;Have you a father and mother?&#34; &#34;Yes ... no ... I have.&#34; &#34;Where are they?&#34; &#34;There ... in Riga.&#34; &#34;What are they?&#34; &#34;Oh, nothing.&#34; &#34;Nothing? Why, what class are they?&#34; &#34;Tradespeople.&#34; &#34;Have you always lived with them?&#34; &#34;Yes.&#34; &#34;How old are you?&#34; &#34;Twenty.&#34; &#34;Why did you leave them?&#34; &#34;Oh, for no reason.&#34; That answer meant &#34;Let me alone; I feel sick, sad.&#34; We were silent. God knows why I did not go away. I felt myself more and more sick and dreary. The images of the previous day began of themselves, apart from my will, flitting through my memory in confusion. I suddenly recalled something I had seen that morning when, full of anxious thoughts, I was hurrying to the office. &#34;I saw them carrying a coffin out yesterday and they nearly dropped it,&#34; I suddenly said aloud, not that I desired to open the conversation, but as it were by accident. &#34;A coffin?&#34; &#34;Yes, in the Haymarket; they were bringing it up out of a cellar.&#34; &#34;From a cellar?&#34; &#34;Not from a cellar, but a basement. Oh, you know ... down below ... from a house of ill-fame. It was filthy all round ... Egg-shells, litter ... a stench. It was loathsome.&#34; Silence. &#34;A nasty day to be buried,&#34; I began, simply to avoid being silent. &#34;Nasty, in what way?&#34; &#34;The snow, the wet.&#34; (I yawned.) &#34;It makes no difference,&#34; she said suddenly, after a brief silence. &#34;No, it&#39;s horrid.&#34; (I yawned again). &#34;The gravediggers must have sworn at getting drenched by the snow. And there must have been water in the grave.&#34; &#34;Why water in the grave?&#34; she asked, with a sort of curiosity, but speaking even more harshly and abruptly than before. I suddenly began to feel provoked. &#34;Why, there must have been water at the bottom a foot deep. You can&#39;t dig a dry grave in Volkovo Cemetery.&#34; &#34;Why?&#34; &#34;Why? Why, the place is waterlogged. It&#39;s a regular marsh. So they bury them in water. I&#39;ve seen it myself ... many times.&#34; (I had never seen it once, indeed I had never been in Volkovo, and had only heard stories of it.) &#34;Do you mean to say, you don&#39;t mind how you die?&#34; &#34;But why should I die?&#34; she answered, as though defending herself. &#34;Why, some day you will die, and you will die just the same as that dead woman. She was ... a girl like you. She died of consumption.&#34; &#34;A wench would have died in hospital ...&#34; (She knows all about it already: she said &#34;wench,&#34; not &#34;girl.&#34;) &#34;She was in debt to her madam,&#34; I retorted, more and more provoked by the discussion; &#34;and went on earning money for her up to the end, though she was in consumption. Some sledge-drivers standing by were talking about her to some soldiers and telling them so. No doubt they knew her. They were laughing. They were going to meet in a pot-house to drink to her memory.&#34; A great deal of this was my invention. Silence followed, profound silence. She did not stir. &#34;And is it better to die in a hospital?&#34; &#34;Isn&#39;t it just the same? Besides, why should I die?&#34; she added irritably. &#34;If not now, a little later.&#34; &#34;Why a little later?&#34; &#34;Why, indeed? Now you are young, pretty, fresh, you fetch a high price. But after another year of this life you will be very different--you will go off.&#34; &#34;In a year?&#34; &#34;Anyway, in a year you will be worth less,&#34; I continued malignantly. &#34;You will go from here to something lower, another house; a year later--to a third, lower and lower, and in seven years you will come to a basement in the Haymarket. That will be if you were lucky. But it would be much worse if you got some disease, consumption, say ... and caught a chill, or something or other. It&#39;s not easy to get over an illness in your way of life. If you catch anything you may not get rid of it. And so you would die.&#34; &#34;Oh, well, then I shall die,&#34; she answered, quite vindictively, and she made a quick movement. &#34;But one is sorry.&#34; &#34;Sorry for whom?&#34; &#34;Sorry for life.&#34; Silence. &#34;Have you been engaged to be married? Eh?&#34; &#34;What&#39;s that to you?&#34; &#34;Oh, I am not cross-examining you. It&#39;s nothing to me. Why are you so cross? Of course you may have had your own troubles. What is it to me? It&#39;s simply that I felt sorry.&#34; &#34;Sorry for whom?&#34; &#34;Sorry for you.&#34; &#34;No need,&#34; she whispered hardly audibly, and again made a faint movement. That incensed me at once. What! I was so gentle with her, and she.... &#34;Why, do you think that you are on the right path?&#34; &#34;I don&#39;t think anything.&#34; &#34;That&#39;s what&#39;s wrong, that you don&#39;t think. Realise it while there is still time. There still is time. You are still young, good-looking; you might love, be married, be happy....&#34; &#34;Not all married women are happy,&#34; she snapped out in the rude abrupt tone she had used at first. &#34;Not all, of course, but anyway it is much better than the life here. Infinitely better. Besides, with love one can live even without happiness. Even in sorrow life is sweet; life is sweet, however one lives. But here what is there but ... foulness? Phew!&#34; I turned away with disgust; I was no longer reasoning coldly. I began to feel myself what I was saying and warmed to the subject. I was already longing to expound the cherished ideas I had brooded over in my corner. Something suddenly flared up in me. An object had appeared before me. &#34;Never mind my being here, I am not an example for you. I am, perhaps, worse than you are. I was drunk when I came here, though,&#34; I hastened, however, to say in self-defence. &#34;Besides, a man is no example for a woman. It&#39;s a different thing. I may degrade and defile myself, but I am not anyone&#39;s slave. I come and go, and that&#39;s an end of it. I shake it off, and I am a different man. But you are a slave from the start. Yes, a slave! You give up everything, your whole freedom. If you want to break your chains afterwards, you won&#39;t be able to; you will be more and more fast in the snares. It is an accursed bondage. I know it. I won&#39;t speak of anything else, maybe you won&#39;t understand, but tell me: no doubt you are in debt to your madam? There, you see,&#34; I added, though she made no answer, but only listened in silence, entirely absorbed, &#34;that&#39;s a bondage for you! You will never buy your freedom. They will see to that. It&#39;s like selling your soul to the devil.... And besides ... perhaps, I too, am just as unlucky--how do you know--and wallow in the mud on purpose, out of misery? You know, men take to drink from grief; well, maybe I am here from grief. Come, tell me, what is there good here? Here you and I ... came together ... just now and did not say one word to one another all the time, and it was only afterwards you began staring at me like a wild creature, and I at you. Is that loving? Is that how one human being should meet another? It&#39;s hideous, that&#39;s what it is!&#34; &#34;Yes!&#34; she assented sharply and hurriedly. I was positively astounded by the promptitude of this &#34;Yes.&#34; So the same thought may have been straying through her mind when she was staring at me just before. So she, too, was capable of certain thoughts? &#34;Damn it all, this was interesting, this was a point of likeness!&#34; I thought, almost rubbing my hands. And indeed it&#39;s easy to turn a young soul like that! It was the exercise of my power that attracted me most. She turned her head nearer to me, and it seemed to me in the darkness that she propped herself on her arm. Perhaps she was scrutinising me. How I regretted that I could not see her eyes. I heard her deep breathing. &#34;Why have you come here?&#34; I asked her, with a note of authority already in my voice. &#34;Oh, I don&#39;t know.&#34; &#34;But how nice it would be to be living in your father&#39;s house! It&#39;s warm and free; you have a home of your own.&#34; &#34;But what if it&#39;s worse than this?&#34; &#34;I must take the right tone,&#34; flashed through my mind. &#34;I may not get far with sentimentality.&#34; But it was only a momentary thought. I swear she really did interest me. Besides, I was exhausted and moody. And cunning so easily goes hand-in-hand with feeling. &#34;Who denies it!&#34; I hastened to answer. &#34;Anything may happen. I am convinced that someone has wronged you, and that you are more sinned against than sinning. Of course, I know nothing of your story, but it&#39;s not likely a girl like you has come here of her own inclination....&#34; &#34;A girl like me?&#34; she whispered, hardly audibly; but I heard it. Damn it all, I was flattering her. That was horrid. But perhaps it was a good thing.... She was silent. &#34;See, Liza, I will tell you about myself. If I had had a home from childhood, I shouldn&#39;t be what I am now. I often think that. However bad it may be at home, anyway they are your father and mother, and not enemies, strangers. Once a year at least, they&#39;ll show their love of you. Anyway, you know you are at home. I grew up without a home; and perhaps that&#39;s why I&#39;ve turned so ... unfeeling.&#34; I waited again. &#34;Perhaps she doesn&#39;t understand,&#34; I thought, &#34;and, indeed, it is absurd--it&#39;s moralising.&#34; &#34;If I were a father and had a daughter, I believe I should love my daughter more than my sons, really,&#34; I began indirectly, as though talking of something else, to distract her attention. I must confess I blushed. &#34;Why so?&#34; she asked. Ah! so she was listening! &#34;I don&#39;t know, Liza. I knew a father who was a stern, austere man, but used to go down on his knees to his daughter, used to kiss her hands, her feet, he couldn&#39;t make enough of her, really. When she danced at parties he used to stand for five hours at a stretch, gazing at her. He was mad over her: I understand that! She would fall asleep tired at night, and he would wake to kiss her in her sleep and make the sign of the cross over her. He would go about in a dirty old coat, he was stingy to everyone else, but would spend his last penny for her, giving her expensive presents, and it was his greatest delight when she was pleased with what he gave her. Fathers always love their daughters more than the mothers do. Some girls live happily at home! And I believe I should never let my daughters marry.&#34; &#34;What next?&#34; she said, with a faint smile. &#34;I should be jealous, I really should. To think that she should kiss anyone else! That she should love a stranger more than her father! It&#39;s painful to imagine it. Of course, that&#39;s all nonsense, of course every father would be reasonable at last. But I believe before I should let her marry, I should worry myself to death; I should find fault with all her suitors. But I should end by letting her marry whom she herself loved. The one whom the daughter loves always seems the worst to the father, you know. That is always so. So many family troubles come from that.&#34; &#34;Some are glad to sell their daughters, rather than marrying them honourably.&#34; Ah, so that was it! &#34;Such a thing, Liza, happens in those accursed families in which there is neither love nor God,&#34; I retorted warmly, &#34;and where there is no love, there is no sense either. There are such families, it&#39;s true, but I am not speaking of them. You must have seen wickedness in your own family, if you talk like that. Truly, you must have been unlucky. H&#39;m! ... that sort of thing mostly comes about through poverty.&#34; &#34;And is it any better with the gentry? Even among the poor, honest people who live happily?&#34; &#34;H&#39;m ... yes. Perhaps. Another thing, Liza, man is fond of reckoning up his troubles, but does not count his joys. If he counted them up as he ought, he would see that every lot has enough happiness provided for it. And what if all goes well with the family, if the blessing of God is upon it, if the husband is a good one, loves you, cherishes you, never leaves you! There is happiness in such a family! Even sometimes there is happiness in the midst of sorrow; and indeed sorrow is everywhere. If you marry YOU WILL FIND OUT FOR YOURSELF. But think of the first years of married life with one you love: what happiness, what happiness there sometimes is in it! And indeed it&#39;s the ordinary thing. In those early days even quarrels with one&#39;s husband end happily. Some women get up quarrels with their husbands just because they love them. Indeed, I knew a woman like that: she seemed to say that because she loved him, she would torment him and make him feel it. You know that you may torment a man on purpose through love. Women are particularly given to that, thinking to themselves &#39;I will love him so, I will make so much of him afterwards, that it&#39;s no sin to torment him a little now.&#39; And all in the house rejoice in the sight of you, and you are happy and gay and peaceful and honourable.... Then there are some women who are jealous. If he went off anywhere--I knew one such woman, she couldn&#39;t restrain herself, but would jump up at night and run off on the sly to find out where he was, whether he was with some other woman. That&#39;s a pity. And the woman knows herself it&#39;s wrong, and her heart fails her and she suffers, but she loves--it&#39;s all through love. And how sweet it is to make up after quarrels, to own herself in the wrong or to forgive him! And they both are so happy all at once--as though they had met anew, been married over again; as though their love had begun afresh. And no one, no one should know what passes between husband and wife if they love one another. And whatever quarrels there may be between them they ought not to call in their own mother to judge between them and tell tales of one another. They are their own judges. Love is a holy mystery and ought to be hidden from all other eyes, whatever happens. That makes it holier and better. They respect one another more, and much is built on respect. And if once there has been love, if they have been married for love, why should love pass away? Surely one can keep it! It is rare that one cannot keep it. And if the husband is kind and straightforward, why should not love last? The first phase of married love will pass, it is true, but then there will come a love that is better still. Then there will be the union of souls, they will have everything in common, there will be no secrets between them. And once they have children, the most difficult times will seem to them happy, so long as there is love and courage. Even toil will be a joy, you may deny yourself bread for your children and even that will be a joy, They will love you for it afterwards; so you are laying by for your future. As the children grow up you feel that you are an example, a support for them; that even after you die your children will always keep your thoughts and feelings, because they have received them from you, they will take on your semblance and likeness. So you see this is a great duty. How can it fail to draw the father and mother nearer? People say it&#39;s a trial to have children. Who says that? It is heavenly happiness! Are you fond of little children, Liza? I am awfully fond of them. You know--a little rosy baby boy at your bosom, and what husband&#39;s heart is not touched, seeing his wife nursing his child! A plump little rosy baby, sprawling and snuggling, chubby little hands and feet, clean tiny little nails, so tiny that it makes one laugh to look at them; eyes that look as if they understand everything. And while it sucks it clutches at your bosom with its little hand, plays. When its father comes up, the child tears itself away from the bosom, flings itself back, looks at its father, laughs, as though it were fearfully funny, and falls to sucking again. Or it will bite its mother&#39;s breast when its little teeth are coming, while it looks sideways at her with its little eyes as though to say, &#39;Look, I am biting!&#39; Is not all that happiness when they are the three together, husband, wife and child? One can forgive a great deal for the sake of such moments. Yes, Liza, one must first learn to live oneself before one blames others!&#34; &#34;It&#39;s by pictures, pictures like that one must get at you,&#34; I thought to myself, though I did speak with real feeling, and all at once I flushed crimson. &#34;What if she were suddenly to burst out laughing, what should I do then?&#34; That idea drove me to fury. Towards the end of my speech I really was excited, and now my vanity was somehow wounded. The silence continued. I almost nudged her. &#34;Why are you--&#34; she began and stopped. But I understood: there was a quiver of something different in her voice, not abrupt, harsh and unyielding as before, but something soft and shamefaced, so shamefaced that I suddenly felt ashamed and guilty. &#34;What?&#34; I asked, with tender curiosity. &#34;Why, you...&#34; &#34;What?&#34; &#34;Why, you ... speak somehow like a book,&#34; she said, and again there was a note of irony in her voice. That remark sent a pang to my heart. It was not what I was expecting. I did not understand that she was hiding her feelings under irony, that this is usually the last refuge of modest and chaste-souled people when the privacy of their soul is coarsely and intrusively invaded, and that their pride makes them refuse to surrender till the last moment and shrink from giving expression to their feelings before you. I ought to have guessed the truth from the timidity with which she had repeatedly approached her sarcasm, only bringing herself to utter it at last with an effort. But I did not guess, and an evil feeling took possession of me. &#34;Wait a bit!&#34; I thought. VII &#34;Oh, hush, Liza! How can you talk about being like a book, when it makes even me, an outsider, feel sick? Though I don&#39;t look at it as an outsider, for, indeed, it touches me to the heart.... Is it possible, is it possible that you do not feel sick at being here yourself? Evidently habit does wonders! God knows what habit can do with anyone. Can you seriously think that you will never grow old, that you will always be good-looking, and that they will keep you here for ever and ever? I say nothing of the loathsomeness of the life here.... Though let me tell you this about it--about your present life, I mean; here though you are young now, attractive, nice, with soul and feeling, yet you know as soon as I came to myself just now I felt at once sick at being here with you! One can only come here when one is drunk. But if you were anywhere else, living as good people live, I should perhaps be more than attracted by you, should fall in love with you, should be glad of a look from you, let alone a word; I should hang about your door, should go down on my knees to you, should look upon you as my betrothed and think it an honour to be allowed to. I should not dare to have an impure thought about you. But here, you see, I know that I have only to whistle and you have to come with me whether you like it or not. I don&#39;t consult your wishes, but you mine. The lowest labourer hires himself as a workman, but he doesn&#39;t make a slave of himself altogether; besides, he knows that he will be free again presently. But when are you free? Only think what you are giving up here? What is it you are making a slave of? It is your soul, together with your body; you are selling your soul which you have no right to dispose of! You give your love to be outraged by every drunkard! Love! But that&#39;s everything, you know, it&#39;s a priceless diamond, it&#39;s a maiden&#39;s treasure, love--why, a man would be ready to give his soul, to face death to gain that love. But how much is your love worth now? You are sold, all of you, body and soul, and there is no need to strive for love when you can have everything without love. And you know there is no greater insult to a girl than that, do you understand? To be sure, I have heard that they comfort you, poor fools, they let you have lovers of your own here. But you know that&#39;s simply a farce, that&#39;s simply a sham, it&#39;s just laughing at you, and you are taken in by it! Why, do you suppose he really loves you, that lover of yours? I don&#39;t believe it. How can he love you when he knows you may be called away from him any minute? He would be a low fellow if he did! Will he have a grain of respect for you? What have you in common with him? He laughs at you and robs you--that is all his love amounts to! You are lucky if he does not beat you. Very likely he does beat you, too. Ask him, if you have got one, whether he will marry you. He will laugh in your face, if he doesn&#39;t spit in it or give you a blow--though maybe he is not worth a bad halfpenny himself. And for what have you ruined your life, if you come to think of it? For the coffee they give you to drink and the plentiful meals? But with what object are they feeding you up? An honest girl couldn&#39;t swallow the food, for she would know what she was being fed for. You are in debt here, and, of course, you will always be in debt, and you will go on in debt to the end, till the visitors here begin to scorn you. And that will soon happen, don&#39;t rely upon your youth--all that flies by express train here, you know. You will be kicked out. And not simply kicked out; long before that she&#39;ll begin nagging at you, scolding you, abusing you, as though you had not sacrificed your health for her, had not thrown away your youth and your soul for her benefit, but as though you had ruined her, beggared her, robbed her. And don&#39;t expect anyone to take your part: the others, your companions, will attack you, too, win her favour, for all are in slavery here, and have lost all conscience and pity here long ago. They have become utterly vile, and nothing on earth is viler, more loathsome, and more insulting than their abuse. And you are laying down everything here, unconditionally, youth and health and beauty and hope, and at twenty-two you will look like a woman of five-and-thirty, and you will be lucky if you are not diseased, pray to God for that! No doubt you are thinking now that you have a gay time and no work to do! Yet there is no work harder or more dreadful in the world or ever has been. One would think that the heart alone would be worn out with tears. And you won&#39;t dare to say a word, not half a word when they drive you away from here; you will go away as though you were to blame. You will change to another house, then to a third, then somewhere else, till you come down at last to the Haymarket. There you will be beaten at every turn; that is good manners there, the visitors don&#39;t know how to be friendly without beating you. You don&#39;t believe that it is so hateful there? Go and look for yourself some time, you can see with your own eyes. Once, one New Year&#39;s Day, I saw a woman at a door. They had turned her out as a joke, to give her a taste of the frost because she had been crying so much, and they shut the door behind her. At nine o&#39;clock in the morning she was already quite drunk, dishevelled, half-naked, covered with bruises, her face was powdered, but she had a black-eye, blood was trickling from her nose and her teeth; some cabman had just given her a drubbing. She was sitting on the stone steps, a salt fish of some sort was in her hand; she was crying, wailing something about her luck and beating with the fish on the steps, and cabmen and drunken soldiers were crowding in the doorway taunting her. You don&#39;t believe that you will ever be like that? I should be sorry to believe it, too, but how do you know; maybe ten years, eight years ago that very woman with the salt fish came here fresh as a cherub, innocent, pure, knowing no evil, blushing at every word. Perhaps she was like you, proud, ready to take offence, not like the others; perhaps she looked like a queen, and knew what happiness was in store for the man who should love her and whom she should love. Do you see how it ended? And what if at that very minute when she was beating on the filthy steps with that fish, drunken and dishevelled--what if at that very minute she recalled the pure early days in her father&#39;s house, when she used to go to school and the neighbour&#39;s son watched for her on the way, declaring that he would love her as long as he lived, that he would devote his life to her, and when they vowed to love one another for ever and be married as soon as they were grown up! No, Liza, it would be happy for you if you were to die soon of consumption in some corner, in some cellar like that woman just now. In the hospital, do you say? You will be lucky if they take you, but what if you are still of use to the madam here? Consumption is a queer disease, it is not like fever. The patient goes on hoping till the last minute and says he is all right. He deludes himself And that just suits your madam. Don&#39;t doubt it, that&#39;s how it is; you have sold your soul, and what is more you owe money, so you daren&#39;t say a word. But when you are dying, all will abandon you, all will turn away from you, for then there will be nothing to get from you. What&#39;s more, they will reproach you for cumbering the place, for being so long over dying. However you beg you won&#39;t get a drink of water without abuse: &#39;Whenever are you going off, you nasty hussy, you won&#39;t let us sleep with your moaning, you make the gentlemen sick.&#39; That&#39;s true, I have heard such things said myself. They will thrust you dying into the filthiest corner in the cellar--in the damp and darkness; what will your thoughts be, lying there alone? When you die, strange hands will lay you out, with grumbling and impatience; no one will bless you, no one will sigh for you, they only want to get rid of you as soon as may be; they will buy a coffin, take you to the grave as they did that poor woman today, and celebrate your memory at the tavern. In the grave, sleet, filth, wet snow--no need to put themselves out for you--&#39;Let her down, Vanuha; it&#39;s just like her luck--even here, she is head-foremost, the hussy. Shorten the cord, you rascal.&#39; &#39;It&#39;s all right as it is.&#39; &#39;All right, is it? Why, she&#39;s on her side! She was a fellow-creature, after all! But, never mind, throw the earth on her.&#39; And they won&#39;t care to waste much time quarrelling over you. They will scatter the wet blue clay as quick as they can and go off to the tavern ... and there your memory on earth will end; other women have children to go to their graves, fathers, husbands. While for you neither tear, nor sigh, nor remembrance; no one in the whole world will ever come to you, your name will vanish from the face of the earth--as though you had never existed, never been born at all! Nothing but filth and mud, however you knock at your coffin lid at night, when the dead arise, however you cry: &#39;Let me out, kind people, to live in the light of day! My life was no life at all; my life has been thrown away like a dish-clout; it was drunk away in the tavern at the Haymarket; let me out, kind people, to live in the world again.&#39;&#34; And I worked myself up to such a pitch that I began to have a lump in my throat myself, and ... and all at once I stopped, sat up in dismay and, bending over apprehensively, began to listen with a beating heart. I had reason to be troubled. I had felt for some time that I was turning her soul upside down and rending her heart, and--and the more I was convinced of it, the more eagerly I desired to gain my object as quickly and as effectually as possible. It was the exercise of my skill that carried me away; yet it was not merely sport.... I knew I was speaking stiffly, artificially, even bookishly, in fact, I could not speak except &#34;like a book.&#34; But that did not trouble me: I knew, I felt that I should be understood and that this very bookishness might be an assistance. But now, having attained my effect, I was suddenly panic-stricken. Never before had I witnessed such despair! She was lying on her face, thrusting her face into the pillow and clutching it in both hands. Her heart was being torn. Her youthful body was shuddering all over as though in convulsions. Suppressed sobs rent her bosom and suddenly burst out in weeping and wailing, then she pressed closer into the pillow: she did not want anyone here, not a living soul, to know of her anguish and her tears. She bit the pillow, bit her hand till it bled (I saw that afterwards), or, thrusting her fingers into her dishevelled hair, seemed rigid with the effort of restraint, holding her breath and clenching her teeth. I began saying something, begging her to calm herself, but felt that I did not dare; and all at once, in a sort of cold shiver, almost in terror, began fumbling in the dark, trying hurriedly to get dressed to go. It was dark; though I tried my best I could not finish dressing quickly. Suddenly I felt a box of matches and a candlestick with a whole candle in it. As soon as the room was lighted up, Liza sprang up, sat up in bed, and with a contorted face, with a half insane smile, looked at me almost senselessly. I sat down beside her and took her hands; she came to herself, made an impulsive movement towards me, would have caught hold of me, but did not dare, and slowly bowed her head before me. &#34;Liza, my dear, I was wrong ... forgive me, my dear,&#34; I began, but she squeezed my hand in her fingers so tightly that I felt I was saying the wrong thing and stopped. &#34;This is my address, Liza, come to me.&#34; &#34;I will come,&#34; she answered resolutely, her head still bowed. &#34;But now I am going, good-bye ... till we meet again.&#34; I got up; she, too, stood up and suddenly flushed all over, gave a shudder, snatched up a shawl that was lying on a chair and muffled herself in it to her chin. As she did this she gave another sickly smile, blushed and looked at me strangely. I felt wretched; I was in haste to get away--to disappear. &#34;Wait a minute,&#34; she said suddenly, in the passage just at the doorway, stopping me with her hand on my overcoat. She put down the candle in hot haste and ran off; evidently she had thought of something or wanted to show me something. As she ran away she flushed, her eyes shone, and there was a smile on her lips--what was the meaning of it? Against my will I waited: she came back a minute later with an expression that seemed to ask forgiveness for something. In fact, it was not the same face, not the same look as the evening before: sullen, mistrustful and obstinate. Her eyes now were imploring, soft, and at the same time trustful, caressing, timid. The expression with which children look at people they are very fond of, of whom they are asking a favour. Her eyes were a light hazel, they were lovely eyes, full of life, and capable of expressing love as well as sullen hatred. Making no explanation, as though I, as a sort of higher being, must understand everything without explanations, she held out a piece of paper to me. Her whole face was positively beaming at that instant with naive, almost childish, triumph. I unfolded it. It was a letter to her from a medical student or someone of that sort--a very high-flown and flowery, but extremely respectful, love-letter. I don&#39;t recall the words now, but I remember well that through the high-flown phrases there was apparent a genuine feeling, which cannot be feigned. When I had finished reading it I met her glowing, questioning, and childishly impatient eyes fixed upon me. She fastened her eyes upon my face and waited impatiently for what I should say. In a few words, hurriedly, but with a sort of joy and pride, she explained to me that she had been to a dance somewhere in a private house, a family of &#34;very nice people, WHO KNEW NOTHING, absolutely nothing, for she had only come here so lately and it had all happened ... and she hadn&#39;t made up her mind to stay and was certainly going away as soon as she had paid her debt...&#34; and at that party there had been the student who had danced with her all the evening. He had talked to her, and it turned out that he had known her in old days at Riga when he was a child, they had played together, but a very long time ago--and he knew her parents, but ABOUT THIS he knew nothing, nothing whatever, and had no suspicion! And the day after the dance (three days ago) he had sent her that letter through the friend with whom she had gone to the party ... and ... well, that was all. She dropped her shining eyes with a sort of bashfulness as she finished. The poor girl was keeping that student&#39;s letter as a precious treasure, and had run to fetch it, her only treasure, because she did not want me to go away without knowing that she, too, was honestly and genuinely loved; that she, too, was addressed respectfully. No doubt that letter was destined to lie in her box and lead to nothing. But none the less, I am certain that she would keep it all her life as a precious treasure, as her pride and justification, and now at such a minute she had thought of that letter and brought it with naive pride to raise herself in my eyes that I might see, that I, too, might think well of her. I said nothing, pressed her hand and went out. I so longed to get away ... I walked all the way home, in spite of the fact that the melting snow was still falling in heavy flakes. I was exhausted, shattered, in bewilderment. But behind the bewilderment the truth was already gleaming. The loathsome truth. VIII It was some time, however, before I consented to recognise that truth. Waking up in the morning after some hours of heavy, leaden sleep, and immediately realising all that had happened on the previous day, I was positively amazed at my last night&#39;s SENTIMENTALITY with Liza, at all those &#34;outcries of horror and pity.&#34; &#34;To think of having such an attack of womanish hysteria, pah!&#34; I concluded. And what did I thrust my address upon her for? What if she comes? Let her come, though; it doesn&#39;t matter.... But OBVIOUSLY, that was not now the chief and the most important matter: I had to make haste and at all costs save my reputation in the eyes of Zverkov and Simonov as quickly as possible; that was the chief business. And I was so taken up that morning that I actually forgot all about Liza. First of all I had at once to repay what I had borrowed the day before from Simonov. I resolved on a desperate measure: to borrow fifteen roubles straight off from Anton Antonitch. As luck would have it he was in the best of humours that morning, and gave it to me at once, on the first asking. I was so delighted at this that, as I signed the IOU with a swaggering air, I told him casually that the night before &#34;I had been keeping it up with some friends at the Hotel de Paris; we were giving a farewell party to a comrade, in fact, I might say a friend of my childhood, and you know--a desperate rake, fearfully spoilt--of course, he belongs to a good family, and has considerable means, a brilliant career; he is witty, charming, a regular Lovelace, you understand; we drank an extra &#39;half-dozen&#39; and ...&#34; And it went off all right; all this was uttered very easily, unconstrainedly and complacently. On reaching home I promptly wrote to Simonov. To this hour I am lost in admiration when I recall the truly gentlemanly, good-humoured, candid tone of my letter. With tact and good-breeding, and, above all, entirely without superfluous words, I blamed myself for all that had happened. I defended myself, &#34;if I really may be allowed to defend myself,&#34; by alleging that being utterly unaccustomed to wine, I had been intoxicated with the first glass, which I said, I had drunk before they arrived, while I was waiting for them at the Hotel de Paris between five and six o&#39;clock. I begged Simonov&#39;s pardon especially; I asked him to convey my explanations to all the others, especially to Zverkov, whom &#34;I seemed to remember as though in a dream&#34; I had insulted. I added that I would have called upon all of them myself, but my head ached, and besides I had not the face to. I was particularly pleased with a certain lightness, almost carelessness (strictly within the bounds of politeness, however), which was apparent in my style, and better than any possible arguments, gave them at once to understand that I took rather an independent view of &#34;all that unpleasantness last night&#34;; that I was by no means so utterly crushed as you, my friends, probably imagine; but on the contrary, looked upon it as a gentleman serenely respecting himself should look upon it. &#34;On a young hero&#39;s past no censure is cast!&#34; &#34;There is actually an aristocratic playfulness about it!&#34; I thought admiringly, as I read over the letter. &#34;And it&#39;s all because I am an intellectual and cultivated man! Another man in my place would not have known how to extricate himself, but here I have got out of it and am as jolly as ever again, and all because I am &#39;a cultivated and educated man of our day.&#39; And, indeed, perhaps, everything was due to the wine yesterday. H&#39;m!&#34; ... No, it was not the wine. I did not drink anything at all between five and six when I was waiting for them. I had lied to Simonov; I had lied shamelessly; and indeed I wasn&#39;t ashamed now.... Hang it all though, the great thing was that I was rid of it. I put six roubles in the letter, sealed it up, and asked Apollon to take it to Simonov. When he learned that there was money in the letter, Apollon became more respectful and agreed to take it. Towards evening I went out for a walk. My head was still aching and giddy after yesterday. But as evening came on and the twilight grew denser, my impressions and, following them, my thoughts, grew more and more different and confused. Something was not dead within me, in the depths of my heart and conscience it would not die, and it showed itself in acute depression. For the most part I jostled my way through the most crowded business streets, along Myeshtchansky Street, along Sadovy Street and in Yusupov Garden. I always liked particularly sauntering along these streets in the dusk, just when there were crowds of working people of all sorts going home from their daily work, with faces looking cross with anxiety. What I liked was just that cheap bustle, that bare prose. On this occasion the jostling of the streets irritated me more than ever, I could not make out what was wrong with me, I could not find the clue, something seemed rising up continually in my soul, painfully, and refusing to be appeased. I returned home completely upset, it was just as though some crime were lying on my conscience. The thought that Liza was coming worried me continually. It seemed queer to me that of all my recollections of yesterday this tormented me, as it were, especially, as it were, quite separately. Everything else I had quite succeeded in forgetting by the evening; I dismissed it all and was still perfectly satisfied with my letter to Simonov. But on this point I was not satisfied at all. It was as though I were worried only by Liza. &#34;What if she comes,&#34; I thought incessantly, &#34;well, it doesn&#39;t matter, let her come! H&#39;m! it&#39;s horrid that she should see, for instance, how I live. Yesterday I seemed such a hero to her, while now, h&#39;m! It&#39;s horrid, though, that I have let myself go so, the room looks like a beggar&#39;s. And I brought myself to go out to dinner in such a suit! And my American leather sofa with the stuffing sticking out. And my dressing-gown, which will not cover me, such tatters, and she will see all this and she will see Apollon. That beast is certain to insult her. He will fasten upon her in order to be rude to me. And I, of course, shall be panic-stricken as usual, I shall begin bowing and scraping before her and pulling my dressing-gown round me, I shall begin smiling, telling lies. Oh, the beastliness! And it isn&#39;t the beastliness of it that matters most! There is something more important, more loathsome, viler! Yes, viler! And to put on that dishonest lying mask again! ...&#34; When I reached that thought I fired up all at once. &#34;Why dishonest? How dishonest? I was speaking sincerely last night. I remember there was real feeling in me, too. What I wanted was to excite an honourable feeling in her.... Her crying was a good thing, it will have a good effect.&#34; Yet I could not feel at ease. All that evening, even when I had come back home, even after nine o&#39;clock, when I calculated that Liza could not possibly come, still she haunted me, and what was worse, she came back to my mind always in the same position. One moment out of all that had happened last night stood vividly before my imagination; the moment when I struck a match and saw her pale, distorted face, with its look of torture. And what a pitiful, what an unnatural, what a distorted smile she had at that moment! But I did not know then, that fifteen years later I should still in my imagination see Liza, always with the pitiful, distorted, inappropriate smile which was on her face at that minute. Next day I was ready again to look upon it all as nonsense, due to over-excited nerves, and, above all, as EXAGGERATED. I was always conscious of that weak point of mine, and sometimes very much afraid of it. &#34;I exaggerate everything, that is where I go wrong,&#34; I repeated to myself every hour. But, however, &#34;Liza will very likely come all the same,&#34; was the refrain with which all my reflections ended. I was so uneasy that I sometimes flew into a fury: &#34;She&#39;ll come, she is certain to come!&#34; I cried, running about the room, &#34;if not today, she will come tomorrow; she&#39;ll find me out! The damnable romanticism of these pure hearts! Oh, the vileness--oh, the silliness--oh, the stupidity of these &#39;wretched sentimental souls!&#39; Why, how fail to understand? How could one fail to understand? ...&#34; But at this point I stopped short, and in great confusion, indeed. And how few, how few words, I thought, in passing, were needed; how little of the idyllic (and affectedly, bookishly, artificially idyllic too) had sufficed to turn a whole human life at once according to my will. That&#39;s virginity, to be sure! Freshness of soil! At times a thought occurred to me, to go to her, &#34;to tell her all,&#34; and beg her not to come to me. But this thought stirred such wrath in me that I believed I should have crushed that &#34;damned&#34; Liza if she had chanced to be near me at the time. I should have insulted her, have spat at her, have turned her out, have struck her! One day passed, however, another and another; she did not come and I began to grow calmer. I felt particularly bold and cheerful after nine o&#39;clock, I even sometimes began dreaming, and rather sweetly: I, for instance, became the salvation of Liza, simply through her coming to me and my talking to her.... I develop her, educate her. Finally, I notice that she loves me, loves me passionately. I pretend not to understand (I don&#39;t know, however, why I pretend, just for effect, perhaps). At last all confusion, transfigured, trembling and sobbing, she flings herself at my feet and says that I am her saviour, and that she loves me better than anything in the world. I am amazed, but.... &#34;Liza,&#34; I say, &#34;can you imagine that I have not noticed your love? I saw it all, I divined it, but I did not dare to approach you first, because I had an influence over you and was afraid that you would force yourself, from gratitude, to respond to my love, would try to rouse in your heart a feeling which was perhaps absent, and I did not wish that ... because it would be tyranny ... it would be indelicate (in short, I launch off at that point into European, inexplicably lofty subtleties a la George Sand), but now, now you are mine, you are my creation, you are pure, you are good, you are my noble wife. &#39;Into my house come bold and free, Its rightful mistress there to be&#39;.&#34; Then we begin living together, go abroad and so on, and so on. In fact, in the end it seemed vulgar to me myself, and I began putting out my tongue at myself. Besides, they won&#39;t let her out, &#34;the hussy!&#34; I thought. They don&#39;t let them go out very readily, especially in the evening (for some reason I fancied she would come in the evening, and at seven o&#39;clock precisely). Though she did say she was not altogether a slave there yet, and had certain rights; so, h&#39;m! Damn it all, she will come, she is sure to come! It was a good thing, in fact, that Apollon distracted my attention at that time by his rudeness. He drove me beyond all patience! He was the bane of my life, the curse laid upon me by Providence. We had been squabbling continually for years, and I hated him. My God, how I hated him! I believe I had never hated anyone in my life as I hated him, especially at some moments. He was an elderly, dignified man, who worked part of his time as a tailor. But for some unknown reason he despised me beyond all measure, and looked down upon me insufferably. Though, indeed, he looked down upon everyone. Simply to glance at that flaxen, smoothly brushed head, at the tuft of hair he combed up on his forehead and oiled with sunflower oil, at that dignified mouth, compressed into the shape of the letter V, made one feel one was confronting a man who never doubted of himself. He was a pedant, to the most extreme point, the greatest pedant I had met on earth, and with that had a vanity only befitting Alexander of Macedon. He was in love with every button on his coat, every nail on his fingers--absolutely in love with them, and he looked it! In his behaviour to me he was a perfect tyrant, he spoke very little to me, and if he chanced to glance at me he gave me a firm, majestically self-confident and invariably ironical look that drove me sometimes to fury. He did his work with the air of doing me the greatest favour, though he did scarcely anything for me, and did not, indeed, consider himself bound to do anything. There could be no doubt that he looked upon me as the greatest fool on earth, and that &#34;he did not get rid of me&#34; was simply that he could get wages from me every month. He consented to do nothing for me for seven roubles a month. Many sins should be forgiven me for what I suffered from him. My hatred reached such a point that sometimes his very step almost threw me into convulsions. What I loathed particularly was his lisp. His tongue must have been a little too long or something of that sort, for he continually lisped, and seemed to be very proud of it, imagining that it greatly added to his dignity. He spoke in a slow, measured tone, with his hands behind his back and his eyes fixed on the ground. He maddened me particularly when he read aloud the psalms to himself behind his partition. Many a battle I waged over that reading! But he was awfully fond of reading aloud in the evenings, in a slow, even, sing-song voice, as though over the dead. It is interesting that that is how he has ended: he hires himself out to read the psalms over the dead, and at the same time he kills rats and makes blacking. But at that time I could not get rid of him, it was as though he were chemically combined with my existence. Besides, nothing would have induced him to consent to leave me. I could not live in furnished lodgings: my lodging was my private solitude, my shell, my cave, in which I concealed myself from all mankind, and Apollon seemed to me, for some reason, an integral part of that flat, and for seven years I could not turn him away. To be two or three days behind with his wages, for instance, was impossible. He would have made such a fuss, I should not have known where to hide my head. But I was so exasperated with everyone during those days, that I made up my mind for some reason and with some object to PUNISH Apollon and not to pay him for a fortnight the wages that were owing him. I had for a long time--for the last two years--been intending to do this, simply in order to teach him not to give himself airs with me, and to show him that if I liked I could withhold his wages. I purposed to say nothing to him about it, and was purposely silent indeed, in order to score off his pride and force him to be the first to speak of his wages. Then I would take the seven roubles out of a drawer, show him I have the money put aside on purpose, but that I won&#39;t, I won&#39;t, I simply won&#39;t pay him his wages, I won&#39;t just because that is &#34;what I wish,&#34; because &#34;I am master, and it is for me to decide,&#34; because he has been disrespectful, because he has been rude; but if he were to ask respectfully I might be softened and give it to him, otherwise he might wait another fortnight, another three weeks, a whole month.... But angry as I was, yet he got the better of me. I could not hold out for four days. He began as he always did begin in such cases, for there had been such cases already, there had been attempts (and it may be observed I knew all this beforehand, I knew his nasty tactics by heart). He would begin by fixing upon me an exceedingly severe stare, keeping it up for several minutes at a time, particularly on meeting me or seeing me out of the house. If I held out and pretended not to notice these stares, he would, still in silence, proceed to further tortures. All at once, A PROPOS of nothing, he would walk softly and smoothly into my room, when I was pacing up and down or reading, stand at the door, one hand behind his back and one foot behind the other, and fix upon me a stare more than severe, utterly contemptuous. If I suddenly asked him what he wanted, he would make me no answer, but continue staring at me persistently for some seconds, then, with a peculiar compression of his lips and a most significant air, deliberately turn round and deliberately go back to his room. Two hours later he would come out again and again present himself before me in the same way. It had happened that in my fury I did not even ask him what he wanted, but simply raised my head sharply and imperiously and began staring back at him. So we stared at one another for two minutes; at last he turned with deliberation and dignity and went back again for two hours. If I were still not brought to reason by all this, but persisted in my revolt, he would suddenly begin sighing while he looked at me, long, deep sighs as though measuring by them the depths of my moral degradation, and, of course, it ended at last by his triumphing completely: I raged and shouted, but still was forced to do what he wanted. This time the usual staring manoeuvres had scarcely begun when I lost my temper and flew at him in a fury. I was irritated beyond endurance apart from him. &#34;Stay,&#34; I cried, in a frenzy, as he was slowly and silently turning, with one hand behind his back, to go to his room. &#34;Stay! Come back, come back, I tell you!&#34; and I must have bawled so unnaturally, that he turned round and even looked at me with some wonder. However, he persisted in saying nothing, and that infuriated me. &#34;How dare you come and look at me like that without being sent for? Answer!&#34; After looking at me calmly for half a minute, he began turning round again. &#34;Stay!&#34; I roared, running up to him, &#34;don&#39;t stir! There. Answer, now: what did you come in to look at?&#34; &#34;If you have any order to give me it&#39;s my duty to carry it out,&#34; he answered, after another silent pause, with a slow, measured lisp, raising his eyebrows and calmly twisting his head from one side to another, all this with exasperating composure. &#34;That&#39;s not what I am asking you about, you torturer!&#34; I shouted, turning crimson with anger. &#34;I&#39;ll tell you why you came here myself: you see, I don&#39;t give you your wages, you are so proud you don&#39;t want to bow down and ask for it, and so you come to punish me with your stupid stares, to worry me and you have no sus-pic-ion how stupid it is--stupid, stupid, stupid, stupid! ...&#34; He would have turned round again without a word, but I seized him. &#34;Listen,&#34; I shouted to him. &#34;Here&#39;s the money, do you see, here it is,&#34; (I took it out of the table drawer); &#34;here&#39;s the seven roubles complete, but you are not going to have it, you ... are ... not ... going ... to ... have it until you come respectfully with bowed head to beg my pardon. Do you hear?&#34; &#34;That cannot be,&#34; he answered, with the most unnatural self-confidence. &#34;It shall be so,&#34; I said, &#34;I give you my word of honour, it shall be!&#34; &#34;And there&#39;s nothing for me to beg your pardon for,&#34; he went on, as though he had not noticed my exclamations at all. &#34;Why, besides, you called me a &#39;torturer,&#39; for which I can summon you at the police-station at any time for insulting behaviour.&#34; &#34;Go, summon me,&#34; I roared, &#34;go at once, this very minute, this very second! You are a torturer all the same! a torturer!&#34; But he merely looked at me, then turned, and regardless of my loud calls to him, he walked to his room with an even step and without looking round. &#34;If it had not been for Liza nothing of this would have happened,&#34; I decided inwardly. Then, after waiting a minute, I went myself behind his screen with a dignified and solemn air, though my heart was beating slowly and violently. &#34;Apollon,&#34; I said quietly and emphatically, though I was breathless, &#34;go at once without a minute&#39;s delay and fetch the police-officer.&#34; He had meanwhile settled himself at his table, put on his spectacles and taken up some sewing. But, hearing my order, he burst into a guffaw. &#34;At once, go this minute! Go on, or else you can&#39;t imagine what will happen.&#34; &#34;You are certainly out of your mind,&#34; he observed, without even raising his head, lisping as deliberately as ever and threading his needle. &#34;Whoever heard of a man sending for the police against himself? And as for being frightened--you are upsetting yourself about nothing, for nothing will come of it.&#34; &#34;Go!&#34; I shrieked, clutching him by the shoulder. I felt I should strike him in a minute. But I did not notice the door from the passage softly and slowly open at that instant and a figure come in, stop short, and begin staring at us in perplexity I glanced, nearly swooned with shame, and rushed back to my room. There, clutching at my hair with both hands, I leaned my head against the wall and stood motionless in that position. Two minutes later I heard Apollon&#39;s deliberate footsteps. &#34;There is some woman asking for you,&#34; he said, looking at me with peculiar severity. Then he stood aside and let in Liza. He would not go away, but stared at us sarcastically. &#34;Go away, go away,&#34; I commanded in desperation. At that moment my clock began whirring and wheezing and struck seven. IX &#34;Into my house come bold and free, Its rightful mistress there to be.&#34; I stood before her crushed, crestfallen, revoltingly confused, and I believe I smiled as I did my utmost to wrap myself in the skirts of my ragged wadded dressing-gown--exactly as I had imagined the scene not long before in a fit of depression. After standing over us for a couple of minutes Apollon went away, but that did not make me more at ease. What made it worse was that she, too, was overwhelmed with confusion, more so, in fact, than I should have expected. At the sight of me, of course. &#34;Sit down,&#34; I said mechanically, moving a chair up to the table, and I sat down on the sofa. She obediently sat down at once and gazed at me open-eyed, evidently expecting something from me at once. This naivete of expectation drove me to fury, but I restrained myself. She ought to have tried not to notice, as though everything had been as usual, while instead of that, she ... and I dimly felt that I should make her pay dearly for ALL THIS. &#34;You have found me in a strange position, Liza,&#34; I began, stammering and knowing that this was the wrong way to begin. &#34;No, no, don&#39;t imagine anything,&#34; I cried, seeing that she had suddenly flushed. &#34;I am not ashamed of my poverty.... On the contrary, I look with pride on my poverty. I am poor but honourable.... One can be poor and honourable,&#34; I muttered. &#34;However ... would you like tea?....&#34; &#34;No,&#34; she was beginning. &#34;Wait a minute.&#34; I leapt up and ran to Apollon. I had to get out of the room somehow. &#34;Apollon,&#34; I whispered in feverish haste, flinging down before him the seven roubles which had remained all the time in my clenched fist, &#34;here are your wages, you see I give them to you; but for that you must come to my rescue: bring me tea and a dozen rusks from the restaurant. If you won&#39;t go, you&#39;ll make me a miserable man! You don&#39;t know what this woman is.... This is--everything! You may be imagining something.... But you don&#39;t know what that woman is! ...&#34; Apollon, who had already sat down to his work and put on his spectacles again, at first glanced askance at the money without speaking or putting down his needle; then, without paying the slightest attention to me or making any answer, he went on busying himself with his needle, which he had not yet threaded. I waited before him for three minutes with my arms crossed A LA NAPOLEON. My temples were moist with sweat. I was pale, I felt it. But, thank God, he must have been moved to pity, looking at me. Having threaded his needle he deliberately got up from his seat, deliberately moved back his chair, deliberately took off his spectacles, deliberately counted the money, and finally asking me over his shoulder: &#34;Shall I get a whole portion?&#34; deliberately walked out of the room. As I was going back to Liza, the thought occurred to me on the way: shouldn&#39;t I run away just as I was in my dressing-gown, no matter where, and then let happen what would? I sat down again. She looked at me uneasily. For some minutes we were silent. &#34;I will kill him,&#34; I shouted suddenly, striking the table with my fist so that the ink spurted out of the inkstand. &#34;What are you saying!&#34; she cried, starting. &#34;I will kill him! kill him!&#34; I shrieked, suddenly striking the table in absolute frenzy, and at the same time fully understanding how stupid it was to be in such a frenzy. &#34;You don&#39;t know, Liza, what that torturer is to me. He is my torturer.... He has gone now to fetch some rusks; he ...&#34; And suddenly I burst into tears. It was an hysterical attack. How ashamed I felt in the midst of my sobs; but still I could not restrain them. She was frightened. &#34;What is the matter? What is wrong?&#34; she cried, fussing about me. &#34;Water, give me water, over there!&#34; I muttered in a faint voice, though I was inwardly conscious that I could have got on very well without water and without muttering in a faint voice. But I was, what is called, PUTTING IT ON, to save appearances, though the attack was a genuine one. She gave me water, looking at me in bewilderment. At that moment Apollon brought in the tea. It suddenly seemed to me that this commonplace, prosaic tea was horribly undignified and paltry after all that had happened, and I blushed crimson. Liza looked at Apollon with positive alarm. He went out without a glance at either of us. &#34;Liza, do you despise me?&#34; I asked, looking at her fixedly, trembling with impatience to know what she was thinking. She was confused, and did not know what to answer. &#34;Drink your tea,&#34; I said to her angrily. I was angry with myself, but, of course, it was she who would have to pay for it. A horrible spite against her suddenly surged up in my heart; I believe I could have killed her. To revenge myself on her I swore inwardly not to say a word to her all the time. &#34;She is the cause of it all,&#34; I thought. Our silence lasted for five minutes. The tea stood on the table; we did not touch it. I had got to the point of purposely refraining from beginning in order to embarrass her further; it was awkward for her to begin alone. Several times she glanced at me with mournful perplexity. I was obstinately silent. I was, of course, myself the chief sufferer, because I was fully conscious of the disgusting meanness of my spiteful stupidity, and yet at the same time I could not restrain myself. &#34;I want to... get away ... from there altogether,&#34; she began, to break the silence in some way, but, poor girl, that was just what she ought not to have spoken about at such a stupid moment to a man so stupid as I was. My heart positively ached with pity for her tactless and unnecessary straightforwardness. But something hideous at once stifled all compassion in me; it even provoked me to greater venom. I did not care what happened. Another five minutes passed. &#34;Perhaps I am in your way,&#34; she began timidly, hardly audibly, and was getting up. But as soon as I saw this first impulse of wounded dignity I positively trembled with spite, and at once burst out. &#34;Why have you come to me, tell me that, please?&#34; I began, gasping for breath and regardless of logical connection in my words. I longed to have it all out at once, at one burst; I did not even trouble how to begin. &#34;Why have you come? Answer, answer,&#34; I cried, hardly knowing what I was doing. &#34;I&#39;ll tell you, my good girl, why you have come. You&#39;ve come because I talked sentimental stuff to you then. So now you are soft as butter and longing for fine sentiments again. So you may as well know that I was laughing at you then. And I am laughing at you now. Why are you shuddering? Yes, I was laughing at you! I had been insulted just before, at dinner, by the fellows who came that evening before me. I came to you, meaning to thrash one of them, an officer; but I didn&#39;t succeed, I didn&#39;t find him; I had to avenge the insult on someone to get back my own again; you turned up, I vented my spleen on you and laughed at you. I had been humiliated, so I wanted to humiliate; I had been treated like a rag, so I wanted to show my power.... That&#39;s what it was, and you imagined I had come there on purpose to save you. Yes? You imagined that? You imagined that?&#34; I knew that she would perhaps be muddled and not take it all in exactly, but I knew, too, that she would grasp the gist of it, very well indeed. And so, indeed, she did. She turned white as a handkerchief, tried to say something, and her lips worked painfully; but she sank on a chair as though she had been felled by an axe. And all the time afterwards she listened to me with her lips parted and her eyes wide open, shuddering with awful terror. The cynicism, the cynicism of my words overwhelmed her.... &#34;Save you!&#34; I went on, jumping up from my chair and running up and down the room before her. &#34;Save you from what? But perhaps I am worse than you myself. Why didn&#39;t you throw it in my teeth when I was giving you that sermon: &#39;But what did you come here yourself for? was it to read us a sermon?&#39; Power, power was what I wanted then, sport was what I wanted, I wanted to wring out your tears, your humiliation, your hysteria--that was what I wanted then! Of course, I couldn&#39;t keep it up then, because I am a wretched creature, I was frightened, and, the devil knows why, gave you my address in my folly. Afterwards, before I got home, I was cursing and swearing at you because of that address, I hated you already because of the lies I had told you. Because I only like playing with words, only dreaming, but, do you know, what I really want is that you should all go to hell. That is what I want. I want peace; yes, I&#39;d sell the whole world for a farthing, straight off, so long as I was left in peace. Is the world to go to pot, or am I to go without my tea? I say that the world may go to pot for me so long as I always get my tea. Did you know that, or not? Well, anyway, I know that I am a blackguard, a scoundrel, an egoist, a sluggard. Here I have been shuddering for the last three days at the thought of your coming. And do you know what has worried me particularly for these three days? That I posed as such a hero to you, and now you would see me in a wretched torn dressing-gown, beggarly, loathsome. I told you just now that I was not ashamed of my poverty; so you may as well know that I am ashamed of it; I am more ashamed of it than of anything, more afraid of it than of being found out if I were a thief, because I am as vain as though I had been skinned and the very air blowing on me hurt. Surely by now you must realise that I shall never forgive you for having found me in this wretched dressing-gown, just as I was flying at Apollon like a spiteful cur. The saviour, the former hero, was flying like a mangy, unkempt sheep-dog at his lackey, and the lackey was jeering at him! And I shall never forgive you for the tears I could not help shedding before you just now, like some silly woman put to shame! And for what I am confessing to you now, I shall never forgive you either! Yes--you must answer for it all because you turned up like this, because I am a blackguard, because I am the nastiest, stupidest, absurdest and most envious of all the worms on earth, who are not a bit better than I am, but, the devil knows why, are never put to confusion; while I shall always be insulted by every louse, that is my doom! And what is it to me that you don&#39;t understand a word of this! And what do I care, what do I care about you, and whether you go to ruin there or not? Do you understand? How I shall hate you now after saying this, for having been here and listening. Why, it&#39;s not once in a lifetime a man speaks out like this, and then it is in hysterics! ... What more do you want? Why do you still stand confronting me, after all this? Why are you worrying me? Why don&#39;t you go?&#34; But at this point a strange thing happened. I was so accustomed to think and imagine everything from books, and to picture everything in the world to myself just as I had made it up in my dreams beforehand, that I could not all at once take in this strange circumstance. What happened was this: Liza, insulted and crushed by me, understood a great deal more than I imagined. She understood from all this what a woman understands first of all, if she feels genuine love, that is, that I was myself unhappy. The frightened and wounded expression on her face was followed first by a look of sorrowful perplexity. When I began calling myself a scoundrel and a blackguard and my tears flowed (the tirade was accompanied throughout by tears) her whole face worked convulsively. She was on the point of getting up and stopping me; when I finished she took no notice of my shouting: &#34;Why are you here, why don&#39;t you go away?&#34; but realised only that it must have been very bitter to me to say all this. Besides, she was so crushed, poor girl; she considered herself infinitely beneath me; how could she feel anger or resentment? She suddenly leapt up from her chair with an irresistible impulse and held out her hands, yearning towards me, though still timid and not daring to stir.... At this point there was a revulsion in my heart too. Then she suddenly rushed to me, threw her arms round me and burst into tears. I, too, could not restrain myself, and sobbed as I never had before. &#34;They won&#39;t let me ... I can&#39;t be good!&#34; I managed to articulate; then I went to the sofa, fell on it face downwards, and sobbed on it for a quarter of an hour in genuine hysterics. She came close to me, put her arms round me and stayed motionless in that position. But the trouble was that the hysterics could not go on for ever, and (I am writing the loathsome truth) lying face downwards on the sofa with my face thrust into my nasty leather pillow, I began by degrees to be aware of a far-away, involuntary but irresistible feeling that it would be awkward now for me to raise my head and look Liza straight in the face. Why was I ashamed? I don&#39;t know, but I was ashamed. The thought, too, came into my overwrought brain that our parts now were completely changed, that she was now the heroine, while I was just a crushed and humiliated creature as she had been before me that night--four days before.... And all this came into my mind during the minutes I was lying on my face on the sofa. My God! surely I was not envious of her then. I don&#39;t know, to this day I cannot decide, and at the time, of course, I was still less able to understand what I was feeling than now. I cannot get on without domineering and tyrannising over someone, but ... there is no explaining anything by reasoning and so it is useless to reason. I conquered myself, however, and raised my head; I had to do so sooner or later ... and I am convinced to this day that it was just because I was ashamed to look at her that another feeling was suddenly kindled and flamed up in my heart ... a feeling of mastery and possession. My eyes gleamed with passion, and I gripped her hands tightly. How I hated her and how I was drawn to her at that minute! The one feeling intensified the other. It was almost like an act of vengeance. At first there was a look of amazement, even of terror on her face, but only for one instant. She warmly and rapturously embraced me. X A quarter of an hour later I was rushing up and down the room in frenzied impatience, from minute to minute I went up to the screen and peeped through the crack at Liza. She was sitting on the ground with her head leaning against the bed, and must have been crying. But she did not go away, and that irritated me. This time she understood it all. I had insulted her finally, but ... there&#39;s no need to describe it. She realised that my outburst of passion had been simply revenge, a fresh humiliation, and that to my earlier, almost causeless hatred was added now a PERSONAL HATRED, born of envy.... Though I do not maintain positively that she understood all this distinctly; but she certainly did fully understand that I was a despicable man, and what was worse, incapable of loving her. I know I shall be told that this is incredible--but it is incredible to be as spiteful and stupid as I was; it may be added that it was strange I should not love her, or at any rate, appreciate her love. Why is it strange? In the first place, by then I was incapable of love, for I repeat, with me loving meant tyrannising and showing my moral superiority. I have never in my life been able to imagine any other sort of love, and have nowadays come to the point of sometimes thinking that love really consists in the right--freely given by the beloved object--to tyrannise over her. Even in my underground dreams I did not imagine love except as a struggle. I began it always with hatred and ended it with moral subjugation, and afterwards I never knew what to do with the subjugated object. And what is there to wonder at in that, since I had succeeded in so corrupting myself, since I was so out of touch with &#34;real life,&#34; as to have actually thought of reproaching her, and putting her to shame for having come to me to hear &#34;fine sentiments&#34;; and did not even guess that she had come not to hear fine sentiments, but to love me, because to a woman all reformation, all salvation from any sort of ruin, and all moral renewal is included in love and can only show itself in that form. I did not hate her so much, however, when I was running about the room and peeping through the crack in the screen. I was only insufferably oppressed by her being here. I wanted her to disappear. I wanted &#34;peace,&#34; to be left alone in my underground world. Real life oppressed me with its novelty so much that I could hardly breathe. But several minutes passed and she still remained, without stirring, as though she were unconscious. I had the shamelessness to tap softly at the screen as though to remind her.... She started, sprang up, and flew to seek her kerchief, her hat, her coat, as though making her escape from me.... Two minutes later she came from behind the screen and looked with heavy eyes at me. I gave a spiteful grin, which was forced, however, to KEEP UP APPEARANCES, and I turned away from her eyes. &#34;Good-bye,&#34; she said, going towards the door. I ran up to her, seized her hand, opened it, thrust something in it and closed it again. Then I turned at once and dashed away in haste to the other corner of the room to avoid seeing, anyway.... I did mean a moment since to tell a lie--to write that I did this accidentally, not knowing what I was doing through foolishness, through losing my head. But I don&#39;t want to lie, and so I will say straight out that I opened her hand and put the money in it ... from spite. It came into my head to do this while I was running up and down the room and she was sitting behind the screen. But this I can say for certain: though I did that cruel thing purposely, it was not an impulse from the heart, but came from my evil brain. This cruelty was so affected, so purposely made up, so completely a product of the brain, of books, that I could not even keep it up a minute--first I dashed away to avoid seeing her, and then in shame and despair rushed after Liza. I opened the door in the passage and began listening. &#34;Liza! Liza!&#34; I cried on the stairs, but in a low voice, not boldly. There was no answer, but I fancied I heard her footsteps, lower down on the stairs. &#34;Liza!&#34; I cried, more loudly. No answer. But at that minute I heard the stiff outer glass door open heavily with a creak and slam violently; the sound echoed up the stairs. She had gone. I went back to my room in hesitation. I felt horribly oppressed. I stood still at the table, beside the chair on which she had sat and looked aimlessly before me. A minute passed, suddenly I started; straight before me on the table I saw.... In short, I saw a crumpled blue five-rouble note, the one I had thrust into her hand a minute before. It was the same note; it could be no other, there was no other in the flat. So she had managed to fling it from her hand on the table at the moment when I had dashed into the further corner. Well! I might have expected that she would do that. Might I have expected it? No, I was such an egoist, I was so lacking in respect for my fellow-creatures that I could not even imagine she would do so. I could not endure it. A minute later I flew like a madman to dress, flinging on what I could at random and ran headlong after her. She could not have got two hundred paces away when I ran out into the street. It was a still night and the snow was coming down in masses and falling almost perpendicularly, covering the pavement and the empty street as though with a pillow. There was no one in the street, no sound was to be heard. The street lamps gave a disconsolate and useless glimmer. I ran two hundred paces to the cross-roads and stopped short. Where had she gone? And why was I running after her? Why? To fall down before her, to sob with remorse, to kiss her feet, to entreat her forgiveness! I longed for that, my whole breast was being rent to pieces, and never, never shall I recall that minute with indifference. But--what for? I thought. Should I not begin to hate her, perhaps, even tomorrow, just because I had kissed her feet today? Should I give her happiness? Had I not recognised that day, for the hundredth time, what I was worth? Should I not torture her? I stood in the snow, gazing into the troubled darkness and pondered this. &#34;And will it not be better?&#34; I mused fantastically, afterwards at home, stifling the living pang of my heart with fantastic dreams. &#34;Will it not be better that she should keep the resentment of the insult for ever? Resentment--why, it is purification; it is a most stinging and painful consciousness! Tomorrow I should have defiled her soul and have exhausted her heart, while now the feeling of insult will never die in her heart, and however loathsome the filth awaiting her--the feeling of insult will elevate and purify her ... by hatred ... h&#39;m! ... perhaps, too, by forgiveness.... Will all that make things easier for her though? ...&#34; And, indeed, I will ask on my own account here, an idle question: which is better--cheap happiness or exalted sufferings? Well, which is better? So I dreamed as I sat at home that evening, almost dead with the pain in my soul. Never had I endured such suffering and remorse, yet could there have been the faintest doubt when I ran out from my lodging that I should turn back half-way? I never met Liza again and I have heard nothing of her. I will add, too, that I remained for a long time afterwards pleased with the phrase about the benefit from resentment and hatred in spite of the fact that I almost fell ill from misery. * * * * * Even now, so many years later, all this is somehow a very evil memory. I have many evil memories now, but ... hadn&#39;t I better end my &#34;Notes&#34; here? I believe I made a mistake in beginning to write them, anyway I have felt ashamed all the time I&#39;ve been writing this story; so it&#39;s hardly literature so much as a corrective punishment. Why, to tell long stories, showing how I have spoiled my life through morally rotting in my corner, through lack of fitting environment, through divorce from real life, and rankling spite in my underground world, would certainly not be interesting; a novel needs a hero, and all the traits for an anti-hero are EXPRESSLY gathered together here, and what matters most, it all produces an unpleasant impression, for we are all divorced from life, we are all cripples, every one of us, more or less. We are so divorced from it that we feel at once a sort of loathing for real life, and so cannot bear to be reminded of it. Why, we have come almost to looking upon real life as an effort, almost as hard work, and we are all privately agreed that it is better in books. And why do we fuss and fume sometimes? Why are we perverse and ask for something else? We don&#39;t know what ourselves. It would be the worse for us if our petulant prayers were answered. Come, try, give any one of us, for instance, a little more independence, untie our hands, widen the spheres of our activity, relax the control and we ... yes, I assure you ... we should be begging to be under control again at once. I know that you will very likely be angry with me for that, and will begin shouting and stamping. Speak for yourself, you will say, and for your miseries in your underground holes, and don&#39;t dare to say all of us--excuse me, gentlemen, I am not justifying myself with that &#34;all of us.&#34; As for what concerns me in particular I have only in my life carried to an extreme what you have not dared to carry halfway, and what&#39;s more, you have taken your cowardice for good sense, and have found comfort in deceiving yourselves. So that perhaps, after all, there is more life in me than in you. Look into it more carefully! Why, we don&#39;t even know what living means now, what it is, and what it is called? Leave us alone without books and we shall be lost and in confusion at once. We shall not know what to join on to, what to cling to, what to love and what to hate, what to respect and what to despise. We are oppressed at being men--men with a real individual body and blood, we are ashamed of it, we think it a disgrace and try to contrive to be some sort of impossible generalised man. We are stillborn, and for generations past have been begotten, not by living fathers, and that suits us better and better. We are developing a taste for it. Soon we shall contrive to be born somehow from an idea. But enough; I don&#39;t want to write more from &#34;Underground.&#34; [The notes of this paradoxalist do not end here, however. He could not refrain from going on with them, but it seems to us that we may stop here.] End of Project Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky *** END OF THIS PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** ***** This file should be named 600.txt or 600.zip ***** This and all associated files of various formats will be found in: http://www.gutenberg.org/6/0/600/ Produced by Judith Boss. HTML version by Al Haines. Updated editions will replace the previous one--the old editions will be renamed. Creating the works from public domain print editions means that no one owns a United States copyright in these works, so the Foundation (and you!) can copy and distribute it in the United States without permission and without paying copyright royalties. Special rules, set forth in the General Terms of Use part of this license, apply to copying and distributing Project Gutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm concept and trademark. Project Gutenberg is a registered trademark, and may not be used if you charge for the eBooks, unless you receive specific permission. If you do not charge anything for copies of this eBook, complying with the rules is very easy. You may use this eBook for nearly any purpose such as creation of derivative works, reports, performances and research. They may be modified and printed and given away--you may do practically ANYTHING with public domain eBooks. Redistribution is subject to the trademark license, especially commercial redistribution. *** START: FULL LICENSE *** THE FULL PROJECT GUTENBERG LICENSE PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK To protect the Project Gutenberg-tm mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase &#34;Project Gutenberg&#34;), you agree to comply with all the terms of the Full Project Gutenberg-tm License (available with this file or online at http://gutenberg.net/license). Section 1. General Terms of Use and Redistributing Project Gutenberg-tm electronic works 1.A. By reading or using any part of this Project Gutenberg-tm electronic work, you indicate that you have read, understand, agree to and accept all the terms of this license and intellectual property (trademark/copyright) agreement. If you do not agree to abide by all the terms of this agreement, you must cease using and return or destroy all copies of Project Gutenberg-tm electronic works in your possession. If you paid a fee for obtaining a copy of or access to a Project Gutenberg-tm electronic work and you do not agree to be bound by the terms of this agreement, you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph 1.E.8. 1.B. &#34;Project Gutenberg&#34; is a registered trademark. It may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement. There are a few things that you can do with most Project Gutenberg-tm electronic works even without complying with the full terms of this agreement. See paragraph 1.C below. There are a lot of things you can do with Project Gutenberg-tm electronic works if you follow the terms of this agreement and help preserve free future access to Project Gutenberg-tm electronic works. See paragraph 1.E below. 1.C. The Project Gutenberg Literary Archive Foundation (&#34;the Foundation&#34; or PGLAF), owns a compilation copyright in the collection of Project Gutenberg-tm electronic works. Nearly all the individual works in the collection are in the public domain in the United States. If an individual work is in the public domain in the United States and you are located in the United States, we do not claim a right to prevent you from copying, distributing, performing, displaying or creating derivative works based on the work as long as all references to Project Gutenberg are removed. Of course, we hope that you will support the Project Gutenberg-tm mission of promoting free access to electronic works by freely sharing Project Gutenberg-tm works in compliance with the terms of this agreement for keeping the Project Gutenberg-tm name associated with the work. You can easily comply with the terms of this agreement by keeping this work in the same format with its attached full Project Gutenberg-tm License when you share it without charge with others. 1.D. The copyright laws of the place where you are located also govern what you can do with this work. Copyright laws in most countries are in a constant state of change. If you are outside the United States, check the laws of your country in addition to the terms of this agreement before downloading, copying, displaying, performing, distributing or creating derivative works based on this work or any other Project Gutenberg-tm work. The Foundation makes no representations concerning the copyright status of any work in any country outside the United States. 1.E. Unless you have removed all references to Project Gutenberg: 1.E.1. The following sentence, with active links to, or other immediate access to, the full Project Gutenberg-tm License must appear prominently whenever any copy of a Project Gutenberg-tm work (any work on which the phrase &#34;Project Gutenberg&#34; appears, or with which the phrase &#34;Project Gutenberg&#34; is associated) is accessed, displayed, performed, viewed, copied or distributed: This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net 1.E.2. If an individual Project Gutenberg-tm electronic work is derived from the public domain (does not contain a notice indicating that it is posted with permission of the copyright holder), the work can be copied and distributed to anyone in the United States without paying any fees or charges. If you are redistributing or providing access to a work with the phrase &#34;Project Gutenberg&#34; associated with or appearing on the work, you must comply either with the requirements of paragraphs 1.E.1 through 1.E.7 or obtain permission for the use of the work and the Project Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or 1.E.9. 1.E.3. If an individual Project Gutenberg-tm electronic work is posted with the permission of the copyright holder, your use and distribution must comply with both paragraphs 1.E.1 through 1.E.7 and any additional terms imposed by the copyright holder. Additional terms will be linked to the Project Gutenberg-tm License for all works posted with the permission of the copyright holder found at the beginning of this work. 1.E.4. Do not unlink or detach or remove the full Project Gutenberg-tm License terms from this work, or any files containing a part of this work or any other work associated with Project Gutenberg-tm. 1.E.5. Do not copy, display, perform, distribute or redistribute this electronic work, or any part of this electronic work, without prominently displaying the sentence set forth in paragraph 1.E.1 with active links or immediate access to the full terms of the Project Gutenberg-tm License. 1.E.6. You may convert to and distribute this work in any binary, compressed, marked up, nonproprietary or proprietary form, including any word processing or hypertext form. However, if you provide access to or distribute copies of a Project Gutenberg-tm work in a format other than &#34;Plain Vanilla ASCII&#34; or other format used in the official version posted on the official Project Gutenberg-tm web site (www.gutenberg.net), you must, at no additional cost, fee or expense to the user, provide a copy, a means of exporting a copy, or a means of obtaining a copy upon request, of the work in its original &#34;Plain Vanilla ASCII&#34; or other form. Any alternate format must include the full Project Gutenberg-tm License as specified in paragraph 1.E.1. 1.E.7. Do not charge a fee for access to, viewing, displaying, performing, copying or distributing any Project Gutenberg-tm works unless you comply with paragraph 1.E.8 or 1.E.9. 1.E.8. You may charge a reasonable fee for copies of or providing access to or distributing Project Gutenberg-tm electronic works provided that - You pay a royalty fee of 20% of the gross profits you derive from the use of Project Gutenberg-tm works calculated using the method you already use to calculate your applicable taxes. The fee is owed to the owner of the Project Gutenberg-tm trademark, but he has agreed to donate royalties under this paragraph to the Project Gutenberg Literary Archive Foundation. Royalty payments must be paid within 60 days following each date on which you prepare (or are legally required to prepare) your periodic tax returns. Royalty payments should be clearly marked as such and sent to the Project Gutenberg Literary Archive Foundation at the address specified in Section 4, &#34;Information about donations to the Project Gutenberg Literary Archive Foundation.&#34; - You provide a full refund of any money paid by a user who notifies you in writing (or by e-mail) within 30 days of receipt that s/he does not agree to the terms of the full Project Gutenberg-tm License. You must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of Project Gutenberg-tm works. - You provide, in accordance with paragraph 1.F.3, a full refund of any money paid for a work or a replacement copy, if a defect in the electronic work is discovered and reported to you within 90 days of receipt of the work. - You comply with all other terms of this agreement for free distribution of Project Gutenberg-tm works. 1.E.9. If you wish to charge a fee or distribute a Project Gutenberg-tm electronic work or group of works on different terms than are set forth in this agreement, you must obtain permission in writing from both the Project Gutenberg Literary Archive Foundation and Michael Hart, the owner of the Project Gutenberg-tm trademark. Contact the Foundation as set forth in Section 3 below. 1.F. 1.F.1. Project Gutenberg volunteers and employees expend considerable effort to identify, do copyright research on, transcribe and proofread public domain works in creating the Project Gutenberg-tm collection. Despite these efforts, Project Gutenberg-tm electronic works, and the medium on which they may be stored, may contain &#34;Defects,&#34; such as, but not limited to, incomplete, inaccurate or corrupt data, transcription errors, a copyright or other intellectual property infringement, a defective or damaged disk or other medium, a computer virus, or computer codes that damage or cannot be read by your equipment. 1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the &#34;Right of Replacement or Refund&#34; described in paragraph 1.F.3, the Project Gutenberg Literary Archive Foundation, the owner of the Project Gutenberg-tm trademark, and any other party distributing a Project Gutenberg-tm electronic work under this agreement, disclaim all liability to you for damages, costs and expenses, including legal fees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE PROVIDED IN PARAGRAPH F3. YOU AGREE THAT THE FOUNDATION, THE TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGE. 1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a defect in this electronic work within 90 days of receiving it, you can receive a refund of the money (if any) you paid for it by sending a written explanation to the person you received the work from. If you received the work on a physical medium, you must return the medium with your written explanation. The person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund. If you received the work electronically, the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund. If the second copy is also defective, you may demand a refund in writing without further opportunities to fix the problem. 1.F.4. Except for the limited right of replacement or refund set forth in paragraph 1.F.3, this work is provided to you &#39;AS-IS&#39; WITH NO OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTIBILITY OR FITNESS FOR ANY PURPOSE. 1.F.5. Some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages. If any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement, the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law. The invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions. 1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the trademark owner, any agent or employee of the Foundation, anyone providing copies of Project Gutenberg-tm electronic works in accordance with this agreement, and any volunteers associated with the production, promotion and distribution of Project Gutenberg-tm electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg-tm work, (b) alteration, modification, or additions or deletions to any Project Gutenberg-tm work, and (c) any Defect you cause. Section 2. Information about the Mission of Project Gutenberg-tm Project Gutenberg-tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers. It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life. Volunteers and financial support to provide volunteers with the assistance they need, is critical to reaching Project Gutenberg-tm&#39;s goals and ensuring that the Project Gutenberg-tm collection will remain freely available for generations to come. In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg-tm and future generations. To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation web page at http://www.pglaf.org. Section 3. Information about the Project Gutenberg Literary Archive Foundation The Project Gutenberg Literary Archive Foundation is a non profit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the Internal Revenue Service. The Foundation&#39;s EIN or federal tax identification number is 64-6221541. Its 501(c)(3) letter is posted at http://pglaf.org/fundraising. Contributions to the Project Gutenberg Literary Archive Foundation are tax deductible to the full extent permitted by U.S. federal laws and your state&#39;s laws. The Foundation&#39;s principal office is located at 4557 Melan Dr. S. Fairbanks, AK, 99712., but its volunteers and employees are scattered throughout numerous locations. Its business office is located at 809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887, email business@pglaf.org. Email contact links and up to date contact information can be found at the Foundation&#39;s web site and official page at http://pglaf.org For additional contact information: Dr. Gregory B. Newby Chief Executive and Director gbnewby@pglaf.org Section 4. Information about Donations to the Project Gutenberg Literary Archive Foundation Project Gutenberg-tm depends upon and cannot survive without wide spread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine readable form accessible by the widest array of equipment including outdated equipment. Many small donations ($1 to $5,000) are particularly important to maintaining tax exempt status with the IRS. The Foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the United States. Compliance requirements are not uniform and it takes a considerable effort, much paperwork and many fees to meet and keep up with these requirements. We do not solicit donations in locations where we have not received written confirmation of compliance. To SEND DONATIONS or determine the status of compliance for any particular state visit http://pglaf.org While we cannot and do not solicit contributions from states where we have not met the solicitation requirements, we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate. International donations are gratefully accepted, but we cannot make any statements concerning tax treatment of donations received from outside the United States. U.S. laws alone swamp our small staff. Please check the Project Gutenberg Web pages for current donation methods and addresses. Donations are accepted in a number of other ways including including checks, online payments and credit card donations. To donate, please visit: http://pglaf.org/donate Section 5. General Information About Project Gutenberg-tm electronic works. Professor Michael S. Hart is the originator of the Project Gutenberg-tm concept of a library of electronic works that could be freely shared with anyone. For thirty years, he produced and distributed Project Gutenberg-tm eBooks with only a loose network of volunteer support. Project Gutenberg-tm eBooks are often created from several printed editions, all of which are confirmed as Public Domain in the U.S. unless a copyright notice is included. Thus, we do not necessarily keep eBooks in compliance with any particular paper edition. Most people start at our Web site which has the main PG search facility: http://www.gutenberg.net This Web site includes information about Project Gutenberg-tm, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks. . Next, we form a clean text with lower case (without special characters, digits and extra spaces) and split it into individual word, for word score computation and formation of the word histogram. . The reason to form a clean text is so that the algorithm won&#39;t treat, i.e. &quot;understanding&quot; and understanding, as two different words. . # generate clean text clean_text = text.lower() # convert all uppercase characters into lowercase characters clean_text = re.sub(r&#39; W&#39;,&#39; &#39;,clean_text) # replace character other than [a-zA-Z0-9] with empty space clean_text = re.sub(r&#39; d&#39;,&#39; &#39;,clean_text) # replace digit with empty space clean_text = re.sub(r&#39; s+&#39;,&#39; &#39;,clean_text) # replace one or more spaces with a single space print(clean_text) . project gutenberg s notes from the underground by feodor dostoevsky this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg net title notes from the underground author feodor dostoevsky posting date september ebook release date july language english start of this project gutenberg ebook notes from the underground produced by judith boss html version by al haines notes from the underground fyodor dostoyevsky part i underground the author of the diary and the diary itself are of course imaginary nevertheless it is clear that such persons as the writer of these notes not only may but positively must exist in our society when we consider the circumstances in the midst of which our society is formed i have tried to expose to the view of the public more distinctly than is commonly done one of the characters of the recent past he is one of the representatives of a generation still living in this fragment entitled underground this person introduces himself and his views and as it were tries to explain the causes owing to which he has made his appearance and was bound to make his appearance in our midst in the second fragment there are added the actual notes of this person concerning certain events in his life author s note i i am a sick man i am a spiteful man i am an unattractive man i believe my liver is diseased however i know nothing at all about my disease and do not know for certain what ails me i don t consult a doctor for it and never have though i have a respect for medicine and doctors besides i am extremely superstitious sufficiently so to respect medicine anyway i am well educated enough not to be superstitious but i am superstitious no i refuse to consult a doctor from spite that you probably will not understand well i understand it though of course i can t explain who it is precisely that i am mortifying in this case by my spite i am perfectly well aware that i cannot pay out the doctors by not consulting them i know better than anyone that by all this i am only injuring myself and no one else but still if i don t consult a doctor it is from spite my liver is bad well let it get worse i have been going on like that for a long time twenty years now i am forty i used to be in the government service but am no longer i was a spiteful official i was rude and took pleasure in being so i did not take bribes you see so i was bound to find a recompense in that at least a poor jest but i will not scratch it out i wrote it thinking it would sound very witty but now that i have seen myself that i only wanted to show off in a despicable way i will not scratch it out on purpose when petitioners used to come for information to the table at which i sat i used to grind my teeth at them and felt intense enjoyment when i succeeded in making anybody unhappy i almost did succeed for the most part they were all timid people of course they were petitioners but of the uppish ones there was one officer in particular i could not endure he simply would not be humble and clanked his sword in a disgusting way i carried on a feud with him for eighteen months over that sword at last i got the better of him he left off clanking it that happened in my youth though but do you know gentlemen what was the chief point about my spite why the whole point the real sting of it lay in the fact that continually even in the moment of the acutest spleen i was inwardly conscious with shame that i was not only not a spiteful but not even an embittered man that i was simply scaring sparrows at random and amusing myself by it i might foam at the mouth but bring me a doll to play with give me a cup of tea with sugar in it and maybe i should be appeased i might even be genuinely touched though probably i should grind my teeth at myself afterwards and lie awake at night with shame for months after that was my way i was lying when i said just now that i was a spiteful official i was lying from spite i was simply amusing myself with the petitioners and with the officer and in reality i never could become spiteful i was conscious every moment in myself of many very many elements absolutely opposite to that i felt them positively swarming in me these opposite elements i knew that they had been swarming in me all my life and craving some outlet from me but i would not let them would not let them purposely would not let them come out they tormented me till i was ashamed they drove me to convulsions and sickened me at last how they sickened me now are not you fancying gentlemen that i am expressing remorse for something now that i am asking your forgiveness for something i am sure you are fancying that however i assure you i do not care if you are it was not only that i could not become spiteful i did not know how to become anything neither spiteful nor kind neither a rascal nor an honest man neither a hero nor an insect now i am living out my life in my corner taunting myself with the spiteful and useless consolation that an intelligent man cannot become anything seriously and it is only the fool who becomes anything yes a man in the nineteenth century must and morally ought to be pre eminently a characterless creature a man of character an active man is pre eminently a limited creature that is my conviction of forty years i am forty years old now and you know forty years is a whole lifetime you know it is extreme old age to live longer than forty years is bad manners is vulgar immoral who does live beyond forty answer that sincerely and honestly i will tell you who do fools and worthless fellows i tell all old men that to their face all these venerable old men all these silver haired and reverend seniors i tell the whole world that to its face i have a right to say so for i shall go on living to sixty myself to seventy to eighty stay let me take breath you imagine no doubt gentlemen that i want to amuse you you are mistaken in that too i am by no means such a mirthful person as you imagine or as you may imagine however irritated by all this babble and i feel that you are irritated you think fit to ask me who i am then my answer is i am a collegiate assessor i was in the service that i might have something to eat and solely for that reason and when last year a distant relation left me six thousand roubles in his will i immediately retired from the service and settled down in my corner i used to live in this corner before but now i have settled down in it my room is a wretched horrid one in the outskirts of the town my servant is an old country woman ill natured from stupidity and moreover there is always a nasty smell about her i am told that the petersburg climate is bad for me and that with my small means it is very expensive to live in petersburg i know all that better than all these sage and experienced counsellors and monitors but i am remaining in petersburg i am not going away from petersburg i am not going away because ech why it is absolutely no matter whether i am going away or not going away but what can a decent man speak of with most pleasure answer of himself well so i will talk about myself ii i want now to tell you gentlemen whether you care to hear it or not why i could not even become an insect i tell you solemnly that i have many times tried to become an insect but i was not equal even to that i swear gentlemen that to be too conscious is an illness a real thorough going illness for man s everyday needs it would have been quite enough to have the ordinary human consciousness that is half or a quarter of the amount which falls to the lot of a cultivated man of our unhappy nineteenth century especially one who has the fatal ill luck to inhabit petersburg the most theoretical and intentional town on the whole terrestrial globe there are intentional and unintentional towns it would have been quite enough for instance to have the consciousness by which all so called direct persons and men of action live i bet you think i am writing all this from affectation to be witty at the expense of men of action and what is more that from ill bred affectation i am clanking a sword like my officer but gentlemen whoever can pride himself on his diseases and even swagger over them though after all everyone does do that people do pride themselves on their diseases and i do may be more than anyone we will not dispute it my contention was absurd but yet i am firmly persuaded that a great deal of consciousness every sort of consciousness in fact is a disease i stick to that let us leave that too for a minute tell me this why does it happen that at the very yes at the very moments when i am most capable of feeling every refinement of all that is sublime and beautiful as they used to say at one time it would as though of design happen to me not only to feel but to do such ugly things such that well in short actions that all perhaps commit but which as though purposely occurred to me at the very time when i was most conscious that they ought not to be committed the more conscious i was of goodness and of all that was sublime and beautiful the more deeply i sank into my mire and the more ready i was to sink in it altogether but the chief point was that all this was as it were not accidental in me but as though it were bound to be so it was as though it were my most normal condition and not in the least disease or depravity so that at last all desire in me to struggle against this depravity passed it ended by my almost believing perhaps actually believing that this was perhaps my normal condition but at first in the beginning what agonies i endured in that struggle i did not believe it was the same with other people and all my life i hid this fact about myself as a secret i was ashamed even now perhaps i am ashamed i got to the point of feeling a sort of secret abnormal despicable enjoyment in returning home to my corner on some disgusting petersburg night acutely conscious that that day i had committed a loathsome action again that what was done could never be undone and secretly inwardly gnawing gnawing at myself for it tearing and consuming myself till at last the bitterness turned into a sort of shameful accursed sweetness and at last into positive real enjoyment yes into enjoyment into enjoyment i insist upon that i have spoken of this because i keep wanting to know for a fact whether other people feel such enjoyment i will explain the enjoyment was just from the too intense consciousness of one s own degradation it was from feeling oneself that one had reached the last barrier that it was horrible but that it could not be otherwise that there was no escape for you that you never could become a different man that even if time and faith were still left you to change into something different you would most likely not wish to change or if you did wish to even then you would do nothing because perhaps in reality there was nothing for you to change into and the worst of it was and the root of it all that it was all in accord with the normal fundamental laws of over acute consciousness and with the inertia that was the direct result of those laws and that consequently one was not only unable to change but could do absolutely nothing thus it would follow as the result of acute consciousness that one is not to blame in being a scoundrel as though that were any consolation to the scoundrel once he has come to realise that he actually is a scoundrel but enough ech i have talked a lot of nonsense but what have i explained how is enjoyment in this to be explained but i will explain it i will get to the bottom of it that is why i have taken up my pen i for instance have a great deal of amour propre i am as suspicious and prone to take offence as a humpback or a dwarf but upon my word i sometimes have had moments when if i had happened to be slapped in the face i should perhaps have been positively glad of it i say in earnest that i should probably have been able to discover even in that a peculiar sort of enjoyment the enjoyment of course of despair but in despair there are the most intense enjoyments especially when one is very acutely conscious of the hopelessness of one s position and when one is slapped in the face why then the consciousness of being rubbed into a pulp would positively overwhelm one the worst of it is look at it which way one will it still turns out that i was always the most to blame in everything and what is most humiliating of all to blame for no fault of my own but so to say through the laws of nature in the first place to blame because i am cleverer than any of the people surrounding me i have always considered myself cleverer than any of the people surrounding me and sometimes would you believe it have been positively ashamed of it at any rate i have all my life as it were turned my eyes away and never could look people straight in the face to blame finally because even if i had had magnanimity i should only have had more suffering from the sense of its uselessness i should certainly have never been able to do anything from being magnanimous neither to forgive for my assailant would perhaps have slapped me from the laws of nature and one cannot forgive the laws of nature nor to forget for even if it were owing to the laws of nature it is insulting all the same finally even if i had wanted to be anything but magnanimous had desired on the contrary to revenge myself on my assailant i could not have revenged myself on any one for anything because i should certainly never have made up my mind to do anything even if i had been able to why should i not have made up my mind about that in particular i want to say a few words iii with people who know how to revenge themselves and to stand up for themselves in general how is it done why when they are possessed let us suppose by the feeling of revenge then for the time there is nothing else but that feeling left in their whole being such a gentleman simply dashes straight for his object like an infuriated bull with its horns down and nothing but a wall will stop him by the way facing the wall such gentlemen that is the direct persons and men of action are genuinely nonplussed for them a wall is not an evasion as for us people who think and consequently do nothing it is not an excuse for turning aside an excuse for which we are always very glad though we scarcely believe in it ourselves as a rule no they are nonplussed in all sincerity the wall has for them something tranquillising morally soothing final maybe even something mysterious but of the wall later well such a direct person i regard as the real normal man as his tender mother nature wished to see him when she graciously brought him into being on the earth i envy such a man till i am green in the face he is stupid i am not disputing that but perhaps the normal man should be stupid how do you know perhaps it is very beautiful in fact and i am the more persuaded of that suspicion if one can call it so by the fact that if you take for instance the antithesis of the normal man that is the man of acute consciousness who has come of course not out of the lap of nature but out of a retort this is almost mysticism gentlemen but i suspect this too this retort made man is sometimes so nonplussed in the presence of his antithesis that with all his exaggerated consciousness he genuinely thinks of himself as a mouse and not a man it may be an acutely conscious mouse yet it is a mouse while the other is a man and therefore et caetera et caetera and the worst of it is he himself his very own self looks on himself as a mouse no one asks him to do so and that is an important point now let us look at this mouse in action let us suppose for instance that it feels insulted too and it almost always does feel insulted and wants to revenge itself too there may even be a greater accumulation of spite in it than in l homme de la nature et de la verite the base and nasty desire to vent that spite on its assailant rankles perhaps even more nastily in it than in l homme de la nature et de la verite for through his innate stupidity the latter looks upon his revenge as justice pure and simple while in consequence of his acute consciousness the mouse does not believe in the justice of it to come at last to the deed itself to the very act of revenge apart from the one fundamental nastiness the luckless mouse succeeds in creating around it so many other nastinesses in the form of doubts and questions adds to the one question so many unsettled questions that there inevitably works up around it a sort of fatal brew a stinking mess made up of its doubts emotions and of the contempt spat upon it by the direct men of action who stand solemnly about it as judges and arbitrators laughing at it till their healthy sides ache of course the only thing left for it is to dismiss all that with a wave of its paw and with a smile of assumed contempt in which it does not even itself believe creep ignominiously into its mouse hole there in its nasty stinking underground home our insulted crushed and ridiculed mouse promptly becomes absorbed in cold malignant and above all everlasting spite for forty years together it will remember its injury down to the smallest most ignominious details and every time will add of itself details still more ignominious spitefully teasing and tormenting itself with its own imagination it will itself be ashamed of its imaginings but yet it will recall it all it will go over and over every detail it will invent unheard of things against itself pretending that those things might happen and will forgive nothing maybe it will begin to revenge itself too but as it were piecemeal in trivial ways from behind the stove incognito without believing either in its own right to vengeance or in the success of its revenge knowing that from all its efforts at revenge it will suffer a hundred times more than he on whom it revenges itself while he i daresay will not even scratch himself on its deathbed it will recall it all over again with interest accumulated over all the years and but it is just in that cold abominable half despair half belief in that conscious burying oneself alive for grief in the underworld for forty years in that acutely recognised and yet partly doubtful hopelessness of one s position in that hell of unsatisfied desires turned inward in that fever of oscillations of resolutions determined for ever and repented of again a minute later that the savour of that strange enjoyment of which i have spoken lies it is so subtle so difficult of analysis that persons who are a little limited or even simply persons of strong nerves will not understand a single atom of it possibly you will add on your own account with a grin people will not understand it either who have never received a slap in the face and in that way you will politely hint to me that i too perhaps have had the experience of a slap in the face in my life and so i speak as one who knows i bet that you are thinking that but set your minds at rest gentlemen i have not received a slap in the face though it is absolutely a matter of indifference to me what you may think about it possibly i even regret myself that i have given so few slaps in the face during my life but enough not another word on that subject of such extreme interest to you i will continue calmly concerning persons with strong nerves who do not understand a certain refinement of enjoyment though in certain circumstances these gentlemen bellow their loudest like bulls though this let us suppose does them the greatest credit yet as i have said already confronted with the impossible they subside at once the impossible means the stone wall what stone wall why of course the laws of nature the deductions of natural science mathematics as soon as they prove to you for instance that you are descended from a monkey then it is no use scowling accept it for a fact when they prove to you that in reality one drop of your own fat must be dearer to you than a hundred thousand of your fellow creatures and that this conclusion is the final solution of all so called virtues and duties and all such prejudices and fancies then you have just to accept it there is no help for it for twice two is a law of mathematics just try refuting it upon my word they will shout at you it is no use protesting it is a case of twice two makes four nature does not ask your permission she has nothing to do with your wishes and whether you like her laws or dislike them you are bound to accept her as she is and consequently all her conclusions a wall you see is a wall and so on and so on merciful heavens but what do i care for the laws of nature and arithmetic when for some reason i dislike those laws and the fact that twice two makes four of course i cannot break through the wall by battering my head against it if i really have not the strength to knock it down but i am not going to be reconciled to it simply because it is a stone wall and i have not the strength as though such a stone wall really were a consolation and really did contain some word of conciliation simply because it is as true as twice two makes four oh absurdity of absurdities how much better it is to understand it all to recognise it all all the impossibilities and the stone wall not to be reconciled to one of those impossibilities and stone walls if it disgusts you to be reconciled to it by the way of the most inevitable logical combinations to reach the most revolting conclusions on the everlasting theme that even for the stone wall you are yourself somehow to blame though again it is as clear as day you are not to blame in the least and therefore grinding your teeth in silent impotence to sink into luxurious inertia brooding on the fact that there is no one even for you to feel vindictive against that you have not and perhaps never will have an object for your spite that it is a sleight of hand a bit of juggling a card sharper s trick that it is simply a mess no knowing what and no knowing who but in spite of all these uncertainties and jugglings still there is an ache in you and the more you do not know the worse the ache iv ha ha ha you will be finding enjoyment in toothache next you cry with a laugh well even in toothache there is enjoyment i answer i had toothache for a whole month and i know there is in that case of course people are not spiteful in silence but moan but they are not candid moans they are malignant moans and the malignancy is the whole point the enjoyment of the sufferer finds expression in those moans if he did not feel enjoyment in them he would not moan it is a good example gentlemen and i will develop it those moans express in the first place all the aimlessness of your pain which is so humiliating to your consciousness the whole legal system of nature on which you spit disdainfully of course but from which you suffer all the same while she does not they express the consciousness that you have no enemy to punish but that you have pain the consciousness that in spite of all possible wagenheims you are in complete slavery to your teeth that if someone wishes it your teeth will leave off aching and if he does not they will go on aching another three months and that finally if you are still contumacious and still protest all that is left you for your own gratification is to thrash yourself or beat your wall with your fist as hard as you can and absolutely nothing more well these mortal insults these jeers on the part of someone unknown end at last in an enjoyment which sometimes reaches the highest degree of voluptuousness i ask you gentlemen listen sometimes to the moans of an educated man of the nineteenth century suffering from toothache on the second or third day of the attack when he is beginning to moan not as he moaned on the first day that is not simply because he has toothache not just as any coarse peasant but as a man affected by progress and european civilisation a man who is divorced from the soil and the national elements as they express it now a days his moans become nasty disgustingly malignant and go on for whole days and nights and of course he knows himself that he is doing himself no sort of good with his moans he knows better than anyone that he is only lacerating and harassing himself and others for nothing he knows that even the audience before whom he is making his efforts and his whole family listen to him with loathing do not put a ha porth of faith in him and inwardly understand that he might moan differently more simply without trills and flourishes and that he is only amusing himself like that from ill humour from malignancy well in all these recognitions and disgraces it is that there lies a voluptuous pleasure as though he would say i am worrying you i am lacerating your hearts i am keeping everyone in the house awake well stay awake then you too feel every minute that i have toothache i am not a hero to you now as i tried to seem before but simply a nasty person an impostor well so be it then i am very glad that you see through me it is nasty for you to hear my despicable moans well let it be nasty here i will let you have a nastier flourish in a minute you do not understand even now gentlemen no it seems our development and our consciousness must go further to understand all the intricacies of this pleasure you laugh delighted my jests gentlemen are of course in bad taste jerky involved lacking self confidence but of course that is because i do not respect myself can a man of perception respect himself at all v come can a man who attempts to find enjoyment in the very feeling of his own degradation possibly have a spark of respect for himself i am not saying this now from any mawkish kind of remorse and indeed i could never endure saying forgive me papa i won t do it again not because i am incapable of saying that on the contrary perhaps just because i have been too capable of it and in what a way too as though of design i used to get into trouble in cases when i was not to blame in any way that was the nastiest part of it at the same time i was genuinely touched and penitent i used to shed tears and of course deceived myself though i was not acting in the least and there was a sick feeling in my heart at the time for that one could not blame even the laws of nature though the laws of nature have continually all my life offended me more than anything it is loathsome to remember it all but it was loathsome even then of course a minute or so later i would realise wrathfully that it was all a lie a revolting lie an affected lie that is all this penitence this emotion these vows of reform you will ask why did i worry myself with such antics answer because it was very dull to sit with one s hands folded and so one began cutting capers that is really it observe yourselves more carefully gentlemen then you will understand that it is so i invented adventures for myself and made up a life so as at least to live in some way how many times it has happened to me well for instance to take offence simply on purpose for nothing and one knows oneself of course that one is offended at nothing that one is putting it on but yet one brings oneself at last to the point of being really offended all my life i have had an impulse to play such pranks so that in the end i could not control it in myself another time twice in fact i tried hard to be in love i suffered too gentlemen i assure you in the depth of my heart there was no faith in my suffering only a faint stir of mockery but yet i did suffer and in the real orthodox way i was jealous beside myself and it was all from ennui gentlemen all from ennui inertia overcame me you know the direct legitimate fruit of consciousness is inertia that is conscious sitting with the hands folded i have referred to this already i repeat i repeat with emphasis all direct persons and men of action are active just because they are stupid and limited how explain that i will tell you in consequence of their limitation they take immediate and secondary causes for primary ones and in that way persuade themselves more quickly and easily than other people do that they have found an infallible foundation for their activity and their minds are at ease and you know that is the chief thing to begin to act you know you must first have your mind completely at ease and no trace of doubt left in it why how am i for example to set my mind at rest where are the primary causes on which i am to build where are my foundations where am i to get them from i exercise myself in reflection and consequently with me every primary cause at once draws after itself another still more primary and so on to infinity that is just the essence of every sort of consciousness and reflection it must be a case of the laws of nature again what is the result of it in the end why just the same remember i spoke just now of vengeance i am sure you did not take it in i said that a man revenges himself because he sees justice in it therefore he has found a primary cause that is justice and so he is at rest on all sides and consequently he carries out his revenge calmly and successfully being persuaded that he is doing a just and honest thing but i see no justice in it i find no sort of virtue in it either and consequently if i attempt to revenge myself it is only out of spite spite of course might overcome everything all my doubts and so might serve quite successfully in place of a primary cause precisely because it is not a cause but what is to be done if i have not even spite i began with that just now you know in consequence again of those accursed laws of consciousness anger in me is subject to chemical disintegration you look into it the object flies off into air your reasons evaporate the criminal is not to be found the wrong becomes not a wrong but a phantom something like the toothache for which no one is to blame and consequently there is only the same outlet left again that is to beat the wall as hard as you can so you give it up with a wave of the hand because you have not found a fundamental cause and try letting yourself be carried away by your feelings blindly without reflection without a primary cause repelling consciousness at least for a time hate or love if only not to sit with your hands folded the day after tomorrow at the latest you will begin despising yourself for having knowingly deceived yourself result a soap bubble and inertia oh gentlemen do you know perhaps i consider myself an intelligent man only because all my life i have been able neither to begin nor to finish anything granted i am a babbler a harmless vexatious babbler like all of us but what is to be done if the direct and sole vocation of every intelligent man is babble that is the intentional pouring of water through a sieve vi oh if i had done nothing simply from laziness heavens how i should have respected myself then i should have respected myself because i should at least have been capable of being lazy there would at least have been one quality as it were positive in me in which i could have believed myself question what is he answer a sluggard how very pleasant it would have been to hear that of oneself it would mean that i was positively defined it would mean that there was something to say about me sluggard why it is a calling and vocation it is a career do not jest it is so i should then be a member of the best club by right and should find my occupation in continually respecting myself i knew a gentleman who prided himself all his life on being a connoisseur of lafitte he considered this as his positive virtue and never doubted himself he died not simply with a tranquil but with a triumphant conscience and he was quite right too then i should have chosen a career for myself i should have been a sluggard and a glutton not a simple one but for instance one with sympathies for everything sublime and beautiful how do you like that i have long had visions of it that sublime and beautiful weighs heavily on my mind at forty but that is at forty then oh then it would have been different i should have found for myself a form of activity in keeping with it to be precise drinking to the health of everything sublime and beautiful i should have snatched at every opportunity to drop a tear into my glass and then to drain it to all that is sublime and beautiful i should then have turned everything into the sublime and the beautiful in the nastiest unquestionable trash i should have sought out the sublime and the beautiful i should have exuded tears like a wet sponge an artist for instance paints a picture worthy of gay at once i drink to the health of the artist who painted the picture worthy of gay because i love all that is sublime and beautiful an author has written as you will at once i drink to the health of anyone you will because i love all that is sublime and beautiful i should claim respect for doing so i should persecute anyone who would not show me respect i should live at ease i should die with dignity why it is charming perfectly charming and what a good round belly i should have grown what a treble chin i should have established what a ruby nose i should have coloured for myself so that everyone would have said looking at me here is an asset here is something real and solid and say what you like it is very agreeable to hear such remarks about oneself in this negative age vii but these are all golden dreams oh tell me who was it first announced who was it first proclaimed that man only does nasty things because he does not know his own interests and that if he were enlightened if his eyes were opened to his real normal interests man would at once cease to do nasty things would at once become good and noble because being enlightened and understanding his real advantage he would see his own advantage in the good and nothing else and we all know that not one man can consciously act against his own interests consequently so to say through necessity he would begin doing good oh the babe oh the pure innocent child why in the first place when in all these thousands of years has there been a time when man has acted only from his own interest what is to be done with the millions of facts that bear witness that men consciously that is fully understanding their real interests have left them in the background and have rushed headlong on another path to meet peril and danger compelled to this course by nobody and by nothing but as it were simply disliking the beaten track and have obstinately wilfully struck out another difficult absurd way seeking it almost in the darkness so i suppose this obstinacy and perversity were pleasanter to them than any advantage advantage what is advantage and will you take it upon yourself to define with perfect accuracy in what the advantage of man consists and what if it so happens that a man s advantage sometimes not only may but even must consist in his desiring in certain cases what is harmful to himself and not advantageous and if so if there can be such a case the whole principle falls into dust what do you think are there such cases you laugh laugh away gentlemen but only answer me have man s advantages been reckoned up with perfect certainty are there not some which not only have not been included but cannot possibly be included under any classification you see you gentlemen have to the best of my knowledge taken your whole register of human advantages from the averages of statistical figures and politico economical formulas your advantages are prosperity wealth freedom peace and so on and so on so that the man who should for instance go openly and knowingly in opposition to all that list would to your thinking and indeed mine too of course be an obscurantist or an absolute madman would not he but you know this is what is surprising why does it so happen that all these statisticians sages and lovers of humanity when they reckon up human advantages invariably leave out one they don t even take it into their reckoning in the form in which it should be taken and the whole reckoning depends upon that it would be no greater matter they would simply have to take it this advantage and add it to the list but the trouble is that this strange advantage does not fall under any classification and is not in place in any list i have a friend for instance ech gentlemen but of course he is your friend too and indeed there is no one no one to whom he is not a friend when he prepares for any undertaking this gentleman immediately explains to you elegantly and clearly exactly how he must act in accordance with the laws of reason and truth what is more he will talk to you with excitement and passion of the true normal interests of man with irony he will upbraid the short sighted fools who do not understand their own interests nor the true significance of virtue and within a quarter of an hour without any sudden outside provocation but simply through something inside him which is stronger than all his interests he will go off on quite a different tack that is act in direct opposition to what he has just been saying about himself in opposition to the laws of reason in opposition to his own advantage in fact in opposition to everything i warn you that my friend is a compound personality and therefore it is difficult to blame him as an individual the fact is gentlemen it seems there must really exist something that is dearer to almost every man than his greatest advantages or not to be illogical there is a most advantageous advantage the very one omitted of which we spoke just now which is more important and more advantageous than all other advantages for the sake of which a man if necessary is ready to act in opposition to all laws that is in opposition to reason honour peace prosperity in fact in opposition to all those excellent and useful things if only he can attain that fundamental most advantageous advantage which is dearer to him than all yes but it s advantage all the same you will retort but excuse me i ll make the point clear and it is not a case of playing upon words what matters is that this advantage is remarkable from the very fact that it breaks down all our classifications and continually shatters every system constructed by lovers of mankind for the benefit of mankind in fact it upsets everything but before i mention this advantage to you i want to compromise myself personally and therefore i boldly declare that all these fine systems all these theories for explaining to mankind their real normal interests in order that inevitably striving to pursue these interests they may at once become good and noble are in my opinion so far mere logical exercises yes logical exercises why to maintain this theory of the regeneration of mankind by means of the pursuit of his own advantage is to my mind almost the same thing as to affirm for instance following buckle that through civilisation mankind becomes softer and consequently less bloodthirsty and less fitted for warfare logically it does seem to follow from his arguments but man has such a predilection for systems and abstract deductions that he is ready to distort the truth intentionally he is ready to deny the evidence of his senses only to justify his logic i take this example because it is the most glaring instance of it only look about you blood is being spilt in streams and in the merriest way as though it were champagne take the whole of the nineteenth century in which buckle lived take napoleon the great and also the present one take north america the eternal union take the farce of schleswig holstein and what is it that civilisation softens in us the only gain of civilisation for mankind is the greater capacity for variety of sensations and absolutely nothing more and through the development of this many sidedness man may come to finding enjoyment in bloodshed in fact this has already happened to him have you noticed that it is the most civilised gentlemen who have been the subtlest slaughterers to whom the attilas and stenka razins could not hold a candle and if they are not so conspicuous as the attilas and stenka razins it is simply because they are so often met with are so ordinary and have become so familiar to us in any case civilisation has made mankind if not more bloodthirsty at least more vilely more loathsomely bloodthirsty in old days he saw justice in bloodshed and with his conscience at peace exterminated those he thought proper now we do think bloodshed abominable and yet we engage in this abomination and with more energy than ever which is worse decide that for yourselves they say that cleopatra excuse an instance from roman history was fond of sticking gold pins into her slave girls breasts and derived gratification from their screams and writhings you will say that that was in the comparatively barbarous times that these are barbarous times too because also comparatively speaking pins are stuck in even now that though man has now learned to see more clearly than in barbarous ages he is still far from having learnt to act as reason and science would dictate but yet you are fully convinced that he will be sure to learn when he gets rid of certain old bad habits and when common sense and science have completely re educated human nature and turned it in a normal direction you are confident that then man will cease from intentional error and will so to say be compelled not to want to set his will against his normal interests that is not all then you say science itself will teach man though to my mind it s a superfluous luxury that he never has really had any caprice or will of his own and that he himself is something of the nature of a piano key or the stop of an organ and that there are besides things called the laws of nature so that everything he does is not done by his willing it but is done of itself by the laws of nature consequently we have only to discover these laws of nature and man will no longer have to answer for his actions and life will become exceedingly easy for him all human actions will then of course be tabulated according to these laws mathematically like tables of logarithms up to and entered in an index or better still there would be published certain edifying works of the nature of encyclopaedic lexicons in which everything will be so clearly calculated and explained that there will be no more incidents or adventures in the world then this is all what you say new economic relations will be established all ready made and worked out with mathematical exactitude so that every possible question will vanish in the twinkling of an eye simply because every possible answer to it will be provided then the palace of crystal will be built then in fact those will be halcyon days of course there is no guaranteeing this is my comment that it will not be for instance frightfully dull then for what will one have to do when everything will be calculated and tabulated but on the other hand everything will be extraordinarily rational of course boredom may lead you to anything it is boredom sets one sticking golden pins into people but all that would not matter what is bad this is my comment again is that i dare say people will be thankful for the gold pins then man is stupid you know phenomenally stupid or rather he is not at all stupid but he is so ungrateful that you could not find another like him in all creation i for instance would not be in the least surprised if all of a sudden a propos of nothing in the midst of general prosperity a gentleman with an ignoble or rather with a reactionary and ironical countenance were to arise and putting his arms akimbo say to us all i say gentleman hadn t we better kick over the whole show and scatter rationalism to the winds simply to send these logarithms to the devil and to enable us to live once more at our own sweet foolish will that again would not matter but what is annoying is that he would be sure to find followers such is the nature of man and all that for the most foolish reason which one would think was hardly worth mentioning that is that man everywhere and at all times whoever he may be has preferred to act as he chose and not in the least as his reason and advantage dictated and one may choose what is contrary to one s own interests and sometimes one positively ought that is my idea one s own free unfettered choice one s own caprice however wild it may be one s own fancy worked up at times to frenzy is that very most advantageous advantage which we have overlooked which comes under no classification and against which all systems and theories are continually being shattered to atoms and how do these wiseacres know that man wants a normal a virtuous choice what has made them conceive that man must want a rationally advantageous choice what man wants is simply independent choice whatever that independence may cost and wherever it may lead and choice of course the devil only knows what choice viii ha ha ha but you know there is no such thing as choice in reality say what you like you will interpose with a chuckle science has succeeded in so far analysing man that we know already that choice and what is called freedom of will is nothing else than stay gentlemen i meant to begin with that myself i confess i was rather frightened i was just going to say that the devil only knows what choice depends on and that perhaps that was a very good thing but i remembered the teaching of science and pulled myself up and here you have begun upon it indeed if there really is some day discovered a formula for all our desires and caprices that is an explanation of what they depend upon by what laws they arise how they develop what they are aiming at in one case and in another and so on that is a real mathematical formula then most likely man will at once cease to feel desire indeed he will be certain to for who would want to choose by rule besides he will at once be transformed from a human being into an organ stop or something of the sort for what is a man without desires without free will and without choice if not a stop in an organ what do you think let us reckon the chances can such a thing happen or not h m you decide our choice is usually mistaken from a false view of our advantage we sometimes choose absolute nonsense because in our foolishness we see in that nonsense the easiest means for attaining a supposed advantage but when all that is explained and worked out on paper which is perfectly possible for it is contemptible and senseless to suppose that some laws of nature man will never understand then certainly so called desires will no longer exist for if a desire should come into conflict with reason we shall then reason and not desire because it will be impossible retaining our reason to be senseless in our desires and in that way knowingly act against reason and desire to injure ourselves and as all choice and reasoning can be really calculated because there will some day be discovered the laws of our so called free will so joking apart there may one day be something like a table constructed of them so that we really shall choose in accordance with it if for instance some day they calculate and prove to me that i made a long nose at someone because i could not help making a long nose at him and that i had to do it in that particular way what freedom is left me especially if i am a learned man and have taken my degree somewhere then i should be able to calculate my whole life for thirty years beforehand in short if this could be arranged there would be nothing left for us to do anyway we should have to understand that and in fact we ought unwearyingly to repeat to ourselves that at such and such a time and in such and such circumstances nature does not ask our leave that we have got to take her as she is and not fashion her to suit our fancy and if we really aspire to formulas and tables of rules and well even to the chemical retort there s no help for it we must accept the retort too or else it will be accepted without our consent yes but here i come to a stop gentlemen you must excuse me for being over philosophical it s the result of forty years underground allow me to indulge my fancy you see gentlemen reason is an excellent thing there s no disputing that but reason is nothing but reason and satisfies only the rational side of man s nature while will is a manifestation of the whole life that is of the whole human life including reason and all the impulses and although our life in this manifestation of it is often worthless yet it is life and not simply extracting square roots here i for instance quite naturally want to live in order to satisfy all my capacities for life and not simply my capacity for reasoning that is not simply one twentieth of my capacity for life what does reason know reason only knows what it has succeeded in learning some things perhaps it will never learn this is a poor comfort but why not say so frankly and human nature acts as a whole with everything that is in it consciously or unconsciously and even if it goes wrong it lives i suspect gentlemen that you are looking at me with compassion you tell me again that an enlightened and developed man such in short as the future man will be cannot consciously desire anything disadvantageous to himself that that can be proved mathematically i thoroughly agree it can by mathematics but i repeat for the hundredth time there is one case one only when man may consciously purposely desire what is injurious to himself what is stupid very stupid simply in order to have the right to desire for himself even what is very stupid and not to be bound by an obligation to desire only what is sensible of course this very stupid thing this caprice of ours may be in reality gentlemen more advantageous for us than anything else on earth especially in certain cases and in particular it may be more advantageous than any advantage even when it does us obvious harm and contradicts the soundest conclusions of our reason concerning our advantage for in any circumstances it preserves for us what is most precious and most important that is our personality our individuality some you see maintain that this really is the most precious thing for mankind choice can of course if it chooses be in agreement with reason and especially if this be not abused but kept within bounds it is profitable and sometimes even praiseworthy but very often and even most often choice is utterly and stubbornly opposed to reason and and do you know that that too is profitable sometimes even praiseworthy gentlemen let us suppose that man is not stupid indeed one cannot refuse to suppose that if only from the one consideration that if man is stupid then who is wise but if he is not stupid he is monstrously ungrateful phenomenally ungrateful in fact i believe that the best definition of man is the ungrateful biped but that is not all that is not his worst defect his worst defect is his perpetual moral obliquity perpetual from the days of the flood to the schleswig holstein period moral obliquity and consequently lack of good sense for it has long been accepted that lack of good sense is due to no other cause than moral obliquity put it to the test and cast your eyes upon the history of mankind what will you see is it a grand spectacle grand if you like take the colossus of rhodes for instance that s worth something with good reason mr anaevsky testifies of it that some say that it is the work of man s hands while others maintain that it has been created by nature herself is it many coloured may be it is many coloured too if one takes the dress uniforms military and civilian of all peoples in all ages that alone is worth something and if you take the undress uniforms you will never get to the end of it no historian would be equal to the job is it monotonous may be it s monotonous too it s fighting and fighting they are fighting now they fought first and they fought last you will admit that it is almost too monotonous in short one may say anything about the history of the world anything that might enter the most disordered imagination the only thing one can t say is that it s rational the very word sticks in one s throat and indeed this is the odd thing that is continually happening there are continually turning up in life moral and rational persons sages and lovers of humanity who make it their object to live all their lives as morally and rationally as possible to be so to speak a light to their neighbours simply in order to show them that it is possible to live morally and rationally in this world and yet we all know that those very people sooner or later have been false to themselves playing some queer trick often a most unseemly one now i ask you what can be expected of man since he is a being endowed with strange qualities shower upon him every earthly blessing drown him in a sea of happiness so that nothing but bubbles of bliss can be seen on the surface give him economic prosperity such that he should have nothing else to do but sleep eat cakes and busy himself with the continuation of his species and even then out of sheer ingratitude sheer spite man would play you some nasty trick he would even risk his cakes and would deliberately desire the most fatal rubbish the most uneconomical absurdity simply to introduce into all this positive good sense his fatal fantastic element it is just his fantastic dreams his vulgar folly that he will desire to retain simply in order to prove to himself as though that were so necessary that men still are men and not the keys of a piano which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar and that is not all even if man really were nothing but a piano key even if this were proved to him by natural science and mathematics even then he would not become reasonable but would purposely do something perverse out of simple ingratitude simply to gain his point and if he does not find means he will contrive destruction and chaos will contrive sufferings of all sorts only to gain his point he will launch a curse upon the world and as only man can curse it is his privilege the primary distinction between him and other animals may be by his curse alone he will attain his object that is convince himself that he is a man and not a piano key if you say that all this too can be calculated and tabulated chaos and darkness and curses so that the mere possibility of calculating it all beforehand would stop it all and reason would reassert itself then man would purposely go mad in order to be rid of reason and gain his point i believe in it i answer for it for the whole work of man really seems to consist in nothing but proving to himself every minute that he is a man and not a piano key it may be at the cost of his skin it may be by cannibalism and this being so can one help being tempted to rejoice that it has not yet come off and that desire still depends on something we don t know you will scream at me that is if you condescend to do so that no one is touching my free will that all they are concerned with is that my will should of itself of its own free will coincide with my own normal interests with the laws of nature and arithmetic good heavens gentlemen what sort of free will is left when we come to tabulation and arithmetic when it will all be a case of twice two make four twice two makes four without my will as if free will meant that ix gentlemen i am joking and i know myself that my jokes are not brilliant but you know one can take everything as a joke i am perhaps jesting against the grain gentlemen i am tormented by questions answer them for me you for instance want to cure men of their old habits and reform their will in accordance with science and good sense but how do you know not only that it is possible but also that it is desirable to reform man in that way and what leads you to the conclusion that man s inclinations need reforming in short how do you know that such a reformation will be a benefit to man and to go to the root of the matter why are you so positively convinced that not to act against his real normal interests guaranteed by the conclusions of reason and arithmetic is certainly always advantageous for man and must always be a law for mankind so far you know this is only your supposition it may be the law of logic but not the law of humanity you think gentlemen perhaps that i am mad allow me to defend myself i agree that man is pre eminently a creative animal predestined to strive consciously for an object and to engage in engineering that is incessantly and eternally to make new roads wherever they may lead but the reason why he wants sometimes to go off at a tangent may just be that he is predestined to make the road and perhaps too that however stupid the direct practical man may be the thought sometimes will occur to him that the road almost always does lead somewhere and that the destination it leads to is less important than the process of making it and that the chief thing is to save the well conducted child from despising engineering and so giving way to the fatal idleness which as we all know is the mother of all the vices man likes to make roads and to create that is a fact beyond dispute but why has he such a passionate love for destruction and chaos also tell me that but on that point i want to say a couple of words myself may it not be that he loves chaos and destruction there can be no disputing that he does sometimes love it because he is instinctively afraid of attaining his object and completing the edifice he is constructing who knows perhaps he only loves that edifice from a distance and is by no means in love with it at close quarters perhaps he only loves building it and does not want to live in it but will leave it when completed for the use of les animaux domestiques such as the ants the sheep and so on now the ants have quite a different taste they have a marvellous edifice of that pattern which endures for ever the ant heap with the ant heap the respectable race of ants began and with the ant heap they will probably end which does the greatest credit to their perseverance and good sense but man is a frivolous and incongruous creature and perhaps like a chess player loves the process of the game not the end of it and who knows there is no saying with certainty perhaps the only goal on earth to which mankind is striving lies in this incessant process of attaining in other words in life itself and not in the thing to be attained which must always be expressed as a formula as positive as twice two makes four and such positiveness is not life gentlemen but is the beginning of death anyway man has always been afraid of this mathematical certainty and i am afraid of it now granted that man does nothing but seek that mathematical certainty he traverses oceans sacrifices his life in the quest but to succeed really to find it dreads i assure you he feels that when he has found it there will be nothing for him to look for when workmen have finished their work they do at least receive their pay they go to the tavern then they are taken to the police station and there is occupation for a week but where can man go anyway one can observe a certain awkwardness about him when he has attained such objects he loves the process of attaining but does not quite like to have attained and that of course is very absurd in fact man is a comical creature there seems to be a kind of jest in it all but yet mathematical certainty is after all something insufferable twice two makes four seems to me simply a piece of insolence twice two makes four is a pert coxcomb who stands with arms akimbo barring your path and spitting i admit that twice two makes four is an excellent thing but if we are to give everything its due twice two makes five is sometimes a very charming thing too and why are you so firmly so triumphantly convinced that only the normal and the positive in other words only what is conducive to welfare is for the advantage of man is not reason in error as regards advantage does not man perhaps love something besides well being perhaps he is just as fond of suffering perhaps suffering is just as great a benefit to him as well being man is sometimes extraordinarily passionately in love with suffering and that is a fact there is no need to appeal to universal history to prove that only ask yourself if you are a man and have lived at all as far as my personal opinion is concerned to care only for well being seems to me positively ill bred whether it s good or bad it is sometimes very pleasant too to smash things i hold no brief for suffering nor for well being either i am standing for my caprice and for its being guaranteed to me when necessary suffering would be out of place in vaudevilles for instance i know that in the palace of crystal it is unthinkable suffering means doubt negation and what would be the good of a palace of crystal if there could be any doubt about it and yet i think man will never renounce real suffering that is destruction and chaos why suffering is the sole origin of consciousness though i did lay it down at the beginning that consciousness is the greatest misfortune for man yet i know man prizes it and would not give it up for any satisfaction consciousness for instance is infinitely superior to twice two makes four once you have mathematical certainty there is nothing left to do or to understand there will be nothing left but to bottle up your five senses and plunge into contemplation while if you stick to consciousness even though the same result is attained you can at least flog yourself at times and that will at any rate liven you up reactionary as it is corporal punishment is better than nothing x you believe in a palace of crystal that can never be destroyed a palace at which one will not be able to put out one s tongue or make a long nose on the sly and perhaps that is just why i am afraid of this edifice that it is of crystal and can never be destroyed and that one cannot put one s tongue out at it even on the sly you see if it were not a palace but a hen house i might creep into it to avoid getting wet and yet i would not call the hen house a palace out of gratitude to it for keeping me dry you laugh and say that in such circumstances a hen house is as good as a mansion yes i answer if one had to live simply to keep out of the rain but what is to be done if i have taken it into my head that that is not the only object in life and that if one must live one had better live in a mansion that is my choice my desire you will only eradicate it when you have changed my preference well do change it allure me with something else give me another ideal but meanwhile i will not take a hen house for a mansion the palace of crystal may be an idle dream it may be that it is inconsistent with the laws of nature and that i have invented it only through my own stupidity through the old fashioned irrational habits of my generation but what does it matter to me that it is inconsistent that makes no difference since it exists in my desires or rather exists as long as my desires exist perhaps you are laughing again laugh away i will put up with any mockery rather than pretend that i am satisfied when i am hungry i know anyway that i will not be put off with a compromise with a recurring zero simply because it is consistent with the laws of nature and actually exists i will not accept as the crown of my desires a block of buildings with tenements for the poor on a lease of a thousand years and perhaps with a sign board of a dentist hanging out destroy my desires eradicate my ideals show me something better and i will follow you you will say perhaps that it is not worth your trouble but in that case i can give you the same answer we are discussing things seriously but if you won t deign to give me your attention i will drop your acquaintance i can retreat into my underground hole but while i am alive and have desires i would rather my hand were withered off than bring one brick to such a building don t remind me that i have just rejected the palace of crystal for the sole reason that one cannot put out one s tongue at it i did not say because i am so fond of putting my tongue out perhaps the thing i resented was that of all your edifices there has not been one at which one could not put out one s tongue on the contrary i would let my tongue be cut off out of gratitude if things could be so arranged that i should lose all desire to put it out it is not my fault that things cannot be so arranged and that one must be satisfied with model flats then why am i made with such desires can i have been constructed simply in order to come to the conclusion that all my construction is a cheat can this be my whole purpose i do not believe it but do you know what i am convinced that we underground folk ought to be kept on a curb though we may sit forty years underground without speaking when we do come out into the light of day and break out we talk and talk and talk xi the long and the short of it is gentlemen that it is better to do nothing better conscious inertia and so hurrah for underground though i have said that i envy the normal man to the last drop of my bile yet i should not care to be in his place such as he is now though i shall not cease envying him no no anyway the underground life is more advantageous there at any rate one can oh but even now i am lying i am lying because i know myself that it is not underground that is better but something different quite different for which i am thirsting but which i cannot find damn underground i will tell you another thing that would be better and that is if i myself believed in anything of what i have just written i swear to you gentlemen there is not one thing not one word of what i have written that i really believe that is i believe it perhaps but at the same time i feel and suspect that i am lying like a cobbler then why have you written all this you will say to me i ought to put you underground for forty years without anything to do and then come to you in your cellar to find out what stage you have reached how can a man be left with nothing to do for forty years isn t that shameful isn t that humiliating you will say perhaps wagging your heads contemptuously you thirst for life and try to settle the problems of life by a logical tangle and how persistent how insolent are your sallies and at the same time what a scare you are in you talk nonsense and are pleased with it you say impudent things and are in continual alarm and apologising for them you declare that you are afraid of nothing and at the same time try to ingratiate yourself in our good opinion you declare that you are gnashing your teeth and at the same time you try to be witty so as to amuse us you know that your witticisms are not witty but you are evidently well satisfied with their literary value you may perhaps have really suffered but you have no respect for your own suffering you may have sincerity but you have no modesty out of the pettiest vanity you expose your sincerity to publicity and ignominy you doubtlessly mean to say something but hide your last word through fear because you have not the resolution to utter it and only have a cowardly impudence you boast of consciousness but you are not sure of your ground for though your mind works yet your heart is darkened and corrupt and you cannot have a full genuine consciousness without a pure heart and how intrusive you are how you insist and grimace lies lies lies of course i have myself made up all the things you say that too is from underground i have been for forty years listening to you through a crack under the floor i have invented them myself there was nothing else i could invent it is no wonder that i have learned it by heart and it has taken a literary form but can you really be so credulous as to think that i will print all this and give it to you to read too and another problem why do i call you gentlemen why do i address you as though you really were my readers such confessions as i intend to make are never printed nor given to other people to read anyway i am not strong minded enough for that and i don t see why i should be but you see a fancy has occurred to me and i want to realise it at all costs let me explain every man has reminiscences which he would not tell to everyone but only to his friends he has other matters in his mind which he would not reveal even to his friends but only to himself and that in secret but there are other things which a man is afraid to tell even to himself and every decent man has a number of such things stored away in his mind the more decent he is the greater the number of such things in his mind anyway i have only lately determined to remember some of my early adventures till now i have always avoided them even with a certain uneasiness now when i am not only recalling them but have actually decided to write an account of them i want to try the experiment whether one can even with oneself be perfectly open and not take fright at the whole truth i will observe in parenthesis that heine says that a true autobiography is almost an impossibility and that man is bound to lie about himself he considers that rousseau certainly told lies about himself in his confessions and even intentionally lied out of vanity i am convinced that heine is right i quite understand how sometimes one may out of sheer vanity attribute regular crimes to oneself and indeed i can very well conceive that kind of vanity but heine judged of people who made their confessions to the public i write only for myself and i wish to declare once and for all that if i write as though i were addressing readers that is simply because it is easier for me to write in that form it is a form an empty form i shall never have readers i have made this plain already i don t wish to be hampered by any restrictions in the compilation of my notes i shall not attempt any system or method i will jot things down as i remember them but here perhaps someone will catch at the word and ask me if you really don t reckon on readers why do you make such compacts with yourself and on paper too that is that you won t attempt any system or method that you jot things down as you remember them and so on and so on why are you explaining why do you apologise well there it is i answer there is a whole psychology in all this though perhaps it is simply that i am a coward and perhaps that i purposely imagine an audience before me in order that i may be more dignified while i write there are perhaps thousands of reasons again what is my object precisely in writing if it is not for the benefit of the public why should i not simply recall these incidents in my own mind without putting them on paper quite so but yet it is more imposing on paper there is something more impressive in it i shall be better able to criticise myself and improve my style besides i shall perhaps obtain actual relief from writing today for instance i am particularly oppressed by one memory of a distant past it came back vividly to my mind a few days ago and has remained haunting me like an annoying tune that one cannot get rid of and yet i must get rid of it somehow i have hundreds of such reminiscences but at times some one stands out from the hundred and oppresses me for some reason i believe that if i write it down i should get rid of it why not try besides i am bored and i never have anything to do writing will be a sort of work they say work makes man kind hearted and honest well here is a chance for me anyway snow is falling today yellow and dingy it fell yesterday too and a few days ago i fancy it is the wet snow that has reminded me of that incident which i cannot shake off now and so let it be a story a propos of the falling snow part ii a propos of the wet snow when from dark error s subjugation my words of passionate exhortation had wrenched thy fainting spirit free and writhing prone in thine affliction thou didst recall with malediction the vice that had encompassed thee and when thy slumbering conscience fretting by recollection s torturing flame thou didst reveal the hideous setting of thy life s current ere i came when suddenly i saw thee sicken and weeping hide thine anguished face revolted maddened horror stricken at memories of foul disgrace nekrassov translated by juliet soskice i at that time i was only twenty four my life was even then gloomy ill regulated and as solitary as that of a savage i made friends with no one and positively avoided talking and buried myself more and more in my hole at work in the office i never looked at anyone and was perfectly well aware that my companions looked upon me not only as a queer fellow but even looked upon me i always fancied this with a sort of loathing i sometimes wondered why it was that nobody except me fancied that he was looked upon with aversion one of the clerks had a most repulsive pock marked face which looked positively villainous i believe i should not have dared to look at anyone with such an unsightly countenance another had such a very dirty old uniform that there was an unpleasant odour in his proximity yet not one of these gentlemen showed the slightest self consciousness either about their clothes or their countenance or their character in any way neither of them ever imagined that they were looked at with repulsion if they had imagined it they would not have minded so long as their superiors did not look at them in that way it is clear to me now that owing to my unbounded vanity and to the high standard i set for myself i often looked at myself with furious discontent which verged on loathing and so i inwardly attributed the same feeling to everyone i hated my face for instance i thought it disgusting and even suspected that there was something base in my expression and so every day when i turned up at the office i tried to behave as independently as possible and to assume a lofty expression so that i might not be suspected of being abject my face may be ugly i thought but let it be lofty expressive and above all extremely intelligent but i was positively and painfully certain that it was impossible for my countenance ever to express those qualities and what was worst of all i thought it actually stupid looking and i would have been quite satisfied if i could have looked intelligent in fact i would even have put up with looking base if at the same time my face could have been thought strikingly intelligent of course i hated my fellow clerks one and all and i despised them all yet at the same time i was as it were afraid of them in fact it happened at times that i thought more highly of them than of myself it somehow happened quite suddenly that i alternated between despising them and thinking them superior to myself a cultivated and decent man cannot be vain without setting a fearfully high standard for himself and without despising and almost hating himself at certain moments but whether i despised them or thought them superior i dropped my eyes almost every time i met anyone i even made experiments whether i could face so and so s looking at me and i was always the first to drop my eyes this worried me to distraction i had a sickly dread too of being ridiculous and so had a slavish passion for the conventional in everything external i loved to fall into the common rut and had a whole hearted terror of any kind of eccentricity in myself but how could i live up to it i was morbidly sensitive as a man of our age should be they were all stupid and as like one another as so many sheep perhaps i was the only one in the office who fancied that i was a coward and a slave and i fancied it just because i was more highly developed but it was not only that i fancied it it really was so i was a coward and a slave i say this without the slightest embarrassment every decent man of our age must be a coward and a slave that is his normal condition of that i am firmly persuaded he is made and constructed to that very end and not only at the present time owing to some casual circumstances but always at all times a decent man is bound to be a coward and a slave it is the law of nature for all decent people all over the earth if anyone of them happens to be valiant about something he need not be comforted nor carried away by that he would show the white feather just the same before something else that is how it invariably and inevitably ends only donkeys and mules are valiant and they only till they are pushed up to the wall it is not worth while to pay attention to them for they really are of no consequence another circumstance too worried me in those days that there was no one like me and i was unlike anyone else i am alone and they are everyone i thought and pondered from that it is evident that i was still a youngster the very opposite sometimes happened it was loathsome sometimes to go to the office things reached such a point that i often came home ill but all at once a propos of nothing there would come a phase of scepticism and indifference everything happened in phases to me and i would laugh myself at my intolerance and fastidiousness i would reproach myself with being romantic at one time i was unwilling to speak to anyone while at other times i would not only talk but go to the length of contemplating making friends with them all my fastidiousness would suddenly for no rhyme or reason vanish who knows perhaps i never had really had it and it had simply been affected and got out of books i have not decided that question even now once i quite made friends with them visited their homes played preference drank vodka talked of promotions but here let me make a digression we russians speaking generally have never had those foolish transcendental romantics german and still more french on whom nothing produces any effect if there were an earthquake if all france perished at the barricades they would still be the same they would not even have the decency to affect a change but would still go on singing their transcendental songs to the hour of their death because they are fools we in russia have no fools that is well known that is what distinguishes us from foreign lands consequently these transcendental natures are not found amongst us in their pure form the idea that they are is due to our realistic journalists and critics of that day always on the look out for kostanzhoglos and uncle pyotr ivanitchs and foolishly accepting them as our ideal they have slandered our romantics taking them for the same transcendental sort as in germany or france on the contrary the characteristics of our romantics are absolutely and directly opposed to the transcendental european type and no european standard can be applied to them allow me to make use of this word romantic an old fashioned and much respected word which has done good service and is familiar to all the characteristics of our romantic are to understand everything to see everything and to see it often incomparably more clearly than our most realistic minds see it to refuse to accept anyone or anything but at the same time not to despise anything to give way to yield from policy never to lose sight of a useful practical object such as rent free quarters at the government expense pensions decorations to keep their eye on that object through all the enthusiasms and volumes of lyrical poems and at the same time to preserve the sublime and the beautiful inviolate within them to the hour of their death and to preserve themselves also incidentally like some precious jewel wrapped in cotton wool if only for the benefit of the sublime and the beautiful our romantic is a man of great breadth and the greatest rogue of all our rogues i assure you i can assure you from experience indeed of course that is if he is intelligent but what am i saying the romantic is always intelligent and i only meant to observe that although we have had foolish romantics they don t count and they were only so because in the flower of their youth they degenerated into germans and to preserve their precious jewel more comfortably settled somewhere out there by preference in weimar or the black forest i for instance genuinely despised my official work and did not openly abuse it simply because i was in it myself and got a salary for it anyway take note i did not openly abuse it our romantic would rather go out of his mind a thing however which very rarely happens than take to open abuse unless he had some other career in view and he is never kicked out at most they would take him to the lunatic asylum as the king of spain if he should go very mad but it is only the thin fair people who go out of their minds in russia innumerable romantics attain later in life to considerable rank in the service their many sidedness is remarkable and what a faculty they have for the most contradictory sensations i was comforted by this thought even in those days and i am of the same opinion now that is why there are so many broad natures among us who never lose their ideal even in the depths of degradation and though they never stir a finger for their ideal though they are arrant thieves and knaves yet they tearfully cherish their first ideal and are extraordinarily honest at heart yes it is only among us that the most incorrigible rogue can be absolutely and loftily honest at heart without in the least ceasing to be a rogue i repeat our romantics frequently become such accomplished rascals i use the term rascals affectionately suddenly display such a sense of reality and practical knowledge that their bewildered superiors and the public generally can only ejaculate in amazement their many sidedness is really amazing and goodness knows what it may develop into later on and what the future has in store for us it is not a poor material i do not say this from any foolish or boastful patriotism but i feel sure that you are again imagining that i am joking or perhaps it s just the contrary and you are convinced that i really think so anyway gentlemen i shall welcome both views as an honour and a special favour and do forgive my digression i did not of course maintain friendly relations with my comrades and soon was at loggerheads with them and in my youth and inexperience i even gave up bowing to them as though i had cut off all relations that however only happened to me once as a rule i was always alone in the first place i spent most of my time at home reading i tried to stifle all that was continually seething within me by means of external impressions and the only external means i had was reading reading of course was a great help exciting me giving me pleasure and pain but at times it bored me fearfully one longed for movement in spite of everything and i plunged all at once into dark underground loathsome vice of the pettiest kind my wretched passions were acute smarting from my continual sickly irritability i had hysterical impulses with tears and convulsions i had no resource except reading that is there was nothing in my surroundings which i could respect and which attracted me i was overwhelmed with depression too i had an hysterical craving for incongruity and for contrast and so i took to vice i have not said all this to justify myself but no i am lying i did want to justify myself i make that little observation for my own benefit gentlemen i don t want to lie i vowed to myself i would not and so furtively timidly in solitude at night i indulged in filthy vice with a feeling of shame which never deserted me even at the most loathsome moments and which at such moments nearly made me curse already even then i had my underground world in my soul i was fearfully afraid of being seen of being met of being recognised i visited various obscure haunts one night as i was passing a tavern i saw through a lighted window some gentlemen fighting with billiard cues and saw one of them thrown out of the window at other times i should have felt very much disgusted but i was in such a mood at the time that i actually envied the gentleman thrown out of the window and i envied him so much that i even went into the tavern and into the billiard room perhaps i thought i ll have a fight too and they ll throw me out of the window i was not drunk but what is one to do depression will drive a man to such a pitch of hysteria but nothing happened it seemed that i was not even equal to being thrown out of the window and i went away without having my fight an officer put me in my place from the first moment i was standing by the billiard table and in my ignorance blocking up the way and he wanted to pass he took me by the shoulders and without a word without a warning or explanation moved me from where i was standing to another spot and passed by as though he had not noticed me i could have forgiven blows but i could not forgive his having moved me without noticing me devil knows what i would have given for a real regular quarrel a more decent a more literary one so to speak i had been treated like a fly this officer was over six foot while i was a spindly little fellow but the quarrel was in my hands i had only to protest and i certainly would have been thrown out of the window but i changed my mind and preferred to beat a resentful retreat i went out of the tavern straight home confused and troubled and the next night i went out again with the same lewd intentions still more furtively abjectly and miserably than before as it were with tears in my eyes but still i did go out again don t imagine though it was cowardice made me slink away from the officer i never have been a coward at heart though i have always been a coward in action don t be in a hurry to laugh i assure you i can explain it all oh if only that officer had been one of the sort who would consent to fight a duel but no he was one of those gentlemen alas long extinct who preferred fighting with cues or like gogol s lieutenant pirogov appealing to the police they did not fight duels and would have thought a duel with a civilian like me an utterly unseemly procedure in any case and they looked upon the duel altogether as something impossible something free thinking and french but they were quite ready to bully especially when they were over six foot i did not slink away through cowardice but through an unbounded vanity i was afraid not of his six foot not of getting a sound thrashing and being thrown out of the window i should have had physical courage enough i assure you but i had not the moral courage what i was afraid of was that everyone present from the insolent marker down to the lowest little stinking pimply clerk in a greasy collar would jeer at me and fail to understand when i began to protest and to address them in literary language for of the point of honour not of honour but of the point of honour point d honneur one cannot speak among us except in literary language you can t allude to the point of honour in ordinary language i was fully convinced the sense of reality in spite of all my romanticism that they would all simply split their sides with laughter and that the officer would not simply beat me that is without insulting me but would certainly prod me in the back with his knee kick me round the billiard table and only then perhaps have pity and drop me out of the window of course this trivial incident could not with me end in that i often met that officer afterwards in the street and noticed him very carefully i am not quite sure whether he recognised me i imagine not i judge from certain signs but i i stared at him with spite and hatred and so it went on for several years my resentment grew even deeper with years at first i began making stealthy inquiries about this officer it was difficult for me to do so for i knew no one but one day i heard someone shout his surname in the street as i was following him at a distance as though i were tied to him and so i learnt his surname another time i followed him to his flat and for ten kopecks learned from the porter where he lived on which storey whether he lived alone or with others and so on in fact everything one could learn from a porter one morning though i had never tried my hand with the pen it suddenly occurred to me to write a satire on this officer in the form of a novel which would unmask his villainy i wrote the novel with relish i did unmask his villainy i even exaggerated it at first i so altered his surname that it could easily be recognised but on second thoughts i changed it and sent the story to the otetchestvenniya zapiski but at that time such attacks were not the fashion and my story was not printed that was a great vexation to me sometimes i was positively choked with resentment at last i determined to challenge my enemy to a duel i composed a splendid charming letter to him imploring him to apologise to me and hinting rather plainly at a duel in case of refusal the letter was so composed that if the officer had had the least understanding of the sublime and the beautiful he would certainly have flung himself on my neck and have offered me his friendship and how fine that would have been how we should have got on together he could have shielded me with his higher rank while i could have improved his mind with my culture and well my ideas and all sorts of things might have happened only fancy this was two years after his insult to me and my challenge would have been a ridiculous anachronism in spite of all the ingenuity of my letter in disguising and explaining away the anachronism but thank god to this day i thank the almighty with tears in my eyes i did not send the letter to him cold shivers run down my back when i think of what might have happened if i had sent it and all at once i revenged myself in the simplest way by a stroke of genius a brilliant thought suddenly dawned upon me sometimes on holidays i used to stroll along the sunny side of the nevsky about four o clock in the afternoon though it was hardly a stroll so much as a series of innumerable miseries humiliations and resentments but no doubt that was just what i wanted i used to wriggle along in a most unseemly fashion like an eel continually moving aside to make way for generals for officers of the guards and the hussars or for ladies at such minutes there used to be a convulsive twinge at my heart and i used to feel hot all down my back at the mere thought of the wretchedness of my attire of the wretchedness and abjectness of my little scurrying figure this was a regular martyrdom a continual intolerable humiliation at the thought which passed into an incessant and direct sensation that i was a mere fly in the eyes of all this world a nasty disgusting fly more intelligent more highly developed more refined in feeling than any of them of course but a fly that was continually making way for everyone insulted and injured by everyone why i inflicted this torture upon myself why i went to the nevsky i don t know i felt simply drawn there at every possible opportunity already then i began to experience a rush of the enjoyment of which i spoke in the first chapter after my affair with the officer i felt even more drawn there than before it was on the nevsky that i met him most frequently there i could admire him he too went there chiefly on holidays he too turned out of his path for generals and persons of high rank and he too wriggled between them like an eel but people like me or even better dressed than me he simply walked over he made straight for them as though there was nothing but empty space before him and never under any circumstances turned aside i gloated over my resentment watching him and always resentfully made way for him it exasperated me that even in the street i could not be on an even footing with him why must you invariably be the first to move aside i kept asking myself in hysterical rage waking up sometimes at three o clock in the morning why is it you and not he there s no regulation about it there s no written law let the making way be equal as it usually is when refined people meet he moves half way and you move half way you pass with mutual respect but that never happened and i always moved aside while he did not even notice my making way for him and lo and behold a bright idea dawned upon me what i thought if i meet him and don t move on one side what if i don t move aside on purpose even if i knock up against him how would that be this audacious idea took such a hold on me that it gave me no peace i was dreaming of it continually horribly and i purposely went more frequently to the nevsky in order to picture more vividly how i should do it when i did do it i was delighted this intention seemed to me more and more practical and possible of course i shall not really push him i thought already more good natured in my joy i will simply not turn aside will run up against him not very violently but just shouldering each other just as much as decency permits i will push against him just as much as he pushes against me at last i made up my mind completely but my preparations took a great deal of time to begin with when i carried out my plan i should need to be looking rather more decent and so i had to think of my get up in case of emergency if for instance there were any sort of public scandal and the public there is of the most recherche the countess walks there prince d walks there all the literary world is there i must be well dressed that inspires respect and of itself puts us on an equal footing in the eyes of the society with this object i asked for some of my salary in advance and bought at tchurkin s a pair of black gloves and a decent hat black gloves seemed to me both more dignified and bon ton than the lemon coloured ones which i had contemplated at first the colour is too gaudy it looks as though one were trying to be conspicuous and i did not take the lemon coloured ones i had got ready long beforehand a good shirt with white bone studs my overcoat was the only thing that held me back the coat in itself was a very good one it kept me warm but it was wadded and it had a raccoon collar which was the height of vulgarity i had to change the collar at any sacrifice and to have a beaver one like an officer s for this purpose i began visiting the gostiny dvor and after several attempts i pitched upon a piece of cheap german beaver though these german beavers soon grow shabby and look wretched yet at first they look exceedingly well and i only needed it for the occasion i asked the price even so it was too expensive after thinking it over thoroughly i decided to sell my raccoon collar the rest of the money a considerable sum for me i decided to borrow from anton antonitch syetotchkin my immediate superior an unassuming person though grave and judicious he never lent money to anyone but i had on entering the service been specially recommended to him by an important personage who had got me my berth i was horribly worried to borrow from anton antonitch seemed to me monstrous and shameful i did not sleep for two or three nights indeed i did not sleep well at that time i was in a fever i had a vague sinking at my heart or else a sudden throbbing throbbing throbbing anton antonitch was surprised at first then he frowned then he reflected and did after all lend me the money receiving from me a written authorisation to take from my salary a fortnight later the sum that he had lent me in this way everything was at last ready the handsome beaver replaced the mean looking raccoon and i began by degrees to get to work it would never have done to act offhand at random the plan had to be carried out skilfully by degrees but i must confess that after many efforts i began to despair we simply could not run into each other i made every preparation i was quite determined it seemed as though we should run into one another directly and before i knew what i was doing i had stepped aside for him again and he had passed without noticing me i even prayed as i approached him that god would grant me determination one time i had made up my mind thoroughly but it ended in my stumbling and falling at his feet because at the very last instant when i was six inches from him my courage failed me he very calmly stepped over me while i flew on one side like a ball that night i was ill again feverish and delirious and suddenly it ended most happily the night before i had made up my mind not to carry out my fatal plan and to abandon it all and with that object i went to the nevsky for the last time just to see how i would abandon it all suddenly three paces from my enemy i unexpectedly made up my mind i closed my eyes and we ran full tilt shoulder to shoulder against one another i did not budge an inch and passed him on a perfectly equal footing he did not even look round and pretended not to notice it but he was only pretending i am convinced of that i am convinced of that to this day of course i got the worst of it he was stronger but that was not the point the point was that i had attained my object i had kept up my dignity i had not yielded a step and had put myself publicly on an equal social footing with him i returned home feeling that i was fully avenged for everything i was delighted i was triumphant and sang italian arias of course i will not describe to you what happened to me three days later if you have read my first chapter you can guess for yourself the officer was afterwards transferred i have not seen him now for fourteen years what is the dear fellow doing now whom is he walking over ii but the period of my dissipation would end and i always felt very sick afterwards it was followed by remorse i tried to drive it away i felt too sick by degrees however i grew used to that too i grew used to everything or rather i voluntarily resigned myself to enduring it but i had a means of escape that reconciled everything that was to find refuge in the sublime and the beautiful in dreams of course i was a terrible dreamer i would dream for three months on end tucked away in my corner and you may believe me that at those moments i had no resemblance to the gentleman who in the perturbation of his chicken heart put a collar of german beaver on his great coat i suddenly became a hero i would not have admitted my six foot lieutenant even if he had called on me i could not even picture him before me then what were my dreams and how i could satisfy myself with them it is hard to say now but at the time i was satisfied with them though indeed even now i am to some extent satisfied with them dreams were particularly sweet and vivid after a spell of dissipation they came with remorse and with tears with curses and transports there were moments of such positive intoxication of such happiness that there was not the faintest trace of irony within me on my honour i had faith hope love i believed blindly at such times that by some miracle by some external circumstance all this would suddenly open out expand that suddenly a vista of suitable activity beneficent good and above all ready made what sort of activity i had no idea but the great thing was that it should be all ready for me would rise up before me and i should come out into the light of day almost riding a white horse and crowned with laurel anything but the foremost place i could not conceive for myself and for that very reason i quite contentedly occupied the lowest in reality either to be a hero or to grovel in the mud there was nothing between that was my ruin for when i was in the mud i comforted myself with the thought that at other times i was a hero and the hero was a cloak for the mud for an ordinary man it was shameful to defile himself but a hero was too lofty to be utterly defiled and so he might defile himself it is worth noting that these attacks of the sublime and the beautiful visited me even during the period of dissipation and just at the times when i was touching the bottom they came in separate spurts as though reminding me of themselves but did not banish the dissipation by their appearance on the contrary they seemed to add a zest to it by contrast and were only sufficiently present to serve as an appetising sauce that sauce was made up of contradictions and sufferings of agonising inward analysis and all these pangs and pin pricks gave a certain piquancy even a significance to my dissipation in fact completely answered the purpose of an appetising sauce there was a certain depth of meaning in it and i could hardly have resigned myself to the simple vulgar direct debauchery of a clerk and have endured all the filthiness of it what could have allured me about it then and have drawn me at night into the street no i had a lofty way of getting out of it all and what loving kindness oh lord what loving kindness i felt at times in those dreams of mine in those flights into the sublime and the beautiful though it was fantastic love though it was never applied to anything human in reality yet there was so much of this love that one did not feel afterwards even the impulse to apply it in reality that would have been superfluous everything however passed satisfactorily by a lazy and fascinating transition into the sphere of art that is into the beautiful forms of life lying ready largely stolen from the poets and novelists and adapted to all sorts of needs and uses i for instance was triumphant over everyone everyone of course was in dust and ashes and was forced spontaneously to recognise my superiority and i forgave them all i was a poet and a grand gentleman i fell in love i came in for countless millions and immediately devoted them to humanity and at the same time i confessed before all the people my shameful deeds which of course were not merely shameful but had in them much that was sublime and beautiful something in the manfred style everyone would kiss me and weep what idiots they would be if they did not while i should go barefoot and hungry preaching new ideas and fighting a victorious austerlitz against the obscurantists then the band would play a march an amnesty would be declared the pope would agree to retire from rome to brazil then there would be a ball for the whole of italy at the villa borghese on the shores of lake como lake como being for that purpose transferred to the neighbourhood of rome then would come a scene in the bushes and so on and so on as though you did not know all about it you will say that it is vulgar and contemptible to drag all this into public after all the tears and transports which i have myself confessed but why is it contemptible can you imagine that i am ashamed of it all and that it was stupider than anything in your life gentlemen and i can assure you that some of these fancies were by no means badly composed it did not all happen on the shores of lake como and yet you are right it really is vulgar and contemptible and most contemptible of all it is that now i am attempting to justify myself to you and even more contemptible than that is my making this remark now but that s enough or there will be no end to it each step will be more contemptible than the last i could never stand more than three months of dreaming at a time without feeling an irresistible desire to plunge into society to plunge into society meant to visit my superior at the office anton antonitch syetotchkin he was the only permanent acquaintance i have had in my life and i wonder at the fact myself now but i only went to see him when that phase came over me and when my dreams had reached such a point of bliss that it became essential at once to embrace my fellows and all mankind and for that purpose i needed at least one human being actually existing i had to call on anton antonitch however on tuesday his at home day so i had always to time my passionate desire to embrace humanity so that it might fall on a tuesday this anton antonitch lived on the fourth storey in a house in five corners in four low pitched rooms one smaller than the other of a particularly frugal and sallow appearance he had two daughters and their aunt who used to pour out the tea of the daughters one was thirteen and another fourteen they both had snub noses and i was awfully shy of them because they were always whispering and giggling together the master of the house usually sat in his study on a leather couch in front of the table with some grey headed gentleman usually a colleague from our office or some other department i never saw more than two or three visitors there always the same they talked about the excise duty about business in the senate about salaries about promotions about his excellency and the best means of pleasing him and so on i had the patience to sit like a fool beside these people for four hours at a stretch listening to them without knowing what to say to them or venturing to say a word i became stupefied several times i felt myself perspiring i was overcome by a sort of paralysis but this was pleasant and good for me on returning home i deferred for a time my desire to embrace all mankind i had however one other acquaintance of a sort simonov who was an old schoolfellow i had a number of schoolfellows indeed in petersburg but i did not associate with them and had even given up nodding to them in the street i believe i had transferred into the department i was in simply to avoid their company and to cut off all connection with my hateful childhood curses on that school and all those terrible years of penal servitude in short i parted from my schoolfellows as soon as i got out into the world there were two or three left to whom i nodded in the street one of them was simonov who had in no way been distinguished at school was of a quiet and equable disposition but i discovered in him a certain independence of character and even honesty i don t even suppose that he was particularly stupid i had at one time spent some rather soulful moments with him but these had not lasted long and had somehow been suddenly clouded over he was evidently uncomfortable at these reminiscences and was i fancy always afraid that i might take up the same tone again i suspected that he had an aversion for me but still i went on going to see him not being quite certain of it and so on one occasion unable to endure my solitude and knowing that as it was thursday anton antonitch s door would be closed i thought of simonov climbing up to his fourth storey i was thinking that the man disliked me and that it was a mistake to go and see him but as it always happened that such reflections impelled me as though purposely to put myself into a false position i went in it was almost a year since i had last seen simonov iii i found two of my old schoolfellows with him they seemed to be discussing an important matter all of them took scarcely any notice of my entrance which was strange for i had not met them for years evidently they looked upon me as something on the level of a common fly i had not been treated like that even at school though they all hated me i knew of course that they must despise me now for my lack of success in the service and for my having let myself sink so low going about badly dressed and so on which seemed to them a sign of my incapacity and insignificance but i had not expected such contempt simonov was positively surprised at my turning up even in old days he had always seemed surprised at my coming all this disconcerted me i sat down feeling rather miserable and began listening to what they were saying they were engaged in warm and earnest conversation about a farewell dinner which they wanted to arrange for the next day to a comrade of theirs called zverkov an officer in the army who was going away to a distant province this zverkov had been all the time at school with me too i had begun to hate him particularly in the upper forms in the lower forms he had simply been a pretty playful boy whom everybody liked i had hated him however even in the lower forms just because he was a pretty and playful boy he was always bad at his lessons and got worse and worse as he went on however he left with a good certificate as he had powerful interests during his last year at school he came in for an estate of two hundred serfs and as almost all of us were poor he took up a swaggering tone among us he was vulgar in the extreme but at the same time he was a good natured fellow even in his swaggering in spite of superficial fantastic and sham notions of honour and dignity all but very few of us positively grovelled before zverkov and the more so the more he swaggered and it was not from any interested motive that they grovelled but simply because he had been favoured by the gifts of nature moreover it was as it were an accepted idea among us that zverkov was a specialist in regard to tact and the social graces this last fact particularly infuriated me i hated the abrupt self confident tone of his voice his admiration of his own witticisms which were often frightfully stupid though he was bold in his language i hated his handsome but stupid face for which i would however have gladly exchanged my intelligent one and the free and easy military manners in fashion in the forties i hated the way in which he used to talk of his future conquests of women he did not venture to begin his attack upon women until he had the epaulettes of an officer and was looking forward to them with impatience and boasted of the duels he would constantly be fighting i remember how i invariably so taciturn suddenly fastened upon zverkov when one day talking at a leisure moment with his schoolfellows of his future relations with the fair sex and growing as sportive as a puppy in the sun he all at once declared that he would not leave a single village girl on his estate unnoticed that that was his droit de seigneur and that if the peasants dared to protest he would have them all flogged and double the tax on them the bearded rascals our servile rabble applauded but i attacked him not from compassion for the girls and their fathers but simply because they were applauding such an insect i got the better of him on that occasion but though zverkov was stupid he was lively and impudent and so laughed it off and in such a way that my victory was not really complete the laugh was on his side he got the better of me on several occasions afterwards but without malice jestingly casually i remained angrily and contemptuously silent and would not answer him when we left school he made advances to me i did not rebuff them for i was flattered but we soon parted and quite naturally afterwards i heard of his barrack room success as a lieutenant and of the fast life he was leading then there came other rumours of his successes in the service by then he had taken to cutting me in the street and i suspected that he was afraid of compromising himself by greeting a personage as insignificant as me i saw him once in the theatre in the third tier of boxes by then he was wearing shoulder straps he was twisting and twirling about ingratiating himself with the daughters of an ancient general in three years he had gone off considerably though he was still rather handsome and adroit one could see that by the time he was thirty he would be corpulent so it was to this zverkov that my schoolfellows were going to give a dinner on his departure they had kept up with him for those three years though privately they did not consider themselves on an equal footing with him i am convinced of that of simonov s two visitors one was ferfitchkin a russianised german a little fellow with the face of a monkey a blockhead who was always deriding everyone a very bitter enemy of mine from our days in the lower forms a vulgar impudent swaggering fellow who affected a most sensitive feeling of personal honour though of course he was a wretched little coward at heart he was one of those worshippers of zverkov who made up to the latter from interested motives and often borrowed money from him simonov s other visitor trudolyubov was a person in no way remarkable a tall young fellow in the army with a cold face fairly honest though he worshipped success of every sort and was only capable of thinking of promotion he was some sort of distant relation of zverkov s and this foolish as it seems gave him a certain importance among us he always thought me of no consequence whatever his behaviour to me though not quite courteous was tolerable well with seven roubles each said trudolyubov twenty one roubles between the three of us we ought to be able to get a good dinner zverkov of course won t pay of course not since we are inviting him simonov decided can you imagine ferfitchkin interrupted hotly and conceitedly like some insolent flunkey boasting of his master the general s decorations can you imagine that zverkov will let us pay alone he will accept from delicacy but he will order half a dozen bottles of champagne do we want half a dozen for the four of us observed trudolyubov taking notice only of the half dozen so the three of us with zverkov for the fourth twenty one roubles at the hotel de paris at five o clock tomorrow simonov who had been asked to make the arrangements concluded finally how twenty one roubles i asked in some agitation with a show of being offended if you count me it will not be twenty one but twenty eight roubles it seemed to me that to invite myself so suddenly and unexpectedly would be positively graceful and that they would all be conquered at once and would look at me with respect do you want to join too simonov observed with no appearance of pleasure seeming to avoid looking at me he knew me through and through it infuriated me that he knew me so thoroughly why not i am an old schoolfellow of his too i believe and i must own i feel hurt that you have left me out i said boiling over again and where were we to find you ferfitchkin put in roughly you never were on good terms with zverkov trudolyubov added frowning but i had already clutched at the idea and would not give it up it seems to me that no one has a right to form an opinion upon that i retorted in a shaking voice as though something tremendous had happened perhaps that is just my reason for wishing it now that i have not always been on good terms with him oh there s no making you out with these refinements trudolyubov jeered we ll put your name down simonov decided addressing me tomorrow at five o clock at the hotel de paris what about the money ferfitchkin began in an undertone indicating me to simonov but he broke off for even simonov was embarrassed that will do said trudolyubov getting up if he wants to come so much let him but it s a private thing between us friends ferfitchkin said crossly as he too picked up his hat it s not an official gathering we do not want at all perhaps they went away ferfitchkin did not greet me in any way as he went out trudolyubov barely nodded simonov with whom i was left tete a tete was in a state of vexation and perplexity and looked at me queerly he did not sit down and did not ask me to h m yes tomorrow then will you pay your subscription now i just ask so as to know he muttered in embarrassment i flushed crimson as i did so i remembered that i had owed simonov fifteen roubles for ages which i had indeed never forgotten though i had not paid it you will understand simonov that i could have no idea when i came here i am very much vexed that i have forgotten all right all right that doesn t matter you can pay tomorrow after the dinner i simply wanted to know please don t he broke off and began pacing the room still more vexed as he walked he began to stamp with his heels am i keeping you i asked after two minutes of silence oh he said starting that is to be truthful yes i have to go and see someone not far from here he added in an apologetic voice somewhat abashed my goodness why didn t you say so i cried seizing my cap with an astonishingly free and easy air which was the last thing i should have expected of myself it s close by not two paces away simonov repeated accompanying me to the front door with a fussy air which did not suit him at all so five o clock punctually tomorrow he called down the stairs after me he was very glad to get rid of me i was in a fury what possessed me what possessed me to force myself upon them i wondered grinding my teeth as i strode along the street for a scoundrel a pig like that zverkov of course i had better not go of course i must just snap my fingers at them i am not bound in any way i ll send simonov a note by tomorrow s post but what made me furious was that i knew for certain that i should go that i should make a point of going and the more tactless the more unseemly my going would be the more certainly i would go and there was a positive obstacle to my going i had no money all i had was nine roubles i had to give seven of that to my servant apollon for his monthly wages that was all i paid him he had to keep himself not to pay him was impossible considering his character but i will talk about that fellow about that plague of mine another time however i knew i should go and should not pay him his wages that night i had the most hideous dreams no wonder all the evening i had been oppressed by memories of my miserable days at school and i could not shake them off i was sent to the school by distant relations upon whom i was dependent and of whom i have heard nothing since they sent me there a forlorn silent boy already crushed by their reproaches already troubled by doubt and looking with savage distrust at everyone my schoolfellows met me with spiteful and merciless jibes because i was not like any of them but i could not endure their taunts i could not give in to them with the ignoble readiness with which they gave in to one another i hated them from the first and shut myself away from everyone in timid wounded and disproportionate pride their coarseness revolted me they laughed cynically at my face at my clumsy figure and yet what stupid faces they had themselves in our school the boys faces seemed in a special way to degenerate and grow stupider how many fine looking boys came to us in a few years they became repulsive even at sixteen i wondered at them morosely even then i was struck by the pettiness of their thoughts the stupidity of their pursuits their games their conversations they had no understanding of such essential things they took no interest in such striking impressive subjects that i could not help considering them inferior to myself it was not wounded vanity that drove me to it and for god s sake do not thrust upon me your hackneyed remarks repeated to nausea that i was only a dreamer while they even then had an understanding of life they understood nothing they had no idea of real life and i swear that that was what made me most indignant with them on the contrary the most obvious striking reality they accepted with fantastic stupidity and even at that time were accustomed to respect success everything that was just but oppressed and looked down upon they laughed at heartlessly and shamefully they took rank for intelligence even at sixteen they were already talking about a snug berth of course a great deal of it was due to their stupidity to the bad examples with which they had always been surrounded in their childhood and boyhood they were monstrously depraved of course a great deal of that too was superficial and an assumption of cynicism of course there were glimpses of youth and freshness even in their depravity but even that freshness was not attractive and showed itself in a certain rakishness i hated them horribly though perhaps i was worse than any of them they repaid me in the same way and did not conceal their aversion for me but by then i did not desire their affection on the contrary i continually longed for their humiliation to escape from their derision i purposely began to make all the progress i could with my studies and forced my way to the very top this impressed them moreover they all began by degrees to grasp that i had already read books none of them could read and understood things not forming part of our school curriculum of which they had not even heard they took a savage and sarcastic view of it but were morally impressed especially as the teachers began to notice me on those grounds the mockery ceased but the hostility remained and cold and strained relations became permanent between us in the end i could not put up with it with years a craving for society for friends developed in me i attempted to get on friendly terms with some of my schoolfellows but somehow or other my intimacy with them was always strained and soon ended of itself once indeed i did have a friend but i was already a tyrant at heart i wanted to exercise unbounded sway over him i tried to instil into him a contempt for his surroundings i required of him a disdainful and complete break with those surroundings i frightened him with my passionate affection i reduced him to tears to hysterics he was a simple and devoted soul but when he devoted himself to me entirely i began to hate him immediately and repulsed him as though all i needed him for was to win a victory over him to subjugate him and nothing else but i could not subjugate all of them my friend was not at all like them either he was in fact a rare exception the first thing i did on leaving school was to give up the special job for which i had been destined so as to break all ties to curse my past and shake the dust from off my feet and goodness knows why after all that i should go trudging off to simonov s early next morning i roused myself and jumped out of bed with excitement as though it were all about to happen at once but i believed that some radical change in my life was coming and would inevitably come that day owing to its rarity perhaps any external event however trivial always made me feel as though some radical change in my life were at hand i went to the office however as usual but sneaked away home two hours earlier to get ready the great thing i thought is not to be the first to arrive or they will think i am overjoyed at coming but there were thousands of such great points to consider and they all agitated and overwhelmed me i polished my boots a second time with my own hands nothing in the world would have induced apollon to clean them twice a day as he considered that it was more than his duties required of him i stole the brushes to clean them from the passage being careful he should not detect it for fear of his contempt then i minutely examined my clothes and thought that everything looked old worn and threadbare i had let myself get too slovenly my uniform perhaps was tidy but i could not go out to dinner in my uniform the worst of it was that on the knee of my trousers was a big yellow stain i had a foreboding that that stain would deprive me of nine tenths of my personal dignity i knew too that it was very poor to think so but this is no time for thinking now i am in for the real thing i thought and my heart sank i knew too perfectly well even then that i was monstrously exaggerating the facts but how could i help it i could not control myself and was already shaking with fever with despair i pictured to myself how coldly and disdainfully that scoundrel zverkov would meet me with what dull witted invincible contempt the blockhead trudolyubov would look at me with what impudent rudeness the insect ferfitchkin would snigger at me in order to curry favour with zverkov how completely simonov would take it all in and how he would despise me for the abjectness of my vanity and lack of spirit and worst of all how paltry unliterary commonplace it would all be of course the best thing would be not to go at all but that was most impossible of all if i feel impelled to do anything i seem to be pitchforked into it i should have jeered at myself ever afterwards so you funked it you funked it you funked the real thing on the contrary i passionately longed to show all that rabble that i was by no means such a spiritless creature as i seemed to myself what is more even in the acutest paroxysm of this cowardly fever i dreamed of getting the upper hand of dominating them carrying them away making them like me if only for my elevation of thought and unmistakable wit they would abandon zverkov he would sit on one side silent and ashamed while i should crush him then perhaps we would be reconciled and drink to our everlasting friendship but what was most bitter and humiliating for me was that i knew even then knew fully and for certain that i needed nothing of all this really that i did not really want to crush to subdue to attract them and that i did not care a straw really for the result even if i did achieve it oh how i prayed for the day to pass quickly in unutterable anguish i went to the window opened the movable pane and looked out into the troubled darkness of the thickly falling wet snow at last my wretched little clock hissed out five i seized my hat and trying not to look at apollon who had been all day expecting his month s wages but in his foolishness was unwilling to be the first to speak about it i slipped between him and the door and jumping into a high class sledge on which i spent my last half rouble i drove up in grand style to the hotel de paris iv i had been certain the day before that i should be the first to arrive but it was not a question of being the first to arrive not only were they not there but i had difficulty in finding our room the table was not laid even what did it mean after a good many questions i elicited from the waiters that the dinner had been ordered not for five but for six o clock this was confirmed at the buffet too i felt really ashamed to go on questioning them it was only twenty five minutes past five if they changed the dinner hour they ought at least to have let me know that is what the post is for and not to have put me in an absurd position in my own eyes and and even before the waiters i sat down the servant began laying the table i felt even more humiliated when he was present towards six o clock they brought in candles though there were lamps burning in the room it had not occurred to the waiter however to bring them in at once when i arrived in the next room two gloomy angry looking persons were eating their dinners in silence at two different tables there was a great deal of noise even shouting in a room further away one could hear the laughter of a crowd of people and nasty little shrieks in french there were ladies at the dinner it was sickening in fact i rarely passed more unpleasant moments so much so that when they did arrive all together punctually at six i was overjoyed to see them as though they were my deliverers and even forgot that it was incumbent upon me to show resentment zverkov walked in at the head of them evidently he was the leading spirit he and all of them were laughing but seeing me zverkov drew himself up a little walked up to me deliberately with a slight rather jaunty bend from the waist he shook hands with me in a friendly but not over friendly fashion with a sort of circumspect courtesy like that of a general as though in giving me his hand he were warding off something i had imagined on the contrary that on coming in he would at once break into his habitual thin shrill laugh and fall to making his insipid jokes and witticisms i had been preparing for them ever since the previous day but i had not expected such condescension such high official courtesy so then he felt himself ineffably superior to me in every respect if he only meant to insult me by that high official tone it would not matter i thought i could pay him back for it one way or another but what if in reality without the least desire to be offensive that sheepshead had a notion in earnest that he was superior to me and could only look at me in a patronising way the very supposition made me gasp i was surprised to hear of your desire to join us he began lisping and drawling which was something new you and i seem to have seen nothing of one another you fight shy of us you shouldn t we are not such terrible people as you think well anyway i am glad to renew our acquaintance and he turned carelessly to put down his hat on the window have you been waiting long trudolyubov inquired i arrived at five o clock as you told me yesterday i answered aloud with an irritability that threatened an explosion didn t you let him know that we had changed the hour said trudolyubov to simonov no i didn t i forgot the latter replied with no sign of regret and without even apologising to me he went off to order the hors d oeuvre so you ve been here a whole hour oh poor fellow zverkov cried ironically for to his notions this was bound to be extremely funny that rascal ferfitchkin followed with his nasty little snigger like a puppy yapping my position struck him too as exquisitely ludicrous and embarrassing it isn t funny at all i cried to ferfitchkin more and more irritated it wasn t my fault but other people s they neglected to let me know it was it was it was simply absurd it s not only absurd but something else as well muttered trudolyubov naively taking my part you are not hard enough upon it it was simply rudeness unintentional of course and how could simonov h m if a trick like that had been played on me observed ferfitchkin i should but you should have ordered something for yourself zverkov interrupted or simply asked for dinner without waiting for us you will allow that i might have done that without your permission i rapped out if i waited it was let us sit down gentlemen cried simonov coming in everything is ready i can answer for the champagne it is capitally frozen you see i did not know your address where was i to look for you he suddenly turned to me but again he seemed to avoid looking at me evidently he had something against me it must have been what happened yesterday all sat down i did the same it was a round table trudolyubov was on my left simonov on my right zverkov was sitting opposite ferfitchkin next to him between him and trudolyubov tell me are you in a government office zverkov went on attending to me seeing that i was embarrassed he seriously thought that he ought to be friendly to me and so to speak cheer me up does he want me to throw a bottle at his head i thought in a fury in my novel surroundings i was unnaturally ready to be irritated in the n office i answered jerkily with my eyes on my plate and ha ave you a go od berth i say what ma a de you leave your original job what ma a de me was that i wanted to leave my original job i drawled more than he hardly able to control myself ferfitchkin went off into a guffaw simonov looked at me ironically trudolyubov left off eating and began looking at me with curiosity zverkov winced but he tried not to notice it and the remuneration what remuneration i mean your sa a lary why are you cross examining me however i told him at once what my salary was i turned horribly red it is not very handsome zverkov observed majestically yes you can t afford to dine at cafes on that ferfitchkin added insolently to my thinking it s very poor trudolyubov observed gravely and how thin you have grown how you have changed added zverkov with a shade of venom in his voice scanning me and my attire with a sort of insolent compassion oh spare his blushes cried ferfitchkin sniggering my dear sir allow me to tell you i am not blushing i broke out at last do you hear i am dining here at this cafe at my own expense not at other people s note that mr ferfitchkin wha at isn t every one here dining at his own expense you would seem to be ferfitchkin flew out at me turning as red as a lobster and looking me in the face with fury tha at i answered feeling i had gone too far and i imagine it would be better to talk of something more intelligent you intend to show off your intelligence i suppose don t disturb yourself that would be quite out of place here why are you clacking away like that my good sir eh have you gone out of your wits in your office enough gentlemen enough zverkov cried authoritatively how stupid it is muttered simonov it really is stupid we have met here a company of friends for a farewell dinner to a comrade and you carry on an altercation said trudolyubov rudely addressing himself to me alone you invited yourself to join us so don t disturb the general harmony enough enough cried zverkov give over gentlemen it s out of place better let me tell you how i nearly got married the day before yesterday and then followed a burlesque narrative of how this gentleman had almost been married two days before there was not a word about the marriage however but the story was adorned with generals colonels and kammer junkers while zverkov almost took the lead among them it was greeted with approving laughter ferfitchkin positively squealed no one paid any attention to me and i sat crushed and humiliated good heavens these are not the people for me i thought and what a fool i have made of myself before them i let ferfitchkin go too far though the brutes imagine they are doing me an honour in letting me sit down with them they don t understand that it s an honour to them and not to me i ve grown thinner my clothes oh damn my trousers zverkov noticed the yellow stain on the knee as soon as he came in but what s the use i must get up at once this very minute take my hat and simply go without a word with contempt and tomorrow i can send a challenge the scoundrels as though i cared about the seven roubles they may think damn it i don t care about the seven roubles i ll go this minute of course i remained i drank sherry and lafitte by the glassful in my discomfiture being unaccustomed to it i was quickly affected my annoyance increased as the wine went to my head i longed all at once to insult them all in a most flagrant manner and then go away to seize the moment and show what i could do so that they would say he s clever though he is absurd and and in fact damn them all i scanned them all insolently with my drowsy eyes but they seemed to have forgotten me altogether they were noisy vociferous cheerful zverkov was talking all the time i began listening zverkov was talking of some exuberant lady whom he had at last led on to declaring her love of course he was lying like a horse and how he had been helped in this affair by an intimate friend of his a prince kolya an officer in the hussars who had three thousand serfs and yet this kolya who has three thousand serfs has not put in an appearance here tonight to see you off i cut in suddenly for one minute every one was silent you are drunk already trudolyubov deigned to notice me at last glancing contemptuously in my direction zverkov without a word examined me as though i were an insect i dropped my eyes simonov made haste to fill up the glasses with champagne trudolyubov raised his glass as did everyone else but me your health and good luck on the journey he cried to zverkov to old times to our future hurrah they all tossed off their glasses and crowded round zverkov to kiss him i did not move my full glass stood untouched before me why aren t you going to drink it roared trudolyubov losing patience and turning menacingly to me i want to make a speech separately on my own account and then i ll drink it mr trudolyubov spiteful brute muttered simonov i drew myself up in my chair and feverishly seized my glass prepared for something extraordinary though i did not know myself precisely what i was going to say silence cried ferfitchkin now for a display of wit zverkov waited very gravely knowing what was coming mr lieutenant zverkov i began let me tell you that i hate phrases phrasemongers and men in corsets that s the first point and there is a second one to follow it there was a general stir the second point is i hate ribaldry and ribald talkers especially ribald talkers the third point i love justice truth and honesty i went on almost mechanically for i was beginning to shiver with horror myself and had no idea how i came to be talking like this i love thought monsieur zverkov i love true comradeship on an equal footing and not h m i love but however why not i will drink your health too mr zverkov seduce the circassian girls shoot the enemies of the fatherland and and to your health monsieur zverkov zverkov got up from his seat bowed to me and said i am very much obliged to you he was frightfully offended and turned pale damn the fellow roared trudolyubov bringing his fist down on the table well he wants a punch in the face for that squealed ferfitchkin we ought to turn him out muttered simonov not a word gentlemen not a movement cried zverkov solemnly checking the general indignation i thank you all but i can show him for myself how much value i attach to his words mr ferfitchkin you will give me satisfaction tomorrow for your words just now i said aloud turning with dignity to ferfitchkin a duel you mean certainly he answered but probably i was so ridiculous as i challenged him and it was so out of keeping with my appearance that everyone including ferfitchkin was prostrate with laughter yes let him alone of course he is quite drunk trudolyubov said with disgust i shall never forgive myself for letting him join us simonov muttered again now is the time to throw a bottle at their heads i thought to myself i picked up the bottle and filled my glass no i d better sit on to the end i went on thinking you would be pleased my friends if i went away nothing will induce me to go i ll go on sitting here and drinking to the end on purpose as a sign that i don t think you of the slightest consequence i will go on sitting and drinking because this is a public house and i paid my entrance money i ll sit here and drink for i look upon you as so many pawns as inanimate pawns i ll sit here and drink and sing if i want to yes sing for i have the right to to sing h m but i did not sing i simply tried not to look at any of them i assumed most unconcerned attitudes and waited with impatience for them to speak first but alas they did not address me and oh how i wished how i wished at that moment to be reconciled to them it struck eight at last nine they moved from the table to the sofa zverkov stretched himself on a lounge and put one foot on a round table wine was brought there he did as a fact order three bottles on his own account i of course was not invited to join them they all sat round him on the sofa they listened to him almost with reverence it was evident that they were fond of him what for what for i wondered from time to time they were moved to drunken enthusiasm and kissed each other they talked of the caucasus of the nature of true passion of snug berths in the service of the income of an hussar called podharzhevsky whom none of them knew personally and rejoiced in the largeness of it of the extraordinary grace and beauty of a princess d whom none of them had ever seen then it came to shakespeare s being immortal i smiled contemptuously and walked up and down the other side of the room opposite the sofa from the table to the stove and back again i tried my very utmost to show them that i could do without them and yet i purposely made a noise with my boots thumping with my heels but it was all in vain they paid no attention i had the patience to walk up and down in front of them from eight o clock till eleven in the same place from the table to the stove and back again i walk up and down to please myself and no one can prevent me the waiter who came into the room stopped from time to time to look at me i was somewhat giddy from turning round so often at moments it seemed to me that i was in delirium during those three hours i was three times soaked with sweat and dry again at times with an intense acute pang i was stabbed to the heart by the thought that ten years twenty years forty years would pass and that even in forty years i would remember with loathing and humiliation those filthiest most ludicrous and most awful moments of my life no one could have gone out of his way to degrade himself more shamelessly and i fully realised it fully and yet i went on pacing up and down from the table to the stove oh if you only knew what thoughts and feelings i am capable of how cultured i am i thought at moments mentally addressing the sofa on which my enemies were sitting but my enemies behaved as though i were not in the room once only once they turned towards me just when zverkov was talking about shakespeare and i suddenly gave a contemptuous laugh i laughed in such an affected and disgusting way that they all at once broke off their conversation and silently and gravely for two minutes watched me walking up and down from the table to the stove taking no notice of them but nothing came of it they said nothing and two minutes later they ceased to notice me again it struck eleven friends cried zverkov getting up from the sofa let us all be off now there of course of course the others assented i turned sharply to zverkov i was so harassed so exhausted that i would have cut my throat to put an end to it i was in a fever my hair soaked with perspiration stuck to my forehead and temples zverkov i beg your pardon i said abruptly and resolutely ferfitchkin yours too and everyone s everyone s i have insulted you all aha a duel is not in your line old man ferfitchkin hissed venomously it sent a sharp pang to my heart no it s not the duel i am afraid of ferfitchkin i am ready to fight you tomorrow after we are reconciled i insist upon it in fact and you cannot refuse i want to show you that i am not afraid of a duel you shall fire first and i shall fire into the air he is comforting himself said simonov he s simply raving said trudolyubov but let us pass why are you barring our way what do you want zverkov answered disdainfully they were all flushed their eyes were bright they had been drinking heavily i ask for your friendship zverkov i insulted you but insulted you insulted me understand sir that you never under any circumstances could possibly insult me and that s enough for you out of the way concluded trudolyubov olympia is mine friends that s agreed cried zverkov we won t dispute your right we won t dispute your right the others answered laughing i stood as though spat upon the party went noisily out of the room trudolyubov struck up some stupid song simonov remained behind for a moment to tip the waiters i suddenly went up to him simonov give me six roubles i said with desperate resolution he looked at me in extreme amazement with vacant eyes he too was drunk you don t mean you are coming with us yes i ve no money he snapped out and with a scornful laugh he went out of the room i clutched at his overcoat it was a nightmare simonov i saw you had money why do you refuse me am i a scoundrel beware of refusing me if you knew if you knew why i am asking my whole future my whole plans depend upon it simonov pulled out the money and almost flung it at me take it if you have no sense of shame he pronounced pitilessly and ran to overtake them i was left for a moment alone disorder the remains of dinner a broken wine glass on the floor spilt wine cigarette ends fumes of drink and delirium in my brain an agonising misery in my heart and finally the waiter who had seen and heard all and was looking inquisitively into my face i am going there i cried either they shall all go down on their knees to beg for my friendship or i will give zverkov a slap in the face v so this is it this is it at last contact with real life i muttered as i ran headlong downstairs this is very different from the pope s leaving rome and going to brazil very different from the ball on lake como you are a scoundrel a thought flashed through my mind if you laugh at this now no matter i cried answering myself now everything is lost there was no trace to be seen of them but that made no difference i knew where they had gone at the steps was standing a solitary night sledge driver in a rough peasant coat powdered over with the still falling wet and as it were warm snow it was hot and steamy the little shaggy piebald horse was also covered with snow and coughing i remember that very well i made a rush for the roughly made sledge but as soon as i raised my foot to get into it the recollection of how simonov had just given me six roubles seemed to double me up and i tumbled into the sledge like a sack no i must do a great deal to make up for all that i cried but i will make up for it or perish on the spot this very night start we set off there was a perfect whirl in my head they won t go down on their knees to beg for my friendship that is a mirage cheap mirage revolting romantic and fantastical that s another ball on lake como and so i am bound to slap zverkov s face it is my duty to and so it is settled i am flying to give him a slap in the face hurry up the driver tugged at the reins as soon as i go in i ll give it him ought i before giving him the slap to say a few words by way of preface no i ll simply go in and give it him they will all be sitting in the drawing room and he with olympia on the sofa that damned olympia she laughed at my looks on one occasion and refused me i ll pull olympia s hair pull zverkov s ears no better one ear and pull him by it round the room maybe they will all begin beating me and will kick me out that s most likely indeed no matter anyway i shall first slap him the initiative will be mine and by the laws of honour that is everything he will be branded and cannot wipe off the slap by any blows by nothing but a duel he will be forced to fight and let them beat me now let them the ungrateful wretches trudolyubov will beat me hardest he is so strong ferfitchkin will be sure to catch hold sideways and tug at my hair but no matter no matter that s what i am going for the blockheads will be forced at last to see the tragedy of it all when they drag me to the door i shall call out to them that in reality they are not worth my little finger get on driver get on i cried to the driver he started and flicked his whip i shouted so savagely we shall fight at daybreak that s a settled thing i ve done with the office ferfitchkin made a joke about it just now but where can i get pistols nonsense i ll get my salary in advance and buy them and powder and bullets that s the second s business and how can it all be done by daybreak and where am i to get a second i have no friends nonsense i cried lashing myself up more and more it s of no consequence the first person i meet in the street is bound to be my second just as he would be bound to pull a drowning man out of water the most eccentric things may happen even if i were to ask the director himself to be my second tomorrow he would be bound to consent if only from a feeling of chivalry and to keep the secret anton antonitch the fact is that at that very minute the disgusting absurdity of my plan and the other side of the question was clearer and more vivid to my imagination than it could be to anyone on earth but get on driver get on you rascal get on ugh sir said the son of toil cold shivers suddenly ran down me wouldn t it be better to go straight home my god my god why did i invite myself to this dinner yesterday but no it s impossible and my walking up and down for three hours from the table to the stove no they they and no one else must pay for my walking up and down they must wipe out this dishonour drive on and what if they give me into custody they won t dare they ll be afraid of the scandal and what if zverkov is so contemptuous that he refuses to fight a duel he is sure to but in that case i ll show them i will turn up at the posting station when he s setting off tomorrow i ll catch him by the leg i ll pull off his coat when he gets into the carriage i ll get my teeth into his hand i ll bite him see what lengths you can drive a desperate man to he may hit me on the head and they may belabour me from behind i will shout to the assembled multitude look at this young puppy who is driving off to captivate the circassian girls after letting me spit in his face of course after that everything will be over the office will have vanished off the face of the earth i shall be arrested i shall be tried i shall be dismissed from the service thrown in prison sent to siberia never mind in fifteen years when they let me out of prison i will trudge off to him a beggar in rags i shall find him in some provincial town he will be married and happy he will have a grown up daughter i shall say to him look monster at my hollow cheeks and my rags i ve lost everything my career my happiness art science the woman i loved and all through you here are pistols i have come to discharge my pistol and and i forgive you then i shall fire into the air and he will hear nothing more of me i was actually on the point of tears though i knew perfectly well at that moment that all this was out of pushkin s silvio and lermontov s masquerade and all at once i felt horribly ashamed so ashamed that i stopped the horse got out of the sledge and stood still in the snow in the middle of the street the driver gazed at me sighing and astonished what was i to do i could not go on there it was evidently stupid and i could not leave things as they were because that would seem as though heavens how could i leave things and after such insults no i cried throwing myself into the sledge again it is ordained it is fate drive on drive on and in my impatience i punched the sledge driver on the back of the neck what are you up to what are you hitting me for the peasant shouted but he whipped up his nag so that it began kicking the wet snow was falling in big flakes i unbuttoned myself regardless of it i forgot everything else for i had finally decided on the slap and felt with horror that it was going to happen now at once and that no force could stop it the deserted street lamps gleamed sullenly in the snowy darkness like torches at a funeral the snow drifted under my great coat under my coat under my cravat and melted there i did not wrap myself up all was lost anyway at last we arrived i jumped out almost unconscious ran up the steps and began knocking and kicking at the door i felt fearfully weak particularly in my legs and knees the door was opened quickly as though they knew i was coming as a fact simonov had warned them that perhaps another gentleman would arrive and this was a place in which one had to give notice and to observe certain precautions it was one of those millinery establishments which were abolished by the police a good time ago by day it really was a shop but at night if one had an introduction one might visit it for other purposes i walked rapidly through the dark shop into the familiar drawing room where there was only one candle burning and stood still in amazement there was no one there where are they i asked somebody but by now of course they had separated before me was standing a person with a stupid smile the madam herself who had seen me before a minute later a door opened and another person came in taking no notice of anything i strode about the room and i believe i talked to myself i felt as though i had been saved from death and was conscious of this joyfully all over i should have given that slap i should certainly certainly have given it but now they were not here and everything had vanished and changed i looked round i could not realise my condition yet i looked mechanically at the girl who had come in and had a glimpse of a fresh young rather pale face with straight dark eyebrows and with grave as it were wondering eyes that attracted me at once i should have hated her if she had been smiling i began looking at her more intently and as it were with effort i had not fully collected my thoughts there was something simple and good natured in her face but something strangely grave i am sure that this stood in her way here and no one of those fools had noticed her she could not however have been called a beauty though she was tall strong looking and well built she was very simply dressed something loathsome stirred within me i went straight up to her i chanced to look into the glass my harassed face struck me as revolting in the extreme pale angry abject with dishevelled hair no matter i am glad of it i thought i am glad that i shall seem repulsive to her i like that vi somewhere behind a screen a clock began wheezing as though oppressed by something as though someone were strangling it after an unnaturally prolonged wheezing there followed a shrill nasty and as it were unexpectedly rapid chime as though someone were suddenly jumping forward it struck two i woke up though i had indeed not been asleep but lying half conscious it was almost completely dark in the narrow cramped low pitched room cumbered up with an enormous wardrobe and piles of cardboard boxes and all sorts of frippery and litter the candle end that had been burning on the table was going out and gave a faint flicker from time to time in a few minutes there would be complete darkness i was not long in coming to myself everything came back to my mind at once without an effort as though it had been in ambush to pounce upon me again and indeed even while i was unconscious a point seemed continually to remain in my memory unforgotten and round it my dreams moved drearily but strange to say everything that had happened to me in that day seemed to me now on waking to be in the far far away past as though i had long long ago lived all that down my head was full of fumes something seemed to be hovering over me rousing me exciting me and making me restless misery and spite seemed surging up in me again and seeking an outlet suddenly i saw beside me two wide open eyes scrutinising me curiously and persistently the look in those eyes was coldly detached sullen as it were utterly remote it weighed upon me a grim idea came into my brain and passed all over my body as a horrible sensation such as one feels when one goes into a damp and mouldy cellar there was something unnatural in those two eyes beginning to look at me only now i recalled too that during those two hours i had not said a single word to this creature and had in fact considered it utterly superfluous in fact the silence had for some reason gratified me now i suddenly realised vividly the hideous idea revolting as a spider of vice which without love grossly and shamelessly begins with that in which true love finds its consummation for a long time we gazed at each other like that but she did not drop her eyes before mine and her expression did not change so that at last i felt uncomfortable what is your name i asked abruptly to put an end to it liza she answered almost in a whisper but somehow far from graciously and she turned her eyes away i was silent what weather the snow it s disgusting i said almost to myself putting my arm under my head despondently and gazing at the ceiling she made no answer this was horrible have you always lived in petersburg i asked a minute later almost angrily turning my head slightly towards her no where do you come from from riga she answered reluctantly are you a german no russian have you been here long where in this house a fortnight she spoke more and more jerkily the candle went out i could no longer distinguish her face have you a father and mother yes no i have where are they there in riga what are they oh nothing nothing why what class are they tradespeople have you always lived with them yes how old are you twenty why did you leave them oh for no reason that answer meant let me alone i feel sick sad we were silent god knows why i did not go away i felt myself more and more sick and dreary the images of the previous day began of themselves apart from my will flitting through my memory in confusion i suddenly recalled something i had seen that morning when full of anxious thoughts i was hurrying to the office i saw them carrying a coffin out yesterday and they nearly dropped it i suddenly said aloud not that i desired to open the conversation but as it were by accident a coffin yes in the haymarket they were bringing it up out of a cellar from a cellar not from a cellar but a basement oh you know down below from a house of ill fame it was filthy all round egg shells litter a stench it was loathsome silence a nasty day to be buried i began simply to avoid being silent nasty in what way the snow the wet i yawned it makes no difference she said suddenly after a brief silence no it s horrid i yawned again the gravediggers must have sworn at getting drenched by the snow and there must have been water in the grave why water in the grave she asked with a sort of curiosity but speaking even more harshly and abruptly than before i suddenly began to feel provoked why there must have been water at the bottom a foot deep you can t dig a dry grave in volkovo cemetery why why why the place is waterlogged it s a regular marsh so they bury them in water i ve seen it myself many times i had never seen it once indeed i had never been in volkovo and had only heard stories of it do you mean to say you don t mind how you die but why should i die she answered as though defending herself why some day you will die and you will die just the same as that dead woman she was a girl like you she died of consumption a wench would have died in hospital she knows all about it already she said wench not girl she was in debt to her madam i retorted more and more provoked by the discussion and went on earning money for her up to the end though she was in consumption some sledge drivers standing by were talking about her to some soldiers and telling them so no doubt they knew her they were laughing they were going to meet in a pot house to drink to her memory a great deal of this was my invention silence followed profound silence she did not stir and is it better to die in a hospital isn t it just the same besides why should i die she added irritably if not now a little later why a little later why indeed now you are young pretty fresh you fetch a high price but after another year of this life you will be very different you will go off in a year anyway in a year you will be worth less i continued malignantly you will go from here to something lower another house a year later to a third lower and lower and in seven years you will come to a basement in the haymarket that will be if you were lucky but it would be much worse if you got some disease consumption say and caught a chill or something or other it s not easy to get over an illness in your way of life if you catch anything you may not get rid of it and so you would die oh well then i shall die she answered quite vindictively and she made a quick movement but one is sorry sorry for whom sorry for life silence have you been engaged to be married eh what s that to you oh i am not cross examining you it s nothing to me why are you so cross of course you may have had your own troubles what is it to me it s simply that i felt sorry sorry for whom sorry for you no need she whispered hardly audibly and again made a faint movement that incensed me at once what i was so gentle with her and she why do you think that you are on the right path i don t think anything that s what s wrong that you don t think realise it while there is still time there still is time you are still young good looking you might love be married be happy not all married women are happy she snapped out in the rude abrupt tone she had used at first not all of course but anyway it is much better than the life here infinitely better besides with love one can live even without happiness even in sorrow life is sweet life is sweet however one lives but here what is there but foulness phew i turned away with disgust i was no longer reasoning coldly i began to feel myself what i was saying and warmed to the subject i was already longing to expound the cherished ideas i had brooded over in my corner something suddenly flared up in me an object had appeared before me never mind my being here i am not an example for you i am perhaps worse than you are i was drunk when i came here though i hastened however to say in self defence besides a man is no example for a woman it s a different thing i may degrade and defile myself but i am not anyone s slave i come and go and that s an end of it i shake it off and i am a different man but you are a slave from the start yes a slave you give up everything your whole freedom if you want to break your chains afterwards you won t be able to you will be more and more fast in the snares it is an accursed bondage i know it i won t speak of anything else maybe you won t understand but tell me no doubt you are in debt to your madam there you see i added though she made no answer but only listened in silence entirely absorbed that s a bondage for you you will never buy your freedom they will see to that it s like selling your soul to the devil and besides perhaps i too am just as unlucky how do you know and wallow in the mud on purpose out of misery you know men take to drink from grief well maybe i am here from grief come tell me what is there good here here you and i came together just now and did not say one word to one another all the time and it was only afterwards you began staring at me like a wild creature and i at you is that loving is that how one human being should meet another it s hideous that s what it is yes she assented sharply and hurriedly i was positively astounded by the promptitude of this yes so the same thought may have been straying through her mind when she was staring at me just before so she too was capable of certain thoughts damn it all this was interesting this was a point of likeness i thought almost rubbing my hands and indeed it s easy to turn a young soul like that it was the exercise of my power that attracted me most she turned her head nearer to me and it seemed to me in the darkness that she propped herself on her arm perhaps she was scrutinising me how i regretted that i could not see her eyes i heard her deep breathing why have you come here i asked her with a note of authority already in my voice oh i don t know but how nice it would be to be living in your father s house it s warm and free you have a home of your own but what if it s worse than this i must take the right tone flashed through my mind i may not get far with sentimentality but it was only a momentary thought i swear she really did interest me besides i was exhausted and moody and cunning so easily goes hand in hand with feeling who denies it i hastened to answer anything may happen i am convinced that someone has wronged you and that you are more sinned against than sinning of course i know nothing of your story but it s not likely a girl like you has come here of her own inclination a girl like me she whispered hardly audibly but i heard it damn it all i was flattering her that was horrid but perhaps it was a good thing she was silent see liza i will tell you about myself if i had had a home from childhood i shouldn t be what i am now i often think that however bad it may be at home anyway they are your father and mother and not enemies strangers once a year at least they ll show their love of you anyway you know you are at home i grew up without a home and perhaps that s why i ve turned so unfeeling i waited again perhaps she doesn t understand i thought and indeed it is absurd it s moralising if i were a father and had a daughter i believe i should love my daughter more than my sons really i began indirectly as though talking of something else to distract her attention i must confess i blushed why so she asked ah so she was listening i don t know liza i knew a father who was a stern austere man but used to go down on his knees to his daughter used to kiss her hands her feet he couldn t make enough of her really when she danced at parties he used to stand for five hours at a stretch gazing at her he was mad over her i understand that she would fall asleep tired at night and he would wake to kiss her in her sleep and make the sign of the cross over her he would go about in a dirty old coat he was stingy to everyone else but would spend his last penny for her giving her expensive presents and it was his greatest delight when she was pleased with what he gave her fathers always love their daughters more than the mothers do some girls live happily at home and i believe i should never let my daughters marry what next she said with a faint smile i should be jealous i really should to think that she should kiss anyone else that she should love a stranger more than her father it s painful to imagine it of course that s all nonsense of course every father would be reasonable at last but i believe before i should let her marry i should worry myself to death i should find fault with all her suitors but i should end by letting her marry whom she herself loved the one whom the daughter loves always seems the worst to the father you know that is always so so many family troubles come from that some are glad to sell their daughters rather than marrying them honourably ah so that was it such a thing liza happens in those accursed families in which there is neither love nor god i retorted warmly and where there is no love there is no sense either there are such families it s true but i am not speaking of them you must have seen wickedness in your own family if you talk like that truly you must have been unlucky h m that sort of thing mostly comes about through poverty and is it any better with the gentry even among the poor honest people who live happily h m yes perhaps another thing liza man is fond of reckoning up his troubles but does not count his joys if he counted them up as he ought he would see that every lot has enough happiness provided for it and what if all goes well with the family if the blessing of god is upon it if the husband is a good one loves you cherishes you never leaves you there is happiness in such a family even sometimes there is happiness in the midst of sorrow and indeed sorrow is everywhere if you marry you will find out for yourself but think of the first years of married life with one you love what happiness what happiness there sometimes is in it and indeed it s the ordinary thing in those early days even quarrels with one s husband end happily some women get up quarrels with their husbands just because they love them indeed i knew a woman like that she seemed to say that because she loved him she would torment him and make him feel it you know that you may torment a man on purpose through love women are particularly given to that thinking to themselves i will love him so i will make so much of him afterwards that it s no sin to torment him a little now and all in the house rejoice in the sight of you and you are happy and gay and peaceful and honourable then there are some women who are jealous if he went off anywhere i knew one such woman she couldn t restrain herself but would jump up at night and run off on the sly to find out where he was whether he was with some other woman that s a pity and the woman knows herself it s wrong and her heart fails her and she suffers but she loves it s all through love and how sweet it is to make up after quarrels to own herself in the wrong or to forgive him and they both are so happy all at once as though they had met anew been married over again as though their love had begun afresh and no one no one should know what passes between husband and wife if they love one another and whatever quarrels there may be between them they ought not to call in their own mother to judge between them and tell tales of one another they are their own judges love is a holy mystery and ought to be hidden from all other eyes whatever happens that makes it holier and better they respect one another more and much is built on respect and if once there has been love if they have been married for love why should love pass away surely one can keep it it is rare that one cannot keep it and if the husband is kind and straightforward why should not love last the first phase of married love will pass it is true but then there will come a love that is better still then there will be the union of souls they will have everything in common there will be no secrets between them and once they have children the most difficult times will seem to them happy so long as there is love and courage even toil will be a joy you may deny yourself bread for your children and even that will be a joy they will love you for it afterwards so you are laying by for your future as the children grow up you feel that you are an example a support for them that even after you die your children will always keep your thoughts and feelings because they have received them from you they will take on your semblance and likeness so you see this is a great duty how can it fail to draw the father and mother nearer people say it s a trial to have children who says that it is heavenly happiness are you fond of little children liza i am awfully fond of them you know a little rosy baby boy at your bosom and what husband s heart is not touched seeing his wife nursing his child a plump little rosy baby sprawling and snuggling chubby little hands and feet clean tiny little nails so tiny that it makes one laugh to look at them eyes that look as if they understand everything and while it sucks it clutches at your bosom with its little hand plays when its father comes up the child tears itself away from the bosom flings itself back looks at its father laughs as though it were fearfully funny and falls to sucking again or it will bite its mother s breast when its little teeth are coming while it looks sideways at her with its little eyes as though to say look i am biting is not all that happiness when they are the three together husband wife and child one can forgive a great deal for the sake of such moments yes liza one must first learn to live oneself before one blames others it s by pictures pictures like that one must get at you i thought to myself though i did speak with real feeling and all at once i flushed crimson what if she were suddenly to burst out laughing what should i do then that idea drove me to fury towards the end of my speech i really was excited and now my vanity was somehow wounded the silence continued i almost nudged her why are you she began and stopped but i understood there was a quiver of something different in her voice not abrupt harsh and unyielding as before but something soft and shamefaced so shamefaced that i suddenly felt ashamed and guilty what i asked with tender curiosity why you what why you speak somehow like a book she said and again there was a note of irony in her voice that remark sent a pang to my heart it was not what i was expecting i did not understand that she was hiding her feelings under irony that this is usually the last refuge of modest and chaste souled people when the privacy of their soul is coarsely and intrusively invaded and that their pride makes them refuse to surrender till the last moment and shrink from giving expression to their feelings before you i ought to have guessed the truth from the timidity with which she had repeatedly approached her sarcasm only bringing herself to utter it at last with an effort but i did not guess and an evil feeling took possession of me wait a bit i thought vii oh hush liza how can you talk about being like a book when it makes even me an outsider feel sick though i don t look at it as an outsider for indeed it touches me to the heart is it possible is it possible that you do not feel sick at being here yourself evidently habit does wonders god knows what habit can do with anyone can you seriously think that you will never grow old that you will always be good looking and that they will keep you here for ever and ever i say nothing of the loathsomeness of the life here though let me tell you this about it about your present life i mean here though you are young now attractive nice with soul and feeling yet you know as soon as i came to myself just now i felt at once sick at being here with you one can only come here when one is drunk but if you were anywhere else living as good people live i should perhaps be more than attracted by you should fall in love with you should be glad of a look from you let alone a word i should hang about your door should go down on my knees to you should look upon you as my betrothed and think it an honour to be allowed to i should not dare to have an impure thought about you but here you see i know that i have only to whistle and you have to come with me whether you like it or not i don t consult your wishes but you mine the lowest labourer hires himself as a workman but he doesn t make a slave of himself altogether besides he knows that he will be free again presently but when are you free only think what you are giving up here what is it you are making a slave of it is your soul together with your body you are selling your soul which you have no right to dispose of you give your love to be outraged by every drunkard love but that s everything you know it s a priceless diamond it s a maiden s treasure love why a man would be ready to give his soul to face death to gain that love but how much is your love worth now you are sold all of you body and soul and there is no need to strive for love when you can have everything without love and you know there is no greater insult to a girl than that do you understand to be sure i have heard that they comfort you poor fools they let you have lovers of your own here but you know that s simply a farce that s simply a sham it s just laughing at you and you are taken in by it why do you suppose he really loves you that lover of yours i don t believe it how can he love you when he knows you may be called away from him any minute he would be a low fellow if he did will he have a grain of respect for you what have you in common with him he laughs at you and robs you that is all his love amounts to you are lucky if he does not beat you very likely he does beat you too ask him if you have got one whether he will marry you he will laugh in your face if he doesn t spit in it or give you a blow though maybe he is not worth a bad halfpenny himself and for what have you ruined your life if you come to think of it for the coffee they give you to drink and the plentiful meals but with what object are they feeding you up an honest girl couldn t swallow the food for she would know what she was being fed for you are in debt here and of course you will always be in debt and you will go on in debt to the end till the visitors here begin to scorn you and that will soon happen don t rely upon your youth all that flies by express train here you know you will be kicked out and not simply kicked out long before that she ll begin nagging at you scolding you abusing you as though you had not sacrificed your health for her had not thrown away your youth and your soul for her benefit but as though you had ruined her beggared her robbed her and don t expect anyone to take your part the others your companions will attack you too win her favour for all are in slavery here and have lost all conscience and pity here long ago they have become utterly vile and nothing on earth is viler more loathsome and more insulting than their abuse and you are laying down everything here unconditionally youth and health and beauty and hope and at twenty two you will look like a woman of five and thirty and you will be lucky if you are not diseased pray to god for that no doubt you are thinking now that you have a gay time and no work to do yet there is no work harder or more dreadful in the world or ever has been one would think that the heart alone would be worn out with tears and you won t dare to say a word not half a word when they drive you away from here you will go away as though you were to blame you will change to another house then to a third then somewhere else till you come down at last to the haymarket there you will be beaten at every turn that is good manners there the visitors don t know how to be friendly without beating you you don t believe that it is so hateful there go and look for yourself some time you can see with your own eyes once one new year s day i saw a woman at a door they had turned her out as a joke to give her a taste of the frost because she had been crying so much and they shut the door behind her at nine o clock in the morning she was already quite drunk dishevelled half naked covered with bruises her face was powdered but she had a black eye blood was trickling from her nose and her teeth some cabman had just given her a drubbing she was sitting on the stone steps a salt fish of some sort was in her hand she was crying wailing something about her luck and beating with the fish on the steps and cabmen and drunken soldiers were crowding in the doorway taunting her you don t believe that you will ever be like that i should be sorry to believe it too but how do you know maybe ten years eight years ago that very woman with the salt fish came here fresh as a cherub innocent pure knowing no evil blushing at every word perhaps she was like you proud ready to take offence not like the others perhaps she looked like a queen and knew what happiness was in store for the man who should love her and whom she should love do you see how it ended and what if at that very minute when she was beating on the filthy steps with that fish drunken and dishevelled what if at that very minute she recalled the pure early days in her father s house when she used to go to school and the neighbour s son watched for her on the way declaring that he would love her as long as he lived that he would devote his life to her and when they vowed to love one another for ever and be married as soon as they were grown up no liza it would be happy for you if you were to die soon of consumption in some corner in some cellar like that woman just now in the hospital do you say you will be lucky if they take you but what if you are still of use to the madam here consumption is a queer disease it is not like fever the patient goes on hoping till the last minute and says he is all right he deludes himself and that just suits your madam don t doubt it that s how it is you have sold your soul and what is more you owe money so you daren t say a word but when you are dying all will abandon you all will turn away from you for then there will be nothing to get from you what s more they will reproach you for cumbering the place for being so long over dying however you beg you won t get a drink of water without abuse whenever are you going off you nasty hussy you won t let us sleep with your moaning you make the gentlemen sick that s true i have heard such things said myself they will thrust you dying into the filthiest corner in the cellar in the damp and darkness what will your thoughts be lying there alone when you die strange hands will lay you out with grumbling and impatience no one will bless you no one will sigh for you they only want to get rid of you as soon as may be they will buy a coffin take you to the grave as they did that poor woman today and celebrate your memory at the tavern in the grave sleet filth wet snow no need to put themselves out for you let her down vanuha it s just like her luck even here she is head foremost the hussy shorten the cord you rascal it s all right as it is all right is it why she s on her side she was a fellow creature after all but never mind throw the earth on her and they won t care to waste much time quarrelling over you they will scatter the wet blue clay as quick as they can and go off to the tavern and there your memory on earth will end other women have children to go to their graves fathers husbands while for you neither tear nor sigh nor remembrance no one in the whole world will ever come to you your name will vanish from the face of the earth as though you had never existed never been born at all nothing but filth and mud however you knock at your coffin lid at night when the dead arise however you cry let me out kind people to live in the light of day my life was no life at all my life has been thrown away like a dish clout it was drunk away in the tavern at the haymarket let me out kind people to live in the world again and i worked myself up to such a pitch that i began to have a lump in my throat myself and and all at once i stopped sat up in dismay and bending over apprehensively began to listen with a beating heart i had reason to be troubled i had felt for some time that i was turning her soul upside down and rending her heart and and the more i was convinced of it the more eagerly i desired to gain my object as quickly and as effectually as possible it was the exercise of my skill that carried me away yet it was not merely sport i knew i was speaking stiffly artificially even bookishly in fact i could not speak except like a book but that did not trouble me i knew i felt that i should be understood and that this very bookishness might be an assistance but now having attained my effect i was suddenly panic stricken never before had i witnessed such despair she was lying on her face thrusting her face into the pillow and clutching it in both hands her heart was being torn her youthful body was shuddering all over as though in convulsions suppressed sobs rent her bosom and suddenly burst out in weeping and wailing then she pressed closer into the pillow she did not want anyone here not a living soul to know of her anguish and her tears she bit the pillow bit her hand till it bled i saw that afterwards or thrusting her fingers into her dishevelled hair seemed rigid with the effort of restraint holding her breath and clenching her teeth i began saying something begging her to calm herself but felt that i did not dare and all at once in a sort of cold shiver almost in terror began fumbling in the dark trying hurriedly to get dressed to go it was dark though i tried my best i could not finish dressing quickly suddenly i felt a box of matches and a candlestick with a whole candle in it as soon as the room was lighted up liza sprang up sat up in bed and with a contorted face with a half insane smile looked at me almost senselessly i sat down beside her and took her hands she came to herself made an impulsive movement towards me would have caught hold of me but did not dare and slowly bowed her head before me liza my dear i was wrong forgive me my dear i began but she squeezed my hand in her fingers so tightly that i felt i was saying the wrong thing and stopped this is my address liza come to me i will come she answered resolutely her head still bowed but now i am going good bye till we meet again i got up she too stood up and suddenly flushed all over gave a shudder snatched up a shawl that was lying on a chair and muffled herself in it to her chin as she did this she gave another sickly smile blushed and looked at me strangely i felt wretched i was in haste to get away to disappear wait a minute she said suddenly in the passage just at the doorway stopping me with her hand on my overcoat she put down the candle in hot haste and ran off evidently she had thought of something or wanted to show me something as she ran away she flushed her eyes shone and there was a smile on her lips what was the meaning of it against my will i waited she came back a minute later with an expression that seemed to ask forgiveness for something in fact it was not the same face not the same look as the evening before sullen mistrustful and obstinate her eyes now were imploring soft and at the same time trustful caressing timid the expression with which children look at people they are very fond of of whom they are asking a favour her eyes were a light hazel they were lovely eyes full of life and capable of expressing love as well as sullen hatred making no explanation as though i as a sort of higher being must understand everything without explanations she held out a piece of paper to me her whole face was positively beaming at that instant with naive almost childish triumph i unfolded it it was a letter to her from a medical student or someone of that sort a very high flown and flowery but extremely respectful love letter i don t recall the words now but i remember well that through the high flown phrases there was apparent a genuine feeling which cannot be feigned when i had finished reading it i met her glowing questioning and childishly impatient eyes fixed upon me she fastened her eyes upon my face and waited impatiently for what i should say in a few words hurriedly but with a sort of joy and pride she explained to me that she had been to a dance somewhere in a private house a family of very nice people who knew nothing absolutely nothing for she had only come here so lately and it had all happened and she hadn t made up her mind to stay and was certainly going away as soon as she had paid her debt and at that party there had been the student who had danced with her all the evening he had talked to her and it turned out that he had known her in old days at riga when he was a child they had played together but a very long time ago and he knew her parents but about this he knew nothing nothing whatever and had no suspicion and the day after the dance three days ago he had sent her that letter through the friend with whom she had gone to the party and well that was all she dropped her shining eyes with a sort of bashfulness as she finished the poor girl was keeping that student s letter as a precious treasure and had run to fetch it her only treasure because she did not want me to go away without knowing that she too was honestly and genuinely loved that she too was addressed respectfully no doubt that letter was destined to lie in her box and lead to nothing but none the less i am certain that she would keep it all her life as a precious treasure as her pride and justification and now at such a minute she had thought of that letter and brought it with naive pride to raise herself in my eyes that i might see that i too might think well of her i said nothing pressed her hand and went out i so longed to get away i walked all the way home in spite of the fact that the melting snow was still falling in heavy flakes i was exhausted shattered in bewilderment but behind the bewilderment the truth was already gleaming the loathsome truth viii it was some time however before i consented to recognise that truth waking up in the morning after some hours of heavy leaden sleep and immediately realising all that had happened on the previous day i was positively amazed at my last night s sentimentality with liza at all those outcries of horror and pity to think of having such an attack of womanish hysteria pah i concluded and what did i thrust my address upon her for what if she comes let her come though it doesn t matter but obviously that was not now the chief and the most important matter i had to make haste and at all costs save my reputation in the eyes of zverkov and simonov as quickly as possible that was the chief business and i was so taken up that morning that i actually forgot all about liza first of all i had at once to repay what i had borrowed the day before from simonov i resolved on a desperate measure to borrow fifteen roubles straight off from anton antonitch as luck would have it he was in the best of humours that morning and gave it to me at once on the first asking i was so delighted at this that as i signed the iou with a swaggering air i told him casually that the night before i had been keeping it up with some friends at the hotel de paris we were giving a farewell party to a comrade in fact i might say a friend of my childhood and you know a desperate rake fearfully spoilt of course he belongs to a good family and has considerable means a brilliant career he is witty charming a regular lovelace you understand we drank an extra half dozen and and it went off all right all this was uttered very easily unconstrainedly and complacently on reaching home i promptly wrote to simonov to this hour i am lost in admiration when i recall the truly gentlemanly good humoured candid tone of my letter with tact and good breeding and above all entirely without superfluous words i blamed myself for all that had happened i defended myself if i really may be allowed to defend myself by alleging that being utterly unaccustomed to wine i had been intoxicated with the first glass which i said i had drunk before they arrived while i was waiting for them at the hotel de paris between five and six o clock i begged simonov s pardon especially i asked him to convey my explanations to all the others especially to zverkov whom i seemed to remember as though in a dream i had insulted i added that i would have called upon all of them myself but my head ached and besides i had not the face to i was particularly pleased with a certain lightness almost carelessness strictly within the bounds of politeness however which was apparent in my style and better than any possible arguments gave them at once to understand that i took rather an independent view of all that unpleasantness last night that i was by no means so utterly crushed as you my friends probably imagine but on the contrary looked upon it as a gentleman serenely respecting himself should look upon it on a young hero s past no censure is cast there is actually an aristocratic playfulness about it i thought admiringly as i read over the letter and it s all because i am an intellectual and cultivated man another man in my place would not have known how to extricate himself but here i have got out of it and am as jolly as ever again and all because i am a cultivated and educated man of our day and indeed perhaps everything was due to the wine yesterday h m no it was not the wine i did not drink anything at all between five and six when i was waiting for them i had lied to simonov i had lied shamelessly and indeed i wasn t ashamed now hang it all though the great thing was that i was rid of it i put six roubles in the letter sealed it up and asked apollon to take it to simonov when he learned that there was money in the letter apollon became more respectful and agreed to take it towards evening i went out for a walk my head was still aching and giddy after yesterday but as evening came on and the twilight grew denser my impressions and following them my thoughts grew more and more different and confused something was not dead within me in the depths of my heart and conscience it would not die and it showed itself in acute depression for the most part i jostled my way through the most crowded business streets along myeshtchansky street along sadovy street and in yusupov garden i always liked particularly sauntering along these streets in the dusk just when there were crowds of working people of all sorts going home from their daily work with faces looking cross with anxiety what i liked was just that cheap bustle that bare prose on this occasion the jostling of the streets irritated me more than ever i could not make out what was wrong with me i could not find the clue something seemed rising up continually in my soul painfully and refusing to be appeased i returned home completely upset it was just as though some crime were lying on my conscience the thought that liza was coming worried me continually it seemed queer to me that of all my recollections of yesterday this tormented me as it were especially as it were quite separately everything else i had quite succeeded in forgetting by the evening i dismissed it all and was still perfectly satisfied with my letter to simonov but on this point i was not satisfied at all it was as though i were worried only by liza what if she comes i thought incessantly well it doesn t matter let her come h m it s horrid that she should see for instance how i live yesterday i seemed such a hero to her while now h m it s horrid though that i have let myself go so the room looks like a beggar s and i brought myself to go out to dinner in such a suit and my american leather sofa with the stuffing sticking out and my dressing gown which will not cover me such tatters and she will see all this and she will see apollon that beast is certain to insult her he will fasten upon her in order to be rude to me and i of course shall be panic stricken as usual i shall begin bowing and scraping before her and pulling my dressing gown round me i shall begin smiling telling lies oh the beastliness and it isn t the beastliness of it that matters most there is something more important more loathsome viler yes viler and to put on that dishonest lying mask again when i reached that thought i fired up all at once why dishonest how dishonest i was speaking sincerely last night i remember there was real feeling in me too what i wanted was to excite an honourable feeling in her her crying was a good thing it will have a good effect yet i could not feel at ease all that evening even when i had come back home even after nine o clock when i calculated that liza could not possibly come still she haunted me and what was worse she came back to my mind always in the same position one moment out of all that had happened last night stood vividly before my imagination the moment when i struck a match and saw her pale distorted face with its look of torture and what a pitiful what an unnatural what a distorted smile she had at that moment but i did not know then that fifteen years later i should still in my imagination see liza always with the pitiful distorted inappropriate smile which was on her face at that minute next day i was ready again to look upon it all as nonsense due to over excited nerves and above all as exaggerated i was always conscious of that weak point of mine and sometimes very much afraid of it i exaggerate everything that is where i go wrong i repeated to myself every hour but however liza will very likely come all the same was the refrain with which all my reflections ended i was so uneasy that i sometimes flew into a fury she ll come she is certain to come i cried running about the room if not today she will come tomorrow she ll find me out the damnable romanticism of these pure hearts oh the vileness oh the silliness oh the stupidity of these wretched sentimental souls why how fail to understand how could one fail to understand but at this point i stopped short and in great confusion indeed and how few how few words i thought in passing were needed how little of the idyllic and affectedly bookishly artificially idyllic too had sufficed to turn a whole human life at once according to my will that s virginity to be sure freshness of soil at times a thought occurred to me to go to her to tell her all and beg her not to come to me but this thought stirred such wrath in me that i believed i should have crushed that damned liza if she had chanced to be near me at the time i should have insulted her have spat at her have turned her out have struck her one day passed however another and another she did not come and i began to grow calmer i felt particularly bold and cheerful after nine o clock i even sometimes began dreaming and rather sweetly i for instance became the salvation of liza simply through her coming to me and my talking to her i develop her educate her finally i notice that she loves me loves me passionately i pretend not to understand i don t know however why i pretend just for effect perhaps at last all confusion transfigured trembling and sobbing she flings herself at my feet and says that i am her saviour and that she loves me better than anything in the world i am amazed but liza i say can you imagine that i have not noticed your love i saw it all i divined it but i did not dare to approach you first because i had an influence over you and was afraid that you would force yourself from gratitude to respond to my love would try to rouse in your heart a feeling which was perhaps absent and i did not wish that because it would be tyranny it would be indelicate in short i launch off at that point into european inexplicably lofty subtleties a la george sand but now now you are mine you are my creation you are pure you are good you are my noble wife into my house come bold and free its rightful mistress there to be then we begin living together go abroad and so on and so on in fact in the end it seemed vulgar to me myself and i began putting out my tongue at myself besides they won t let her out the hussy i thought they don t let them go out very readily especially in the evening for some reason i fancied she would come in the evening and at seven o clock precisely though she did say she was not altogether a slave there yet and had certain rights so h m damn it all she will come she is sure to come it was a good thing in fact that apollon distracted my attention at that time by his rudeness he drove me beyond all patience he was the bane of my life the curse laid upon me by providence we had been squabbling continually for years and i hated him my god how i hated him i believe i had never hated anyone in my life as i hated him especially at some moments he was an elderly dignified man who worked part of his time as a tailor but for some unknown reason he despised me beyond all measure and looked down upon me insufferably though indeed he looked down upon everyone simply to glance at that flaxen smoothly brushed head at the tuft of hair he combed up on his forehead and oiled with sunflower oil at that dignified mouth compressed into the shape of the letter v made one feel one was confronting a man who never doubted of himself he was a pedant to the most extreme point the greatest pedant i had met on earth and with that had a vanity only befitting alexander of macedon he was in love with every button on his coat every nail on his fingers absolutely in love with them and he looked it in his behaviour to me he was a perfect tyrant he spoke very little to me and if he chanced to glance at me he gave me a firm majestically self confident and invariably ironical look that drove me sometimes to fury he did his work with the air of doing me the greatest favour though he did scarcely anything for me and did not indeed consider himself bound to do anything there could be no doubt that he looked upon me as the greatest fool on earth and that he did not get rid of me was simply that he could get wages from me every month he consented to do nothing for me for seven roubles a month many sins should be forgiven me for what i suffered from him my hatred reached such a point that sometimes his very step almost threw me into convulsions what i loathed particularly was his lisp his tongue must have been a little too long or something of that sort for he continually lisped and seemed to be very proud of it imagining that it greatly added to his dignity he spoke in a slow measured tone with his hands behind his back and his eyes fixed on the ground he maddened me particularly when he read aloud the psalms to himself behind his partition many a battle i waged over that reading but he was awfully fond of reading aloud in the evenings in a slow even sing song voice as though over the dead it is interesting that that is how he has ended he hires himself out to read the psalms over the dead and at the same time he kills rats and makes blacking but at that time i could not get rid of him it was as though he were chemically combined with my existence besides nothing would have induced him to consent to leave me i could not live in furnished lodgings my lodging was my private solitude my shell my cave in which i concealed myself from all mankind and apollon seemed to me for some reason an integral part of that flat and for seven years i could not turn him away to be two or three days behind with his wages for instance was impossible he would have made such a fuss i should not have known where to hide my head but i was so exasperated with everyone during those days that i made up my mind for some reason and with some object to punish apollon and not to pay him for a fortnight the wages that were owing him i had for a long time for the last two years been intending to do this simply in order to teach him not to give himself airs with me and to show him that if i liked i could withhold his wages i purposed to say nothing to him about it and was purposely silent indeed in order to score off his pride and force him to be the first to speak of his wages then i would take the seven roubles out of a drawer show him i have the money put aside on purpose but that i won t i won t i simply won t pay him his wages i won t just because that is what i wish because i am master and it is for me to decide because he has been disrespectful because he has been rude but if he were to ask respectfully i might be softened and give it to him otherwise he might wait another fortnight another three weeks a whole month but angry as i was yet he got the better of me i could not hold out for four days he began as he always did begin in such cases for there had been such cases already there had been attempts and it may be observed i knew all this beforehand i knew his nasty tactics by heart he would begin by fixing upon me an exceedingly severe stare keeping it up for several minutes at a time particularly on meeting me or seeing me out of the house if i held out and pretended not to notice these stares he would still in silence proceed to further tortures all at once a propos of nothing he would walk softly and smoothly into my room when i was pacing up and down or reading stand at the door one hand behind his back and one foot behind the other and fix upon me a stare more than severe utterly contemptuous if i suddenly asked him what he wanted he would make me no answer but continue staring at me persistently for some seconds then with a peculiar compression of his lips and a most significant air deliberately turn round and deliberately go back to his room two hours later he would come out again and again present himself before me in the same way it had happened that in my fury i did not even ask him what he wanted but simply raised my head sharply and imperiously and began staring back at him so we stared at one another for two minutes at last he turned with deliberation and dignity and went back again for two hours if i were still not brought to reason by all this but persisted in my revolt he would suddenly begin sighing while he looked at me long deep sighs as though measuring by them the depths of my moral degradation and of course it ended at last by his triumphing completely i raged and shouted but still was forced to do what he wanted this time the usual staring manoeuvres had scarcely begun when i lost my temper and flew at him in a fury i was irritated beyond endurance apart from him stay i cried in a frenzy as he was slowly and silently turning with one hand behind his back to go to his room stay come back come back i tell you and i must have bawled so unnaturally that he turned round and even looked at me with some wonder however he persisted in saying nothing and that infuriated me how dare you come and look at me like that without being sent for answer after looking at me calmly for half a minute he began turning round again stay i roared running up to him don t stir there answer now what did you come in to look at if you have any order to give me it s my duty to carry it out he answered after another silent pause with a slow measured lisp raising his eyebrows and calmly twisting his head from one side to another all this with exasperating composure that s not what i am asking you about you torturer i shouted turning crimson with anger i ll tell you why you came here myself you see i don t give you your wages you are so proud you don t want to bow down and ask for it and so you come to punish me with your stupid stares to worry me and you have no sus pic ion how stupid it is stupid stupid stupid stupid he would have turned round again without a word but i seized him listen i shouted to him here s the money do you see here it is i took it out of the table drawer here s the seven roubles complete but you are not going to have it you are not going to have it until you come respectfully with bowed head to beg my pardon do you hear that cannot be he answered with the most unnatural self confidence it shall be so i said i give you my word of honour it shall be and there s nothing for me to beg your pardon for he went on as though he had not noticed my exclamations at all why besides you called me a torturer for which i can summon you at the police station at any time for insulting behaviour go summon me i roared go at once this very minute this very second you are a torturer all the same a torturer but he merely looked at me then turned and regardless of my loud calls to him he walked to his room with an even step and without looking round if it had not been for liza nothing of this would have happened i decided inwardly then after waiting a minute i went myself behind his screen with a dignified and solemn air though my heart was beating slowly and violently apollon i said quietly and emphatically though i was breathless go at once without a minute s delay and fetch the police officer he had meanwhile settled himself at his table put on his spectacles and taken up some sewing but hearing my order he burst into a guffaw at once go this minute go on or else you can t imagine what will happen you are certainly out of your mind he observed without even raising his head lisping as deliberately as ever and threading his needle whoever heard of a man sending for the police against himself and as for being frightened you are upsetting yourself about nothing for nothing will come of it go i shrieked clutching him by the shoulder i felt i should strike him in a minute but i did not notice the door from the passage softly and slowly open at that instant and a figure come in stop short and begin staring at us in perplexity i glanced nearly swooned with shame and rushed back to my room there clutching at my hair with both hands i leaned my head against the wall and stood motionless in that position two minutes later i heard apollon s deliberate footsteps there is some woman asking for you he said looking at me with peculiar severity then he stood aside and let in liza he would not go away but stared at us sarcastically go away go away i commanded in desperation at that moment my clock began whirring and wheezing and struck seven ix into my house come bold and free its rightful mistress there to be i stood before her crushed crestfallen revoltingly confused and i believe i smiled as i did my utmost to wrap myself in the skirts of my ragged wadded dressing gown exactly as i had imagined the scene not long before in a fit of depression after standing over us for a couple of minutes apollon went away but that did not make me more at ease what made it worse was that she too was overwhelmed with confusion more so in fact than i should have expected at the sight of me of course sit down i said mechanically moving a chair up to the table and i sat down on the sofa she obediently sat down at once and gazed at me open eyed evidently expecting something from me at once this naivete of expectation drove me to fury but i restrained myself she ought to have tried not to notice as though everything had been as usual while instead of that she and i dimly felt that i should make her pay dearly for all this you have found me in a strange position liza i began stammering and knowing that this was the wrong way to begin no no don t imagine anything i cried seeing that she had suddenly flushed i am not ashamed of my poverty on the contrary i look with pride on my poverty i am poor but honourable one can be poor and honourable i muttered however would you like tea no she was beginning wait a minute i leapt up and ran to apollon i had to get out of the room somehow apollon i whispered in feverish haste flinging down before him the seven roubles which had remained all the time in my clenched fist here are your wages you see i give them to you but for that you must come to my rescue bring me tea and a dozen rusks from the restaurant if you won t go you ll make me a miserable man you don t know what this woman is this is everything you may be imagining something but you don t know what that woman is apollon who had already sat down to his work and put on his spectacles again at first glanced askance at the money without speaking or putting down his needle then without paying the slightest attention to me or making any answer he went on busying himself with his needle which he had not yet threaded i waited before him for three minutes with my arms crossed a la napoleon my temples were moist with sweat i was pale i felt it but thank god he must have been moved to pity looking at me having threaded his needle he deliberately got up from his seat deliberately moved back his chair deliberately took off his spectacles deliberately counted the money and finally asking me over his shoulder shall i get a whole portion deliberately walked out of the room as i was going back to liza the thought occurred to me on the way shouldn t i run away just as i was in my dressing gown no matter where and then let happen what would i sat down again she looked at me uneasily for some minutes we were silent i will kill him i shouted suddenly striking the table with my fist so that the ink spurted out of the inkstand what are you saying she cried starting i will kill him kill him i shrieked suddenly striking the table in absolute frenzy and at the same time fully understanding how stupid it was to be in such a frenzy you don t know liza what that torturer is to me he is my torturer he has gone now to fetch some rusks he and suddenly i burst into tears it was an hysterical attack how ashamed i felt in the midst of my sobs but still i could not restrain them she was frightened what is the matter what is wrong she cried fussing about me water give me water over there i muttered in a faint voice though i was inwardly conscious that i could have got on very well without water and without muttering in a faint voice but i was what is called putting it on to save appearances though the attack was a genuine one she gave me water looking at me in bewilderment at that moment apollon brought in the tea it suddenly seemed to me that this commonplace prosaic tea was horribly undignified and paltry after all that had happened and i blushed crimson liza looked at apollon with positive alarm he went out without a glance at either of us liza do you despise me i asked looking at her fixedly trembling with impatience to know what she was thinking she was confused and did not know what to answer drink your tea i said to her angrily i was angry with myself but of course it was she who would have to pay for it a horrible spite against her suddenly surged up in my heart i believe i could have killed her to revenge myself on her i swore inwardly not to say a word to her all the time she is the cause of it all i thought our silence lasted for five minutes the tea stood on the table we did not touch it i had got to the point of purposely refraining from beginning in order to embarrass her further it was awkward for her to begin alone several times she glanced at me with mournful perplexity i was obstinately silent i was of course myself the chief sufferer because i was fully conscious of the disgusting meanness of my spiteful stupidity and yet at the same time i could not restrain myself i want to get away from there altogether she began to break the silence in some way but poor girl that was just what she ought not to have spoken about at such a stupid moment to a man so stupid as i was my heart positively ached with pity for her tactless and unnecessary straightforwardness but something hideous at once stifled all compassion in me it even provoked me to greater venom i did not care what happened another five minutes passed perhaps i am in your way she began timidly hardly audibly and was getting up but as soon as i saw this first impulse of wounded dignity i positively trembled with spite and at once burst out why have you come to me tell me that please i began gasping for breath and regardless of logical connection in my words i longed to have it all out at once at one burst i did not even trouble how to begin why have you come answer answer i cried hardly knowing what i was doing i ll tell you my good girl why you have come you ve come because i talked sentimental stuff to you then so now you are soft as butter and longing for fine sentiments again so you may as well know that i was laughing at you then and i am laughing at you now why are you shuddering yes i was laughing at you i had been insulted just before at dinner by the fellows who came that evening before me i came to you meaning to thrash one of them an officer but i didn t succeed i didn t find him i had to avenge the insult on someone to get back my own again you turned up i vented my spleen on you and laughed at you i had been humiliated so i wanted to humiliate i had been treated like a rag so i wanted to show my power that s what it was and you imagined i had come there on purpose to save you yes you imagined that you imagined that i knew that she would perhaps be muddled and not take it all in exactly but i knew too that she would grasp the gist of it very well indeed and so indeed she did she turned white as a handkerchief tried to say something and her lips worked painfully but she sank on a chair as though she had been felled by an axe and all the time afterwards she listened to me with her lips parted and her eyes wide open shuddering with awful terror the cynicism the cynicism of my words overwhelmed her save you i went on jumping up from my chair and running up and down the room before her save you from what but perhaps i am worse than you myself why didn t you throw it in my teeth when i was giving you that sermon but what did you come here yourself for was it to read us a sermon power power was what i wanted then sport was what i wanted i wanted to wring out your tears your humiliation your hysteria that was what i wanted then of course i couldn t keep it up then because i am a wretched creature i was frightened and the devil knows why gave you my address in my folly afterwards before i got home i was cursing and swearing at you because of that address i hated you already because of the lies i had told you because i only like playing with words only dreaming but do you know what i really want is that you should all go to hell that is what i want i want peace yes i d sell the whole world for a farthing straight off so long as i was left in peace is the world to go to pot or am i to go without my tea i say that the world may go to pot for me so long as i always get my tea did you know that or not well anyway i know that i am a blackguard a scoundrel an egoist a sluggard here i have been shuddering for the last three days at the thought of your coming and do you know what has worried me particularly for these three days that i posed as such a hero to you and now you would see me in a wretched torn dressing gown beggarly loathsome i told you just now that i was not ashamed of my poverty so you may as well know that i am ashamed of it i am more ashamed of it than of anything more afraid of it than of being found out if i were a thief because i am as vain as though i had been skinned and the very air blowing on me hurt surely by now you must realise that i shall never forgive you for having found me in this wretched dressing gown just as i was flying at apollon like a spiteful cur the saviour the former hero was flying like a mangy unkempt sheep dog at his lackey and the lackey was jeering at him and i shall never forgive you for the tears i could not help shedding before you just now like some silly woman put to shame and for what i am confessing to you now i shall never forgive you either yes you must answer for it all because you turned up like this because i am a blackguard because i am the nastiest stupidest absurdest and most envious of all the worms on earth who are not a bit better than i am but the devil knows why are never put to confusion while i shall always be insulted by every louse that is my doom and what is it to me that you don t understand a word of this and what do i care what do i care about you and whether you go to ruin there or not do you understand how i shall hate you now after saying this for having been here and listening why it s not once in a lifetime a man speaks out like this and then it is in hysterics what more do you want why do you still stand confronting me after all this why are you worrying me why don t you go but at this point a strange thing happened i was so accustomed to think and imagine everything from books and to picture everything in the world to myself just as i had made it up in my dreams beforehand that i could not all at once take in this strange circumstance what happened was this liza insulted and crushed by me understood a great deal more than i imagined she understood from all this what a woman understands first of all if she feels genuine love that is that i was myself unhappy the frightened and wounded expression on her face was followed first by a look of sorrowful perplexity when i began calling myself a scoundrel and a blackguard and my tears flowed the tirade was accompanied throughout by tears her whole face worked convulsively she was on the point of getting up and stopping me when i finished she took no notice of my shouting why are you here why don t you go away but realised only that it must have been very bitter to me to say all this besides she was so crushed poor girl she considered herself infinitely beneath me how could she feel anger or resentment she suddenly leapt up from her chair with an irresistible impulse and held out her hands yearning towards me though still timid and not daring to stir at this point there was a revulsion in my heart too then she suddenly rushed to me threw her arms round me and burst into tears i too could not restrain myself and sobbed as i never had before they won t let me i can t be good i managed to articulate then i went to the sofa fell on it face downwards and sobbed on it for a quarter of an hour in genuine hysterics she came close to me put her arms round me and stayed motionless in that position but the trouble was that the hysterics could not go on for ever and i am writing the loathsome truth lying face downwards on the sofa with my face thrust into my nasty leather pillow i began by degrees to be aware of a far away involuntary but irresistible feeling that it would be awkward now for me to raise my head and look liza straight in the face why was i ashamed i don t know but i was ashamed the thought too came into my overwrought brain that our parts now were completely changed that she was now the heroine while i was just a crushed and humiliated creature as she had been before me that night four days before and all this came into my mind during the minutes i was lying on my face on the sofa my god surely i was not envious of her then i don t know to this day i cannot decide and at the time of course i was still less able to understand what i was feeling than now i cannot get on without domineering and tyrannising over someone but there is no explaining anything by reasoning and so it is useless to reason i conquered myself however and raised my head i had to do so sooner or later and i am convinced to this day that it was just because i was ashamed to look at her that another feeling was suddenly kindled and flamed up in my heart a feeling of mastery and possession my eyes gleamed with passion and i gripped her hands tightly how i hated her and how i was drawn to her at that minute the one feeling intensified the other it was almost like an act of vengeance at first there was a look of amazement even of terror on her face but only for one instant she warmly and rapturously embraced me x a quarter of an hour later i was rushing up and down the room in frenzied impatience from minute to minute i went up to the screen and peeped through the crack at liza she was sitting on the ground with her head leaning against the bed and must have been crying but she did not go away and that irritated me this time she understood it all i had insulted her finally but there s no need to describe it she realised that my outburst of passion had been simply revenge a fresh humiliation and that to my earlier almost causeless hatred was added now a personal hatred born of envy though i do not maintain positively that she understood all this distinctly but she certainly did fully understand that i was a despicable man and what was worse incapable of loving her i know i shall be told that this is incredible but it is incredible to be as spiteful and stupid as i was it may be added that it was strange i should not love her or at any rate appreciate her love why is it strange in the first place by then i was incapable of love for i repeat with me loving meant tyrannising and showing my moral superiority i have never in my life been able to imagine any other sort of love and have nowadays come to the point of sometimes thinking that love really consists in the right freely given by the beloved object to tyrannise over her even in my underground dreams i did not imagine love except as a struggle i began it always with hatred and ended it with moral subjugation and afterwards i never knew what to do with the subjugated object and what is there to wonder at in that since i had succeeded in so corrupting myself since i was so out of touch with real life as to have actually thought of reproaching her and putting her to shame for having come to me to hear fine sentiments and did not even guess that she had come not to hear fine sentiments but to love me because to a woman all reformation all salvation from any sort of ruin and all moral renewal is included in love and can only show itself in that form i did not hate her so much however when i was running about the room and peeping through the crack in the screen i was only insufferably oppressed by her being here i wanted her to disappear i wanted peace to be left alone in my underground world real life oppressed me with its novelty so much that i could hardly breathe but several minutes passed and she still remained without stirring as though she were unconscious i had the shamelessness to tap softly at the screen as though to remind her she started sprang up and flew to seek her kerchief her hat her coat as though making her escape from me two minutes later she came from behind the screen and looked with heavy eyes at me i gave a spiteful grin which was forced however to keep up appearances and i turned away from her eyes good bye she said going towards the door i ran up to her seized her hand opened it thrust something in it and closed it again then i turned at once and dashed away in haste to the other corner of the room to avoid seeing anyway i did mean a moment since to tell a lie to write that i did this accidentally not knowing what i was doing through foolishness through losing my head but i don t want to lie and so i will say straight out that i opened her hand and put the money in it from spite it came into my head to do this while i was running up and down the room and she was sitting behind the screen but this i can say for certain though i did that cruel thing purposely it was not an impulse from the heart but came from my evil brain this cruelty was so affected so purposely made up so completely a product of the brain of books that i could not even keep it up a minute first i dashed away to avoid seeing her and then in shame and despair rushed after liza i opened the door in the passage and began listening liza liza i cried on the stairs but in a low voice not boldly there was no answer but i fancied i heard her footsteps lower down on the stairs liza i cried more loudly no answer but at that minute i heard the stiff outer glass door open heavily with a creak and slam violently the sound echoed up the stairs she had gone i went back to my room in hesitation i felt horribly oppressed i stood still at the table beside the chair on which she had sat and looked aimlessly before me a minute passed suddenly i started straight before me on the table i saw in short i saw a crumpled blue five rouble note the one i had thrust into her hand a minute before it was the same note it could be no other there was no other in the flat so she had managed to fling it from her hand on the table at the moment when i had dashed into the further corner well i might have expected that she would do that might i have expected it no i was such an egoist i was so lacking in respect for my fellow creatures that i could not even imagine she would do so i could not endure it a minute later i flew like a madman to dress flinging on what i could at random and ran headlong after her she could not have got two hundred paces away when i ran out into the street it was a still night and the snow was coming down in masses and falling almost perpendicularly covering the pavement and the empty street as though with a pillow there was no one in the street no sound was to be heard the street lamps gave a disconsolate and useless glimmer i ran two hundred paces to the cross roads and stopped short where had she gone and why was i running after her why to fall down before her to sob with remorse to kiss her feet to entreat her forgiveness i longed for that my whole breast was being rent to pieces and never never shall i recall that minute with indifference but what for i thought should i not begin to hate her perhaps even tomorrow just because i had kissed her feet today should i give her happiness had i not recognised that day for the hundredth time what i was worth should i not torture her i stood in the snow gazing into the troubled darkness and pondered this and will it not be better i mused fantastically afterwards at home stifling the living pang of my heart with fantastic dreams will it not be better that she should keep the resentment of the insult for ever resentment why it is purification it is a most stinging and painful consciousness tomorrow i should have defiled her soul and have exhausted her heart while now the feeling of insult will never die in her heart and however loathsome the filth awaiting her the feeling of insult will elevate and purify her by hatred h m perhaps too by forgiveness will all that make things easier for her though and indeed i will ask on my own account here an idle question which is better cheap happiness or exalted sufferings well which is better so i dreamed as i sat at home that evening almost dead with the pain in my soul never had i endured such suffering and remorse yet could there have been the faintest doubt when i ran out from my lodging that i should turn back half way i never met liza again and i have heard nothing of her i will add too that i remained for a long time afterwards pleased with the phrase about the benefit from resentment and hatred in spite of the fact that i almost fell ill from misery even now so many years later all this is somehow a very evil memory i have many evil memories now but hadn t i better end my notes here i believe i made a mistake in beginning to write them anyway i have felt ashamed all the time i ve been writing this story so it s hardly literature so much as a corrective punishment why to tell long stories showing how i have spoiled my life through morally rotting in my corner through lack of fitting environment through divorce from real life and rankling spite in my underground world would certainly not be interesting a novel needs a hero and all the traits for an anti hero are expressly gathered together here and what matters most it all produces an unpleasant impression for we are all divorced from life we are all cripples every one of us more or less we are so divorced from it that we feel at once a sort of loathing for real life and so cannot bear to be reminded of it why we have come almost to looking upon real life as an effort almost as hard work and we are all privately agreed that it is better in books and why do we fuss and fume sometimes why are we perverse and ask for something else we don t know what ourselves it would be the worse for us if our petulant prayers were answered come try give any one of us for instance a little more independence untie our hands widen the spheres of our activity relax the control and we yes i assure you we should be begging to be under control again at once i know that you will very likely be angry with me for that and will begin shouting and stamping speak for yourself you will say and for your miseries in your underground holes and don t dare to say all of us excuse me gentlemen i am not justifying myself with that all of us as for what concerns me in particular i have only in my life carried to an extreme what you have not dared to carry halfway and what s more you have taken your cowardice for good sense and have found comfort in deceiving yourselves so that perhaps after all there is more life in me than in you look into it more carefully why we don t even know what living means now what it is and what it is called leave us alone without books and we shall be lost and in confusion at once we shall not know what to join on to what to cling to what to love and what to hate what to respect and what to despise we are oppressed at being men men with a real individual body and blood we are ashamed of it we think it a disgrace and try to contrive to be some sort of impossible generalised man we are stillborn and for generations past have been begotten not by living fathers and that suits us better and better we are developing a taste for it soon we shall contrive to be born somehow from an idea but enough i don t want to write more from underground the notes of this paradoxalist do not end here however he could not refrain from going on with them but it seems to us that we may stop here end of project gutenberg s notes from the underground by feodor dostoevsky end of this project gutenberg ebook notes from the underground this file should be named txt or zip this and all associated files of various formats will be found in http www gutenberg org produced by judith boss html version by al haines updated editions will replace the previous one the old editions will be renamed creating the works from public domain print editions means that no one owns a united states copyright in these works so the foundation and you can copy and distribute it in the united states without permission and without paying copyright royalties special rules set forth in the general terms of use part of this license apply to copying and distributing project gutenberg tm electronic works to protect the project gutenberg tm concept and trademark project gutenberg is a registered trademark and may not be used if you charge for the ebooks unless you receive specific permission if you do not charge anything for copies of this ebook complying with the rules is very easy you may use this ebook for nearly any purpose such as creation of derivative works reports performances and research they may be modified and printed and given away you may do practically anything with public domain ebooks redistribution is subject to the trademark license especially commercial redistribution start full license the full project gutenberg license please read this before you distribute or use this work to protect the project gutenberg tm mission of promoting the free distribution of electronic works by using or distributing this work or any other work associated in any way with the phrase project gutenberg you agree to comply with all the terms of the full project gutenberg tm license available with this file or online at http gutenberg net license section general terms of use and redistributing project gutenberg tm electronic works a by reading or using any part of this project gutenberg tm electronic work you indicate that you have read understand agree to and accept all the terms of this license and intellectual property trademark copyright agreement if you do not agree to abide by all the terms of this agreement you must cease using and return or destroy all copies of project gutenberg tm electronic works in your possession if you paid a fee for obtaining a copy of or access to a project gutenberg tm electronic work and you do not agree to be bound by the terms of this agreement you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph e b project gutenberg is a registered trademark it may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement there are a few things that you can do with most project gutenberg tm electronic works even without complying with the full terms of this agreement see paragraph c below there are a lot of things you can do with project gutenberg tm electronic works if you follow the terms of this agreement and help preserve free future access to project gutenberg tm electronic works see paragraph e below c the project gutenberg literary archive foundation the foundation or pglaf owns a compilation copyright in the collection of project gutenberg tm electronic works nearly all the individual works in the collection are in the public domain in the united states if an individual work is in the public domain in the united states and you are located in the united states we do not claim a right to prevent you from copying distributing performing displaying or creating derivative works based on the work as long as all references to project gutenberg are removed of course we hope that you will support the project gutenberg tm mission of promoting free access to electronic works by freely sharing project gutenberg tm works in compliance with the terms of this agreement for keeping the project gutenberg tm name associated with the work you can easily comply with the terms of this agreement by keeping this work in the same format with its attached full project gutenberg tm license when you share it without charge with others d the copyright laws of the place where you are located also govern what you can do with this work copyright laws in most countries are in a constant state of change if you are outside the united states check the laws of your country in addition to the terms of this agreement before downloading copying displaying performing distributing or creating derivative works based on this work or any other project gutenberg tm work the foundation makes no representations concerning the copyright status of any work in any country outside the united states e unless you have removed all references to project gutenberg e the following sentence with active links to or other immediate access to the full project gutenberg tm license must appear prominently whenever any copy of a project gutenberg tm work any work on which the phrase project gutenberg appears or with which the phrase project gutenberg is associated is accessed displayed performed viewed copied or distributed this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg net e if an individual project gutenberg tm electronic work is derived from the public domain does not contain a notice indicating that it is posted with permission of the copyright holder the work can be copied and distributed to anyone in the united states without paying any fees or charges if you are redistributing or providing access to a work with the phrase project gutenberg associated with or appearing on the work you must comply either with the requirements of paragraphs e through e or obtain permission for the use of the work and the project gutenberg tm trademark as set forth in paragraphs e or e e if an individual project gutenberg tm electronic work is posted with the permission of the copyright holder your use and distribution must comply with both paragraphs e through e and any additional terms imposed by the copyright holder additional terms will be linked to the project gutenberg tm license for all works posted with the permission of the copyright holder found at the beginning of this work e do not unlink or detach or remove the full project gutenberg tm license terms from this work or any files containing a part of this work or any other work associated with project gutenberg tm e do not copy display perform distribute or redistribute this electronic work or any part of this electronic work without prominently displaying the sentence set forth in paragraph e with active links or immediate access to the full terms of the project gutenberg tm license e you may convert to and distribute this work in any binary compressed marked up nonproprietary or proprietary form including any word processing or hypertext form however if you provide access to or distribute copies of a project gutenberg tm work in a format other than plain vanilla ascii or other format used in the official version posted on the official project gutenberg tm web site www gutenberg net you must at no additional cost fee or expense to the user provide a copy a means of exporting a copy or a means of obtaining a copy upon request of the work in its original plain vanilla ascii or other form any alternate format must include the full project gutenberg tm license as specified in paragraph e e do not charge a fee for access to viewing displaying performing copying or distributing any project gutenberg tm works unless you comply with paragraph e or e e you may charge a reasonable fee for copies of or providing access to or distributing project gutenberg tm electronic works provided that you pay a royalty fee of of the gross profits you derive from the use of project gutenberg tm works calculated using the method you already use to calculate your applicable taxes the fee is owed to the owner of the project gutenberg tm trademark but he has agreed to donate royalties under this paragraph to the project gutenberg literary archive foundation royalty payments must be paid within days following each date on which you prepare or are legally required to prepare your periodic tax returns royalty payments should be clearly marked as such and sent to the project gutenberg literary archive foundation at the address specified in section information about donations to the project gutenberg literary archive foundation you provide a full refund of any money paid by a user who notifies you in writing or by e mail within days of receipt that s he does not agree to the terms of the full project gutenberg tm license you must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of project gutenberg tm works you provide in accordance with paragraph f a full refund of any money paid for a work or a replacement copy if a defect in the electronic work is discovered and reported to you within days of receipt of the work you comply with all other terms of this agreement for free distribution of project gutenberg tm works e if you wish to charge a fee or distribute a project gutenberg tm electronic work or group of works on different terms than are set forth in this agreement you must obtain permission in writing from both the project gutenberg literary archive foundation and michael hart the owner of the project gutenberg tm trademark contact the foundation as set forth in section below f f project gutenberg volunteers and employees expend considerable effort to identify do copyright research on transcribe and proofread public domain works in creating the project gutenberg tm collection despite these efforts project gutenberg tm electronic works and the medium on which they may be stored may contain defects such as but not limited to incomplete inaccurate or corrupt data transcription errors a copyright or other intellectual property infringement a defective or damaged disk or other medium a computer virus or computer codes that damage or cannot be read by your equipment f limited warranty disclaimer of damages except for the right of replacement or refund described in paragraph f the project gutenberg literary archive foundation the owner of the project gutenberg tm trademark and any other party distributing a project gutenberg tm electronic work under this agreement disclaim all liability to you for damages costs and expenses including legal fees you agree that you have no remedies for negligence strict liability breach of warranty or breach of contract except those provided in paragraph f you agree that the foundation the trademark owner and any distributor under this agreement will not be liable to you for actual direct indirect consequential punitive or incidental damages even if you give notice of the possibility of such damage f limited right of replacement or refund if you discover a defect in this electronic work within days of receiving it you can receive a refund of the money if any you paid for it by sending a written explanation to the person you received the work from if you received the work on a physical medium you must return the medium with your written explanation the person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund if you received the work electronically the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund if the second copy is also defective you may demand a refund in writing without further opportunities to fix the problem f except for the limited right of replacement or refund set forth in paragraph f this work is provided to you as is with no other warranties of any kind express or implied including but not limited to warranties of merchantibility or fitness for any purpose f some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages if any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law the invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions f indemnity you agree to indemnify and hold the foundation the trademark owner any agent or employee of the foundation anyone providing copies of project gutenberg tm electronic works in accordance with this agreement and any volunteers associated with the production promotion and distribution of project gutenberg tm electronic works harmless from all liability costs and expenses including legal fees that arise directly or indirectly from any of the following which you do or cause to occur a distribution of this or any project gutenberg tm work b alteration modification or additions or deletions to any project gutenberg tm work and c any defect you cause section information about the mission of project gutenberg tm project gutenberg tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete old middle aged and new computers it exists because of the efforts of hundreds of volunteers and donations from people in all walks of life volunteers and financial support to provide volunteers with the assistance they need is critical to reaching project gutenberg tm s goals and ensuring that the project gutenberg tm collection will remain freely available for generations to come in the project gutenberg literary archive foundation was created to provide a secure and permanent future for project gutenberg tm and future generations to learn more about the project gutenberg literary archive foundation and how your efforts and donations can help see sections and and the foundation web page at http www pglaf org section information about the project gutenberg literary archive foundation the project gutenberg literary archive foundation is a non profit c educational corporation organized under the laws of the state of mississippi and granted tax exempt status by the internal revenue service the foundation s ein or federal tax identification number is its c letter is posted at http pglaf org fundraising contributions to the project gutenberg literary archive foundation are tax deductible to the full extent permitted by u s federal laws and your state s laws the foundation s principal office is located at melan dr s fairbanks ak but its volunteers and employees are scattered throughout numerous locations its business office is located at north west salt lake city ut email business pglaf org email contact links and up to date contact information can be found at the foundation s web site and official page at http pglaf org for additional contact information dr gregory b newby chief executive and director gbnewby pglaf org section information about donations to the project gutenberg literary archive foundation project gutenberg tm depends upon and cannot survive without wide spread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine readable form accessible by the widest array of equipment including outdated equipment many small donations to are particularly important to maintaining tax exempt status with the irs the foundation is committed to complying with the laws regulating charities and charitable donations in all states of the united states compliance requirements are not uniform and it takes a considerable effort much paperwork and many fees to meet and keep up with these requirements we do not solicit donations in locations where we have not received written confirmation of compliance to send donations or determine the status of compliance for any particular state visit http pglaf org while we cannot and do not solicit contributions from states where we have not met the solicitation requirements we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate international donations are gratefully accepted but we cannot make any statements concerning tax treatment of donations received from outside the united states u s laws alone swamp our small staff please check the project gutenberg web pages for current donation methods and addresses donations are accepted in a number of other ways including including checks online payments and credit card donations to donate please visit http pglaf org donate section general information about project gutenberg tm electronic works professor michael s hart is the originator of the project gutenberg tm concept of a library of electronic works that could be freely shared with anyone for thirty years he produced and distributed project gutenberg tm ebooks with only a loose network of volunteer support project gutenberg tm ebooks are often created from several printed editions all of which are confirmed as public domain in the u s unless a copyright notice is included thus we do not necessarily keep ebooks in compliance with any particular paper edition most people start at our web site which has the main pg search facility http www gutenberg net this web site includes information about project gutenberg tm including how to make donations to the project gutenberg literary archive foundation how to help produce our new ebooks and how to subscribe to our email newsletter to hear about new ebooks . Split text into sentences . We split (tokenize) the text into sentences using NLTK sent_tokenize() method. We will evaluate the importance of each of sentences, then decide if we should each include in our summary. . # split (tokenize) the sentences sentences = nltk.sent_tokenize(text) print(sentences) . [&#34; ufeffProject Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.&#34;, &#39;You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: Notes from the Underground Author: Feodor Dostoevsky Posting Date: September 13, 2008 [EBook #600] Release Date: July, 1996 Language: English *** START OF THIS PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Produced by Judith Boss.&#39;, &#39;HTML version by Al Haines.&#39;, &#39;Notes from the Underground FYODOR DOSTOYEVSKY PART I Underground* *The author of the diary and the diary itself are, of course, imaginary.&#39;, &#39;Nevertheless it is clear that such persons as the writer of these notes not only may, but positively must, exist in our society, when we consider the circumstances in the midst of which our society is formed.&#39;, &#39;I have tried to expose to the view of the public more distinctly than is commonly done, one of the characters of the recent past.&#39;, &#39;He is one of the representatives of a generation still living.&#39;, &#39;In this fragment, entitled &#34;Underground,&#34; this person introduces himself and his views, and, as it were, tries to explain the causes owing to which he has made his appearance and was bound to make his appearance in our midst.&#39;, &#34;In the second fragment there are added the actual notes of this person concerning certain events in his life.--AUTHOR&#39;S NOTE.&#34;, &#39;I I am a sick man....&#39;, &#39;I am a spiteful man.&#39;, &#39;I am an unattractive man.&#39;, &#39;I believe my liver is diseased.&#39;, &#39;However, I know nothing at all about my disease, and do not know for certain what ails me.&#39;, &#34;I don&#39;t consult a doctor for it, and never have, though I have a respect for medicine and doctors.&#34;, &#39;Besides, I am extremely superstitious, sufficiently so to respect medicine, anyway (I am well-educated enough not to be superstitious, but I am superstitious).&#39;, &#39;No, I refuse to consult a doctor from spite.&#39;, &#39;That you probably will not understand.&#39;, &#39;Well, I understand it, though.&#39;, &#39;Of course, I can &#39;t explain who it is precisely that I am mortifying in this case by my spite: I am perfectly well aware that I cannot &#34;pay out&#34; the doctors by not consulting them; I know better than anyone that by all this I am only injuring myself and no one else.&#39;, &#34;But still, if I don&#39;t consult a doctor it is from spite.&#34;, &#39;My liver is bad, well--let it get worse!&#39;, &#39;I have been going on like that for a long time--twenty years.&#39;, &#39;Now I am forty.&#39;, &#39;I used to be in the government service, but am no longer.&#39;, &#39;I was a spiteful official.&#39;, &#39;I was rude and took pleasure in being so.&#39;, &#39;I did not take bribes, you see, so I was bound to find a recompense in that, at least.&#39;, &#39;(A poor jest, but I will not scratch it out.&#39;, &#39;I wrote it thinking it would sound very witty; but now that I have seen myself that I only wanted to show off in a despicable way, I will not scratch it out on purpose!)&#39;, &#39;When petitioners used to come for information to the table at which I sat, I used to grind my teeth at them, and felt intense enjoyment when I succeeded in making anybody unhappy.&#39;, &#39;I almost did succeed.&#39;, &#39;For the most part they were all timid people--of course, they were petitioners.&#39;, &#39;But of the uppish ones there was one officer in particular I could not endure.&#39;, &#39;He simply would not be humble, and clanked his sword in a disgusting way.&#39;, &#39;I carried on a feud with him for eighteen months over that sword.&#39;, &#39;At last I got the better of him.&#39;, &#39;He left off clanking it.&#39;, &#39;That happened in my youth, though.&#39;, &#39;But do you know, gentlemen, what was the chief point about my spite?&#39;, &#39;Why, the whole point, the real sting of it lay in the fact that continually, even in the moment of the acutest spleen, I was inwardly conscious with shame that I was not only not a spiteful but not even an embittered man, that I was simply scaring sparrows at random and amusing myself by it.&#39;, &#39;I might foam at the mouth, but bring me a doll to play with, give me a cup of tea with sugar in it, and maybe I should be appeased.&#39;, &#39;I might even be genuinely touched, though probably I should grind my teeth at myself afterwards and lie awake at night with shame for months after.&#39;, &#39;That was my way.&#39;, &#39;I was lying when I said just now that I was a spiteful official.&#39;, &#39;I was lying from spite.&#39;, &#39;I was simply amusing myself with the petitioners and with the officer, and in reality I never could become spiteful.&#39;, &#39;I was conscious every moment in myself of many, very many elements absolutely opposite to that.&#39;, &#39;I felt them positively swarming in me, these opposite elements.&#39;, &#39;I knew that they had been swarming in me all my life and craving some outlet from me, but I would not let them, would not let them, purposely would not let them come out.&#39;, &#39;They tormented me till I was ashamed: they drove me to convulsions and--sickened me, at last, how they sickened me!&#39;, &#39;Now, are not you fancying, gentlemen, that I am expressing remorse for something now, that I am asking your forgiveness for something?&#39;, &#39;I am sure you are fancying that ...&#39;, &#39;However, I assure you I do not care if you are....&#39;, &#39;It was not only that I could not become spiteful, I did not know how to become anything; neither spiteful nor kind, neither a rascal nor an honest man, neither a hero nor an insect.&#39;, &#39;Now, I am living out my life in my corner, taunting myself with the spiteful and useless consolation that an intelligent man cannot become anything seriously, and it is only the fool who becomes anything.&#39;, &#39;Yes, a man in the nineteenth century must and morally ought to be pre-eminently a characterless creature; a man of character, an active man is pre-eminently a limited creature.&#39;, &#39;That is my conviction of forty years.&#39;, &#39;I am forty years old now, and you know forty years is a whole lifetime; you know it is extreme old age.&#39;, &#39;To live longer than forty years is bad manners, is vulgar, immoral.&#39;, &#39;Who does live beyond forty?&#39;, &#39;Answer that, sincerely and honestly I will tell you who do: fools and worthless fellows.&#39;, &#39;I tell all old men that to their face, all these venerable old men, all these silver-haired and reverend seniors!&#39;, &#39;I tell the whole world that to its face!&#39;, &#39;I have a right to say so, for I shall go on living to sixty myself.&#39;, &#39;To seventy!&#39;, &#39;To eighty!&#39;, &#39;... Stay, let me take breath ... You imagine no doubt, gentlemen, that I want to amuse you.&#39;, &#39;You are mistaken in that, too.&#39;, &#39;I am by no means such a mirthful person as you imagine, or as you may imagine; however, irritated by all this babble (and I feel that you are irritated) you think fit to ask me who I am--then my answer is, I am a collegiate assessor.&#39;, &#39;I was in the service that I might have something to eat (and solely for that reason), and when last year a distant relation left me six thousand roubles in his will I immediately retired from the service and settled down in my corner.&#39;, &#39;I used to live in this corner before, but now I have settled down in it.&#39;, &#39;My room is a wretched, horrid one in the outskirts of the town.&#39;, &#39;My servant is an old country-woman, ill-natured from stupidity, and, moreover, there is always a nasty smell about her.&#39;, &#39;I am told that the Petersburg climate is bad for me, and that with my small means it is very expensive to live in Petersburg.&#39;, &#39;I know all that better than all these sage and experienced counsellors and monitors....&#39;, &#39;But I am remaining in Petersburg; I am not going away from Petersburg!&#39;, &#39;I am not going away because ... ech!&#39;, &#39;Why, it is absolutely no matter whether I am going away or not going away.&#39;, &#39;But what can a decent man speak of with most pleasure?&#39;, &#39;Answer: Of himself.&#39;, &#39;Well, so I will talk about myself.&#39;, &#39;II I want now to tell you, gentlemen, whether you care to hear it or not, why I could not even become an insect.&#39;, &#39;I tell you solemnly, that I have many times tried to become an insect.&#39;, &#39;But I was not equal even to that.&#39;, &#39;I swear, gentlemen, that to be too conscious is an illness--a real thorough-going illness.&#39;, &#34;For man&#39;s everyday needs, it would have been quite enough to have the ordinary human consciousness, that is, half or a quarter of the amount which falls to the lot of a cultivated man of our unhappy nineteenth century, especially one who has the fatal ill-luck to inhabit Petersburg, the most theoretical and intentional town on the whole terrestrial globe.&#34;, &#39;(There are intentional and unintentional towns.)&#39;, &#39;It would have been quite enough, for instance, to have the consciousness by which all so-called direct persons and men of action live.&#39;, &#39;I bet you think I am writing all this from affectation, to be witty at the expense of men of action; and what is more, that from ill-bred affectation, I am clanking a sword like my officer.&#39;, &#39;But, gentlemen, whoever can pride himself on his diseases and even swagger over them?&#39;, &#39;Though, after all, everyone does do that; people do pride themselves on their diseases, and I do, may be, more than anyone.&#39;, &#39;We will not dispute it; my contention was absurd.&#39;, &#39;But yet I am firmly persuaded that a great deal of consciousness, every sort of consciousness, in fact, is a disease.&#39;, &#39;I stick to that.&#39;, &#39;Let us leave that, too, for a minute.&#39;, &#39;Tell me this: why does it happen that at the very, yes, at the very moments when I am most capable of feeling every refinement of all that is &#34;sublime and beautiful,&#34; as they used to say at one time, it would, as though of design, happen to me not only to feel but to do such ugly things, such that ... Well, in short, actions that all, perhaps, commit; but which, as though purposely, occurred to me at the very time when I was most conscious that they ought not to be committed.&#39;, &#39;The more conscious I was of goodness and of all that was &#34;sublime and beautiful,&#34; the more deeply I sank into my mire and the more ready I was to sink in it altogether.&#39;, &#39;But the chief point was that all this was, as it were, not accidental in me, but as though it were bound to be so.&#39;, &#39;It was as though it were my most normal condition, and not in the least disease or depravity, so that at last all desire in me to struggle against this depravity passed.&#39;, &#39;It ended by my almost believing (perhaps actually believing) that this was perhaps my normal condition.&#39;, &#39;But at first, in the beginning, what agonies I endured in that struggle!&#39;, &#39;I did not believe it was the same with other people, and all my life I hid this fact about myself as a secret.&#39;, &#39;I was ashamed (even now, perhaps, I am ashamed): I got to the point of feeling a sort of secret abnormal, despicable enjoyment in returning home to my corner on some disgusting Petersburg night, acutely conscious that that day I had committed a loathsome action again, that what was done could never be undone, and secretly, inwardly gnawing, gnawing at myself for it, tearing and consuming myself till at last the bitterness turned into a sort of shameful accursed sweetness, and at last--into positive real enjoyment!&#39;, &#39;Yes, into enjoyment, into enjoyment!&#39;, &#39;I insist upon that.&#39;, &#39;I have spoken of this because I keep wanting to know for a fact whether other people feel such enjoyment?&#39;, &#34;I will explain; the enjoyment was just from the too intense consciousness of one&#39;s own degradation; it was from feeling oneself that one had reached the last barrier, that it was horrible, but that it could not be otherwise; that there was no escape for you; that you never could become a different man; that even if time and faith were still left you to change into something different you would most likely not wish to change; or if you did wish to, even then you would do nothing; because perhaps in reality there was nothing for you to change into.&#34;, &#39;And the worst of it was, and the root of it all, that it was all in accord with the normal fundamental laws of over-acute consciousness, and with the inertia that was the direct result of those laws, and that consequently one was not only unable to change but could do absolutely nothing.&#39;, &#39;Thus it would follow, as the result of acute consciousness, that one is not to blame in being a scoundrel; as though that were any consolation to the scoundrel once he has come to realise that he actually is a scoundrel.&#39;, &#39;But enough.... Ech, I have talked a lot of nonsense, but what have I explained?&#39;, &#39;How is enjoyment in this to be explained?&#39;, &#39;But I will explain it.&#39;, &#39;I will get to the bottom of it!&#39;, &#39;That is why I have taken up my pen....&#39;, &#39;I, for instance, have a great deal of AMOUR PROPRE.&#39;, &#39;I am as suspicious and prone to take offence as a humpback or a dwarf.&#39;, &#39;But upon my word I sometimes have had moments when if I had happened to be slapped in the face I should, perhaps, have been positively glad of it.&#39;, &#34;I say, in earnest, that I should probably have been able to discover even in that a peculiar sort of enjoyment--the enjoyment, of course, of despair; but in despair there are the most intense enjoyments, especially when one is very acutely conscious of the hopelessness of one&#39;s position.&#34;, &#39;And when one is slapped in the face--why then the consciousness of being rubbed into a pulp would positively overwhelm one.&#39;, &#39;The worst of it is, look at it which way one will, it still turns out that I was always the most to blame in everything.&#39;, &#39;And what is most humiliating of all, to blame for no fault of my own but, so to say, through the laws of nature.&#39;, &#39;In the first place, to blame because I am cleverer than any of the people surrounding me.&#39;, &#39;(I have always considered myself cleverer than any of the people surrounding me, and sometimes, would you believe it, have been positively ashamed of it.&#39;, &#39;At any rate, I have all my life, as it were, turned my eyes away and never could look people straight in the face.)&#39;, &#39;To blame, finally, because even if I had had magnanimity, I should only have had more suffering from the sense of its uselessness.&#39;, &#39;I should certainly have never been able to do anything from being magnanimous--neither to forgive, for my assailant would perhaps have slapped me from the laws of nature, and one cannot forgive the laws of nature; nor to forget, for even if it were owing to the laws of nature, it is insulting all the same.&#39;, &#39;Finally, even if I had wanted to be anything but magnanimous, had desired on the contrary to revenge myself on my assailant, I could not have revenged myself on any one for anything because I should certainly never have made up my mind to do anything, even if I had been able to.&#39;, &#39;Why should I not have made up my mind?&#39;, &#39;About that in particular I want to say a few words.&#39;, &#39;III With people who know how to revenge themselves and to stand up for themselves in general, how is it done?&#39;, &#39;Why, when they are possessed, let us suppose, by the feeling of revenge, then for the time there is nothing else but that feeling left in their whole being.&#39;, &#39;Such a gentleman simply dashes straight for his object like an infuriated bull with its horns down, and nothing but a wall will stop him.&#39;, &#39;(By the way: facing the wall, such gentlemen--that is, the &#34;direct&#34; persons and men of action--are genuinely nonplussed.&#39;, &#39;For them a wall is not an evasion, as for us people who think and consequently do nothing; it is not an excuse for turning aside, an excuse for which we are always very glad, though we scarcely believe in it ourselves, as a rule.&#39;, &#39;No, they are nonplussed in all sincerity.&#39;, &#39;The wall has for them something tranquillising, morally soothing, final--maybe even something mysterious ... but of the wall later.)&#39;, &#39;Well, such a direct person I regard as the real normal man, as his tender mother nature wished to see him when she graciously brought him into being on the earth.&#39;, &#39;I envy such a man till I am green in the face.&#39;, &#39;He is stupid.&#39;, &#39;I am not disputing that, but perhaps the normal man should be stupid, how do you know?&#39;, &#39;Perhaps it is very beautiful, in fact.&#39;, &#39;And I am the more persuaded of that suspicion, if one can call it so, by the fact that if you take, for instance, the antithesis of the normal man, that is, the man of acute consciousness, who has come, of course, not out of the lap of nature but out of a retort (this is almost mysticism, gentlemen, but I suspect this, too), this retort-made man is sometimes so nonplussed in the presence of his antithesis that with all his exaggerated consciousness he genuinely thinks of himself as a mouse and not a man.&#39;, &#39;It may be an acutely conscious mouse, yet it is a mouse, while the other is a man, and therefore, et caetera, et caetera.&#39;, &#39;And the worst of it is, he himself, his very own self, looks on himself as a mouse; no one asks him to do so; and that is an important point.&#39;, &#39;Now let us look at this mouse in action.&#39;, &#39;Let us suppose, for instance, that it feels insulted, too (and it almost always does feel insulted), and wants to revenge itself, too.&#39;, &#34;There may even be a greater accumulation of spite in it than in L&#39;HOMME DE LA NATURE ET DE LA VERITE.&#34;, &#34;The base and nasty desire to vent that spite on its assailant rankles perhaps even more nastily in it than in L&#39;HOMME DE LA NATURE ET DE LA VERITE.&#34;, &#39;For through his innate stupidity the latter looks upon his revenge as justice pure and simple; while in consequence of his acute consciousness the mouse does not believe in the justice of it.&#39;, &#39;To come at last to the deed itself, to the very act of revenge.&#39;, &#39;Apart from the one fundamental nastiness the luckless mouse succeeds in creating around it so many other nastinesses in the form of doubts and questions, adds to the one question so many unsettled questions that there inevitably works up around it a sort of fatal brew, a stinking mess, made up of its doubts, emotions, and of the contempt spat upon it by the direct men of action who stand solemnly about it as judges and arbitrators, laughing at it till their healthy sides ache.&#39;, &#39;Of course the only thing left for it is to dismiss all that with a wave of its paw, and, with a smile of assumed contempt in which it does not even itself believe, creep ignominiously into its mouse-hole.&#39;, &#39;There in its nasty, stinking, underground home our insulted, crushed and ridiculed mouse promptly becomes absorbed in cold, malignant and, above all, everlasting spite.&#39;, &#39;For forty years together it will remember its injury down to the smallest, most ignominious details, and every time will add, of itself, details still more ignominious, spitefully teasing and tormenting itself with its own imagination.&#39;, &#39;It will itself be ashamed of its imaginings, but yet it will recall it all, it will go over and over every detail, it will invent unheard of things against itself, pretending that those things might happen, and will forgive nothing.&#39;, &#39;Maybe it will begin to revenge itself, too, but, as it were, piecemeal, in trivial ways, from behind the stove, incognito, without believing either in its own right to vengeance, or in the success of its revenge, knowing that from all its efforts at revenge it will suffer a hundred times more than he on whom it revenges itself, while he, I daresay, will not even scratch himself.&#39;, &#39;On its deathbed it will recall it all over again, with interest accumulated over all the years and ...&#39;, &#34;But it is just in that cold, abominable half despair, half belief, in that conscious burying oneself alive for grief in the underworld for forty years, in that acutely recognised and yet partly doubtful hopelessness of one&#39;s position, in that hell of unsatisfied desires turned inward, in that fever of oscillations, of resolutions determined for ever and repented of again a minute later--that the savour of that strange enjoyment of which I have spoken lies.&#34;, &#39;It is so subtle, so difficult of analysis, that persons who are a little limited, or even simply persons of strong nerves, will not understand a single atom of it.&#39;, &#39;&#34;Possibly,&#34; you will add on your own account with a grin, &#34;people will not understand it either who have never received a slap in the face,&#34; and in that way you will politely hint to me that I, too, perhaps, have had the experience of a slap in the face in my life, and so I speak as one who knows.&#39;, &#39;I bet that you are thinking that.&#39;, &#39;But set your minds at rest, gentlemen, I have not received a slap in the face, though it is absolutely a matter of indifference to me what you may think about it.&#39;, &#39;Possibly, I even regret, myself, that I have given so few slaps in the face during my life.&#39;, &#39;But enough ... not another word on that subject of such extreme interest to you.&#39;, &#39;I will continue calmly concerning persons with strong nerves who do not understand a certain refinement of enjoyment.&#39;, &#39;Though in certain circumstances these gentlemen bellow their loudest like bulls, though this, let us suppose, does them the greatest credit, yet, as I have said already, confronted with the impossible they subside at once.&#39;, &#39;The impossible means the stone wall!&#39;, &#39;What stone wall?&#39;, &#39;Why, of course, the laws of nature, the deductions of natural science, mathematics.&#39;, &#39;As soon as they prove to you, for instance, that you are descended from a monkey, then it is no use scowling, accept it for a fact.&#39;, &#39;When they prove to you that in reality one drop of your own fat must be dearer to you than a hundred thousand of your fellow-creatures, and that this conclusion is the final solution of all so-called virtues and duties and all such prejudices and fancies, then you have just to accept it, there is no help for it, for twice two is a law of mathematics.&#39;, &#39;Just try refuting it.&#39;, &#39;&#34;Upon my word, they will shout at you, it is no use protesting: it is a case of twice two makes four!&#39;, &#39;Nature does not ask your permission, she has nothing to do with your wishes, and whether you like her laws or dislike them, you are bound to accept her as she is, and consequently all her conclusions.&#39;, &#39;A wall, you see, is a wall ... and so on, and so on.&#34;&#39;, &#39;Merciful Heavens!&#39;, &#39;but what do I care for the laws of nature and arithmetic, when, for some reason I dislike those laws and the fact that twice two makes four?&#39;, &#39;Of course I cannot break through the wall by battering my head against it if I really have not the strength to knock it down, but I am not going to be reconciled to it simply because it is a stone wall and I have not the strength.&#39;, &#39;As though such a stone wall really were a consolation, and really did contain some word of conciliation, simply because it is as true as twice two makes four.&#39;, &#39;Oh, absurdity of absurdities!&#39;, &#34;How much better it is to understand it all, to recognise it all, all the impossibilities and the stone wall; not to be reconciled to one of those impossibilities and stone walls if it disgusts you to be reconciled to it; by the way of the most inevitable, logical combinations to reach the most revolting conclusions on the everlasting theme, that even for the stone wall you are yourself somehow to blame, though again it is as clear as day you are not to blame in the least, and therefore grinding your teeth in silent impotence to sink into luxurious inertia, brooding on the fact that there is no one even for you to feel vindictive against, that you have not, and perhaps never will have, an object for your spite, that it is a sleight of hand, a bit of juggling, a card-sharper&#39;s trick, that it is simply a mess, no knowing what and no knowing who, but in spite of all these uncertainties and jugglings, still there is an ache in you, and the more you do not know, the worse the ache.&#34;, &#39;IV &#34;Ha, ha, ha!&#39;, &#39;You will be finding enjoyment in toothache next,&#34; you cry, with a laugh.&#39;, &#39;&#34;Well, even in toothache there is enjoyment,&#34; I answer.&#39;, &#39;I had toothache for a whole month and I know there is.&#39;, &#39;In that case, of course, people are not spiteful in silence, but moan; but they are not candid moans, they are malignant moans, and the malignancy is the whole point.&#39;, &#39;The enjoyment of the sufferer finds expression in those moans; if he did not feel enjoyment in them he would not moan.&#39;, &#39;It is a good example, gentlemen, and I will develop it.&#39;, &#39;Those moans express in the first place all the aimlessness of your pain, which is so humiliating to your consciousness; the whole legal system of nature on which you spit disdainfully, of course, but from which you suffer all the same while she does not.&#39;, &#39;They express the consciousness that you have no enemy to punish, but that you have pain; the consciousness that in spite of all possible Wagenheims you are in complete slavery to your teeth; that if someone wishes it, your teeth will leave off aching, and if he does not, they will go on aching another three months; and that finally if you are still contumacious and still protest, all that is left you for your own gratification is to thrash yourself or beat your wall with your fist as hard as you can, and absolutely nothing more.&#39;, &#39;Well, these mortal insults, these jeers on the part of someone unknown, end at last in an enjoyment which sometimes reaches the highest degree of voluptuousness.&#39;, &#39;I ask you, gentlemen, listen sometimes to the moans of an educated man of the nineteenth century suffering from toothache, on the second or third day of the attack, when he is beginning to moan, not as he moaned on the first day, that is, not simply because he has toothache, not just as any coarse peasant, but as a man affected by progress and European civilisation, a man who is &#34;divorced from the soil and the national elements,&#34; as they express it now-a-days.&#39;, &#39;His moans become nasty, disgustingly malignant, and go on for whole days and nights.&#39;, &#34;And of course he knows himself that he is doing himself no sort of good with his moans; he knows better than anyone that he is only lacerating and harassing himself and others for nothing; he knows that even the audience before whom he is making his efforts, and his whole family, listen to him with loathing, do not put a ha&#39;porth of faith in him, and inwardly understand that he might moan differently, more simply, without trills and flourishes, and that he is only amusing himself like that from ill-humour, from malignancy.&#34;, &#39;Well, in all these recognitions and disgraces it is that there lies a voluptuous pleasure.&#39;, &#39;As though he would say: &#34;I am worrying you, I am lacerating your hearts, I am keeping everyone in the house awake.&#39;, &#39;Well, stay awake then, you, too, feel every minute that I have toothache.&#39;, &#39;I am not a hero to you now, as I tried to seem before, but simply a nasty person, an impostor.&#39;, &#39;Well, so be it, then!&#39;, &#39;I am very glad that you see through me.&#39;, &#39;It is nasty for you to hear my despicable moans: well, let it be nasty; here I will let you have a nastier flourish in a minute....&#34; You do not understand even now, gentlemen?&#39;, &#39;No, it seems our development and our consciousness must go further to understand all the intricacies of this pleasure.&#39;, &#39;You laugh?&#39;, &#39;Delighted.&#39;, &#39;My jests, gentlemen, are of course in bad taste, jerky, involved, lacking self-confidence.&#39;, &#39;But of course that is because I do not respect myself.&#39;, &#39;Can a man of perception respect himself at all?&#39;, &#39;V Come, can a man who attempts to find enjoyment in the very feeling of his own degradation possibly have a spark of respect for himself?&#39;, &#39;I am not saying this now from any mawkish kind of remorse.&#39;, &#39;And, indeed, I could never endure saying, &#34;Forgive me, Papa, I won &#39;t do it again,&#34; not because I am incapable of saying that--on the contrary, perhaps just because I have been too capable of it, and in what a way, too.&#39;, &#39;As though of design I used to get into trouble in cases when I was not to blame in any way.&#39;, &#39;That was the nastiest part of it.&#39;, &#39;At the same time I was genuinely touched and penitent, I used to shed tears and, of course, deceived myself, though I was not acting in the least and there was a sick feeling in my heart at the time.... For that one could not blame even the laws of nature, though the laws of nature have continually all my life offended me more than anything.&#39;, &#39;It is loathsome to remember it all, but it was loathsome even then.&#39;, &#39;Of course, a minute or so later I would realise wrathfully that it was all a lie, a revolting lie, an affected lie, that is, all this penitence, this emotion, these vows of reform.&#39;, &#34;You will ask why did I worry myself with such antics: answer, because it was very dull to sit with one&#39;s hands folded, and so one began cutting capers.&#34;, &#39;That is really it.&#39;, &#39;Observe yourselves more carefully, gentlemen, then you will understand that it is so.&#39;, &#39;I invented adventures for myself and made up a life, so as at least to live in some way.&#39;, &#39;How many times it has happened to me--well, for instance, to take offence simply on purpose, for nothing; and one knows oneself, of course, that one is offended at nothing; that one is putting it on, but yet one brings oneself at last to the point of being really offended.&#39;, &#39;All my life I have had an impulse to play such pranks, so that in the end I could not control it in myself.&#39;, &#39;Another time, twice, in fact, I tried hard to be in love.&#39;, &#39;I suffered, too, gentlemen, I assure you.&#39;, &#39;In the depth of my heart there was no faith in my suffering, only a faint stir of mockery, but yet I did suffer, and in the real, orthodox way; I was jealous, beside myself ... and it was all from ENNUI, gentlemen, all from ENNUI; inertia overcame me.&#39;, &#39;You know the direct, legitimate fruit of consciousness is inertia, that is, conscious sitting-with-the-hands-folded.&#39;, &#39;I have referred to this already.&#39;, &#39;I repeat, I repeat with emphasis: all &#34;direct&#34; persons and men of action are active just because they are stupid and limited.&#39;, &#39;How explain that?&#39;, &#39;I will tell you: in consequence of their limitation they take immediate and secondary causes for primary ones, and in that way persuade themselves more quickly and easily than other people do that they have found an infallible foundation for their activity, and their minds are at ease and you know that is the chief thing.&#39;, &#39;To begin to act, you know, you must first have your mind completely at ease and no trace of doubt left in it.&#39;, &#39;Why, how am I, for example, to set my mind at rest?&#39;, &#39;Where are the primary causes on which I am to build?&#39;, &#39;Where are my foundations?&#39;, &#39;Where am I to get them from?&#39;, &#39;I exercise myself in reflection, and consequently with me every primary cause at once draws after itself another still more primary, and so on to infinity.&#39;, &#39;That is just the essence of every sort of consciousness and reflection.&#39;, &#39;It must be a case of the laws of nature again.&#39;, &#39;What is the result of it in the end?&#39;, &#39;Why, just the same.&#39;, &#39;Remember I spoke just now of vengeance.&#39;, &#39;(I am sure you did not take it in.)&#39;, &#39;I said that a man revenges himself because he sees justice in it.&#39;, &#39;Therefore he has found a primary cause, that is, justice.&#39;, &#39;And so he is at rest on all sides, and consequently he carries out his revenge calmly and successfully, being persuaded that he is doing a just and honest thing.&#39;, &#39;But I see no justice in it, I find no sort of virtue in it either, and consequently if I attempt to revenge myself, it is only out of spite.&#39;, &#39;Spite, of course, might overcome everything, all my doubts, and so might serve quite successfully in place of a primary cause, precisely because it is not a cause.&#39;, &#39;But what is to be done if I have not even spite (I began with that just now, you know).&#39;, &#39;In consequence again of those accursed laws of consciousness, anger in me is subject to chemical disintegration.&#39;, &#39;You look into it, the object flies off into air, your reasons evaporate, the criminal is not to be found, the wrong becomes not a wrong but a phantom, something like the toothache, for which no one is to blame, and consequently there is only the same outlet left again--that is, to beat the wall as hard as you can.&#39;, &#39;So you give it up with a wave of the hand because you have not found a fundamental cause.&#39;, &#39;And try letting yourself be carried away by your feelings, blindly, without reflection, without a primary cause, repelling consciousness at least for a time; hate or love, if only not to sit with your hands folded.&#39;, &#39;The day after tomorrow, at the latest, you will begin despising yourself for having knowingly deceived yourself.&#39;, &#39;Result: a soap-bubble and inertia.&#39;, &#39;Oh, gentlemen, do you know, perhaps I consider myself an intelligent man, only because all my life I have been able neither to begin nor to finish anything.&#39;, &#39;Granted I am a babbler, a harmless vexatious babbler, like all of us.&#39;, &#39;But what is to be done if the direct and sole vocation of every intelligent man is babble, that is, the intentional pouring of water through a sieve?&#39;, &#39;VI Oh, if I had done nothing simply from laziness!&#39;, &#39;Heavens, how I should have respected myself, then.&#39;, &#39;I should have respected myself because I should at least have been capable of being lazy; there would at least have been one quality, as it were, positive in me, in which I could have believed myself.&#39;, &#39;Question: What is he?&#39;, &#39;Answer: A sluggard; how very pleasant it would have been to hear that of oneself!&#39;, &#39;It would mean that I was positively defined, it would mean that there was something to say about me.&#39;, &#39;&#34;Sluggard&#34;--why, it is a calling and vocation, it is a career.&#39;, &#39;Do not jest, it is so.&#39;, &#39;I should then be a member of the best club by right, and should find my occupation in continually respecting myself.&#39;, &#39;I knew a gentleman who prided himself all his life on being a connoisseur of Lafitte.&#39;, &#39;He considered this as his positive virtue, and never doubted himself.&#39;, &#39;He died, not simply with a tranquil, but with a triumphant conscience, and he was quite right, too.&#39;, &#39;Then I should have chosen a career for myself, I should have been a sluggard and a glutton, not a simple one, but, for instance, one with sympathies for everything sublime and beautiful.&#39;, &#39;How do you like that?&#39;, &#39;I have long had visions of it.&#39;, &#39;That &#34;sublime and beautiful&#34; weighs heavily on my mind at forty But that is at forty; then--oh, then it would have been different!&#39;, &#39;I should have found for myself a form of activity in keeping with it, to be precise, drinking to the health of everything &#34;sublime and beautiful.&#34;&#39;, &#39;I should have snatched at every opportunity to drop a tear into my glass and then to drain it to all that is &#34;sublime and beautiful.&#34;&#39;, &#39;I should then have turned everything into the sublime and the beautiful; in the nastiest, unquestionable trash, I should have sought out the sublime and the beautiful.&#39;, &#39;I should have exuded tears like a wet sponge.&#39;, &#39;An artist, for instance, paints a picture worthy of Gay.&#39;, &#39;At once I drink to the health of the artist who painted the picture worthy of Gay, because I love all that is &#34;sublime and beautiful.&#34;&#39;, &#39;An author has written AS YOU WILL: at once I drink to the health of &#34;anyone you will&#34; because I love all that is &#34;sublime and beautiful.&#34;&#39;, &#39;I should claim respect for doing so.&#39;, &#39;I should persecute anyone who would not show me respect.&#39;, &#39;I should live at ease, I should die with dignity, why, it is charming, perfectly charming!&#39;, &#39;And what a good round belly I should have grown, what a treble chin I should have established, what a ruby nose I should have coloured for myself, so that everyone would have said, looking at me: &#34;Here is an asset!&#39;, &#39;Here is something real and solid!&#34;&#39;, &#39;And, say what you like, it is very agreeable to hear such remarks about oneself in this negative age.&#39;, &#39;VII But these are all golden dreams.&#39;, &#39;Oh, tell me, who was it first announced, who was it first proclaimed, that man only does nasty things because he does not know his own interests; and that if he were enlightened, if his eyes were opened to his real normal interests, man would at once cease to do nasty things, would at once become good and noble because, being enlightened and understanding his real advantage, he would see his own advantage in the good and nothing else, and we all know that not one man can, consciously, act against his own interests, consequently, so to say, through necessity, he would begin doing good?&#39;, &#39;Oh, the babe!&#39;, &#39;Oh, the pure, innocent child!&#39;, &#39;Why, in the first place, when in all these thousands of years has there been a time when man has acted only from his own interest?&#39;, &#39;What is to be done with the millions of facts that bear witness that men, CONSCIOUSLY, that is fully understanding their real interests, have left them in the background and have rushed headlong on another path, to meet peril and danger, compelled to this course by nobody and by nothing, but, as it were, simply disliking the beaten track, and have obstinately, wilfully, struck out another difficult, absurd way, seeking it almost in the darkness.&#39;, &#39;So, I suppose, this obstinacy and perversity were pleasanter to them than any advantage....&#39;, &#39;Advantage!&#39;, &#39;What is advantage?&#39;, &#39;And will you take it upon yourself to define with perfect accuracy in what the advantage of man consists?&#39;, &#34;And what if it so happens that a man&#39;s advantage, SOMETIMES, not only may, but even must, consist in his desiring in certain cases what is harmful to himself and not advantageous.&#34;, &#39;And if so, if there can be such a case, the whole principle falls into dust.&#39;, &#39;What do you think--are there such cases?&#39;, &#34;You laugh; laugh away, gentlemen, but only answer me: have man&#39;s advantages been reckoned up with perfect certainty?&#34;, &#39;Are there not some which not only have not been included but cannot possibly be included under any classification?&#39;, &#39;You see, you gentlemen have, to the best of my knowledge, taken your whole register of human advantages from the averages of statistical figures and politico-economical formulas.&#39;, &#39;Your advantages are prosperity, wealth, freedom, peace--and so on, and so on.&#39;, &#39;So that the man who should, for instance, go openly and knowingly in opposition to all that list would to your thinking, and indeed mine, too, of course, be an obscurantist or an absolute madman: would not he?&#39;, &#39;But, you know, this is what is surprising: why does it so happen that all these statisticians, sages and lovers of humanity, when they reckon up human advantages invariably leave out one?&#39;, &#34;They don&#39;t even take it into their reckoning in the form in which it should be taken, and the whole reckoning depends upon that.&#34;, &#39;It would be no greater matter, they would simply have to take it, this advantage, and add it to the list.&#39;, &#39;But the trouble is, that this strange advantage does not fall under any classification and is not in place in any list.&#39;, &#39;I have a friend for instance ... Ech!&#39;, &#39;gentlemen, but of course he is your friend, too; and indeed there is no one, no one to whom he is not a friend!&#39;, &#39;When he prepares for any undertaking this gentleman immediately explains to you, elegantly and clearly, exactly how he must act in accordance with the laws of reason and truth.&#39;, &#39;What is more, he will talk to you with excitement and passion of the true normal interests of man; with irony he will upbraid the short-sighted fools who do not understand their own interests, nor the true significance of virtue; and, within a quarter of an hour, without any sudden outside provocation, but simply through something inside him which is stronger than all his interests, he will go off on quite a different tack--that is, act in direct opposition to what he has just been saying about himself, in opposition to the laws of reason, in opposition to his own advantage, in fact in opposition to everything ...&#39;, &#39;I warn you that my friend is a compound personality and therefore it is difficult to blame him as an individual.&#39;, &#39;The fact is, gentlemen, it seems there must really exist something that is dearer to almost every man than his greatest advantages, or (not to be illogical) there is a most advantageous advantage (the very one omitted of which we spoke just now) which is more important and more advantageous than all other advantages, for the sake of which a man if necessary is ready to act in opposition to all laws; that is, in opposition to reason, honour, peace, prosperity--in fact, in opposition to all those excellent and useful things if only he can attain that fundamental, most advantageous advantage which is dearer to him than all.&#39;, &#39;&#34;Yes, but it &#39;s advantage all the same,&#34; you will retort.&#39;, &#34;But excuse me, I&#39;ll make the point clear, and it is not a case of playing upon words.&#34;, &#39;What matters is, that this advantage is remarkable from the very fact that it breaks down all our classifications, and continually shatters every system constructed by lovers of mankind for the benefit of mankind.&#39;, &#39;In fact, it upsets everything.&#39;, &#39;But before I mention this advantage to you, I want to compromise myself personally, and therefore I boldly declare that all these fine systems, all these theories for explaining to mankind their real normal interests, in order that inevitably striving to pursue these interests they may at once become good and noble--are, in my opinion, so far, mere logical exercises!&#39;, &#39;Yes, logical exercises.&#39;, &#39;Why, to maintain this theory of the regeneration of mankind by means of the pursuit of his own advantage is to my mind almost the same thing ... as to affirm, for instance, following Buckle, that through civilisation mankind becomes softer, and consequently less bloodthirsty and less fitted for warfare.&#39;, &#39;Logically it does seem to follow from his arguments.&#39;, &#39;But man has such a predilection for systems and abstract deductions that he is ready to distort the truth intentionally, he is ready to deny the evidence of his senses only to justify his logic.&#39;, &#39;I take this example because it is the most glaring instance of it.&#39;, &#39;Only look about you: blood is being spilt in streams, and in the merriest way, as though it were champagne.&#39;, &#39;Take the whole of the nineteenth century in which Buckle lived.&#39;, &#39;Take Napoleon--the Great and also the present one.&#39;, &#39;Take North America--the eternal union.&#39;, &#39;Take the farce of Schleswig-Holstein.... And what is it that civilisation softens in us?&#39;, &#39;The only gain of civilisation for mankind is the greater capacity for variety of sensations--and absolutely nothing more.&#39;, &#39;And through the development of this many-sidedness man may come to finding enjoyment in bloodshed.&#39;, &#39;In fact, this has already happened to him.&#39;, &#39;Have you noticed that it is the most civilised gentlemen who have been the subtlest slaughterers, to whom the Attilas and Stenka Razins could not hold a candle, and if they are not so conspicuous as the Attilas and Stenka Razins it is simply because they are so often met with, are so ordinary and have become so familiar to us.&#39;, &#39;In any case civilisation has made mankind if not more bloodthirsty, at least more vilely, more loathsomely bloodthirsty.&#39;, &#39;In old days he saw justice in bloodshed and with his conscience at peace exterminated those he thought proper.&#39;, &#39;Now we do think bloodshed abominable and yet we engage in this abomination, and with more energy than ever.&#39;, &#39;Which is worse?&#39;, &#39;Decide that for yourselves.&#39;, &#34;They say that Cleopatra (excuse an instance from Roman history) was fond of sticking gold pins into her slave-girls&#39; breasts and derived gratification from their screams and writhings.&#34;, &#39;You will say that that was in the comparatively barbarous times; that these are barbarous times too, because also, comparatively speaking, pins are stuck in even now; that though man has now learned to see more clearly than in barbarous ages, he is still far from having learnt to act as reason and science would dictate.&#39;, &#39;But yet you are fully convinced that he will be sure to learn when he gets rid of certain old bad habits, and when common sense and science have completely re-educated human nature and turned it in a normal direction.&#39;, &#39;You are confident that then man will cease from INTENTIONAL error and will, so to say, be compelled not to want to set his will against his normal interests.&#39;, &#34;That is not all; then, you say, science itself will teach man (though to my mind it&#39;s a superfluous luxury) that he never has really had any caprice or will of his own, and that he himself is something of the nature of a piano-key or the stop of an organ, and that there are, besides, things called the laws of nature; so that everything he does is not done by his willing it, but is done of itself, by the laws of nature.&#34;, &#39;Consequently we have only to discover these laws of nature, and man will no longer have to answer for his actions and life will become exceedingly easy for him.&#39;, &#39;All human actions will then, of course, be tabulated according to these laws, mathematically, like tables of logarithms up to 108,000, and entered in an index; or, better still, there would be published certain edifying works of the nature of encyclopaedic lexicons, in which everything will be so clearly calculated and explained that there will be no more incidents or adventures in the world.&#39;, &#39;Then--this is all what you say--new economic relations will be established, all ready-made and worked out with mathematical exactitude, so that every possible question will vanish in the twinkling of an eye, simply because every possible answer to it will be provided.&#39;, &#39;Then the &#34;Palace of Crystal&#34; will be built.&#39;, &#39;Then ...&#39;, &#39;In fact, those will be halcyon days.&#39;, &#39;Of course there is no guaranteeing (this is my comment) that it will not be, for instance, frightfully dull then (for what will one have to do when everything will be calculated and tabulated), but on the other hand everything will be extraordinarily rational.&#39;, &#39;Of course boredom may lead you to anything.&#39;, &#39;It is boredom sets one sticking golden pins into people, but all that would not matter.&#39;, &#39;What is bad (this is my comment again) is that I dare say people will be thankful for the gold pins then.&#39;, &#39;Man is stupid, you know, phenomenally stupid; or rather he is not at all stupid, but he is so ungrateful that you could not find another like him in all creation.&#39;, &#39;I, for instance, would not be in the least surprised if all of a sudden, A PROPOS of nothing, in the midst of general prosperity a gentleman with an ignoble, or rather with a reactionary and ironical, countenance were to arise and, putting his arms akimbo, say to us all: &#34;I say, gentleman, hadn &#39;t we better kick over the whole show and scatter rationalism to the winds, simply to send these logarithms to the devil, and to enable us to live once more at our own sweet foolish will!&#34;&#39;, &#39;That again would not matter, but what is annoying is that he would be sure to find followers--such is the nature of man.&#39;, &#39;And all that for the most foolish reason, which, one would think, was hardly worth mentioning: that is, that man everywhere and at all times, whoever he may be, has preferred to act as he chose and not in the least as his reason and advantage dictated.&#39;, &#34;And one may choose what is contrary to one&#39;s own interests, and sometimes one POSITIVELY OUGHT (that is my idea).&#34;, &#39;One &#39;s own free unfettered choice, one &#39;s own caprice, however wild it may be, one &#39;s own fancy worked up at times to frenzy--is that very &#34;most advantageous advantage&#34; which we have overlooked, which comes under no classification and against which all systems and theories are continually being shattered to atoms.&#39;, &#39;And how do these wiseacres know that man wants a normal, a virtuous choice?&#39;, &#39;What has made them conceive that man must want a rationally advantageous choice?&#39;, &#39;What man wants is simply INDEPENDENT choice, whatever that independence may cost and wherever it may lead.&#39;, &#39;And choice, of course, the devil only knows what choice.&#39;, &#39;VIII &#34;Ha!&#39;, &#39;ha!&#39;, &#39;ha!&#39;, &#39;But you know there is no such thing as choice in reality, say what you like,&#34; you will interpose with a chuckle.&#39;, &#39;&#34;Science has succeeded in so far analysing man that we know already that choice and what is called freedom of will is nothing else than--&#34; Stay, gentlemen, I meant to begin with that myself I confess, I was rather frightened.&#39;, &#39;I was just going to say that the devil only knows what choice depends on, and that perhaps that was a very good thing, but I remembered the teaching of science ... and pulled myself up.&#39;, &#39;And here you have begun upon it.&#39;, &#39;Indeed, if there really is some day discovered a formula for all our desires and caprices--that is, an explanation of what they depend upon, by what laws they arise, how they develop, what they are aiming at in one case and in another and so on, that is a real mathematical formula--then, most likely, man will at once cease to feel desire, indeed, he will be certain to.&#39;, &#39;For who would want to choose by rule?&#39;, &#39;Besides, he will at once be transformed from a human being into an organ-stop or something of the sort; for what is a man without desires, without free will and without choice, if not a stop in an organ?&#39;, &#39;What do you think?&#39;, &#39;Let us reckon the chances--can such a thing happen or not?&#39;, &#39;&#34;H &#39;m!&#34;&#39;, &#39;you decide.&#39;, &#39;&#34;Our choice is usually mistaken from a false view of our advantage.&#39;, &#39;We sometimes choose absolute nonsense because in our foolishness we see in that nonsense the easiest means for attaining a supposed advantage.&#39;, &#39;But when all that is explained and worked out on paper (which is perfectly possible, for it is contemptible and senseless to suppose that some laws of nature man will never understand), then certainly so-called desires will no longer exist.&#39;, &#39;For if a desire should come into conflict with reason we shall then reason and not desire, because it will be impossible retaining our reason to be SENSELESS in our desires, and in that way knowingly act against reason and desire to injure ourselves.&#39;, &#39;And as all choice and reasoning can be really calculated--because there will some day be discovered the laws of our so-called free will--so, joking apart, there may one day be something like a table constructed of them, so that we really shall choose in accordance with it.&#39;, &#39;If, for instance, some day they calculate and prove to me that I made a long nose at someone because I could not help making a long nose at him and that I had to do it in that particular way, what FREEDOM is left me, especially if I am a learned man and have taken my degree somewhere?&#39;, &#39;Then I should be able to calculate my whole life for thirty years beforehand.&#39;, &#39;In short, if this could be arranged there would be nothing left for us to do; anyway, we should have to understand that.&#39;, &#39;And, in fact, we ought unwearyingly to repeat to ourselves that at such and such a time and in such and such circumstances nature does not ask our leave; that we have got to take her as she is and not fashion her to suit our fancy, and if we really aspire to formulas and tables of rules, and well, even ... to the chemical retort, there &#39;s no help for it, we must accept the retort too, or else it will be accepted without our consent....&#34; Yes, but here I come to a stop!&#39;, &#34;Gentlemen, you must excuse me for being over-philosophical; it&#39;s the result of forty years underground!&#34;, &#39;Allow me to indulge my fancy.&#39;, &#34;You see, gentlemen, reason is an excellent thing, there&#39;s no disputing that, but reason is nothing but reason and satisfies only the rational side of man&#39;s nature, while will is a manifestation of the whole life, that is, of the whole human life including reason and all the impulses.&#34;, &#39;And although our life, in this manifestation of it, is often worthless, yet it is life and not simply extracting square roots.&#39;, &#39;Here I, for instance, quite naturally want to live, in order to satisfy all my capacities for life, and not simply my capacity for reasoning, that is, not simply one twentieth of my capacity for life.&#39;, &#39;What does reason know?&#39;, &#39;Reason only knows what it has succeeded in learning (some things, perhaps, it will never learn; this is a poor comfort, but why not say so frankly?)&#39;, &#39;and human nature acts as a whole, with everything that is in it, consciously or unconsciously, and, even if it goes wrong, it lives.&#39;, &#39;I suspect, gentlemen, that you are looking at me with compassion; you tell me again that an enlightened and developed man, such, in short, as the future man will be, cannot consciously desire anything disadvantageous to himself, that that can be proved mathematically.&#39;, &#39;I thoroughly agree, it can--by mathematics.&#39;, &#39;But I repeat for the hundredth time, there is one case, one only, when man may consciously, purposely, desire what is injurious to himself, what is stupid, very stupid--simply in order to have the right to desire for himself even what is very stupid and not to be bound by an obligation to desire only what is sensible.&#39;, &#39;Of course, this very stupid thing, this caprice of ours, may be in reality, gentlemen, more advantageous for us than anything else on earth, especially in certain cases.&#39;, &#39;And in particular it may be more advantageous than any advantage even when it does us obvious harm, and contradicts the soundest conclusions of our reason concerning our advantage--for in any circumstances it preserves for us what is most precious and most important--that is, our personality, our individuality.&#39;, &#39;Some, you see, maintain that this really is the most precious thing for mankind; choice can, of course, if it chooses, be in agreement with reason; and especially if this be not abused but kept within bounds.&#39;, &#39;It is profitable and sometimes even praiseworthy.&#39;, &#39;But very often, and even most often, choice is utterly and stubbornly opposed to reason ... and ... and ... do you know that that, too, is profitable, sometimes even praiseworthy?&#39;, &#39;Gentlemen, let us suppose that man is not stupid.&#39;, &#39;(Indeed one cannot refuse to suppose that, if only from the one consideration, that, if man is stupid, then who is wise?)&#39;, &#39;But if he is not stupid, he is monstrously ungrateful!&#39;, &#39;Phenomenally ungrateful.&#39;, &#39;In fact, I believe that the best definition of man is the ungrateful biped.&#39;, &#39;But that is not all, that is not his worst defect; his worst defect is his perpetual moral obliquity, perpetual--from the days of the Flood to the Schleswig-Holstein period.&#39;, &#39;Moral obliquity and consequently lack of good sense; for it has long been accepted that lack of good sense is due to no other cause than moral obliquity.&#39;, &#39;Put it to the test and cast your eyes upon the history of mankind.&#39;, &#39;What will you see?&#39;, &#39;Is it a grand spectacle?&#39;, &#39;Grand, if you like.&#39;, &#34;Take the Colossus of Rhodes, for instance, that&#39;s worth something.&#34;, &#34;With good reason Mr. Anaevsky testifies of it that some say that it is the work of man&#39;s hands, while others maintain that it has been created by nature herself.&#34;, &#39;Is it many-coloured?&#39;, &#39;May be it is many-coloured, too: if one takes the dress uniforms, military and civilian, of all peoples in all ages--that alone is worth something, and if you take the undress uniforms you will never get to the end of it; no historian would be equal to the job.&#39;, &#39;Is it monotonous?&#39;, &#34;May be it&#39;s monotonous too: it&#39;s fighting and fighting; they are fighting now, they fought first and they fought last--you will admit, that it is almost too monotonous.&#34;, &#39;In short, one may say anything about the history of the world--anything that might enter the most disordered imagination.&#39;, &#34;The only thing one can&#39;t say is that it&#39;s rational.&#34;, &#34;The very word sticks in one&#39;s throat.&#34;, &#39;And, indeed, this is the odd thing that is continually happening: there are continually turning up in life moral and rational persons, sages and lovers of humanity who make it their object to live all their lives as morally and rationally as possible, to be, so to speak, a light to their neighbours simply in order to show them that it is possible to live morally and rationally in this world.&#39;, &#39;And yet we all know that those very people sooner or later have been false to themselves, playing some queer trick, often a most unseemly one.&#39;, &#39;Now I ask you: what can be expected of man since he is a being endowed with strange qualities?&#39;, &#39;Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick.&#39;, &#39;He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element.&#39;, &#39;It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself--as though that were so necessary--that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar.&#39;, &#39;And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point.&#39;, &#39;And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point!&#39;, &#39;He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object--that is, convince himself that he is a man and not a piano-key!&#39;, &#39;If you say that all this, too, can be calculated and tabulated--chaos and darkness and curses, so that the mere possibility of calculating it all beforehand would stop it all, and reason would reassert itself, then man would purposely go mad in order to be rid of reason and gain his point!&#39;, &#39;I believe in it, I answer for it, for the whole work of man really seems to consist in nothing but proving to himself every minute that he is a man and not a piano-key!&#39;, &#39;It may be at the cost of his skin, it may be by cannibalism!&#39;, &#34;And this being so, can one help being tempted to rejoice that it has not yet come off, and that desire still depends on something we don&#39;t know?&#34;, &#39;You will scream at me (that is, if you condescend to do so) that no one is touching my free will, that all they are concerned with is that my will should of itself, of its own free will, coincide with my own normal interests, with the laws of nature and arithmetic.&#39;, &#39;Good heavens, gentlemen, what sort of free will is left when we come to tabulation and arithmetic, when it will all be a case of twice two make four?&#39;, &#39;Twice two makes four without my will.&#39;, &#39;As if free will meant that!&#39;, &#39;IX Gentlemen, I am joking, and I know myself that my jokes are not brilliant, but you know one can take everything as a joke.&#39;, &#39;I am, perhaps, jesting against the grain.&#39;, &#39;Gentlemen, I am tormented by questions; answer them for me.&#39;, &#39;You, for instance, want to cure men of their old habits and reform their will in accordance with science and good sense.&#39;, &#39;But how do you know, not only that it is possible, but also that it is DESIRABLE to reform man in that way?&#39;, &#34;And what leads you to the conclusion that man&#39;s inclinations NEED reforming?&#34;, &#39;In short, how do you know that such a reformation will be a benefit to man?&#39;, &#39;And to go to the root of the matter, why are you so positively convinced that not to act against his real normal interests guaranteed by the conclusions of reason and arithmetic is certainly always advantageous for man and must always be a law for mankind?&#39;, &#39;So far, you know, this is only your supposition.&#39;, &#39;It may be the law of logic, but not the law of humanity.&#39;, &#39;You think, gentlemen, perhaps that I am mad?&#39;, &#39;Allow me to defend myself.&#39;, &#39;I agree that man is pre-eminently a creative animal, predestined to strive consciously for an object and to engage in engineering--that is, incessantly and eternally to make new roads, WHEREVER THEY MAY LEAD.&#39;, &#39;But the reason why he wants sometimes to go off at a tangent may just be that he is PREDESTINED to make the road, and perhaps, too, that however stupid the &#34;direct&#34; practical man may be, the thought sometimes will occur to him that the road almost always does lead SOMEWHERE, and that the destination it leads to is less important than the process of making it, and that the chief thing is to save the well-conducted child from despising engineering, and so giving way to the fatal idleness, which, as we all know, is the mother of all the vices.&#39;, &#39;Man likes to make roads and to create, that is a fact beyond dispute.&#39;, &#39;But why has he such a passionate love for destruction and chaos also?&#39;, &#39;Tell me that!&#39;, &#39;But on that point I want to say a couple of words myself.&#39;, &#39;May it not be that he loves chaos and destruction (there can be no disputing that he does sometimes love it) because he is instinctively afraid of attaining his object and completing the edifice he is constructing?&#39;, &#39;Who knows, perhaps he only loves that edifice from a distance, and is by no means in love with it at close quarters; perhaps he only loves building it and does not want to live in it, but will leave it, when completed, for the use of LES ANIMAUX DOMESTIQUES--such as the ants, the sheep, and so on.&#39;, &#39;Now the ants have quite a different taste.&#39;, &#39;They have a marvellous edifice of that pattern which endures for ever--the ant-heap.&#39;, &#39;With the ant-heap the respectable race of ants began and with the ant-heap they will probably end, which does the greatest credit to their perseverance and good sense.&#39;, &#39;But man is a frivolous and incongruous creature, and perhaps, like a chess player, loves the process of the game, not the end of it.&#39;, &#39;And who knows (there is no saying with certainty), perhaps the only goal on earth to which mankind is striving lies in this incessant process of attaining, in other words, in life itself, and not in the thing to be attained, which must always be expressed as a formula, as positive as twice two makes four, and such positiveness is not life, gentlemen, but is the beginning of death.&#39;, &#39;Anyway, man has always been afraid of this mathematical certainty, and I am afraid of it now.&#39;, &#39;Granted that man does nothing but seek that mathematical certainty, he traverses oceans, sacrifices his life in the quest, but to succeed, really to find it, dreads, I assure you.&#39;, &#39;He feels that when he has found it there will be nothing for him to look for.&#39;, &#39;When workmen have finished their work they do at least receive their pay, they go to the tavern, then they are taken to the police-station--and there is occupation for a week.&#39;, &#39;But where can man go?&#39;, &#39;Anyway, one can observe a certain awkwardness about him when he has attained such objects.&#39;, &#39;He loves the process of attaining, but does not quite like to have attained, and that, of course, is very absurd.&#39;, &#39;In fact, man is a comical creature; there seems to be a kind of jest in it all.&#39;, &#39;But yet mathematical certainty is after all, something insufferable.&#39;, &#39;Twice two makes four seems to me simply a piece of insolence.&#39;, &#39;Twice two makes four is a pert coxcomb who stands with arms akimbo barring your path and spitting.&#39;, &#39;I admit that twice two makes four is an excellent thing, but if we are to give everything its due, twice two makes five is sometimes a very charming thing too.&#39;, &#39;And why are you so firmly, so triumphantly, convinced that only the normal and the positive--in other words, only what is conducive to welfare--is for the advantage of man?&#39;, &#39;Is not reason in error as regards advantage?&#39;, &#39;Does not man, perhaps, love something besides well-being?&#39;, &#39;Perhaps he is just as fond of suffering?&#39;, &#39;Perhaps suffering is just as great a benefit to him as well-being?&#39;, &#39;Man is sometimes extraordinarily, passionately, in love with suffering, and that is a fact.&#39;, &#39;There is no need to appeal to universal history to prove that; only ask yourself, if you are a man and have lived at all.&#39;, &#39;As far as my personal opinion is concerned, to care only for well-being seems to me positively ill-bred.&#39;, &#34;Whether it&#39;s good or bad, it is sometimes very pleasant, too, to smash things.&#34;, &#39;I hold no brief for suffering nor for well-being either.&#39;, &#39;I am standing for ... my caprice, and for its being guaranteed to me when necessary.&#39;, &#39;Suffering would be out of place in vaudevilles, for instance; I know that.&#39;, &#39;In the &#34;Palace of Crystal&#34; it is unthinkable; suffering means doubt, negation, and what would be the good of a &#34;palace of crystal&#34; if there could be any doubt about it?&#39;, &#39;And yet I think man will never renounce real suffering, that is, destruction and chaos.&#39;, &#39;Why, suffering is the sole origin of consciousness.&#39;, &#39;Though I did lay it down at the beginning that consciousness is the greatest misfortune for man, yet I know man prizes it and would not give it up for any satisfaction.&#39;, &#39;Consciousness, for instance, is infinitely superior to twice two makes four.&#39;, &#39;Once you have mathematical certainty there is nothing left to do or to understand.&#39;, &#39;There will be nothing left but to bottle up your five senses and plunge into contemplation.&#39;, &#39;While if you stick to consciousness, even though the same result is attained, you can at least flog yourself at times, and that will, at any rate, liven you up.&#39;, &#39;Reactionary as it is, corporal punishment is better than nothing.&#39;, &#34;X You believe in a palace of crystal that can never be destroyed--a palace at which one will not be able to put out one&#39;s tongue or make a long nose on the sly.&#34;, &#34;And perhaps that is just why I am afraid of this edifice, that it is of crystal and can never be destroyed and that one cannot put one&#39;s tongue out at it even on the sly.&#34;, &#39;You see, if it were not a palace, but a hen-house, I might creep into it to avoid getting wet, and yet I would not call the hen-house a palace out of gratitude to it for keeping me dry.&#39;, &#39;You laugh and say that in such circumstances a hen-house is as good as a mansion.&#39;, &#39;Yes, I answer, if one had to live simply to keep out of the rain.&#39;, &#39;But what is to be done if I have taken it into my head that that is not the only object in life, and that if one must live one had better live in a mansion?&#39;, &#39;That is my choice, my desire.&#39;, &#39;You will only eradicate it when you have changed my preference.&#39;, &#39;Well, do change it, allure me with something else, give me another ideal.&#39;, &#39;But meanwhile I will not take a hen-house for a mansion.&#39;, &#39;The palace of crystal may be an idle dream, it may be that it is inconsistent with the laws of nature and that I have invented it only through my own stupidity, through the old-fashioned irrational habits of my generation.&#39;, &#39;But what does it matter to me that it is inconsistent?&#39;, &#39;That makes no difference since it exists in my desires, or rather exists as long as my desires exist.&#39;, &#39;Perhaps you are laughing again?&#39;, &#39;Laugh away; I will put up with any mockery rather than pretend that I am satisfied when I am hungry.&#39;, &#39;I know, anyway, that I will not be put off with a compromise, with a recurring zero, simply because it is consistent with the laws of nature and actually exists.&#39;, &#39;I will not accept as the crown of my desires a block of buildings with tenements for the poor on a lease of a thousand years, and perhaps with a sign-board of a dentist hanging out.&#39;, &#39;Destroy my desires, eradicate my ideals, show me something better, and I will follow you.&#39;, &#39;You will say, perhaps, that it is not worth your trouble; but in that case I can give you the same answer.&#39;, &#34;We are discussing things seriously; but if you won&#39;t deign to give me your attention, I will drop your acquaintance.&#34;, &#39;I can retreat into my underground hole.&#39;, &#39;But while I am alive and have desires I would rather my hand were withered off than bring one brick to such a building!&#39;, &#34;Don&#39;t remind me that I have just rejected the palace of crystal for the sole reason that one cannot put out one&#39;s tongue at it.&#34;, &#39;I did not say because I am so fond of putting my tongue out.&#39;, &#34;Perhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one&#39;s tongue.&#34;, &#39;On the contrary, I would let my tongue be cut off out of gratitude if things could be so arranged that I should lose all desire to put it out.&#39;, &#39;It is not my fault that things cannot be so arranged, and that one must be satisfied with model flats.&#39;, &#39;Then why am I made with such desires?&#39;, &#39;Can I have been constructed simply in order to come to the conclusion that all my construction is a cheat?&#39;, &#39;Can this be my whole purpose?&#39;, &#39;I do not believe it.&#39;, &#39;But do you know what: I am convinced that we underground folk ought to be kept on a curb.&#39;, &#39;Though we may sit forty years underground without speaking, when we do come out into the light of day and break out we talk and talk and talk.... XI The long and the short of it is, gentlemen, that it is better to do nothing!&#39;, &#39;Better conscious inertia!&#39;, &#39;And so hurrah for underground!&#39;, &#39;Though I have said that I envy the normal man to the last drop of my bile, yet I should not care to be in his place such as he is now (though I shall not cease envying him).&#39;, &#39;No, no; anyway the underground life is more advantageous.&#39;, &#39;There, at any rate, one can ... Oh, but even now I am lying!&#39;, &#39;I am lying because I know myself that it is not underground that is better, but something different, quite different, for which I am thirsting, but which I cannot find!&#39;, &#39;Damn underground!&#39;, &#39;I will tell you another thing that would be better, and that is, if I myself believed in anything of what I have just written.&#39;, &#39;I swear to you, gentlemen, there is not one thing, not one word of what I have written that I really believe.&#39;, &#39;That is, I believe it, perhaps, but at the same time I feel and suspect that I am lying like a cobbler.&#39;, &#39;&#34;Then why have you written all this?&#34;&#39;, &#39;you will say to me.&#39;, &#39;&#34;I ought to put you underground for forty years without anything to do and then come to you in your cellar, to find out what stage you have reached!&#39;, &#39;How can a man be left with nothing to do for forty years?&#34;&#39;, &#39;&#34;Isn &#39;t that shameful, isn &#39;t that humiliating?&#34;&#39;, &#39;you will say, perhaps, wagging your heads contemptuously.&#39;, &#39;&#34;You thirst for life and try to settle the problems of life by a logical tangle.&#39;, &#39;And how persistent, how insolent are your sallies, and at the same time what a scare you are in!&#39;, &#39;You talk nonsense and are pleased with it; you say impudent things and are in continual alarm and apologising for them.&#39;, &#39;You declare that you are afraid of nothing and at the same time try to ingratiate yourself in our good opinion.&#39;, &#39;You declare that you are gnashing your teeth and at the same time you try to be witty so as to amuse us.&#39;, &#39;You know that your witticisms are not witty, but you are evidently well satisfied with their literary value.&#39;, &#39;You may, perhaps, have really suffered, but you have no respect for your own suffering.&#39;, &#39;You may have sincerity, but you have no modesty; out of the pettiest vanity you expose your sincerity to publicity and ignominy.&#39;, &#39;You doubtlessly mean to say something, but hide your last word through fear, because you have not the resolution to utter it, and only have a cowardly impudence.&#39;, &#39;You boast of consciousness, but you are not sure of your ground, for though your mind works, yet your heart is darkened and corrupt, and you cannot have a full, genuine consciousness without a pure heart.&#39;, &#39;And how intrusive you are, how you insist and grimace!&#39;, &#39;Lies, lies, lies!&#34;&#39;, &#39;Of course I have myself made up all the things you say.&#39;, &#39;That, too, is from underground.&#39;, &#39;I have been for forty years listening to you through a crack under the floor.&#39;, &#39;I have invented them myself, there was nothing else I could invent.&#39;, &#39;It is no wonder that I have learned it by heart and it has taken a literary form....&#39;, &#39;But can you really be so credulous as to think that I will print all this and give it to you to read too?&#39;, &#39;And another problem: why do I call you &#34;gentlemen,&#34; why do I address you as though you really were my readers?&#39;, &#39;Such confessions as I intend to make are never printed nor given to other people to read.&#39;, &#34;Anyway, I am not strong-minded enough for that, and I don&#39;t see why I should be.&#34;, &#39;But you see a fancy has occurred to me and I want to realise it at all costs.&#39;, &#39;Let me explain.&#39;, &#39;Every man has reminiscences which he would not tell to everyone, but only to his friends.&#39;, &#39;He has other matters in his mind which he would not reveal even to his friends, but only to himself, and that in secret.&#39;, &#39;But there are other things which a man is afraid to tell even to himself, and every decent man has a number of such things stored away in his mind.&#39;, &#39;The more decent he is, the greater the number of such things in his mind.&#39;, &#39;Anyway, I have only lately determined to remember some of my early adventures.&#39;, &#39;Till now I have always avoided them, even with a certain uneasiness.&#39;, &#39;Now, when I am not only recalling them, but have actually decided to write an account of them, I want to try the experiment whether one can, even with oneself, be perfectly open and not take fright at the whole truth.&#39;, &#39;I will observe, in parenthesis, that Heine says that a true autobiography is almost an impossibility, and that man is bound to lie about himself.&#39;, &#39;He considers that Rousseau certainly told lies about himself in his confessions, and even intentionally lied, out of vanity.&#39;, &#39;I am convinced that Heine is right; I quite understand how sometimes one may, out of sheer vanity, attribute regular crimes to oneself, and indeed I can very well conceive that kind of vanity.&#39;, &#39;But Heine judged of people who made their confessions to the public.&#39;, &#39;I write only for myself, and I wish to declare once and for all that if I write as though I were addressing readers, that is simply because it is easier for me to write in that form.&#39;, &#39;It is a form, an empty form--I shall never have readers.&#39;, &#39;I have made this plain already ...&#39;, &#34;I don&#39;t wish to be hampered by any restrictions in the compilation of my notes.&#34;, &#39;I shall not attempt any system or method.&#39;, &#39;I will jot things down as I remember them.&#39;, &#34;But here, perhaps, someone will catch at the word and ask me: if you really don&#39;t reckon on readers, why do you make such compacts with yourself--and on paper too--that is, that you won&#39;t attempt any system or method, that you jot things down as you remember them, and so on, and so on?&#34;, &#39;Why are you explaining?&#39;, &#39;Why do you apologise?&#39;, &#39;Well, there it is, I answer.&#39;, &#39;There is a whole psychology in all this, though.&#39;, &#39;Perhaps it is simply that I am a coward.&#39;, &#39;And perhaps that I purposely imagine an audience before me in order that I may be more dignified while I write.&#39;, &#39;There are perhaps thousands of reasons.&#39;, &#39;Again, what is my object precisely in writing?&#39;, &#39;If it is not for the benefit of the public why should I not simply recall these incidents in my own mind without putting them on paper?&#39;, &#39;Quite so; but yet it is more imposing on paper.&#39;, &#39;There is something more impressive in it; I shall be better able to criticise myself and improve my style.&#39;, &#39;Besides, I shall perhaps obtain actual relief from writing.&#39;, &#39;Today, for instance, I am particularly oppressed by one memory of a distant past.&#39;, &#39;It came back vividly to my mind a few days ago, and has remained haunting me like an annoying tune that one cannot get rid of.&#39;, &#39;And yet I must get rid of it somehow.&#39;, &#39;I have hundreds of such reminiscences; but at times some one stands out from the hundred and oppresses me.&#39;, &#39;For some reason I believe that if I write it down I should get rid of it.&#39;, &#39;Why not try?&#39;, &#39;Besides, I am bored, and I never have anything to do.&#39;, &#39;Writing will be a sort of work.&#39;, &#39;They say work makes man kind-hearted and honest.&#39;, &#39;Well, here is a chance for me, anyway.&#39;, &#39;Snow is falling today, yellow and dingy.&#39;, &#39;It fell yesterday, too, and a few days ago.&#39;, &#39;I fancy it is the wet snow that has reminded me of that incident which I cannot shake off now.&#39;, &#39;And so let it be a story A PROPOS of the falling snow.&#39;, &#34;PART II A Propos of the Wet Snow When from dark error&#39;s subjugation My words of passionate exhortation Had wrenched thy fainting spirit free; And writhing prone in thine affliction Thou didst recall with malediction The vice that had encompassed thee: And when thy slumbering conscience, fretting By recollection&#39;s torturing flame, Thou didst reveal the hideous setting Of thy life&#39;s current ere I came: When suddenly I saw thee sicken, And weeping, hide thine anguished face, Revolted, maddened, horror-stricken, At memories of foul disgrace.&#34;, &#39;NEKRASSOV (translated by Juliet Soskice).&#39;, &#39;I AT THAT TIME I was only twenty-four.&#39;, &#39;My life was even then gloomy, ill-regulated, and as solitary as that of a savage.&#39;, &#39;I made friends with no one and positively avoided talking, and buried myself more and more in my hole.&#39;, &#39;At work in the office I never looked at anyone, and was perfectly well aware that my companions looked upon me, not only as a queer fellow, but even looked upon me--I always fancied this--with a sort of loathing.&#39;, &#39;I sometimes wondered why it was that nobody except me fancied that he was looked upon with aversion?&#39;, &#39;One of the clerks had a most repulsive, pock-marked face, which looked positively villainous.&#39;, &#39;I believe I should not have dared to look at anyone with such an unsightly countenance.&#39;, &#39;Another had such a very dirty old uniform that there was an unpleasant odour in his proximity.&#39;, &#39;Yet not one of these gentlemen showed the slightest self-consciousness--either about their clothes or their countenance or their character in any way.&#39;, &#39;Neither of them ever imagined that they were looked at with repulsion; if they had imagined it they would not have minded--so long as their superiors did not look at them in that way.&#39;, &#39;It is clear to me now that, owing to my unbounded vanity and to the high standard I set for myself, I often looked at myself with furious discontent, which verged on loathing, and so I inwardly attributed the same feeling to everyone.&#39;, &#39;I hated my face, for instance: I thought it disgusting, and even suspected that there was something base in my expression, and so every day when I turned up at the office I tried to behave as independently as possible, and to assume a lofty expression, so that I might not be suspected of being abject.&#39;, &#39;&#34;My face may be ugly,&#34; I thought, &#34;but let it be lofty, expressive, and, above all, EXTREMELY intelligent.&#34;&#39;, &#39;But I was positively and painfully certain that it was impossible for my countenance ever to express those qualities.&#39;, &#39;And what was worst of all, I thought it actually stupid looking, and I would have been quite satisfied if I could have looked intelligent.&#39;, &#39;In fact, I would even have put up with looking base if, at the same time, my face could have been thought strikingly intelligent.&#39;, &#39;Of course, I hated my fellow clerks one and all, and I despised them all, yet at the same time I was, as it were, afraid of them.&#39;, &#39;In fact, it happened at times that I thought more highly of them than of myself.&#39;, &#39;It somehow happened quite suddenly that I alternated between despising them and thinking them superior to myself.&#39;, &#39;A cultivated and decent man cannot be vain without setting a fearfully high standard for himself, and without despising and almost hating himself at certain moments.&#39;, &#39;But whether I despised them or thought them superior I dropped my eyes almost every time I met anyone.&#39;, &#34;I even made experiments whether I could face so and so&#39;s looking at me, and I was always the first to drop my eyes.&#34;, &#39;This worried me to distraction.&#39;, &#39;I had a sickly dread, too, of being ridiculous, and so had a slavish passion for the conventional in everything external.&#39;, &#39;I loved to fall into the common rut, and had a whole-hearted terror of any kind of eccentricity in myself.&#39;, &#39;But how could I live up to it?&#39;, &#39;I was morbidly sensitive as a man of our age should be.&#39;, &#39;They were all stupid, and as like one another as so many sheep.&#39;, &#39;Perhaps I was the only one in the office who fancied that I was a coward and a slave, and I fancied it just because I was more highly developed.&#39;, &#39;But it was not only that I fancied it, it really was so.&#39;, &#39;I was a coward and a slave.&#39;, &#39;I say this without the slightest embarrassment.&#39;, &#39;Every decent man of our age must be a coward and a slave.&#39;, &#39;That is his normal condition.&#39;, &#39;Of that I am firmly persuaded.&#39;, &#39;He is made and constructed to that very end.&#39;, &#39;And not only at the present time owing to some casual circumstances, but always, at all times, a decent man is bound to be a coward and a slave.&#39;, &#39;It is the law of nature for all decent people all over the earth.&#39;, &#39;If anyone of them happens to be valiant about something, he need not be comforted nor carried away by that; he would show the white feather just the same before something else.&#39;, &#39;That is how it invariably and inevitably ends.&#39;, &#39;Only donkeys and mules are valiant, and they only till they are pushed up to the wall.&#39;, &#39;It is not worth while to pay attention to them for they really are of no consequence.&#39;, &#39;Another circumstance, too, worried me in those days: that there was no one like me and I was unlike anyone else.&#39;, &#39;&#34;I am alone and they are EVERYONE,&#34; I thought--and pondered.&#39;, &#39;From that it is evident that I was still a youngster.&#39;, &#39;The very opposite sometimes happened.&#39;, &#39;It was loathsome sometimes to go to the office; things reached such a point that I often came home ill.&#39;, &#39;But all at once, A PROPOS of nothing, there would come a phase of scepticism and indifference (everything happened in phases to me), and I would laugh myself at my intolerance and fastidiousness, I would reproach myself with being ROMANTIC.&#39;, &#39;At one time I was unwilling to speak to anyone, while at other times I would not only talk, but go to the length of contemplating making friends with them.&#39;, &#39;All my fastidiousness would suddenly, for no rhyme or reason, vanish.&#39;, &#39;Who knows, perhaps I never had really had it, and it had simply been affected, and got out of books.&#39;, &#39;I have not decided that question even now.&#39;, &#39;Once I quite made friends with them, visited their homes, played preference, drank vodka, talked of promotions....&#39;, &#39;But here let me make a digression.&#39;, &#39;We Russians, speaking generally, have never had those foolish transcendental &#34;romantics&#34;--German, and still more French--on whom nothing produces any effect; if there were an earthquake, if all France perished at the barricades, they would still be the same, they would not even have the decency to affect a change, but would still go on singing their transcendental songs to the hour of their death, because they are fools.&#39;, &#39;We, in Russia, have no fools; that is well known.&#39;, &#39;That is what distinguishes us from foreign lands.&#39;, &#39;Consequently these transcendental natures are not found amongst us in their pure form.&#39;, &#39;The idea that they are is due to our &#34;realistic&#34; journalists and critics of that day, always on the look out for Kostanzhoglos and Uncle Pyotr Ivanitchs and foolishly accepting them as our ideal; they have slandered our romantics, taking them for the same transcendental sort as in Germany or France.&#39;, &#39;On the contrary, the characteristics of our &#34;romantics&#34; are absolutely and directly opposed to the transcendental European type, and no European standard can be applied to them.&#39;, &#39;(Allow me to make use of this word &#34;romantic&#34;--an old-fashioned and much respected word which has done good service and is familiar to all.)&#39;, &#39;The characteristics of our romantic are to understand everything, TO SEE EVERYTHING AND TO SEE IT OFTEN INCOMPARABLY MORE CLEARLY THAN OUR MOST REALISTIC MINDS SEE IT; to refuse to accept anyone or anything, but at the same time not to despise anything; to give way, to yield, from policy; never to lose sight of a useful practical object (such as rent-free quarters at the government expense, pensions, decorations), to keep their eye on that object through all the enthusiasms and volumes of lyrical poems, and at the same time to preserve &#34;the sublime and the beautiful&#34; inviolate within them to the hour of their death, and to preserve themselves also, incidentally, like some precious jewel wrapped in cotton wool if only for the benefit of &#34;the sublime and the beautiful.&#34;&#39;, &#39;Our &#34;romantic&#34; is a man of great breadth and the greatest rogue of all our rogues, I assure you....&#39;, &#39;I can assure you from experience, indeed.&#39;, &#39;Of course, that is, if he is intelligent.&#39;, &#39;But what am I saying!&#39;, &#34;The romantic is always intelligent, and I only meant to observe that although we have had foolish romantics they don&#39;t count, and they were only so because in the flower of their youth they degenerated into Germans, and to preserve their precious jewel more comfortably, settled somewhere out there--by preference in Weimar or the Black Forest.&#34;, &#39;I, for instance, genuinely despised my official work and did not openly abuse it simply because I was in it myself and got a salary for it.&#39;, &#39;Anyway, take note, I did not openly abuse it.&#39;, &#39;Our romantic would rather go out of his mind--a thing, however, which very rarely happens--than take to open abuse, unless he had some other career in view; and he is never kicked out.&#39;, &#39;At most, they would take him to the lunatic asylum as &#34;the King of Spain&#34; if he should go very mad.&#39;, &#39;But it is only the thin, fair people who go out of their minds in Russia.&#39;, &#39;Innumerable &#34;romantics&#34; attain later in life to considerable rank in the service.&#39;, &#39;Their many-sidedness is remarkable!&#39;, &#39;And what a faculty they have for the most contradictory sensations!&#39;, &#39;I was comforted by this thought even in those days, and I am of the same opinion now.&#39;, &#39;That is why there are so many &#34;broad natures&#34; among us who never lose their ideal even in the depths of degradation; and though they never stir a finger for their ideal, though they are arrant thieves and knaves, yet they tearfully cherish their first ideal and are extraordinarily honest at heart.&#39;, &#39;Yes, it is only among us that the most incorrigible rogue can be absolutely and loftily honest at heart without in the least ceasing to be a rogue.&#39;, &#39;I repeat, our romantics, frequently, become such accomplished rascals (I use the term &#34;rascals&#34; affectionately), suddenly display such a sense of reality and practical knowledge that their bewildered superiors and the public generally can only ejaculate in amazement.&#39;, &#39;Their many-sidedness is really amazing, and goodness knows what it may develop into later on, and what the future has in store for us.&#39;, &#39;It is not a poor material!&#39;, &#39;I do not say this from any foolish or boastful patriotism.&#39;, &#39;But I feel sure that you are again imagining that I am joking.&#39;, &#34;Or perhaps it&#39;s just the contrary and you are convinced that I really think so.&#34;, &#39;Anyway, gentlemen, I shall welcome both views as an honour and a special favour.&#39;, &#39;And do forgive my digression.&#39;, &#39;I did not, of course, maintain friendly relations with my comrades and soon was at loggerheads with them, and in my youth and inexperience I even gave up bowing to them, as though I had cut off all relations.&#39;, &#39;That, however, only happened to me once.&#39;, &#39;As a rule, I was always alone.&#39;, &#39;In the first place I spent most of my time at home, reading.&#39;, &#39;I tried to stifle all that was continually seething within me by means of external impressions.&#39;, &#39;And the only external means I had was reading.&#39;, &#39;Reading, of course, was a great help--exciting me, giving me pleasure and pain.&#39;, &#39;But at times it bored me fearfully.&#39;, &#39;One longed for movement in spite of everything, and I plunged all at once into dark, underground, loathsome vice of the pettiest kind.&#39;, &#39;My wretched passions were acute, smarting, from my continual, sickly irritability I had hysterical impulses, with tears and convulsions.&#39;, &#39;I had no resource except reading, that is, there was nothing in my surroundings which I could respect and which attracted me.&#39;, &#39;I was overwhelmed with depression, too; I had an hysterical craving for incongruity and for contrast, and so I took to vice.&#39;, &#39;I have not said all this to justify myself....&#39;, &#39;But, no!&#39;, &#39;I am lying.&#39;, &#39;I did want to justify myself.&#39;, &#39;I make that little observation for my own benefit, gentlemen.&#39;, &#34;I don&#39;t want to lie.&#34;, &#39;I vowed to myself I would not.&#39;, &#39;And so, furtively, timidly, in solitude, at night, I indulged in filthy vice, with a feeling of shame which never deserted me, even at the most loathsome moments, and which at such moments nearly made me curse.&#39;, &#39;Already even then I had my underground world in my soul.&#39;, &#39;I was fearfully afraid of being seen, of being met, of being recognised.&#39;, &#39;I visited various obscure haunts.&#39;, &#39;One night as I was passing a tavern I saw through a lighted window some gentlemen fighting with billiard cues, and saw one of them thrown out of the window.&#39;, &#39;At other times I should have felt very much disgusted, but I was in such a mood at the time, that I actually envied the gentleman thrown out of the window--and I envied him so much that I even went into the tavern and into the billiard-room.&#39;, &#39;&#34;Perhaps,&#34; I thought, &#34;I &#39;ll have a fight, too, and they &#39;ll throw me out of the window.&#34;&#39;, &#39;I was not drunk--but what is one to do--depression will drive a man to such a pitch of hysteria?&#39;, &#39;But nothing happened.&#39;, &#39;It seemed that I was not even equal to being thrown out of the window and I went away without having my fight.&#39;, &#39;An officer put me in my place from the first moment.&#39;, &#39;I was standing by the billiard-table and in my ignorance blocking up the way, and he wanted to pass; he took me by the shoulders and without a word--without a warning or explanation--moved me from where I was standing to another spot and passed by as though he had not noticed me.&#39;, &#39;I could have forgiven blows, but I could not forgive his having moved me without noticing me.&#39;, &#39;Devil knows what I would have given for a real regular quarrel--a more decent, a more LITERARY one, so to speak.&#39;, &#39;I had been treated like a fly.&#39;, &#39;This officer was over six foot, while I was a spindly little fellow.&#39;, &#39;But the quarrel was in my hands.&#39;, &#39;I had only to protest and I certainly would have been thrown out of the window.&#39;, &#39;But I changed my mind and preferred to beat a resentful retreat.&#39;, &#39;I went out of the tavern straight home, confused and troubled, and the next night I went out again with the same lewd intentions, still more furtively, abjectly and miserably than before, as it were, with tears in my eyes--but still I did go out again.&#39;, &#34;Don&#39;t imagine, though, it was cowardice made me slink away from the officer; I never have been a coward at heart, though I have always been a coward in action.&#34;, &#34;Don&#39;t be in a hurry to laugh--I assure you I can explain it all.&#34;, &#39;Oh, if only that officer had been one of the sort who would consent to fight a duel!&#39;, &#39;But no, he was one of those gentlemen (alas, long extinct!)&#39;, &#34;who preferred fighting with cues or, like Gogol&#39;s Lieutenant Pirogov, appealing to the police.&#34;, &#39;They did not fight duels and would have thought a duel with a civilian like me an utterly unseemly procedure in any case--and they looked upon the duel altogether as something impossible, something free-thinking and French.&#39;, &#39;But they were quite ready to bully, especially when they were over six foot.&#39;, &#39;I did not slink away through cowardice, but through an unbounded vanity.&#39;, &#39;I was afraid not of his six foot, not of getting a sound thrashing and being thrown out of the window; I should have had physical courage enough, I assure you; but I had not the moral courage.&#39;, &#39;What I was afraid of was that everyone present, from the insolent marker down to the lowest little stinking, pimply clerk in a greasy collar, would jeer at me and fail to understand when I began to protest and to address them in literary language.&#39;, &#34;For of the point of honour--not of honour, but of the point of honour (POINT D&#39;HONNEUR)--one cannot speak among us except in literary language.&#34;, &#39;You can &#39;t allude to the &#34;point of honour&#34; in ordinary language.&#39;, &#39;I was fully convinced (the sense of reality, in spite of all my romanticism!)&#39;, &#39;that they would all simply split their sides with laughter, and that the officer would not simply beat me, that is, without insulting me, but would certainly prod me in the back with his knee, kick me round the billiard-table, and only then perhaps have pity and drop me out of the window.&#39;, &#39;Of course, this trivial incident could not with me end in that.&#39;, &#39;I often met that officer afterwards in the street and noticed him very carefully.&#39;, &#39;I am not quite sure whether he recognised me, I imagine not; I judge from certain signs.&#39;, &#39;But I--I stared at him with spite and hatred and so it went on ... for several years!&#39;, &#39;My resentment grew even deeper with years.&#39;, &#39;At first I began making stealthy inquiries about this officer.&#39;, &#39;It was difficult for me to do so, for I knew no one.&#39;, &#39;But one day I heard someone shout his surname in the street as I was following him at a distance, as though I were tied to him--and so I learnt his surname.&#39;, &#39;Another time I followed him to his flat, and for ten kopecks learned from the porter where he lived, on which storey, whether he lived alone or with others, and so on--in fact, everything one could learn from a porter.&#39;, &#39;One morning, though I had never tried my hand with the pen, it suddenly occurred to me to write a satire on this officer in the form of a novel which would unmask his villainy.&#39;, &#39;I wrote the novel with relish.&#39;, &#39;I did unmask his villainy, I even exaggerated it; at first I so altered his surname that it could easily be recognised, but on second thoughts I changed it, and sent the story to the OTETCHESTVENNIYA ZAPISKI.&#39;, &#39;But at that time such attacks were not the fashion and my story was not printed.&#39;, &#39;That was a great vexation to me.&#39;, &#39;Sometimes I was positively choked with resentment.&#39;, &#39;At last I determined to challenge my enemy to a duel.&#39;, &#39;I composed a splendid, charming letter to him, imploring him to apologise to me, and hinting rather plainly at a duel in case of refusal.&#39;, &#39;The letter was so composed that if the officer had had the least understanding of the sublime and the beautiful he would certainly have flung himself on my neck and have offered me his friendship.&#39;, &#39;And how fine that would have been!&#39;, &#39;How we should have got on together!&#39;, &#39;&#34;He could have shielded me with his higher rank, while I could have improved his mind with my culture, and, well ... my ideas, and all sorts of things might have happened.&#34;&#39;, &#39;Only fancy, this was two years after his insult to me, and my challenge would have been a ridiculous anachronism, in spite of all the ingenuity of my letter in disguising and explaining away the anachronism.&#39;, &#39;But, thank God (to this day I thank the Almighty with tears in my eyes) I did not send the letter to him.&#39;, &#39;Cold shivers run down my back when I think of what might have happened if I had sent it.&#39;, &#39;And all at once I revenged myself in the simplest way, by a stroke of genius!&#39;, &#39;A brilliant thought suddenly dawned upon me.&#39;, &#34;Sometimes on holidays I used to stroll along the sunny side of the Nevsky about four o&#39;clock in the afternoon.&#34;, &#39;Though it was hardly a stroll so much as a series of innumerable miseries, humiliations and resentments; but no doubt that was just what I wanted.&#39;, &#39;I used to wriggle along in a most unseemly fashion, like an eel, continually moving aside to make way for generals, for officers of the guards and the hussars, or for ladies.&#39;, &#39;At such minutes there used to be a convulsive twinge at my heart, and I used to feel hot all down my back at the mere thought of the wretchedness of my attire, of the wretchedness and abjectness of my little scurrying figure.&#39;, &#39;This was a regular martyrdom, a continual, intolerable humiliation at the thought, which passed into an incessant and direct sensation, that I was a mere fly in the eyes of all this world, a nasty, disgusting fly--more intelligent, more highly developed, more refined in feeling than any of them, of course--but a fly that was continually making way for everyone, insulted and injured by everyone.&#39;, &#34;Why I inflicted this torture upon myself, why I went to the Nevsky, I don&#39;t know.&#34;, &#39;I felt simply drawn there at every possible opportunity.&#39;, &#39;Already then I began to experience a rush of the enjoyment of which I spoke in the first chapter.&#39;, &#39;After my affair with the officer I felt even more drawn there than before: it was on the Nevsky that I met him most frequently, there I could admire him.&#39;, &#39;He, too, went there chiefly on holidays, He, too, turned out of his path for generals and persons of high rank, and he too, wriggled between them like an eel; but people, like me, or even better dressed than me, he simply walked over; he made straight for them as though there was nothing but empty space before him, and never, under any circumstances, turned aside.&#39;, &#39;I gloated over my resentment watching him and ... always resentfully made way for him.&#39;, &#39;It exasperated me that even in the street I could not be on an even footing with him.&#39;, &#39;&#34;Why must you invariably be the first to move aside?&#34;&#39;, &#34;I kept asking myself in hysterical rage, waking up sometimes at three o&#39;clock in the morning.&#34;, &#39;&#34;Why is it you and not he?&#39;, &#34;There&#39;s no regulation about it; there&#39;s no written law.&#34;, &#39;Let the making way be equal as it usually is when refined people meet; he moves half-way and you move half-way; you pass with mutual respect.&#34;&#39;, &#39;But that never happened, and I always moved aside, while he did not even notice my making way for him.&#39;, &#39;And lo and behold a bright idea dawned upon me!&#39;, &#39;&#34;What,&#34; I thought, &#34;if I meet him and don &#39;t move on one side?&#39;, &#34;What if I don&#39;t move aside on purpose, even if I knock up against him?&#34;, &#39;How would that be?&#34;&#39;, &#39;This audacious idea took such a hold on me that it gave me no peace.&#39;, &#39;I was dreaming of it continually, horribly, and I purposely went more frequently to the Nevsky in order to picture more vividly how I should do it when I did do it.&#39;, &#39;I was delighted.&#39;, &#39;This intention seemed to me more and more practical and possible.&#39;, &#39;&#34;Of course I shall not really push him,&#34; I thought, already more good-natured in my joy.&#39;, &#39;&#34;I will simply not turn aside, will run up against him, not very violently, but just shouldering each other--just as much as decency permits.&#39;, &#39;I will push against him just as much as he pushes against me.&#34;&#39;, &#39;At last I made up my mind completely.&#39;, &#39;But my preparations took a great deal of time.&#39;, &#39;To begin with, when I carried out my plan I should need to be looking rather more decent, and so I had to think of my get-up.&#39;, &#39;&#34;In case of emergency, if, for instance, there were any sort of public scandal (and the public there is of the most RECHERCHE: the Countess walks there; Prince D. walks there; all the literary world is there), I must be well dressed; that inspires respect and of itself puts us on an equal footing in the eyes of the society.&#34;&#39;, &#34;With this object I asked for some of my salary in advance, and bought at Tchurkin&#39;s a pair of black gloves and a decent hat.&#34;, &#39;Black gloves seemed to me both more dignified and BON TON than the lemon-coloured ones which I had contemplated at first.&#39;, &#39;&#34;The colour is too gaudy, it looks as though one were trying to be conspicuous,&#34; and I did not take the lemon-coloured ones.&#39;, &#39;I had got ready long beforehand a good shirt, with white bone studs; my overcoat was the only thing that held me back.&#39;, &#39;The coat in itself was a very good one, it kept me warm; but it was wadded and it had a raccoon collar which was the height of vulgarity.&#39;, &#34;I had to change the collar at any sacrifice, and to have a beaver one like an officer&#39;s.&#34;, &#39;For this purpose I began visiting the Gostiny Dvor and after several attempts I pitched upon a piece of cheap German beaver.&#39;, &#39;Though these German beavers soon grow shabby and look wretched, yet at first they look exceedingly well, and I only needed it for the occasion.&#39;, &#39;I asked the price; even so, it was too expensive.&#39;, &#39;After thinking it over thoroughly I decided to sell my raccoon collar.&#39;, &#39;The rest of the money--a considerable sum for me, I decided to borrow from Anton Antonitch Syetotchkin, my immediate superior, an unassuming person, though grave and judicious.&#39;, &#39;He never lent money to anyone, but I had, on entering the service, been specially recommended to him by an important personage who had got me my berth.&#39;, &#39;I was horribly worried.&#39;, &#39;To borrow from Anton Antonitch seemed to me monstrous and shameful.&#39;, &#39;I did not sleep for two or three nights.&#39;, &#39;Indeed, I did not sleep well at that time, I was in a fever; I had a vague sinking at my heart or else a sudden throbbing, throbbing, throbbing!&#39;, &#39;Anton Antonitch was surprised at first, then he frowned, then he reflected, and did after all lend me the money, receiving from me a written authorisation to take from my salary a fortnight later the sum that he had lent me.&#39;, &#39;In this way everything was at last ready.&#39;, &#39;The handsome beaver replaced the mean-looking raccoon, and I began by degrees to get to work.&#39;, &#39;It would never have done to act offhand, at random; the plan had to be carried out skilfully, by degrees.&#39;, &#39;But I must confess that after many efforts I began to despair: we simply could not run into each other.&#39;, &#39;I made every preparation, I was quite determined--it seemed as though we should run into one another directly--and before I knew what I was doing I had stepped aside for him again and he had passed without noticing me.&#39;, &#39;I even prayed as I approached him that God would grant me determination.&#39;, &#39;One time I had made up my mind thoroughly, but it ended in my stumbling and falling at his feet because at the very last instant when I was six inches from him my courage failed me.&#39;, &#39;He very calmly stepped over me, while I flew on one side like a ball.&#39;, &#39;That night I was ill again, feverish and delirious.&#39;, &#39;And suddenly it ended most happily.&#39;, &#39;The night before I had made up my mind not to carry out my fatal plan and to abandon it all, and with that object I went to the Nevsky for the last time, just to see how I would abandon it all.&#39;, &#39;Suddenly, three paces from my enemy, I unexpectedly made up my mind--I closed my eyes, and we ran full tilt, shoulder to shoulder, against one another!&#39;, &#39;I did not budge an inch and passed him on a perfectly equal footing!&#39;, &#39;He did not even look round and pretended not to notice it; but he was only pretending, I am convinced of that.&#39;, &#39;I am convinced of that to this day!&#39;, &#39;Of course, I got the worst of it--he was stronger, but that was not the point.&#39;, &#39;The point was that I had attained my object, I had kept up my dignity, I had not yielded a step, and had put myself publicly on an equal social footing with him.&#39;, &#39;I returned home feeling that I was fully avenged for everything.&#39;, &#39;I was delighted.&#39;, &#39;I was triumphant and sang Italian arias.&#39;, &#39;Of course, I will not describe to you what happened to me three days later; if you have read my first chapter you can guess for yourself.&#39;, &#39;The officer was afterwards transferred; I have not seen him now for fourteen years.&#39;, &#39;What is the dear fellow doing now?&#39;, &#39;Whom is he walking over?&#39;, &#39;II But the period of my dissipation would end and I always felt very sick afterwards.&#39;, &#39;It was followed by remorse--I tried to drive it away; I felt too sick.&#39;, &#39;By degrees, however, I grew used to that too.&#39;, &#39;I grew used to everything, or rather I voluntarily resigned myself to enduring it.&#39;, &#39;But I had a means of escape that reconciled everything--that was to find refuge in &#34;the sublime and the beautiful,&#34; in dreams, of course.&#39;, &#39;I was a terrible dreamer, I would dream for three months on end, tucked away in my corner, and you may believe me that at those moments I had no resemblance to the gentleman who, in the perturbation of his chicken heart, put a collar of German beaver on his great-coat.&#39;, &#39;I suddenly became a hero.&#39;, &#39;I would not have admitted my six-foot lieutenant even if he had called on me.&#39;, &#39;I could not even picture him before me then.&#39;, &#39;What were my dreams and how I could satisfy myself with them--it is hard to say now, but at the time I was satisfied with them.&#39;, &#39;Though, indeed, even now, I am to some extent satisfied with them.&#39;, &#39;Dreams were particularly sweet and vivid after a spell of dissipation; they came with remorse and with tears, with curses and transports.&#39;, &#39;There were moments of such positive intoxication, of such happiness, that there was not the faintest trace of irony within me, on my honour.&#39;, &#39;I had faith, hope, love.&#39;, &#39;I believed blindly at such times that by some miracle, by some external circumstance, all this would suddenly open out, expand; that suddenly a vista of suitable activity--beneficent, good, and, above all, READY MADE (what sort of activity I had no idea, but the great thing was that it should be all ready for me)--would rise up before me--and I should come out into the light of day, almost riding a white horse and crowned with laurel.&#39;, &#39;Anything but the foremost place I could not conceive for myself, and for that very reason I quite contentedly occupied the lowest in reality.&#39;, &#39;Either to be a hero or to grovel in the mud--there was nothing between.&#39;, &#39;That was my ruin, for when I was in the mud I comforted myself with the thought that at other times I was a hero, and the hero was a cloak for the mud: for an ordinary man it was shameful to defile himself, but a hero was too lofty to be utterly defiled, and so he might defile himself.&#39;, &#39;It is worth noting that these attacks of the &#34;sublime and the beautiful&#34; visited me even during the period of dissipation and just at the times when I was touching the bottom.&#39;, &#39;They came in separate spurts, as though reminding me of themselves, but did not banish the dissipation by their appearance.&#39;, &#39;On the contrary, they seemed to add a zest to it by contrast, and were only sufficiently present to serve as an appetising sauce.&#39;, &#39;That sauce was made up of contradictions and sufferings, of agonising inward analysis, and all these pangs and pin-pricks gave a certain piquancy, even a significance to my dissipation--in fact, completely answered the purpose of an appetising sauce.&#39;, &#39;There was a certain depth of meaning in it.&#39;, &#39;And I could hardly have resigned myself to the simple, vulgar, direct debauchery of a clerk and have endured all the filthiness of it.&#39;, &#39;What could have allured me about it then and have drawn me at night into the street?&#39;, &#39;No, I had a lofty way of getting out of it all.&#39;, &#39;And what loving-kindness, oh Lord, what loving-kindness I felt at times in those dreams of mine!&#39;, &#39;in those &#34;flights into the sublime and the beautiful&#34;; though it was fantastic love, though it was never applied to anything human in reality, yet there was so much of this love that one did not feel afterwards even the impulse to apply it in reality; that would have been superfluous.&#39;, &#39;Everything, however, passed satisfactorily by a lazy and fascinating transition into the sphere of art, that is, into the beautiful forms of life, lying ready, largely stolen from the poets and novelists and adapted to all sorts of needs and uses.&#39;, &#39;I, for instance, was triumphant over everyone; everyone, of course, was in dust and ashes, and was forced spontaneously to recognise my superiority, and I forgave them all.&#39;, &#39;I was a poet and a grand gentleman, I fell in love; I came in for countless millions and immediately devoted them to humanity, and at the same time I confessed before all the people my shameful deeds, which, of course, were not merely shameful, but had in them much that was &#34;sublime and beautiful&#34; something in the Manfred style.&#39;, &#39;Everyone would kiss me and weep (what idiots they would be if they did not), while I should go barefoot and hungry preaching new ideas and fighting a victorious Austerlitz against the obscurantists.&#39;, &#39;Then the band would play a march, an amnesty would be declared, the Pope would agree to retire from Rome to Brazil; then there would be a ball for the whole of Italy at the Villa Borghese on the shores of Lake Como, Lake Como being for that purpose transferred to the neighbourhood of Rome; then would come a scene in the bushes, and so on, and so on--as though you did not know all about it?&#39;, &#39;You will say that it is vulgar and contemptible to drag all this into public after all the tears and transports which I have myself confessed.&#39;, &#39;But why is it contemptible?&#39;, &#39;Can you imagine that I am ashamed of it all, and that it was stupider than anything in your life, gentlemen?&#39;, &#39;And I can assure you that some of these fancies were by no means badly composed....&#39;, &#39;It did not all happen on the shores of Lake Como.&#39;, &#39;And yet you are right--it really is vulgar and contemptible.&#39;, &#39;And most contemptible of all it is that now I am attempting to justify myself to you.&#39;, &#39;And even more contemptible than that is my making this remark now.&#39;, &#34;But that&#39;s enough, or there will be no end to it; each step will be more contemptible than the last....&#34;, &#39;I could never stand more than three months of dreaming at a time without feeling an irresistible desire to plunge into society.&#39;, &#39;To plunge into society meant to visit my superior at the office, Anton Antonitch Syetotchkin.&#39;, &#39;He was the only permanent acquaintance I have had in my life, and I wonder at the fact myself now.&#39;, &#39;But I only went to see him when that phase came over me, and when my dreams had reached such a point of bliss that it became essential at once to embrace my fellows and all mankind; and for that purpose I needed, at least, one human being, actually existing.&#39;, &#39;I had to call on Anton Antonitch, however, on Tuesday--his at-home day; so I had always to time my passionate desire to embrace humanity so that it might fall on a Tuesday.&#39;, &#39;This Anton Antonitch lived on the fourth storey in a house in Five Corners, in four low-pitched rooms, one smaller than the other, of a particularly frugal and sallow appearance.&#39;, &#39;He had two daughters and their aunt, who used to pour out the tea.&#39;, &#39;Of the daughters one was thirteen and another fourteen, they both had snub noses, and I was awfully shy of them because they were always whispering and giggling together.&#39;, &#39;The master of the house usually sat in his study on a leather couch in front of the table with some grey-headed gentleman, usually a colleague from our office or some other department.&#39;, &#39;I never saw more than two or three visitors there, always the same.&#39;, &#39;They talked about the excise duty; about business in the senate, about salaries, about promotions, about His Excellency, and the best means of pleasing him, and so on.&#39;, &#39;I had the patience to sit like a fool beside these people for four hours at a stretch, listening to them without knowing what to say to them or venturing to say a word.&#39;, &#39;I became stupefied, several times I felt myself perspiring, I was overcome by a sort of paralysis; but this was pleasant and good for me.&#39;, &#39;On returning home I deferred for a time my desire to embrace all mankind.&#39;, &#39;I had however one other acquaintance of a sort, Simonov, who was an old schoolfellow.&#39;, &#39;I had a number of schoolfellows, indeed, in Petersburg, but I did not associate with them and had even given up nodding to them in the street.&#39;, &#39;I believe I had transferred into the department I was in simply to avoid their company and to cut off all connection with my hateful childhood.&#39;, &#39;Curses on that school and all those terrible years of penal servitude!&#39;, &#39;In short, I parted from my schoolfellows as soon as I got out into the world.&#39;, &#39;There were two or three left to whom I nodded in the street.&#39;, &#34;One of them was Simonov, who had in no way been distinguished at school, was of a quiet and equable disposition; but I discovered in him a certain independence of character and even honesty I don&#39;t even suppose that he was particularly stupid.&#34;, &#39;I had at one time spent some rather soulful moments with him, but these had not lasted long and had somehow been suddenly clouded over.&#39;, &#39;He was evidently uncomfortable at these reminiscences, and was, I fancy, always afraid that I might take up the same tone again.&#39;, &#39;I suspected that he had an aversion for me, but still I went on going to see him, not being quite certain of it.&#39;, &#34;And so on one occasion, unable to endure my solitude and knowing that as it was Thursday Anton Antonitch&#39;s door would be closed, I thought of Simonov.&#34;, &#39;Climbing up to his fourth storey I was thinking that the man disliked me and that it was a mistake to go and see him.&#39;, &#39;But as it always happened that such reflections impelled me, as though purposely, to put myself into a false position, I went in.&#39;, &#39;It was almost a year since I had last seen Simonov.&#39;, &#39;III I found two of my old schoolfellows with him.&#39;, &#39;They seemed to be discussing an important matter.&#39;, &#39;All of them took scarcely any notice of my entrance, which was strange, for I had not met them for years.&#39;, &#39;Evidently they looked upon me as something on the level of a common fly.&#39;, &#39;I had not been treated like that even at school, though they all hated me.&#39;, &#39;I knew, of course, that they must despise me now for my lack of success in the service, and for my having let myself sink so low, going about badly dressed and so on--which seemed to them a sign of my incapacity and insignificance.&#39;, &#39;But I had not expected such contempt.&#39;, &#39;Simonov was positively surprised at my turning up.&#39;, &#39;Even in old days he had always seemed surprised at my coming.&#39;, &#39;All this disconcerted me: I sat down, feeling rather miserable, and began listening to what they were saying.&#39;, &#39;They were engaged in warm and earnest conversation about a farewell dinner which they wanted to arrange for the next day to a comrade of theirs called Zverkov, an officer in the army, who was going away to a distant province.&#39;, &#39;This Zverkov had been all the time at school with me too.&#39;, &#39;I had begun to hate him particularly in the upper forms.&#39;, &#39;In the lower forms he had simply been a pretty, playful boy whom everybody liked.&#39;, &#39;I had hated him, however, even in the lower forms, just because he was a pretty and playful boy.&#39;, &#39;He was always bad at his lessons and got worse and worse as he went on; however, he left with a good certificate, as he had powerful interests.&#39;, &#39;During his last year at school he came in for an estate of two hundred serfs, and as almost all of us were poor he took up a swaggering tone among us.&#39;, &#39;He was vulgar in the extreme, but at the same time he was a good-natured fellow, even in his swaggering.&#39;, &#39;In spite of superficial, fantastic and sham notions of honour and dignity, all but very few of us positively grovelled before Zverkov, and the more so the more he swaggered.&#39;, &#39;And it was not from any interested motive that they grovelled, but simply because he had been favoured by the gifts of nature.&#39;, &#39;Moreover, it was, as it were, an accepted idea among us that Zverkov was a specialist in regard to tact and the social graces.&#39;, &#39;This last fact particularly infuriated me.&#39;, &#39;I hated the abrupt self-confident tone of his voice, his admiration of his own witticisms, which were often frightfully stupid, though he was bold in his language; I hated his handsome, but stupid face (for which I would, however, have gladly exchanged my intelligent one), and the free-and-easy military manners in fashion in the &#34; &#39;forties.&#34;&#39;, &#39;I hated the way in which he used to talk of his future conquests of women (he did not venture to begin his attack upon women until he had the epaulettes of an officer, and was looking forward to them with impatience), and boasted of the duels he would constantly be fighting.&#39;, &#39;I remember how I, invariably so taciturn, suddenly fastened upon Zverkov, when one day talking at a leisure moment with his schoolfellows of his future relations with the fair sex, and growing as sportive as a puppy in the sun, he all at once declared that he would not leave a single village girl on his estate unnoticed, that that was his DROIT DE SEIGNEUR, and that if the peasants dared to protest he would have them all flogged and double the tax on them, the bearded rascals.&#39;, &#39;Our servile rabble applauded, but I attacked him, not from compassion for the girls and their fathers, but simply because they were applauding such an insect.&#39;, &#39;I got the better of him on that occasion, but though Zverkov was stupid he was lively and impudent, and so laughed it off, and in such a way that my victory was not really complete; the laugh was on his side.&#39;, &#39;He got the better of me on several occasions afterwards, but without malice, jestingly, casually.&#39;, &#39;I remained angrily and contemptuously silent and would not answer him.&#39;, &#39;When we left school he made advances to me; I did not rebuff them, for I was flattered, but we soon parted and quite naturally.&#39;, &#39;Afterwards I heard of his barrack-room success as a lieutenant, and of the fast life he was leading.&#39;, &#39;Then there came other rumours--of his successes in the service.&#39;, &#39;By then he had taken to cutting me in the street, and I suspected that he was afraid of compromising himself by greeting a personage as insignificant as me.&#39;, &#39;I saw him once in the theatre, in the third tier of boxes.&#39;, &#39;By then he was wearing shoulder-straps.&#39;, &#39;He was twisting and twirling about, ingratiating himself with the daughters of an ancient General.&#39;, &#39;In three years he had gone off considerably, though he was still rather handsome and adroit.&#39;, &#39;One could see that by the time he was thirty he would be corpulent.&#39;, &#39;So it was to this Zverkov that my schoolfellows were going to give a dinner on his departure.&#39;, &#39;They had kept up with him for those three years, though privately they did not consider themselves on an equal footing with him, I am convinced of that.&#39;, &#34;Of Simonov&#39;s two visitors, one was Ferfitchkin, a Russianised German--a little fellow with the face of a monkey, a blockhead who was always deriding everyone, a very bitter enemy of mine from our days in the lower forms--a vulgar, impudent, swaggering fellow, who affected a most sensitive feeling of personal honour, though, of course, he was a wretched little coward at heart.&#34;, &#39;He was one of those worshippers of Zverkov who made up to the latter from interested motives, and often borrowed money from him.&#39;, &#34;Simonov&#39;s other visitor, Trudolyubov, was a person in no way remarkable--a tall young fellow, in the army, with a cold face, fairly honest, though he worshipped success of every sort, and was only capable of thinking of promotion.&#34;, &#34;He was some sort of distant relation of Zverkov&#39;s, and this, foolish as it seems, gave him a certain importance among us.&#34;, &#39;He always thought me of no consequence whatever; his behaviour to me, though not quite courteous, was tolerable.&#39;, &#39;&#34;Well, with seven roubles each,&#34; said Trudolyubov, &#34;twenty-one roubles between the three of us, we ought to be able to get a good dinner.&#39;, &#39;Zverkov, of course, won &#39;t pay.&#34;&#39;, &#39;&#34;Of course not, since we are inviting him,&#34; Simonov decided.&#39;, &#39;&#34;Can you imagine,&#34; Ferfitchkin interrupted hotly and conceitedly, like some insolent flunkey boasting of his master the General &#39;s decorations, &#34;can you imagine that Zverkov will let us pay alone?&#39;, &#39;He will accept from delicacy, but he will order half a dozen bottles of champagne.&#34;&#39;, &#39;&#34;Do we want half a dozen for the four of us?&#34;&#39;, &#39;observed Trudolyubov, taking notice only of the half dozen.&#39;, &#39;&#34;So the three of us, with Zverkov for the fourth, twenty-one roubles, at the Hotel de Paris at five o &#39;clock tomorrow,&#34; Simonov, who had been asked to make the arrangements, concluded finally.&#39;, &#39;&#34;How twenty-one roubles?&#34;&#39;, &#39;I asked in some agitation, with a show of being offended; &#34;if you count me it will not be twenty-one, but twenty-eight roubles.&#34;&#39;, &#39;It seemed to me that to invite myself so suddenly and unexpectedly would be positively graceful, and that they would all be conquered at once and would look at me with respect.&#39;, &#39;&#34;Do you want to join, too?&#34;&#39;, &#39;Simonov observed, with no appearance of pleasure, seeming to avoid looking at me.&#39;, &#39;He knew me through and through.&#39;, &#39;It infuriated me that he knew me so thoroughly.&#39;, &#39;&#34;Why not?&#39;, &#39;I am an old schoolfellow of his, too, I believe, and I must own I feel hurt that you have left me out,&#34; I said, boiling over again.&#39;, &#39;&#34;And where were we to find you?&#34;&#39;, &#39;Ferfitchkin put in roughly.&#39;, &#39;&#34;You never were on good terms with Zverkov,&#34; Trudolyubov added, frowning.&#39;, &#39;But I had already clutched at the idea and would not give it up.&#39;, &#39;&#34;It seems to me that no one has a right to form an opinion upon that,&#34; I retorted in a shaking voice, as though something tremendous had happened.&#39;, &#39;&#34;Perhaps that is just my reason for wishing it now, that I have not always been on good terms with him.&#34;&#39;, &#39;&#34;Oh, there &#39;s no making you out ... with these refinements,&#34; Trudolyubov jeered.&#39;, &#39;&#34;We &#39;ll put your name down,&#34; Simonov decided, addressing me.&#39;, &#39;&#34;Tomorrow at five-o &#39;clock at the Hotel de Paris.&#34;&#39;, &#39;&#34;What about the money?&#34;&#39;, &#39;Ferfitchkin began in an undertone, indicating me to Simonov, but he broke off, for even Simonov was embarrassed.&#39;, &#39;&#34;That will do,&#34; said Trudolyubov, getting up.&#39;, &#39;&#34;If he wants to come so much, let him.&#34;&#39;, &#39;&#34;But it &#39;s a private thing, between us friends,&#34; Ferfitchkin said crossly, as he, too, picked up his hat.&#39;, &#39;&#34;It &#39;s not an official gathering.&#34;&#39;, &#39;&#34;We do not want at all, perhaps ...&#34; They went away.&#39;, &#39;Ferfitchkin did not greet me in any way as he went out, Trudolyubov barely nodded.&#39;, &#39;Simonov, with whom I was left TETE-A-TETE, was in a state of vexation and perplexity, and looked at me queerly.&#39;, &#39;He did not sit down and did not ask me to.&#39;, &#39;&#34;H &#39;m ... yes ... tomorrow, then.&#39;, &#39;Will you pay your subscription now?&#39;, &#39;I just ask so as to know,&#34; he muttered in embarrassment.&#39;, &#39;I flushed crimson, as I did so I remembered that I had owed Simonov fifteen roubles for ages--which I had, indeed, never forgotten, though I had not paid it.&#39;, &#39;&#34;You will understand, Simonov, that I could have no idea when I came here....&#39;, &#39;I am very much vexed that I have forgotten....&#34; &#34;All right, all right, that doesn &#39;t matter.&#39;, &#39;You can pay tomorrow after the dinner.&#39;, &#39;I simply wanted to know....&#39;, &#39;Please don &#39;t...&#34; He broke off and began pacing the room still more vexed.&#39;, &#39;As he walked he began to stamp with his heels.&#39;, &#39;&#34;Am I keeping you?&#34;&#39;, &#39;I asked, after two minutes of silence.&#39;, &#39;&#34;Oh!&#34;&#39;, &#39;he said, starting, &#34;that is--to be truthful--yes.&#39;, &#39;I have to go and see someone ... not far from here,&#34; he added in an apologetic voice, somewhat abashed.&#39;, &#39;&#34;My goodness, why didn &#39;t you say so?&#34;&#39;, &#39;I cried, seizing my cap, with an astonishingly free-and-easy air, which was the last thing I should have expected of myself.&#39;, &#39;&#34;It &#39;s close by ... not two paces away,&#34; Simonov repeated, accompanying me to the front door with a fussy air which did not suit him at all.&#39;, &#39;&#34;So five o &#39;clock, punctually, tomorrow,&#34; he called down the stairs after me.&#39;, &#39;He was very glad to get rid of me.&#39;, &#39;I was in a fury.&#39;, &#39;&#34;What possessed me, what possessed me to force myself upon them?&#34;&#39;, &#39;I wondered, grinding my teeth as I strode along the street, &#34;for a scoundrel, a pig like that Zverkov!&#39;, &#39;Of course I had better not go; of course, I must just snap my fingers at them.&#39;, &#39;I am not bound in any way.&#39;, &#39;I &#39;ll send Simonov a note by tomorrow &#39;s post....&#34; But what made me furious was that I knew for certain that I should go, that I should make a point of going; and the more tactless, the more unseemly my going would be, the more certainly I would go.&#39;, &#39;And there was a positive obstacle to my going: I had no money.&#39;, &#39;All I had was nine roubles, I had to give seven of that to my servant, Apollon, for his monthly wages.&#39;, &#39;That was all I paid him--he had to keep himself.&#39;, &#39;Not to pay him was impossible, considering his character.&#39;, &#39;But I will talk about that fellow, about that plague of mine, another time.&#39;, &#39;However, I knew I should go and should not pay him his wages.&#39;, &#39;That night I had the most hideous dreams.&#39;, &#39;No wonder; all the evening I had been oppressed by memories of my miserable days at school, and I could not shake them off.&#39;, &#39;I was sent to the school by distant relations, upon whom I was dependent and of whom I have heard nothing since--they sent me there a forlorn, silent boy, already crushed by their reproaches, already troubled by doubt, and looking with savage distrust at everyone.&#39;, &#39;My schoolfellows met me with spiteful and merciless jibes because I was not like any of them.&#39;, &#39;But I could not endure their taunts; I could not give in to them with the ignoble readiness with which they gave in to one another.&#39;, &#39;I hated them from the first, and shut myself away from everyone in timid, wounded and disproportionate pride.&#39;, &#39;Their coarseness revolted me.&#39;, &#39;They laughed cynically at my face, at my clumsy figure; and yet what stupid faces they had themselves.&#39;, &#34;In our school the boys&#39; faces seemed in a special way to degenerate and grow stupider.&#34;, &#39;How many fine-looking boys came to us!&#39;, &#39;In a few years they became repulsive.&#39;, &#39;Even at sixteen I wondered at them morosely; even then I was struck by the pettiness of their thoughts, the stupidity of their pursuits, their games, their conversations.&#39;, &#39;They had no understanding of such essential things, they took no interest in such striking, impressive subjects, that I could not help considering them inferior to myself.&#39;, &#39;It was not wounded vanity that drove me to it, and for God &#39;s sake do not thrust upon me your hackneyed remarks, repeated to nausea, that &#34;I was only a dreamer,&#34; while they even then had an understanding of life.&#39;, &#39;They understood nothing, they had no idea of real life, and I swear that that was what made me most indignant with them.&#39;, &#39;On the contrary, the most obvious, striking reality they accepted with fantastic stupidity and even at that time were accustomed to respect success.&#39;, &#39;Everything that was just, but oppressed and looked down upon, they laughed at heartlessly and shamefully.&#39;, &#39;They took rank for intelligence; even at sixteen they were already talking about a snug berth.&#39;, &#39;Of course, a great deal of it was due to their stupidity, to the bad examples with which they had always been surrounded in their childhood and boyhood.&#39;, &#39;They were monstrously depraved.&#39;, &#39;Of course a great deal of that, too, was superficial and an assumption of cynicism; of course there were glimpses of youth and freshness even in their depravity; but even that freshness was not attractive, and showed itself in a certain rakishness.&#39;, &#39;I hated them horribly, though perhaps I was worse than any of them.&#39;, &#39;They repaid me in the same way, and did not conceal their aversion for me.&#39;, &#39;But by then I did not desire their affection: on the contrary, I continually longed for their humiliation.&#39;, &#39;To escape from their derision I purposely began to make all the progress I could with my studies and forced my way to the very top.&#39;, &#39;This impressed them.&#39;, &#39;Moreover, they all began by degrees to grasp that I had already read books none of them could read, and understood things (not forming part of our school curriculum) of which they had not even heard.&#39;, &#39;They took a savage and sarcastic view of it, but were morally impressed, especially as the teachers began to notice me on those grounds.&#39;, &#39;The mockery ceased, but the hostility remained, and cold and strained relations became permanent between us.&#39;, &#39;In the end I could not put up with it: with years a craving for society, for friends, developed in me.&#39;, &#39;I attempted to get on friendly terms with some of my schoolfellows; but somehow or other my intimacy with them was always strained and soon ended of itself.&#39;, &#39;Once, indeed, I did have a friend.&#39;, &#39;But I was already a tyrant at heart; I wanted to exercise unbounded sway over him; I tried to instil into him a contempt for his surroundings; I required of him a disdainful and complete break with those surroundings.&#39;, &#39;I frightened him with my passionate affection; I reduced him to tears, to hysterics.&#39;, &#39;He was a simple and devoted soul; but when he devoted himself to me entirely I began to hate him immediately and repulsed him--as though all I needed him for was to win a victory over him, to subjugate him and nothing else.&#39;, &#39;But I could not subjugate all of them; my friend was not at all like them either, he was, in fact, a rare exception.&#39;, &#34;The first thing I did on leaving school was to give up the special job for which I had been destined so as to break all ties, to curse my past and shake the dust from off my feet.... And goodness knows why, after all that, I should go trudging off to Simonov&#39;s!&#34;, &#39;Early next morning I roused myself and jumped out of bed with excitement, as though it were all about to happen at once.&#39;, &#39;But I believed that some radical change in my life was coming, and would inevitably come that day.&#39;, &#39;Owing to its rarity, perhaps, any external event, however trivial, always made me feel as though some radical change in my life were at hand.&#39;, &#39;I went to the office, however, as usual, but sneaked away home two hours earlier to get ready.&#39;, &#39;The great thing, I thought, is not to be the first to arrive, or they will think I am overjoyed at coming.&#39;, &#39;But there were thousands of such great points to consider, and they all agitated and overwhelmed me.&#39;, &#39;I polished my boots a second time with my own hands; nothing in the world would have induced Apollon to clean them twice a day, as he considered that it was more than his duties required of him.&#39;, &#39;I stole the brushes to clean them from the passage, being careful he should not detect it, for fear of his contempt.&#39;, &#39;Then I minutely examined my clothes and thought that everything looked old, worn and threadbare.&#39;, &#39;I had let myself get too slovenly.&#39;, &#39;My uniform, perhaps, was tidy, but I could not go out to dinner in my uniform.&#39;, &#39;The worst of it was that on the knee of my trousers was a big yellow stain.&#39;, &#39;I had a foreboding that that stain would deprive me of nine-tenths of my personal dignity.&#39;, &#39;I knew, too, that it was very poor to think so.&#39;, &#39;&#34;But this is no time for thinking: now I am in for the real thing,&#34; I thought, and my heart sank.&#39;, &#39;I knew, too, perfectly well even then, that I was monstrously exaggerating the facts.&#39;, &#39;But how could I help it?&#39;, &#39;I could not control myself and was already shaking with fever.&#39;, &#39;With despair I pictured to myself how coldly and disdainfully that &#34;scoundrel&#34; Zverkov would meet me; with what dull-witted, invincible contempt the blockhead Trudolyubov would look at me; with what impudent rudeness the insect Ferfitchkin would snigger at me in order to curry favour with Zverkov; how completely Simonov would take it all in, and how he would despise me for the abjectness of my vanity and lack of spirit--and, worst of all, how paltry, UNLITERARY, commonplace it would all be.&#39;, &#39;Of course, the best thing would be not to go at all.&#39;, &#39;But that was most impossible of all: if I feel impelled to do anything, I seem to be pitchforked into it.&#39;, &#39;I should have jeered at myself ever afterwards: &#34;So you funked it, you funked it, you funked the REAL THING!&#34;&#39;, &#39;On the contrary, I passionately longed to show all that &#34;rabble&#34; that I was by no means such a spiritless creature as I seemed to myself.&#39;, &#39;What is more, even in the acutest paroxysm of this cowardly fever, I dreamed of getting the upper hand, of dominating them, carrying them away, making them like me--if only for my &#34;elevation of thought and unmistakable wit.&#34;&#39;, &#39;They would abandon Zverkov, he would sit on one side, silent and ashamed, while I should crush him.&#39;, &#39;Then, perhaps, we would be reconciled and drink to our everlasting friendship; but what was most bitter and humiliating for me was that I knew even then, knew fully and for certain, that I needed nothing of all this really, that I did not really want to crush, to subdue, to attract them, and that I did not care a straw really for the result, even if I did achieve it.&#39;, &#39;Oh, how I prayed for the day to pass quickly!&#39;, &#39;In unutterable anguish I went to the window, opened the movable pane and looked out into the troubled darkness of the thickly falling wet snow.&#39;, &#39;At last my wretched little clock hissed out five.&#39;, &#34;I seized my hat and, trying not to look at Apollon, who had been all day expecting his month&#39;s wages, but in his foolishness was unwilling to be the first to speak about it, I slipped between him and the door and, jumping into a high-class sledge, on which I spent my last half rouble, I drove up in grand style to the Hotel de Paris.&#34;, &#39;IV I had been certain the day before that I should be the first to arrive.&#39;, &#39;But it was not a question of being the first to arrive.&#39;, &#39;Not only were they not there, but I had difficulty in finding our room.&#39;, &#39;The table was not laid even.&#39;, &#39;What did it mean?&#39;, &#34;After a good many questions I elicited from the waiters that the dinner had been ordered not for five, but for six o&#39;clock.&#34;, &#39;This was confirmed at the buffet too.&#39;, &#39;I felt really ashamed to go on questioning them.&#39;, &#39;It was only twenty-five minutes past five.&#39;, &#39;If they changed the dinner hour they ought at least to have let me know--that is what the post is for, and not to have put me in an absurd position in my own eyes and ... and even before the waiters.&#39;, &#39;I sat down; the servant began laying the table; I felt even more humiliated when he was present.&#39;, &#34;Towards six o&#39;clock they brought in candles, though there were lamps burning in the room.&#34;, &#39;It had not occurred to the waiter, however, to bring them in at once when I arrived.&#39;, &#39;In the next room two gloomy, angry-looking persons were eating their dinners in silence at two different tables.&#39;, &#39;There was a great deal of noise, even shouting, in a room further away; one could hear the laughter of a crowd of people, and nasty little shrieks in French: there were ladies at the dinner.&#39;, &#39;It was sickening, in fact.&#39;, &#39;I rarely passed more unpleasant moments, so much so that when they did arrive all together punctually at six I was overjoyed to see them, as though they were my deliverers, and even forgot that it was incumbent upon me to show resentment.&#39;, &#39;Zverkov walked in at the head of them; evidently he was the leading spirit.&#39;, &#39;He and all of them were laughing; but, seeing me, Zverkov drew himself up a little, walked up to me deliberately with a slight, rather jaunty bend from the waist.&#39;, &#39;He shook hands with me in a friendly, but not over-friendly, fashion, with a sort of circumspect courtesy like that of a General, as though in giving me his hand he were warding off something.&#39;, &#39;I had imagined, on the contrary, that on coming in he would at once break into his habitual thin, shrill laugh and fall to making his insipid jokes and witticisms.&#39;, &#39;I had been preparing for them ever since the previous day, but I had not expected such condescension, such high-official courtesy.&#39;, &#39;So, then, he felt himself ineffably superior to me in every respect!&#39;, &#39;If he only meant to insult me by that high-official tone, it would not matter, I thought--I could pay him back for it one way or another.&#39;, &#39;But what if, in reality, without the least desire to be offensive, that sheepshead had a notion in earnest that he was superior to me and could only look at me in a patronising way?&#39;, &#39;The very supposition made me gasp.&#39;, &#39;&#34;I was surprised to hear of your desire to join us,&#34; he began, lisping and drawling, which was something new.&#39;, &#39;&#34;You and I seem to have seen nothing of one another.&#39;, &#39;You fight shy of us.&#39;, &#34;You shouldn&#39;t.&#34;, &#39;We are not such terrible people as you think.&#39;, &#39;Well, anyway, I am glad to renew our acquaintance.&#34;&#39;, &#39;And he turned carelessly to put down his hat on the window.&#39;, &#39;&#34;Have you been waiting long?&#34;&#39;, &#39;Trudolyubov inquired.&#39;, &#39;&#34;I arrived at five o &#39;clock as you told me yesterday,&#34; I answered aloud, with an irritability that threatened an explosion.&#39;, &#39;&#34;Didn &#39;t you let him know that we had changed the hour?&#34;&#39;, &#39;said Trudolyubov to Simonov.&#39;, &#39;&#34;No, I didn &#39;t.&#39;, &#39;I forgot,&#34; the latter replied, with no sign of regret, and without even apologising to me he went off to order the HORS D &#39;OEUVRE.&#39;, &#39;&#34;So you &#39;ve been here a whole hour?&#39;, &#39;Oh, poor fellow!&#34;&#39;, &#39;Zverkov cried ironically, for to his notions this was bound to be extremely funny.&#39;, &#39;That rascal Ferfitchkin followed with his nasty little snigger like a puppy yapping.&#39;, &#39;My position struck him, too, as exquisitely ludicrous and embarrassing.&#39;, &#39;&#34;It isn &#39;t funny at all!&#34;&#39;, &#39;I cried to Ferfitchkin, more and more irritated.&#39;, &#39;&#34;It wasn &#39;t my fault, but other people &#39;s.&#39;, &#39;They neglected to let me know.&#39;, &#39;It was ... it was ... it was simply absurd.&#34;&#39;, &#39;&#34;It &#39;s not only absurd, but something else as well,&#34; muttered Trudolyubov, naively taking my part.&#39;, &#39;&#34;You are not hard enough upon it.&#39;, &#39;It was simply rudeness--unintentional, of course.&#39;, &#39;And how could Simonov ...&#39;, &#39;h &#39;m!&#34;&#39;, &#39;&#34;If a trick like that had been played on me,&#34; observed Ferfitchkin, &#34;I should ...&#34; &#34;But you should have ordered something for yourself,&#34; Zverkov interrupted, &#34;or simply asked for dinner without waiting for us.&#34;&#39;, &#39;&#34;You will allow that I might have done that without your permission,&#34; I rapped out.&#39;, &#39;&#34;If I waited, it was ...&#34; &#34;Let us sit down, gentlemen,&#34; cried Simonov, coming in.&#39;, &#39;&#34;Everything is ready; I can answer for the champagne; it is capitally frozen.... You see, I did not know your address, where was I to look for you?&#34;&#39;, &#39;he suddenly turned to me, but again he seemed to avoid looking at me.&#39;, &#39;Evidently he had something against me.&#39;, &#39;It must have been what happened yesterday.&#39;, &#39;All sat down; I did the same.&#39;, &#39;It was a round table.&#39;, &#39;Trudolyubov was on my left, Simonov on my right, Zverkov was sitting opposite, Ferfitchkin next to him, between him and Trudolyubov.&#39;, &#39;&#34;Tell me, are you ... in a government office?&#34;&#39;, &#39;Zverkov went on attending to me.&#39;, &#39;Seeing that I was embarrassed he seriously thought that he ought to be friendly to me, and, so to speak, cheer me up.&#39;, &#39;&#34;Does he want me to throw a bottle at his head?&#34;&#39;, &#39;I thought, in a fury.&#39;, &#39;In my novel surroundings I was unnaturally ready to be irritated.&#39;, &#39;&#34;In the N- office,&#34; I answered jerkily, with my eyes on my plate.&#39;, &#39;&#34;And ha-ave you a go-od berth?&#39;, &#39;I say, what ma-a-de you leave your original job?&#34;&#39;, &#39;&#34;What ma-a-de me was that I wanted to leave my original job,&#34; I drawled more than he, hardly able to control myself.&#39;, &#39;Ferfitchkin went off into a guffaw.&#39;, &#39;Simonov looked at me ironically.&#39;, &#39;Trudolyubov left off eating and began looking at me with curiosity.&#39;, &#39;Zverkov winced, but he tried not to notice it.&#39;, &#39;&#34;And the remuneration?&#34;&#39;, &#39;&#34;What remuneration?&#34;&#39;, &#39;&#34;I mean, your sa-a-lary?&#34;&#39;, &#39;&#34;Why are you cross-examining me?&#34;&#39;, &#39;However, I told him at once what my salary was.&#39;, &#39;I turned horribly red.&#39;, &#39;&#34;It is not very handsome,&#34; Zverkov observed majestically.&#39;, &#39;&#34;Yes, you can &#39;t afford to dine at cafes on that,&#34; Ferfitchkin added insolently.&#39;, &#39;&#34;To my thinking it &#39;s very poor,&#34; Trudolyubov observed gravely.&#39;, &#39;&#34;And how thin you have grown!&#39;, &#39;How you have changed!&#34;&#39;, &#39;added Zverkov, with a shade of venom in his voice, scanning me and my attire with a sort of insolent compassion.&#39;, &#39;&#34;Oh, spare his blushes,&#34; cried Ferfitchkin, sniggering.&#39;, &#39;&#34;My dear sir, allow me to tell you I am not blushing,&#34; I broke out at last; &#34;do you hear?&#39;, &#34;I am dining here, at this cafe, at my own expense, not at other people&#39;s--note that, Mr.&#34;, &#39;Ferfitchkin.&#34;&#39;, &#39;&#34;Wha-at?&#39;, &#34;Isn&#39;t every one here dining at his own expense?&#34;, &#39;You would seem to be ...&#34; Ferfitchkin flew out at me, turning as red as a lobster, and looking me in the face with fury.&#39;, &#39;&#34;Tha-at,&#34; I answered, feeling I had gone too far, &#34;and I imagine it would be better to talk of something more intelligent.&#34;&#39;, &#39;&#34;You intend to show off your intelligence, I suppose?&#34;&#39;, &#39;&#34;Don &#39;t disturb yourself, that would be quite out of place here.&#34;&#39;, &#39;&#34;Why are you clacking away like that, my good sir, eh?&#39;, &#39;Have you gone out of your wits in your office?&#34;&#39;, &#39;&#34;Enough, gentlemen, enough!&#34;&#39;, &#39;Zverkov cried, authoritatively.&#39;, &#39;&#34;How stupid it is!&#34;&#39;, &#39;muttered Simonov.&#39;, &#39;&#34;It really is stupid.&#39;, &#39;We have met here, a company of friends, for a farewell dinner to a comrade and you carry on an altercation,&#34; said Trudolyubov, rudely addressing himself to me alone.&#39;, &#39;&#34;You invited yourself to join us, so don &#39;t disturb the general harmony.&#34;&#39;, &#39;&#34;Enough, enough!&#34;&#39;, &#39;cried Zverkov.&#39;, &#39;&#34;Give over, gentlemen, it &#39;s out of place.&#39;, &#39;Better let me tell you how I nearly got married the day before yesterday....&#34; And then followed a burlesque narrative of how this gentleman had almost been married two days before.&#39;, &#39;There was not a word about the marriage, however, but the story was adorned with generals, colonels and kammer-junkers, while Zverkov almost took the lead among them.&#39;, &#39;It was greeted with approving laughter; Ferfitchkin positively squealed.&#39;, &#39;No one paid any attention to me, and I sat crushed and humiliated.&#39;, &#39;&#34;Good Heavens, these are not the people for me!&#34;&#39;, &#39;I thought.&#39;, &#39;&#34;And what a fool I have made of myself before them!&#39;, &#39;I let Ferfitchkin go too far, though.&#39;, &#39;The brutes imagine they are doing me an honour in letting me sit down with them.&#39;, &#34;They don&#39;t understand that it&#39;s an honour to them and not to me!&#34;, &#34;I&#39;ve grown thinner!&#34;, &#39;My clothes!&#39;, &#39;Oh, damn my trousers!&#39;, &#39;Zverkov noticed the yellow stain on the knee as soon as he came in....&#39;, &#34;But what&#39;s the use!&#34;, &#39;I must get up at once, this very minute, take my hat and simply go without a word ... with contempt!&#39;, &#39;And tomorrow I can send a challenge.&#39;, &#39;The scoundrels!&#39;, &#39;As though I cared about the seven roubles.&#39;, &#39;They may think....&#39;, &#39;Damn it!&#39;, &#34;I don&#39;t care about the seven roubles.&#34;, &#39;I &#39;ll go this minute!&#34;&#39;, &#39;Of course I remained.&#39;, &#39;I drank sherry and Lafitte by the glassful in my discomfiture.&#39;, &#39;Being unaccustomed to it, I was quickly affected.&#39;, &#39;My annoyance increased as the wine went to my head.&#39;, &#39;I longed all at once to insult them all in a most flagrant manner and then go away.&#39;, &#39;To seize the moment and show what I could do, so that they would say, &#34;He &#39;s clever, though he is absurd,&#34; and ... and ... in fact, damn them all!&#39;, &#39;I scanned them all insolently with my drowsy eyes.&#39;, &#39;But they seemed to have forgotten me altogether.&#39;, &#39;They were noisy, vociferous, cheerful.&#39;, &#39;Zverkov was talking all the time.&#39;, &#39;I began listening.&#39;, &#39;Zverkov was talking of some exuberant lady whom he had at last led on to declaring her love (of course, he was lying like a horse), and how he had been helped in this affair by an intimate friend of his, a Prince Kolya, an officer in the hussars, who had three thousand serfs.&#39;, &#39;&#34;And yet this Kolya, who has three thousand serfs, has not put in an appearance here tonight to see you off,&#34; I cut in suddenly.&#39;, &#39;For one minute every one was silent.&#39;, &#39;&#34;You are drunk already.&#34;&#39;, &#39;Trudolyubov deigned to notice me at last, glancing contemptuously in my direction.&#39;, &#39;Zverkov, without a word, examined me as though I were an insect.&#39;, &#39;I dropped my eyes.&#39;, &#39;Simonov made haste to fill up the glasses with champagne.&#39;, &#39;Trudolyubov raised his glass, as did everyone else but me.&#39;, &#39;&#34;Your health and good luck on the journey!&#34;&#39;, &#39;he cried to Zverkov.&#39;, &#39;&#34;To old times, to our future, hurrah!&#34;&#39;, &#39;They all tossed off their glasses, and crowded round Zverkov to kiss him.&#39;, &#39;I did not move; my full glass stood untouched before me.&#39;, &#39;&#34;Why, aren &#39;t you going to drink it?&#34;&#39;, &#39;roared Trudolyubov, losing patience and turning menacingly to me.&#39;, &#39;&#34;I want to make a speech separately, on my own account ... and then I &#39;ll drink it, Mr.&#39;, &#39;Trudolyubov.&#34;&#39;, &#39;&#34;Spiteful brute!&#34;&#39;, &#39;muttered Simonov.&#39;, &#39;I drew myself up in my chair and feverishly seized my glass, prepared for something extraordinary, though I did not know myself precisely what I was going to say.&#39;, &#39;&#34;SILENCE!&#34;&#39;, &#39;cried Ferfitchkin.&#39;, &#39;&#34;Now for a display of wit!&#34;&#39;, &#39;Zverkov waited very gravely, knowing what was coming.&#39;, &#39;&#34;Mr.&#39;, &#39;Lieutenant Zverkov,&#34; I began, &#34;let me tell you that I hate phrases, phrasemongers and men in corsets ... that &#39;s the first point, and there is a second one to follow it.&#34;&#39;, &#39;There was a general stir.&#39;, &#39;&#34;The second point is: I hate ribaldry and ribald talkers.&#39;, &#39;Especially ribald talkers!&#39;, &#39;The third point: I love justice, truth and honesty.&#34;&#39;, &#39;I went on almost mechanically, for I was beginning to shiver with horror myself and had no idea how I came to be talking like this.&#39;, &#39;&#34;I love thought, Monsieur Zverkov; I love true comradeship, on an equal footing and not ... H &#39;m ...&#39;, &#39;I love ...&#39;, &#39;But, however, why not?&#39;, &#39;I will drink your health, too, Mr. Zverkov.&#39;, &#39;Seduce the Circassian girls, shoot the enemies of the fatherland and ... and ... to your health, Monsieur Zverkov!&#34;&#39;, &#39;Zverkov got up from his seat, bowed to me and said: &#34;I am very much obliged to you.&#34;&#39;, &#39;He was frightfully offended and turned pale.&#39;, &#39;&#34;Damn the fellow!&#34;&#39;, &#39;roared Trudolyubov, bringing his fist down on the table.&#39;, &#39;&#34;Well, he wants a punch in the face for that,&#34; squealed Ferfitchkin.&#39;, &#39;&#34;We ought to turn him out,&#34; muttered Simonov.&#39;, &#39;&#34;Not a word, gentlemen, not a movement!&#34;&#39;, &#39;cried Zverkov solemnly, checking the general indignation.&#39;, &#39;&#34;I thank you all, but I can show him for myself how much value I attach to his words.&#34;&#39;, &#39;&#34;Mr. Ferfitchkin, you will give me satisfaction tomorrow for your words just now!&#34;&#39;, &#39;I said aloud, turning with dignity to Ferfitchkin.&#39;, &#39;&#34;A duel, you mean?&#39;, &#39;Certainly,&#34; he answered.&#39;, &#39;But probably I was so ridiculous as I challenged him and it was so out of keeping with my appearance that everyone including Ferfitchkin was prostrate with laughter.&#39;, &#39;&#34;Yes, let him alone, of course!&#39;, &#39;He is quite drunk,&#34; Trudolyubov said with disgust.&#39;, &#39;&#34;I shall never forgive myself for letting him join us,&#34; Simonov muttered again.&#39;, &#39;&#34;Now is the time to throw a bottle at their heads,&#34; I thought to myself.&#39;, &#39;I picked up the bottle ... and filled my glass.... &#34;No, I &#39;d better sit on to the end,&#34; I went on thinking; &#34;you would be pleased, my friends, if I went away.&#39;, &#39;Nothing will induce me to go.&#39;, &#34;I&#39;ll go on sitting here and drinking to the end, on purpose, as a sign that I don&#39;t think you of the slightest consequence.&#34;, &#39;I will go on sitting and drinking, because this is a public-house and I paid my entrance money.&#39;, &#34;I&#39;ll sit here and drink, for I look upon you as so many pawns, as inanimate pawns.&#34;, &#34;I&#39;ll sit here and drink ... and sing if I want to, yes, sing, for I have the right to ... to sing ...&#34;, &#39;H &#39;m!&#34;&#39;, &#39;But I did not sing.&#39;, &#39;I simply tried not to look at any of them.&#39;, &#39;I assumed most unconcerned attitudes and waited with impatience for them to speak FIRST.&#39;, &#39;But alas, they did not address me!&#39;, &#39;And oh, how I wished, how I wished at that moment to be reconciled to them!&#39;, &#39;It struck eight, at last nine.&#39;, &#39;They moved from the table to the sofa.&#39;, &#39;Zverkov stretched himself on a lounge and put one foot on a round table.&#39;, &#39;Wine was brought there.&#39;, &#39;He did, as a fact, order three bottles on his own account.&#39;, &#39;I, of course, was not invited to join them.&#39;, &#39;They all sat round him on the sofa.&#39;, &#39;They listened to him, almost with reverence.&#39;, &#39;It was evident that they were fond of him.&#39;, &#39;&#34;What for?&#39;, &#39;What for?&#34;&#39;, &#39;I wondered.&#39;, &#39;From time to time they were moved to drunken enthusiasm and kissed each other.&#39;, &#34;They talked of the Caucasus, of the nature of true passion, of snug berths in the service, of the income of an hussar called Podharzhevsky, whom none of them knew personally, and rejoiced in the largeness of it, of the extraordinary grace and beauty of a Princess D., whom none of them had ever seen; then it came to Shakespeare&#39;s being immortal.&#34;, &#39;I smiled contemptuously and walked up and down the other side of the room, opposite the sofa, from the table to the stove and back again.&#39;, &#39;I tried my very utmost to show them that I could do without them, and yet I purposely made a noise with my boots, thumping with my heels.&#39;, &#39;But it was all in vain.&#39;, &#39;They paid no attention.&#39;, &#34;I had the patience to walk up and down in front of them from eight o&#39;clock till eleven, in the same place, from the table to the stove and back again.&#34;, &#39;&#34;I walk up and down to please myself and no one can prevent me.&#34;&#39;, &#39;The waiter who came into the room stopped, from time to time, to look at me.&#39;, &#39;I was somewhat giddy from turning round so often; at moments it seemed to me that I was in delirium.&#39;, &#39;During those three hours I was three times soaked with sweat and dry again.&#39;, &#39;At times, with an intense, acute pang I was stabbed to the heart by the thought that ten years, twenty years, forty years would pass, and that even in forty years I would remember with loathing and humiliation those filthiest, most ludicrous, and most awful moments of my life.&#39;, &#39;No one could have gone out of his way to degrade himself more shamelessly, and I fully realised it, fully, and yet I went on pacing up and down from the table to the stove.&#39;, &#39;&#34;Oh, if you only knew what thoughts and feelings I am capable of, how cultured I am!&#34;&#39;, &#39;I thought at moments, mentally addressing the sofa on which my enemies were sitting.&#39;, &#39;But my enemies behaved as though I were not in the room.&#39;, &#39;Once--only once--they turned towards me, just when Zverkov was talking about Shakespeare, and I suddenly gave a contemptuous laugh.&#39;, &#39;I laughed in such an affected and disgusting way that they all at once broke off their conversation, and silently and gravely for two minutes watched me walking up and down from the table to the stove, TAKING NO NOTICE OF THEM.&#39;, &#39;But nothing came of it: they said nothing, and two minutes later they ceased to notice me again.&#39;, &#39;It struck eleven.&#39;, &#39;&#34;Friends,&#34; cried Zverkov getting up from the sofa, &#34;let us all be off now, THERE!&#34;&#39;, &#39;&#34;Of course, of course,&#34; the others assented.&#39;, &#39;I turned sharply to Zverkov.&#39;, &#39;I was so harassed, so exhausted, that I would have cut my throat to put an end to it.&#39;, &#39;I was in a fever; my hair, soaked with perspiration, stuck to my forehead and temples.&#39;, &#39;&#34;Zverkov, I beg your pardon,&#34; I said abruptly and resolutely.&#39;, &#39;&#34;Ferfitchkin, yours too, and everyone &#39;s, everyone &#39;s: I have insulted you all!&#34;&#39;, &#39;&#34;Aha!&#39;, &#39;A duel is not in your line, old man,&#34; Ferfitchkin hissed venomously.&#39;, &#39;It sent a sharp pang to my heart.&#39;, &#39;&#34;No, it &#39;s not the duel I am afraid of, Ferfitchkin!&#39;, &#39;I am ready to fight you tomorrow, after we are reconciled.&#39;, &#39;I insist upon it, in fact, and you cannot refuse.&#39;, &#39;I want to show you that I am not afraid of a duel.&#39;, &#39;You shall fire first and I shall fire into the air.&#34;&#39;, &#39;&#34;He is comforting himself,&#34; said Simonov.&#39;, &#39;&#34;He &#39;s simply raving,&#34; said Trudolyubov.&#39;, &#39;&#34;But let us pass.&#39;, &#39;Why are you barring our way?&#39;, &#39;What do you want?&#34;&#39;, &#39;Zverkov answered disdainfully.&#39;, &#39;They were all flushed, their eyes were bright: they had been drinking heavily.&#39;, &#39;&#34;I ask for your friendship, Zverkov; I insulted you, but ...&#34; &#34;Insulted?&#39;, &#39;YOU insulted ME?&#39;, &#39;Understand, sir, that you never, under any circumstances, could possibly insult ME.&#34;&#39;, &#39;&#34;And that &#39;s enough for you.&#39;, &#39;Out of the way!&#34;&#39;, &#39;concluded Trudolyubov.&#39;, &#39;&#34;Olympia is mine, friends, that &#39;s agreed!&#34;&#39;, &#39;cried Zverkov.&#39;, &#39;&#34;We won &#39;t dispute your right, we won &#39;t dispute your right,&#34; the others answered, laughing.&#39;, &#39;I stood as though spat upon.&#39;, &#39;The party went noisily out of the room.&#39;, &#39;Trudolyubov struck up some stupid song.&#39;, &#39;Simonov remained behind for a moment to tip the waiters.&#39;, &#39;I suddenly went up to him.&#39;, &#39;&#34;Simonov!&#39;, &#39;give me six roubles!&#34;&#39;, &#39;I said, with desperate resolution.&#39;, &#39;He looked at me in extreme amazement, with vacant eyes.&#39;, &#39;He, too, was drunk.&#39;, &#39;&#34;You don &#39;t mean you are coming with us?&#34;&#39;, &#39;&#34;Yes.&#34;&#39;, &#39;&#34;I &#39;ve no money,&#34; he snapped out, and with a scornful laugh he went out of the room.&#39;, &#39;I clutched at his overcoat.&#39;, &#39;It was a nightmare.&#39;, &#39;&#34;Simonov, I saw you had money.&#39;, &#39;Why do you refuse me?&#39;, &#39;Am I a scoundrel?&#39;, &#39;Beware of refusing me: if you knew, if you knew why I am asking!&#39;, &#39;My whole future, my whole plans depend upon it!&#34;&#39;, &#39;Simonov pulled out the money and almost flung it at me.&#39;, &#39;&#34;Take it, if you have no sense of shame!&#34;&#39;, &#39;he pronounced pitilessly, and ran to overtake them.&#39;, &#39;I was left for a moment alone.&#39;, &#39;Disorder, the remains of dinner, a broken wine-glass on the floor, spilt wine, cigarette ends, fumes of drink and delirium in my brain, an agonising misery in my heart and finally the waiter, who had seen and heard all and was looking inquisitively into my face.&#39;, &#39;&#34;I am going there!&#34;&#39;, &#39;I cried.&#39;, &#39;&#34;Either they shall all go down on their knees to beg for my friendship, or I will give Zverkov a slap in the face!&#34;&#39;, &#39;V &#34;So this is it, this is it at last--contact with real life,&#34; I muttered as I ran headlong downstairs.&#39;, &#39;&#34;This is very different from the Pope &#39;s leaving Rome and going to Brazil, very different from the ball on Lake Como!&#34;&#39;, &#39;&#34;You are a scoundrel,&#34; a thought flashed through my mind, &#34;if you laugh at this now.&#34;&#39;, &#39;&#34;No matter!&#34;&#39;, &#39;I cried, answering myself.&#39;, &#39;&#34;Now everything is lost!&#34;&#39;, &#39;There was no trace to be seen of them, but that made no difference--I knew where they had gone.&#39;, &#39;At the steps was standing a solitary night sledge-driver in a rough peasant coat, powdered over with the still falling, wet, and as it were warm, snow.&#39;, &#39;It was hot and steamy.&#39;, &#39;The little shaggy piebald horse was also covered with snow and coughing, I remember that very well.&#39;, &#39;I made a rush for the roughly made sledge; but as soon as I raised my foot to get into it, the recollection of how Simonov had just given me six roubles seemed to double me up and I tumbled into the sledge like a sack.&#39;, &#39;&#34;No, I must do a great deal to make up for all that,&#34; I cried.&#39;, &#39;&#34;But I will make up for it or perish on the spot this very night.&#39;, &#39;Start!&#34;&#39;, &#39;We set off.&#39;, &#39;There was a perfect whirl in my head.&#39;, &#39;&#34;They won &#39;t go down on their knees to beg for my friendship.&#39;, &#34;That is a mirage, cheap mirage, revolting, romantic and fantastical--that&#39;s another ball on Lake Como.&#34;, &#34;And so I am bound to slap Zverkov&#39;s face!&#34;, &#39;It is my duty to.&#39;, &#39;And so it is settled; I am flying to give him a slap in the face.&#39;, &#39;Hurry up!&#34;&#39;, &#39;The driver tugged at the reins.&#39;, &#39;&#34;As soon as I go in I &#39;ll give it him.&#39;, &#39;Ought I before giving him the slap to say a few words by way of preface?&#39;, &#39;No.&#39;, &#34;I&#39;ll simply go in and give it him.&#34;, &#39;They will all be sitting in the drawing-room, and he with Olympia on the sofa.&#39;, &#39;That damned Olympia!&#39;, &#39;She laughed at my looks on one occasion and refused me.&#39;, &#34;I&#39;ll pull Olympia&#39;s hair, pull Zverkov&#39;s ears!&#34;, &#39;No, better one ear, and pull him by it round the room.&#39;, &#39;Maybe they will all begin beating me and will kick me out.&#39;, &#34;That&#39;s most likely, indeed.&#34;, &#39;No matter!&#39;, &#39;Anyway, I shall first slap him; the initiative will be mine; and by the laws of honour that is everything: he will be branded and cannot wipe off the slap by any blows, by nothing but a duel.&#39;, &#39;He will be forced to fight.&#39;, &#39;And let them beat me now.&#39;, &#39;Let them, the ungrateful wretches!&#39;, &#39;Trudolyubov will beat me hardest, he is so strong; Ferfitchkin will be sure to catch hold sideways and tug at my hair.&#39;, &#39;But no matter, no matter!&#39;, &#34;That&#39;s what I am going for.&#34;, &#39;The blockheads will be forced at last to see the tragedy of it all!&#39;, &#39;When they drag me to the door I shall call out to them that in reality they are not worth my little finger.&#39;, &#39;Get on, driver, get on!&#34;&#39;, &#39;I cried to the driver.&#39;, &#39;He started and flicked his whip, I shouted so savagely.&#39;, &#39;&#34;We shall fight at daybreak, that &#39;s a settled thing.&#39;, &#34;I&#39;ve done with the office.&#34;, &#39;Ferfitchkin made a joke about it just now.&#39;, &#39;But where can I get pistols?&#39;, &#39;Nonsense!&#39;, &#34;I&#39;ll get my salary in advance and buy them.&#34;, &#39;And powder, and bullets?&#39;, &#34;That&#39;s the second&#39;s business.&#34;, &#39;And how can it all be done by daybreak?&#39;, &#39;and where am I to get a second?&#39;, &#39;I have no friends.&#39;, &#39;Nonsense!&#34;&#39;, &#39;I cried, lashing myself up more and more.&#39;, &#39;&#34;It &#39;s of no consequence!&#39;, &#39;The first person I meet in the street is bound to be my second, just as he would be bound to pull a drowning man out of water.&#39;, &#39;The most eccentric things may happen.&#39;, &#39;Even if I were to ask the director himself to be my second tomorrow, he would be bound to consent, if only from a feeling of chivalry, and to keep the secret!&#39;, &#39;Anton Antonitch....&#34; The fact is, that at that very minute the disgusting absurdity of my plan and the other side of the question was clearer and more vivid to my imagination than it could be to anyone on earth.&#39;, &#39;But .... &#34;Get on, driver, get on, you rascal, get on!&#34;&#39;, &#39;&#34;Ugh, sir!&#34;&#39;, &#39;said the son of toil.&#39;, &#39;Cold shivers suddenly ran down me.&#39;, &#34;Wouldn&#39;t it be better ... to go straight home?&#34;, &#39;My God, my God!&#39;, &#39;Why did I invite myself to this dinner yesterday?&#39;, &#34;But no, it&#39;s impossible.&#34;, &#39;And my walking up and down for three hours from the table to the stove?&#39;, &#39;No, they, they and no one else must pay for my walking up and down!&#39;, &#39;They must wipe out this dishonour!&#39;, &#39;Drive on!&#39;, &#39;And what if they give me into custody?&#39;, &#34;They won&#39;t dare!&#34;, &#34;They&#39;ll be afraid of the scandal.&#34;, &#39;And what if Zverkov is so contemptuous that he refuses to fight a duel?&#39;, &#34;He is sure to; but in that case I&#39;ll show them ...&#34;, &#34;I will turn up at the posting station when he&#39;s setting off tomorrow, I&#39;ll catch him by the leg, I&#39;ll pull off his coat when he gets into the carriage.&#34;, &#34;I&#39;ll get my teeth into his hand, I&#39;ll bite him.&#34;, &#39;&#34;See what lengths you can drive a desperate man to!&#34;&#39;, &#39;He may hit me on the head and they may belabour me from behind.&#39;, &#39;I will shout to the assembled multitude: &#34;Look at this young puppy who is driving off to captivate the Circassian girls after letting me spit in his face!&#34;&#39;, &#39;Of course, after that everything will be over!&#39;, &#39;The office will have vanished off the face of the earth.&#39;, &#39;I shall be arrested, I shall be tried, I shall be dismissed from the service, thrown in prison, sent to Siberia.&#39;, &#39;Never mind!&#39;, &#39;In fifteen years when they let me out of prison I will trudge off to him, a beggar, in rags.&#39;, &#39;I shall find him in some provincial town.&#39;, &#39;He will be married and happy.&#39;, &#39;He will have a grown-up daughter....&#39;, &#39;I shall say to him: &#34;Look, monster, at my hollow cheeks and my rags!&#39;, &#34;I&#39;ve lost everything--my career, my happiness, art, science, THE WOMAN I LOVED, and all through you.&#34;, &#39;Here are pistols.&#39;, &#39;I have come to discharge my pistol and ... and I ... forgive you.&#39;, &#39;Then I shall fire into the air and he will hear nothing more of me....&#34; I was actually on the point of tears, though I knew perfectly well at that moment that all this was out of Pushkin &#39;s SILVIO and Lermontov &#39;s MASQUERADE.&#39;, &#39;And all at once I felt horribly ashamed, so ashamed that I stopped the horse, got out of the sledge, and stood still in the snow in the middle of the street.&#39;, &#39;The driver gazed at me, sighing and astonished.&#39;, &#39;What was I to do?&#39;, &#39;I could not go on there--it was evidently stupid, and I could not leave things as they were, because that would seem as though ... Heavens, how could I leave things!&#39;, &#39;And after such insults!&#39;, &#39;&#34;No!&#34;&#39;, &#39;I cried, throwing myself into the sledge again.&#39;, &#39;&#34;It is ordained!&#39;, &#39;It is fate!&#39;, &#39;Drive on, drive on!&#34;&#39;, &#39;And in my impatience I punched the sledge-driver on the back of the neck.&#39;, &#39;&#34;What are you up to?&#39;, &#39;What are you hitting me for?&#34;&#39;, &#39;the peasant shouted, but he whipped up his nag so that it began kicking.&#39;, &#39;The wet snow was falling in big flakes; I unbuttoned myself, regardless of it.&#39;, &#39;I forgot everything else, for I had finally decided on the slap, and felt with horror that it was going to happen NOW, AT ONCE, and that NO FORCE COULD STOP IT.&#39;, &#39;The deserted street lamps gleamed sullenly in the snowy darkness like torches at a funeral.&#39;, &#39;The snow drifted under my great-coat, under my coat, under my cravat, and melted there.&#39;, &#39;I did not wrap myself up--all was lost, anyway.&#39;, &#39;At last we arrived.&#39;, &#39;I jumped out, almost unconscious, ran up the steps and began knocking and kicking at the door.&#39;, &#39;I felt fearfully weak, particularly in my legs and knees.&#39;, &#39;The door was opened quickly as though they knew I was coming.&#39;, &#39;As a fact, Simonov had warned them that perhaps another gentleman would arrive, and this was a place in which one had to give notice and to observe certain precautions.&#39;, &#39;It was one of those &#34;millinery establishments&#34; which were abolished by the police a good time ago.&#39;, &#39;By day it really was a shop; but at night, if one had an introduction, one might visit it for other purposes.&#39;, &#39;I walked rapidly through the dark shop into the familiar drawing-room, where there was only one candle burning, and stood still in amazement: there was no one there.&#39;, &#39;&#34;Where are they?&#34;&#39;, &#39;I asked somebody.&#39;, &#39;But by now, of course, they had separated.&#39;, &#39;Before me was standing a person with a stupid smile, the &#34;madam&#34; herself, who had seen me before.&#39;, &#39;A minute later a door opened and another person came in.&#39;, &#39;Taking no notice of anything I strode about the room, and, I believe, I talked to myself.&#39;, &#39;I felt as though I had been saved from death and was conscious of this, joyfully, all over: I should have given that slap, I should certainly, certainly have given it!&#39;, &#39;But now they were not here and ... everything had vanished and changed!&#39;, &#39;I looked round.&#39;, &#39;I could not realise my condition yet.&#39;, &#39;I looked mechanically at the girl who had come in: and had a glimpse of a fresh, young, rather pale face, with straight, dark eyebrows, and with grave, as it were wondering, eyes that attracted me at once; I should have hated her if she had been smiling.&#39;, &#39;I began looking at her more intently and, as it were, with effort.&#39;, &#39;I had not fully collected my thoughts.&#39;, &#39;There was something simple and good-natured in her face, but something strangely grave.&#39;, &#39;I am sure that this stood in her way here, and no one of those fools had noticed her.&#39;, &#39;She could not, however, have been called a beauty, though she was tall, strong-looking, and well built.&#39;, &#39;She was very simply dressed.&#39;, &#39;Something loathsome stirred within me.&#39;, &#39;I went straight up to her.&#39;, &#39;I chanced to look into the glass.&#39;, &#39;My harassed face struck me as revolting in the extreme, pale, angry, abject, with dishevelled hair.&#39;, &#39;&#34;No matter, I am glad of it,&#34; I thought; &#34;I am glad that I shall seem repulsive to her; I like that.&#34;&#39;, &#39;VI ... Somewhere behind a screen a clock began wheezing, as though oppressed by something, as though someone were strangling it.&#39;, &#39;After an unnaturally prolonged wheezing there followed a shrill, nasty, and as it were unexpectedly rapid, chime--as though someone were suddenly jumping forward.&#39;, &#39;It struck two.&#39;, &#39;I woke up, though I had indeed not been asleep but lying half-conscious.&#39;, &#39;It was almost completely dark in the narrow, cramped, low-pitched room, cumbered up with an enormous wardrobe and piles of cardboard boxes and all sorts of frippery and litter.&#39;, &#39;The candle end that had been burning on the table was going out and gave a faint flicker from time to time.&#39;, &#39;In a few minutes there would be complete darkness.&#39;, &#39;I was not long in coming to myself; everything came back to my mind at once, without an effort, as though it had been in ambush to pounce upon me again.&#39;, &#39;And, indeed, even while I was unconscious a point seemed continually to remain in my memory unforgotten, and round it my dreams moved drearily.&#39;, &#39;But strange to say, everything that had happened to me in that day seemed to me now, on waking, to be in the far, far away past, as though I had long, long ago lived all that down.&#39;, &#39;My head was full of fumes.&#39;, &#39;Something seemed to be hovering over me, rousing me, exciting me, and making me restless.&#39;, &#39;Misery and spite seemed surging up in me again and seeking an outlet.&#39;, &#39;Suddenly I saw beside me two wide open eyes scrutinising me curiously and persistently.&#39;, &#39;The look in those eyes was coldly detached, sullen, as it were utterly remote; it weighed upon me.&#39;, &#39;A grim idea came into my brain and passed all over my body, as a horrible sensation, such as one feels when one goes into a damp and mouldy cellar.&#39;, &#39;There was something unnatural in those two eyes, beginning to look at me only now.&#39;, &#39;I recalled, too, that during those two hours I had not said a single word to this creature, and had, in fact, considered it utterly superfluous; in fact, the silence had for some reason gratified me.&#39;, &#39;Now I suddenly realised vividly the hideous idea--revolting as a spider--of vice, which, without love, grossly and shamelessly begins with that in which true love finds its consummation.&#39;, &#39;For a long time we gazed at each other like that, but she did not drop her eyes before mine and her expression did not change, so that at last I felt uncomfortable.&#39;, &#39;&#34;What is your name?&#34;&#39;, &#39;I asked abruptly, to put an end to it.&#39;, &#39;&#34;Liza,&#34; she answered almost in a whisper, but somehow far from graciously, and she turned her eyes away.&#39;, &#39;I was silent.&#39;, &#39;&#34;What weather!&#39;, &#39;The snow ... it &#39;s disgusting!&#34;&#39;, &#39;I said, almost to myself, putting my arm under my head despondently, and gazing at the ceiling.&#39;, &#39;She made no answer.&#39;, &#39;This was horrible.&#39;, &#39;&#34;Have you always lived in Petersburg?&#34;&#39;, &#39;I asked a minute later, almost angrily, turning my head slightly towards her.&#39;, &#39;&#34;No.&#34;&#39;, &#39;&#34;Where do you come from?&#34;&#39;, &#39;&#34;From Riga,&#34; she answered reluctantly.&#39;, &#39;&#34;Are you a German?&#34;&#39;, &#39;&#34;No, Russian.&#34;&#39;, &#39;&#34;Have you been here long?&#34;&#39;, &#39;&#34;Where?&#34;&#39;, &#39;&#34;In this house?&#34;&#39;, &#39;&#34;A fortnight.&#34;&#39;, &#39;She spoke more and more jerkily.&#39;, &#39;The candle went out; I could no longer distinguish her face.&#39;, &#39;&#34;Have you a father and mother?&#34;&#39;, &#39;&#34;Yes ... no ...&#39;, &#39;I have.&#34;&#39;, &#39;&#34;Where are they?&#34;&#39;, &#39;&#34;There ... in Riga.&#34;&#39;, &#39;&#34;What are they?&#34;&#39;, &#39;&#34;Oh, nothing.&#34;&#39;, &#39;&#34;Nothing?&#39;, &#39;Why, what class are they?&#34;&#39;, &#39;&#34;Tradespeople.&#34;&#39;, &#39;&#34;Have you always lived with them?&#34;&#39;, &#39;&#34;Yes.&#34;&#39;, &#39;&#34;How old are you?&#34;&#39;, &#39;&#34;Twenty.&#34;&#39;, &#39;&#34;Why did you leave them?&#34;&#39;, &#39;&#34;Oh, for no reason.&#34;&#39;, &#39;That answer meant &#34;Let me alone; I feel sick, sad.&#34;&#39;, &#39;We were silent.&#39;, &#39;God knows why I did not go away.&#39;, &#39;I felt myself more and more sick and dreary.&#39;, &#39;The images of the previous day began of themselves, apart from my will, flitting through my memory in confusion.&#39;, &#39;I suddenly recalled something I had seen that morning when, full of anxious thoughts, I was hurrying to the office.&#39;, &#39;&#34;I saw them carrying a coffin out yesterday and they nearly dropped it,&#34; I suddenly said aloud, not that I desired to open the conversation, but as it were by accident.&#39;, &#39;&#34;A coffin?&#34;&#39;, &#39;&#34;Yes, in the Haymarket; they were bringing it up out of a cellar.&#34;&#39;, &#39;&#34;From a cellar?&#34;&#39;, &#39;&#34;Not from a cellar, but a basement.&#39;, &#39;Oh, you know ... down below ... from a house of ill-fame.&#39;, &#39;It was filthy all round ... Egg-shells, litter ... a stench.&#39;, &#39;It was loathsome.&#34;&#39;, &#39;Silence.&#39;, &#39;&#34;A nasty day to be buried,&#34; I began, simply to avoid being silent.&#39;, &#39;&#34;Nasty, in what way?&#34;&#39;, &#39;&#34;The snow, the wet.&#34;&#39;, &#39;(I yawned.)&#39;, &#39;&#34;It makes no difference,&#34; she said suddenly, after a brief silence.&#39;, &#39;&#34;No, it &#39;s horrid.&#34;&#39;, &#39;(I yawned again).&#39;, &#39;&#34;The gravediggers must have sworn at getting drenched by the snow.&#39;, &#39;And there must have been water in the grave.&#34;&#39;, &#39;&#34;Why water in the grave?&#34;&#39;, &#39;she asked, with a sort of curiosity, but speaking even more harshly and abruptly than before.&#39;, &#39;I suddenly began to feel provoked.&#39;, &#39;&#34;Why, there must have been water at the bottom a foot deep.&#39;, &#39;You can &#39;t dig a dry grave in Volkovo Cemetery.&#34;&#39;, &#39;&#34;Why?&#34;&#39;, &#39;&#34;Why?&#39;, &#39;Why, the place is waterlogged.&#39;, &#34;It&#39;s a regular marsh.&#34;, &#39;So they bury them in water.&#39;, &#39;I &#39;ve seen it myself ... many times.&#34;&#39;, &#39;(I had never seen it once, indeed I had never been in Volkovo, and had only heard stories of it.)&#39;, &#39;&#34;Do you mean to say, you don &#39;t mind how you die?&#34;&#39;, &#39;&#34;But why should I die?&#34;&#39;, &#39;she answered, as though defending herself.&#39;, &#39;&#34;Why, some day you will die, and you will die just the same as that dead woman.&#39;, &#39;She was ... a girl like you.&#39;, &#39;She died of consumption.&#34;&#39;, &#39;&#34;A wench would have died in hospital ...&#34; (She knows all about it already: she said &#34;wench,&#34; not &#34;girl.&#34;)&#39;, &#39;&#34;She was in debt to her madam,&#34; I retorted, more and more provoked by the discussion; &#34;and went on earning money for her up to the end, though she was in consumption.&#39;, &#39;Some sledge-drivers standing by were talking about her to some soldiers and telling them so.&#39;, &#39;No doubt they knew her.&#39;, &#39;They were laughing.&#39;, &#39;They were going to meet in a pot-house to drink to her memory.&#34;&#39;, &#39;A great deal of this was my invention.&#39;, &#39;Silence followed, profound silence.&#39;, &#39;She did not stir.&#39;, &#39;&#34;And is it better to die in a hospital?&#34;&#39;, &#39;&#34;Isn &#39;t it just the same?&#39;, &#39;Besides, why should I die?&#34;&#39;, &#39;she added irritably.&#39;, &#39;&#34;If not now, a little later.&#34;&#39;, &#39;&#34;Why a little later?&#34;&#39;, &#39;&#34;Why, indeed?&#39;, &#39;Now you are young, pretty, fresh, you fetch a high price.&#39;, &#39;But after another year of this life you will be very different--you will go off.&#34;&#39;, &#39;&#34;In a year?&#34;&#39;, &#39;&#34;Anyway, in a year you will be worth less,&#34; I continued malignantly.&#39;, &#39;&#34;You will go from here to something lower, another house; a year later--to a third, lower and lower, and in seven years you will come to a basement in the Haymarket.&#39;, &#39;That will be if you were lucky.&#39;, &#39;But it would be much worse if you got some disease, consumption, say ... and caught a chill, or something or other.&#39;, &#34;It&#39;s not easy to get over an illness in your way of life.&#34;, &#39;If you catch anything you may not get rid of it.&#39;, &#39;And so you would die.&#34;&#39;, &#39;&#34;Oh, well, then I shall die,&#34; she answered, quite vindictively, and she made a quick movement.&#39;, &#39;&#34;But one is sorry.&#34;&#39;, &#39;&#34;Sorry for whom?&#34;&#39;, &#39;&#34;Sorry for life.&#34;&#39;, &#39;Silence.&#39;, &#39;&#34;Have you been engaged to be married?&#39;, &#39;Eh?&#34;&#39;, &#39;&#34;What &#39;s that to you?&#34;&#39;, &#39;&#34;Oh, I am not cross-examining you.&#39;, &#34;It&#39;s nothing to me.&#34;, &#39;Why are you so cross?&#39;, &#39;Of course you may have had your own troubles.&#39;, &#39;What is it to me?&#39;, &#39;It &#39;s simply that I felt sorry.&#34;&#39;, &#39;&#34;Sorry for whom?&#34;&#39;, &#39;&#34;Sorry for you.&#34;&#39;, &#39;&#34;No need,&#34; she whispered hardly audibly, and again made a faint movement.&#39;, &#39;That incensed me at once.&#39;, &#39;What!&#39;, &#39;I was so gentle with her, and she.... &#34;Why, do you think that you are on the right path?&#34;&#39;, &#39;&#34;I don &#39;t think anything.&#34;&#39;, &#39;&#34;That &#39;s what &#39;s wrong, that you don &#39;t think.&#39;, &#39;Realise it while there is still time.&#39;, &#39;There still is time.&#39;, &#39;You are still young, good-looking; you might love, be married, be happy....&#34; &#34;Not all married women are happy,&#34; she snapped out in the rude abrupt tone she had used at first.&#39;, &#39;&#34;Not all, of course, but anyway it is much better than the life here.&#39;, &#39;Infinitely better.&#39;, &#39;Besides, with love one can live even without happiness.&#39;, &#39;Even in sorrow life is sweet; life is sweet, however one lives.&#39;, &#39;But here what is there but ... foulness?&#39;, &#39;Phew!&#34;&#39;, &#39;I turned away with disgust; I was no longer reasoning coldly.&#39;, &#39;I began to feel myself what I was saying and warmed to the subject.&#39;, &#39;I was already longing to expound the cherished ideas I had brooded over in my corner.&#39;, &#39;Something suddenly flared up in me.&#39;, &#39;An object had appeared before me.&#39;, &#39;&#34;Never mind my being here, I am not an example for you.&#39;, &#39;I am, perhaps, worse than you are.&#39;, &#39;I was drunk when I came here, though,&#34; I hastened, however, to say in self-defence.&#39;, &#39;&#34;Besides, a man is no example for a woman.&#39;, &#34;It&#39;s a different thing.&#34;, &#34;I may degrade and defile myself, but I am not anyone&#39;s slave.&#34;, &#34;I come and go, and that&#39;s an end of it.&#34;, &#39;I shake it off, and I am a different man.&#39;, &#39;But you are a slave from the start.&#39;, &#39;Yes, a slave!&#39;, &#39;You give up everything, your whole freedom.&#39;, &#34;If you want to break your chains afterwards, you won&#39;t be able to; you will be more and more fast in the snares.&#34;, &#39;It is an accursed bondage.&#39;, &#39;I know it.&#39;, &#34;I won&#39;t speak of anything else, maybe you won&#39;t understand, but tell me: no doubt you are in debt to your madam?&#34;, &#39;There, you see,&#34; I added, though she made no answer, but only listened in silence, entirely absorbed, &#34;that &#39;s a bondage for you!&#39;, &#39;You will never buy your freedom.&#39;, &#39;They will see to that.&#39;, &#34;It&#39;s like selling your soul to the devil.... And besides ... perhaps, I too, am just as unlucky--how do you know--and wallow in the mud on purpose, out of misery?&#34;, &#39;You know, men take to drink from grief; well, maybe I am here from grief.&#39;, &#39;Come, tell me, what is there good here?&#39;, &#39;Here you and I ... came together ... just now and did not say one word to one another all the time, and it was only afterwards you began staring at me like a wild creature, and I at you.&#39;, &#39;Is that loving?&#39;, &#39;Is that how one human being should meet another?&#39;, &#39;It &#39;s hideous, that &#39;s what it is!&#34;&#39;, &#39;&#34;Yes!&#34;&#39;, &#39;she assented sharply and hurriedly.&#39;, &#39;I was positively astounded by the promptitude of this &#34;Yes.&#34;&#39;, &#39;So the same thought may have been straying through her mind when she was staring at me just before.&#39;, &#39;So she, too, was capable of certain thoughts?&#39;, &#39;&#34;Damn it all, this was interesting, this was a point of likeness!&#34;&#39;, &#39;I thought, almost rubbing my hands.&#39;, &#34;And indeed it&#39;s easy to turn a young soul like that!&#34;, &#39;It was the exercise of my power that attracted me most.&#39;, &#39;She turned her head nearer to me, and it seemed to me in the darkness that she propped herself on her arm.&#39;, &#39;Perhaps she was scrutinising me.&#39;, &#39;How I regretted that I could not see her eyes.&#39;, &#39;I heard her deep breathing.&#39;, &#39;&#34;Why have you come here?&#34;&#39;, &#39;I asked her, with a note of authority already in my voice.&#39;, &#39;&#34;Oh, I don &#39;t know.&#34;&#39;, &#39;&#34;But how nice it would be to be living in your father &#39;s house!&#39;, &#39;It &#39;s warm and free; you have a home of your own.&#34;&#39;, &#39;&#34;But what if it &#39;s worse than this?&#34;&#39;, &#39;&#34;I must take the right tone,&#34; flashed through my mind.&#39;, &#39;&#34;I may not get far with sentimentality.&#34;&#39;, &#39;But it was only a momentary thought.&#39;, &#39;I swear she really did interest me.&#39;, &#39;Besides, I was exhausted and moody.&#39;, &#39;And cunning so easily goes hand-in-hand with feeling.&#39;, &#39;&#34;Who denies it!&#34;&#39;, &#39;I hastened to answer.&#39;, &#39;&#34;Anything may happen.&#39;, &#39;I am convinced that someone has wronged you, and that you are more sinned against than sinning.&#39;, &#39;Of course, I know nothing of your story, but it &#39;s not likely a girl like you has come here of her own inclination....&#34; &#34;A girl like me?&#34;&#39;, &#39;she whispered, hardly audibly; but I heard it.&#39;, &#39;Damn it all, I was flattering her.&#39;, &#39;That was horrid.&#39;, &#39;But perhaps it was a good thing.... She was silent.&#39;, &#39;&#34;See, Liza, I will tell you about myself.&#39;, &#34;If I had had a home from childhood, I shouldn&#39;t be what I am now.&#34;, &#39;I often think that.&#39;, &#39;However bad it may be at home, anyway they are your father and mother, and not enemies, strangers.&#39;, &#34;Once a year at least, they&#39;ll show their love of you.&#34;, &#39;Anyway, you know you are at home.&#39;, &#34;I grew up without a home; and perhaps that&#39;s why I&#39;ve turned so ...&#34;, &#39;unfeeling.&#34;&#39;, &#39;I waited again.&#39;, &#39;&#34;Perhaps she doesn &#39;t understand,&#34; I thought, &#34;and, indeed, it is absurd--it &#39;s moralising.&#34;&#39;, &#39;&#34;If I were a father and had a daughter, I believe I should love my daughter more than my sons, really,&#34; I began indirectly, as though talking of something else, to distract her attention.&#39;, &#39;I must confess I blushed.&#39;, &#39;&#34;Why so?&#34;&#39;, &#39;she asked.&#39;, &#39;Ah!&#39;, &#39;so she was listening!&#39;, &#39;&#34;I don &#39;t know, Liza.&#39;, &#34;I knew a father who was a stern, austere man, but used to go down on his knees to his daughter, used to kiss her hands, her feet, he couldn&#39;t make enough of her, really.&#34;, &#39;When she danced at parties he used to stand for five hours at a stretch, gazing at her.&#39;, &#39;He was mad over her: I understand that!&#39;, &#39;She would fall asleep tired at night, and he would wake to kiss her in her sleep and make the sign of the cross over her.&#39;, &#39;He would go about in a dirty old coat, he was stingy to everyone else, but would spend his last penny for her, giving her expensive presents, and it was his greatest delight when she was pleased with what he gave her.&#39;, &#39;Fathers always love their daughters more than the mothers do.&#39;, &#39;Some girls live happily at home!&#39;, &#39;And I believe I should never let my daughters marry.&#34;&#39;, &#39;&#34;What next?&#34;&#39;, &#39;she said, with a faint smile.&#39;, &#39;&#34;I should be jealous, I really should.&#39;, &#39;To think that she should kiss anyone else!&#39;, &#39;That she should love a stranger more than her father!&#39;, &#34;It&#39;s painful to imagine it.&#34;, &#34;Of course, that&#39;s all nonsense, of course every father would be reasonable at last.&#34;, &#39;But I believe before I should let her marry, I should worry myself to death; I should find fault with all her suitors.&#39;, &#39;But I should end by letting her marry whom she herself loved.&#39;, &#39;The one whom the daughter loves always seems the worst to the father, you know.&#39;, &#39;That is always so.&#39;, &#39;So many family troubles come from that.&#34;&#39;, &#39;&#34;Some are glad to sell their daughters, rather than marrying them honourably.&#34;&#39;, &#39;Ah, so that was it!&#39;, &#39;&#34;Such a thing, Liza, happens in those accursed families in which there is neither love nor God,&#34; I retorted warmly, &#34;and where there is no love, there is no sense either.&#39;, &#34;There are such families, it&#39;s true, but I am not speaking of them.&#34;, &#39;You must have seen wickedness in your own family, if you talk like that.&#39;, &#39;Truly, you must have been unlucky.&#39;, &#34;H&#39;m!&#34;, &#39;... that sort of thing mostly comes about through poverty.&#34;&#39;, &#39;&#34;And is it any better with the gentry?&#39;, &#39;Even among the poor, honest people who live happily?&#34;&#39;, &#39;&#34;H &#39;m ... yes.&#39;, &#39;Perhaps.&#39;, &#39;Another thing, Liza, man is fond of reckoning up his troubles, but does not count his joys.&#39;, &#39;If he counted them up as he ought, he would see that every lot has enough happiness provided for it.&#39;, &#39;And what if all goes well with the family, if the blessing of God is upon it, if the husband is a good one, loves you, cherishes you, never leaves you!&#39;, &#39;There is happiness in such a family!&#39;, &#39;Even sometimes there is happiness in the midst of sorrow; and indeed sorrow is everywhere.&#39;, &#39;If you marry YOU WILL FIND OUT FOR YOURSELF.&#39;, &#39;But think of the first years of married life with one you love: what happiness, what happiness there sometimes is in it!&#39;, &#34;And indeed it&#39;s the ordinary thing.&#34;, &#34;In those early days even quarrels with one&#39;s husband end happily.&#34;, &#39;Some women get up quarrels with their husbands just because they love them.&#39;, &#39;Indeed, I knew a woman like that: she seemed to say that because she loved him, she would torment him and make him feel it.&#39;, &#39;You know that you may torment a man on purpose through love.&#39;, &#34;Women are particularly given to that, thinking to themselves &#39;I will love him so, I will make so much of him afterwards, that it&#39;s no sin to torment him a little now.&#39;&#34;, &#39;And all in the house rejoice in the sight of you, and you are happy and gay and peaceful and honourable.... Then there are some women who are jealous.&#39;, &#34;If he went off anywhere--I knew one such woman, she couldn&#39;t restrain herself, but would jump up at night and run off on the sly to find out where he was, whether he was with some other woman.&#34;, &#34;That&#39;s a pity.&#34;, &#34;And the woman knows herself it&#39;s wrong, and her heart fails her and she suffers, but she loves--it&#39;s all through love.&#34;, &#39;And how sweet it is to make up after quarrels, to own herself in the wrong or to forgive him!&#39;, &#39;And they both are so happy all at once--as though they had met anew, been married over again; as though their love had begun afresh.&#39;, &#39;And no one, no one should know what passes between husband and wife if they love one another.&#39;, &#39;And whatever quarrels there may be between them they ought not to call in their own mother to judge between them and tell tales of one another.&#39;, &#39;They are their own judges.&#39;, &#39;Love is a holy mystery and ought to be hidden from all other eyes, whatever happens.&#39;, &#39;That makes it holier and better.&#39;, &#39;They respect one another more, and much is built on respect.&#39;, &#39;And if once there has been love, if they have been married for love, why should love pass away?&#39;, &#39;Surely one can keep it!&#39;, &#39;It is rare that one cannot keep it.&#39;, &#39;And if the husband is kind and straightforward, why should not love last?&#39;, &#39;The first phase of married love will pass, it is true, but then there will come a love that is better still.&#39;, &#39;Then there will be the union of souls, they will have everything in common, there will be no secrets between them.&#39;, &#39;And once they have children, the most difficult times will seem to them happy, so long as there is love and courage.&#39;, &#39;Even toil will be a joy, you may deny yourself bread for your children and even that will be a joy, They will love you for it afterwards; so you are laying by for your future.&#39;, &#39;As the children grow up you feel that you are an example, a support for them; that even after you die your children will always keep your thoughts and feelings, because they have received them from you, they will take on your semblance and likeness.&#39;, &#39;So you see this is a great duty.&#39;, &#39;How can it fail to draw the father and mother nearer?&#39;, &#34;People say it&#39;s a trial to have children.&#34;, &#39;Who says that?&#39;, &#39;It is heavenly happiness!&#39;, &#39;Are you fond of little children, Liza?&#39;, &#39;I am awfully fond of them.&#39;, &#34;You know--a little rosy baby boy at your bosom, and what husband&#39;s heart is not touched, seeing his wife nursing his child!&#34;, &#39;A plump little rosy baby, sprawling and snuggling, chubby little hands and feet, clean tiny little nails, so tiny that it makes one laugh to look at them; eyes that look as if they understand everything.&#39;, &#39;And while it sucks it clutches at your bosom with its little hand, plays.&#39;, &#39;When its father comes up, the child tears itself away from the bosom, flings itself back, looks at its father, laughs, as though it were fearfully funny, and falls to sucking again.&#39;, &#34;Or it will bite its mother&#39;s breast when its little teeth are coming, while it looks sideways at her with its little eyes as though to say, &#39;Look, I am biting!&#39;&#34;, &#39;Is not all that happiness when they are the three together, husband, wife and child?&#39;, &#39;One can forgive a great deal for the sake of such moments.&#39;, &#39;Yes, Liza, one must first learn to live oneself before one blames others!&#34;&#39;, &#39;&#34;It &#39;s by pictures, pictures like that one must get at you,&#34; I thought to myself, though I did speak with real feeling, and all at once I flushed crimson.&#39;, &#39;&#34;What if she were suddenly to burst out laughing, what should I do then?&#34;&#39;, &#39;That idea drove me to fury.&#39;, &#39;Towards the end of my speech I really was excited, and now my vanity was somehow wounded.&#39;, &#39;The silence continued.&#39;, &#39;I almost nudged her.&#39;, &#39;&#34;Why are you--&#34; she began and stopped.&#39;, &#39;But I understood: there was a quiver of something different in her voice, not abrupt, harsh and unyielding as before, but something soft and shamefaced, so shamefaced that I suddenly felt ashamed and guilty.&#39;, &#39;&#34;What?&#34;&#39;, &#39;I asked, with tender curiosity.&#39;, &#39;&#34;Why, you...&#34; &#34;What?&#34;&#39;, &#39;&#34;Why, you ... speak somehow like a book,&#34; she said, and again there was a note of irony in her voice.&#39;, &#39;That remark sent a pang to my heart.&#39;, &#39;It was not what I was expecting.&#39;, &#39;I did not understand that she was hiding her feelings under irony, that this is usually the last refuge of modest and chaste-souled people when the privacy of their soul is coarsely and intrusively invaded, and that their pride makes them refuse to surrender till the last moment and shrink from giving expression to their feelings before you.&#39;, &#39;I ought to have guessed the truth from the timidity with which she had repeatedly approached her sarcasm, only bringing herself to utter it at last with an effort.&#39;, &#39;But I did not guess, and an evil feeling took possession of me.&#39;, &#39;&#34;Wait a bit!&#34;&#39;, &#39;I thought.&#39;, &#39;VII &#34;Oh, hush, Liza!&#39;, &#39;How can you talk about being like a book, when it makes even me, an outsider, feel sick?&#39;, &#34;Though I don&#39;t look at it as an outsider, for, indeed, it touches me to the heart.... Is it possible, is it possible that you do not feel sick at being here yourself?&#34;, &#39;Evidently habit does wonders!&#39;, &#39;God knows what habit can do with anyone.&#39;, &#39;Can you seriously think that you will never grow old, that you will always be good-looking, and that they will keep you here for ever and ever?&#39;, &#39;I say nothing of the loathsomeness of the life here....&#39;, &#39;Though let me tell you this about it--about your present life, I mean; here though you are young now, attractive, nice, with soul and feeling, yet you know as soon as I came to myself just now I felt at once sick at being here with you!&#39;, &#39;One can only come here when one is drunk.&#39;, &#39;But if you were anywhere else, living as good people live, I should perhaps be more than attracted by you, should fall in love with you, should be glad of a look from you, let alone a word; I should hang about your door, should go down on my knees to you, should look upon you as my betrothed and think it an honour to be allowed to.&#39;, &#39;I should not dare to have an impure thought about you.&#39;, &#39;But here, you see, I know that I have only to whistle and you have to come with me whether you like it or not.&#39;, &#34;I don&#39;t consult your wishes, but you mine.&#34;, &#34;The lowest labourer hires himself as a workman, but he doesn&#39;t make a slave of himself altogether; besides, he knows that he will be free again presently.&#34;, &#39;But when are you free?&#39;, &#39;Only think what you are giving up here?&#39;, &#39;What is it you are making a slave of?&#39;, &#39;It is your soul, together with your body; you are selling your soul which you have no right to dispose of!&#39;, &#39;You give your love to be outraged by every drunkard!&#39;, &#39;Love!&#39;, &#34;But that&#39;s everything, you know, it&#39;s a priceless diamond, it&#39;s a maiden&#39;s treasure, love--why, a man would be ready to give his soul, to face death to gain that love.&#34;, &#39;But how much is your love worth now?&#39;, &#39;You are sold, all of you, body and soul, and there is no need to strive for love when you can have everything without love.&#39;, &#39;And you know there is no greater insult to a girl than that, do you understand?&#39;, &#39;To be sure, I have heard that they comfort you, poor fools, they let you have lovers of your own here.&#39;, &#34;But you know that&#39;s simply a farce, that&#39;s simply a sham, it&#39;s just laughing at you, and you are taken in by it!&#34;, &#39;Why, do you suppose he really loves you, that lover of yours?&#39;, &#34;I don&#39;t believe it.&#34;, &#39;How can he love you when he knows you may be called away from him any minute?&#39;, &#39;He would be a low fellow if he did!&#39;, &#39;Will he have a grain of respect for you?&#39;, &#39;What have you in common with him?&#39;, &#39;He laughs at you and robs you--that is all his love amounts to!&#39;, &#39;You are lucky if he does not beat you.&#39;, &#39;Very likely he does beat you, too.&#39;, &#39;Ask him, if you have got one, whether he will marry you.&#39;, &#34;He will laugh in your face, if he doesn&#39;t spit in it or give you a blow--though maybe he is not worth a bad halfpenny himself.&#34;, &#39;And for what have you ruined your life, if you come to think of it?&#39;, &#39;For the coffee they give you to drink and the plentiful meals?&#39;, &#39;But with what object are they feeding you up?&#39;, &#34;An honest girl couldn&#39;t swallow the food, for she would know what she was being fed for.&#34;, &#39;You are in debt here, and, of course, you will always be in debt, and you will go on in debt to the end, till the visitors here begin to scorn you.&#39;, &#34;And that will soon happen, don&#39;t rely upon your youth--all that flies by express train here, you know.&#34;, &#39;You will be kicked out.&#39;, &#34;And not simply kicked out; long before that she&#39;ll begin nagging at you, scolding you, abusing you, as though you had not sacrificed your health for her, had not thrown away your youth and your soul for her benefit, but as though you had ruined her, beggared her, robbed her.&#34;, &#34;And don&#39;t expect anyone to take your part: the others, your companions, will attack you, too, win her favour, for all are in slavery here, and have lost all conscience and pity here long ago.&#34;, &#39;They have become utterly vile, and nothing on earth is viler, more loathsome, and more insulting than their abuse.&#39;, &#39;And you are laying down everything here, unconditionally, youth and health and beauty and hope, and at twenty-two you will look like a woman of five-and-thirty, and you will be lucky if you are not diseased, pray to God for that!&#39;, &#39;No doubt you are thinking now that you have a gay time and no work to do!&#39;, &#39;Yet there is no work harder or more dreadful in the world or ever has been.&#39;, &#39;One would think that the heart alone would be worn out with tears.&#39;, &#34;And you won&#39;t dare to say a word, not half a word when they drive you away from here; you will go away as though you were to blame.&#34;, &#39;You will change to another house, then to a third, then somewhere else, till you come down at last to the Haymarket.&#39;, &#34;There you will be beaten at every turn; that is good manners there, the visitors don&#39;t know how to be friendly without beating you.&#34;, &#34;You don&#39;t believe that it is so hateful there?&#34;, &#39;Go and look for yourself some time, you can see with your own eyes.&#39;, &#34;Once, one New Year&#39;s Day, I saw a woman at a door.&#34;, &#39;They had turned her out as a joke, to give her a taste of the frost because she had been crying so much, and they shut the door behind her.&#39;, &#34;At nine o&#39;clock in the morning she was already quite drunk, dishevelled, half-naked, covered with bruises, her face was powdered, but she had a black-eye, blood was trickling from her nose and her teeth; some cabman had just given her a drubbing.&#34;, &#39;She was sitting on the stone steps, a salt fish of some sort was in her hand; she was crying, wailing something about her luck and beating with the fish on the steps, and cabmen and drunken soldiers were crowding in the doorway taunting her.&#39;, &#34;You don&#39;t believe that you will ever be like that?&#34;, &#39;I should be sorry to believe it, too, but how do you know; maybe ten years, eight years ago that very woman with the salt fish came here fresh as a cherub, innocent, pure, knowing no evil, blushing at every word.&#39;, &#39;Perhaps she was like you, proud, ready to take offence, not like the others; perhaps she looked like a queen, and knew what happiness was in store for the man who should love her and whom she should love.&#39;, &#39;Do you see how it ended?&#39;, &#34;And what if at that very minute when she was beating on the filthy steps with that fish, drunken and dishevelled--what if at that very minute she recalled the pure early days in her father&#39;s house, when she used to go to school and the neighbour&#39;s son watched for her on the way, declaring that he would love her as long as he lived, that he would devote his life to her, and when they vowed to love one another for ever and be married as soon as they were grown up!&#34;, &#39;No, Liza, it would be happy for you if you were to die soon of consumption in some corner, in some cellar like that woman just now.&#39;, &#39;In the hospital, do you say?&#39;, &#39;You will be lucky if they take you, but what if you are still of use to the madam here?&#39;, &#39;Consumption is a queer disease, it is not like fever.&#39;, &#39;The patient goes on hoping till the last minute and says he is all right.&#39;, &#39;He deludes himself And that just suits your madam.&#39;, &#34;Don&#39;t doubt it, that&#39;s how it is; you have sold your soul, and what is more you owe money, so you daren&#39;t say a word.&#34;, &#39;But when you are dying, all will abandon you, all will turn away from you, for then there will be nothing to get from you.&#39;, &#34;What&#39;s more, they will reproach you for cumbering the place, for being so long over dying.&#34;, &#34;However you beg you won&#39;t get a drink of water without abuse: &#39;Whenever are you going off, you nasty hussy, you won&#39;t let us sleep with your moaning, you make the gentlemen sick.&#39;&#34;, &#34;That&#39;s true, I have heard such things said myself.&#34;, &#39;They will thrust you dying into the filthiest corner in the cellar--in the damp and darkness; what will your thoughts be, lying there alone?&#39;, &#39;When you die, strange hands will lay you out, with grumbling and impatience; no one will bless you, no one will sigh for you, they only want to get rid of you as soon as may be; they will buy a coffin, take you to the grave as they did that poor woman today, and celebrate your memory at the tavern.&#39;, &#34;In the grave, sleet, filth, wet snow--no need to put themselves out for you--&#39;Let her down, Vanuha; it&#39;s just like her luck--even here, she is head-foremost, the hussy.&#34;, &#34;Shorten the cord, you rascal.&#39;&#34;, &#34;&#39;It&#39;s all right as it is.&#39;&#34;, &#34;&#39;All right, is it?&#34;, &#34;Why, she&#39;s on her side!&#34;, &#39;She was a fellow-creature, after all!&#39;, &#34;But, never mind, throw the earth on her.&#39;&#34;, &#34;And they won&#39;t care to waste much time quarrelling over you.&#34;, &#39;They will scatter the wet blue clay as quick as they can and go off to the tavern ... and there your memory on earth will end; other women have children to go to their graves, fathers, husbands.&#39;, &#39;While for you neither tear, nor sigh, nor remembrance; no one in the whole world will ever come to you, your name will vanish from the face of the earth--as though you had never existed, never been born at all!&#39;, &#34;Nothing but filth and mud, however you knock at your coffin lid at night, when the dead arise, however you cry: &#39;Let me out, kind people, to live in the light of day!&#34;, &#39;My life was no life at all; my life has been thrown away like a dish-clout; it was drunk away in the tavern at the Haymarket; let me out, kind people, to live in the world again. &#39;&#34;&#39;, &#39;And I worked myself up to such a pitch that I began to have a lump in my throat myself, and ... and all at once I stopped, sat up in dismay and, bending over apprehensively, began to listen with a beating heart.&#39;, &#39;I had reason to be troubled.&#39;, &#39;I had felt for some time that I was turning her soul upside down and rending her heart, and--and the more I was convinced of it, the more eagerly I desired to gain my object as quickly and as effectually as possible.&#39;, &#39;It was the exercise of my skill that carried me away; yet it was not merely sport....&#39;, &#39;I knew I was speaking stiffly, artificially, even bookishly, in fact, I could not speak except &#34;like a book.&#34;&#39;, &#39;But that did not trouble me: I knew, I felt that I should be understood and that this very bookishness might be an assistance.&#39;, &#39;But now, having attained my effect, I was suddenly panic-stricken.&#39;, &#39;Never before had I witnessed such despair!&#39;, &#39;She was lying on her face, thrusting her face into the pillow and clutching it in both hands.&#39;, &#39;Her heart was being torn.&#39;, &#39;Her youthful body was shuddering all over as though in convulsions.&#39;, &#39;Suppressed sobs rent her bosom and suddenly burst out in weeping and wailing, then she pressed closer into the pillow: she did not want anyone here, not a living soul, to know of her anguish and her tears.&#39;, &#39;She bit the pillow, bit her hand till it bled (I saw that afterwards), or, thrusting her fingers into her dishevelled hair, seemed rigid with the effort of restraint, holding her breath and clenching her teeth.&#39;, &#39;I began saying something, begging her to calm herself, but felt that I did not dare; and all at once, in a sort of cold shiver, almost in terror, began fumbling in the dark, trying hurriedly to get dressed to go.&#39;, &#39;It was dark; though I tried my best I could not finish dressing quickly.&#39;, &#39;Suddenly I felt a box of matches and a candlestick with a whole candle in it.&#39;, &#39;As soon as the room was lighted up, Liza sprang up, sat up in bed, and with a contorted face, with a half insane smile, looked at me almost senselessly.&#39;, &#39;I sat down beside her and took her hands; she came to herself, made an impulsive movement towards me, would have caught hold of me, but did not dare, and slowly bowed her head before me.&#39;, &#39;&#34;Liza, my dear, I was wrong ... forgive me, my dear,&#34; I began, but she squeezed my hand in her fingers so tightly that I felt I was saying the wrong thing and stopped.&#39;, &#39;&#34;This is my address, Liza, come to me.&#34;&#39;, &#39;&#34;I will come,&#34; she answered resolutely, her head still bowed.&#39;, &#39;&#34;But now I am going, good-bye ... till we meet again.&#34;&#39;, &#39;I got up; she, too, stood up and suddenly flushed all over, gave a shudder, snatched up a shawl that was lying on a chair and muffled herself in it to her chin.&#39;, &#39;As she did this she gave another sickly smile, blushed and looked at me strangely.&#39;, &#39;I felt wretched; I was in haste to get away--to disappear.&#39;, &#39;&#34;Wait a minute,&#34; she said suddenly, in the passage just at the doorway, stopping me with her hand on my overcoat.&#39;, &#39;She put down the candle in hot haste and ran off; evidently she had thought of something or wanted to show me something.&#39;, &#39;As she ran away she flushed, her eyes shone, and there was a smile on her lips--what was the meaning of it?&#39;, &#39;Against my will I waited: she came back a minute later with an expression that seemed to ask forgiveness for something.&#39;, &#39;In fact, it was not the same face, not the same look as the evening before: sullen, mistrustful and obstinate.&#39;, &#39;Her eyes now were imploring, soft, and at the same time trustful, caressing, timid.&#39;, &#39;The expression with which children look at people they are very fond of, of whom they are asking a favour.&#39;, &#39;Her eyes were a light hazel, they were lovely eyes, full of life, and capable of expressing love as well as sullen hatred.&#39;, &#39;Making no explanation, as though I, as a sort of higher being, must understand everything without explanations, she held out a piece of paper to me.&#39;, &#39;Her whole face was positively beaming at that instant with naive, almost childish, triumph.&#39;, &#39;I unfolded it.&#39;, &#39;It was a letter to her from a medical student or someone of that sort--a very high-flown and flowery, but extremely respectful, love-letter.&#39;, &#34;I don&#39;t recall the words now, but I remember well that through the high-flown phrases there was apparent a genuine feeling, which cannot be feigned.&#34;, &#39;When I had finished reading it I met her glowing, questioning, and childishly impatient eyes fixed upon me.&#39;, &#39;She fastened her eyes upon my face and waited impatiently for what I should say.&#39;, &#39;In a few words, hurriedly, but with a sort of joy and pride, she explained to me that she had been to a dance somewhere in a private house, a family of &#34;very nice people, WHO KNEW NOTHING, absolutely nothing, for she had only come here so lately and it had all happened ... and she hadn &#39;t made up her mind to stay and was certainly going away as soon as she had paid her debt...&#34; and at that party there had been the student who had danced with her all the evening.&#39;, &#39;He had talked to her, and it turned out that he had known her in old days at Riga when he was a child, they had played together, but a very long time ago--and he knew her parents, but ABOUT THIS he knew nothing, nothing whatever, and had no suspicion!&#39;, &#39;And the day after the dance (three days ago) he had sent her that letter through the friend with whom she had gone to the party ... and ... well, that was all.&#39;, &#39;She dropped her shining eyes with a sort of bashfulness as she finished.&#39;, &#34;The poor girl was keeping that student&#39;s letter as a precious treasure, and had run to fetch it, her only treasure, because she did not want me to go away without knowing that she, too, was honestly and genuinely loved; that she, too, was addressed respectfully.&#34;, &#39;No doubt that letter was destined to lie in her box and lead to nothing.&#39;, &#39;But none the less, I am certain that she would keep it all her life as a precious treasure, as her pride and justification, and now at such a minute she had thought of that letter and brought it with naive pride to raise herself in my eyes that I might see, that I, too, might think well of her.&#39;, &#39;I said nothing, pressed her hand and went out.&#39;, &#39;I so longed to get away ...&#39;, &#39;I walked all the way home, in spite of the fact that the melting snow was still falling in heavy flakes.&#39;, &#39;I was exhausted, shattered, in bewilderment.&#39;, &#39;But behind the bewilderment the truth was already gleaming.&#39;, &#39;The loathsome truth.&#39;, &#39;VIII It was some time, however, before I consented to recognise that truth.&#39;, &#39;Waking up in the morning after some hours of heavy, leaden sleep, and immediately realising all that had happened on the previous day, I was positively amazed at my last night &#39;s SENTIMENTALITY with Liza, at all those &#34;outcries of horror and pity.&#34;&#39;, &#39;&#34;To think of having such an attack of womanish hysteria, pah!&#34;&#39;, &#39;I concluded.&#39;, &#39;And what did I thrust my address upon her for?&#39;, &#39;What if she comes?&#39;, &#34;Let her come, though; it doesn&#39;t matter....&#34;, &#39;But OBVIOUSLY, that was not now the chief and the most important matter: I had to make haste and at all costs save my reputation in the eyes of Zverkov and Simonov as quickly as possible; that was the chief business.&#39;, &#39;And I was so taken up that morning that I actually forgot all about Liza.&#39;, &#39;First of all I had at once to repay what I had borrowed the day before from Simonov.&#39;, &#39;I resolved on a desperate measure: to borrow fifteen roubles straight off from Anton Antonitch.&#39;, &#39;As luck would have it he was in the best of humours that morning, and gave it to me at once, on the first asking.&#39;, &#39;I was so delighted at this that, as I signed the IOU with a swaggering air, I told him casually that the night before &#34;I had been keeping it up with some friends at the Hotel de Paris; we were giving a farewell party to a comrade, in fact, I might say a friend of my childhood, and you know--a desperate rake, fearfully spoilt--of course, he belongs to a good family, and has considerable means, a brilliant career; he is witty, charming, a regular Lovelace, you understand; we drank an extra &#39;half-dozen &#39; and ...&#34; And it went off all right; all this was uttered very easily, unconstrainedly and complacently.&#39;, &#39;On reaching home I promptly wrote to Simonov.&#39;, &#39;To this hour I am lost in admiration when I recall the truly gentlemanly, good-humoured, candid tone of my letter.&#39;, &#39;With tact and good-breeding, and, above all, entirely without superfluous words, I blamed myself for all that had happened.&#39;, &#39;I defended myself, &#34;if I really may be allowed to defend myself,&#34; by alleging that being utterly unaccustomed to wine, I had been intoxicated with the first glass, which I said, I had drunk before they arrived, while I was waiting for them at the Hotel de Paris between five and six o &#39;clock.&#39;, &#39;I begged Simonov &#39;s pardon especially; I asked him to convey my explanations to all the others, especially to Zverkov, whom &#34;I seemed to remember as though in a dream&#34; I had insulted.&#39;, &#39;I added that I would have called upon all of them myself, but my head ached, and besides I had not the face to.&#39;, &#39;I was particularly pleased with a certain lightness, almost carelessness (strictly within the bounds of politeness, however), which was apparent in my style, and better than any possible arguments, gave them at once to understand that I took rather an independent view of &#34;all that unpleasantness last night&#34;; that I was by no means so utterly crushed as you, my friends, probably imagine; but on the contrary, looked upon it as a gentleman serenely respecting himself should look upon it.&#39;, &#39;&#34;On a young hero &#39;s past no censure is cast!&#34;&#39;, &#39;&#34;There is actually an aristocratic playfulness about it!&#34;&#39;, &#39;I thought admiringly, as I read over the letter.&#39;, &#39;&#34;And it &#39;s all because I am an intellectual and cultivated man!&#39;, &#34;Another man in my place would not have known how to extricate himself, but here I have got out of it and am as jolly as ever again, and all because I am &#39;a cultivated and educated man of our day.&#39;&#34;, &#39;And, indeed, perhaps, everything was due to the wine yesterday.&#39;, &#39;H &#39;m!&#34;&#39;, &#39;... No, it was not the wine.&#39;, &#39;I did not drink anything at all between five and six when I was waiting for them.&#39;, &#34;I had lied to Simonov; I had lied shamelessly; and indeed I wasn&#39;t ashamed now....&#34;, &#39;Hang it all though, the great thing was that I was rid of it.&#39;, &#39;I put six roubles in the letter, sealed it up, and asked Apollon to take it to Simonov.&#39;, &#39;When he learned that there was money in the letter, Apollon became more respectful and agreed to take it.&#39;, &#39;Towards evening I went out for a walk.&#39;, &#39;My head was still aching and giddy after yesterday.&#39;, &#39;But as evening came on and the twilight grew denser, my impressions and, following them, my thoughts, grew more and more different and confused.&#39;, &#39;Something was not dead within me, in the depths of my heart and conscience it would not die, and it showed itself in acute depression.&#39;, &#39;For the most part I jostled my way through the most crowded business streets, along Myeshtchansky Street, along Sadovy Street and in Yusupov Garden.&#39;, &#39;I always liked particularly sauntering along these streets in the dusk, just when there were crowds of working people of all sorts going home from their daily work, with faces looking cross with anxiety.&#39;, &#39;What I liked was just that cheap bustle, that bare prose.&#39;, &#39;On this occasion the jostling of the streets irritated me more than ever, I could not make out what was wrong with me, I could not find the clue, something seemed rising up continually in my soul, painfully, and refusing to be appeased.&#39;, &#39;I returned home completely upset, it was just as though some crime were lying on my conscience.&#39;, &#39;The thought that Liza was coming worried me continually.&#39;, &#39;It seemed queer to me that of all my recollections of yesterday this tormented me, as it were, especially, as it were, quite separately.&#39;, &#39;Everything else I had quite succeeded in forgetting by the evening; I dismissed it all and was still perfectly satisfied with my letter to Simonov.&#39;, &#39;But on this point I was not satisfied at all.&#39;, &#39;It was as though I were worried only by Liza.&#39;, &#39;&#34;What if she comes,&#34; I thought incessantly, &#34;well, it doesn &#39;t matter, let her come!&#39;, &#34;H&#39;m!&#34;, &#34;it&#39;s horrid that she should see, for instance, how I live.&#34;, &#34;Yesterday I seemed such a hero to her, while now, h&#39;m!&#34;, &#34;It&#39;s horrid, though, that I have let myself go so, the room looks like a beggar&#39;s.&#34;, &#39;And I brought myself to go out to dinner in such a suit!&#39;, &#39;And my American leather sofa with the stuffing sticking out.&#39;, &#39;And my dressing-gown, which will not cover me, such tatters, and she will see all this and she will see Apollon.&#39;, &#39;That beast is certain to insult her.&#39;, &#39;He will fasten upon her in order to be rude to me.&#39;, &#39;And I, of course, shall be panic-stricken as usual, I shall begin bowing and scraping before her and pulling my dressing-gown round me, I shall begin smiling, telling lies.&#39;, &#39;Oh, the beastliness!&#39;, &#34;And it isn&#39;t the beastliness of it that matters most!&#34;, &#39;There is something more important, more loathsome, viler!&#39;, &#39;Yes, viler!&#39;, &#39;And to put on that dishonest lying mask again!&#39;, &#39;...&#34; When I reached that thought I fired up all at once.&#39;, &#39;&#34;Why dishonest?&#39;, &#39;How dishonest?&#39;, &#39;I was speaking sincerely last night.&#39;, &#39;I remember there was real feeling in me, too.&#39;, &#39;What I wanted was to excite an honourable feeling in her....&#39;, &#39;Her crying was a good thing, it will have a good effect.&#34;&#39;, &#39;Yet I could not feel at ease.&#39;, &#34;All that evening, even when I had come back home, even after nine o&#39;clock, when I calculated that Liza could not possibly come, still she haunted me, and what was worse, she came back to my mind always in the same position.&#34;, &#39;One moment out of all that had happened last night stood vividly before my imagination; the moment when I struck a match and saw her pale, distorted face, with its look of torture.&#39;, &#39;And what a pitiful, what an unnatural, what a distorted smile she had at that moment!&#39;, &#39;But I did not know then, that fifteen years later I should still in my imagination see Liza, always with the pitiful, distorted, inappropriate smile which was on her face at that minute.&#39;, &#39;Next day I was ready again to look upon it all as nonsense, due to over-excited nerves, and, above all, as EXAGGERATED.&#39;, &#39;I was always conscious of that weak point of mine, and sometimes very much afraid of it.&#39;, &#39;&#34;I exaggerate everything, that is where I go wrong,&#34; I repeated to myself every hour.&#39;, &#39;But, however, &#34;Liza will very likely come all the same,&#34; was the refrain with which all my reflections ended.&#39;, &#39;I was so uneasy that I sometimes flew into a fury: &#34;She &#39;ll come, she is certain to come!&#34;&#39;, &#39;I cried, running about the room, &#34;if not today, she will come tomorrow; she &#39;ll find me out!&#39;, &#39;The damnable romanticism of these pure hearts!&#39;, &#34;Oh, the vileness--oh, the silliness--oh, the stupidity of these &#39;wretched sentimental souls!&#39;&#34;, &#39;Why, how fail to understand?&#39;, &#39;How could one fail to understand?&#39;, &#39;...&#34; But at this point I stopped short, and in great confusion, indeed.&#39;, &#39;And how few, how few words, I thought, in passing, were needed; how little of the idyllic (and affectedly, bookishly, artificially idyllic too) had sufficed to turn a whole human life at once according to my will.&#39;, &#34;That&#39;s virginity, to be sure!&#34;, &#39;Freshness of soil!&#39;, &#39;At times a thought occurred to me, to go to her, &#34;to tell her all,&#34; and beg her not to come to me.&#39;, &#39;But this thought stirred such wrath in me that I believed I should have crushed that &#34;damned&#34; Liza if she had chanced to be near me at the time.&#39;, &#39;I should have insulted her, have spat at her, have turned her out, have struck her!&#39;, &#39;One day passed, however, another and another; she did not come and I began to grow calmer.&#39;, &#34;I felt particularly bold and cheerful after nine o&#39;clock, I even sometimes began dreaming, and rather sweetly: I, for instance, became the salvation of Liza, simply through her coming to me and my talking to her....&#34;, &#39;I develop her, educate her.&#39;, &#39;Finally, I notice that she loves me, loves me passionately.&#39;, &#34;I pretend not to understand (I don&#39;t know, however, why I pretend, just for effect, perhaps).&#34;, &#39;At last all confusion, transfigured, trembling and sobbing, she flings herself at my feet and says that I am her saviour, and that she loves me better than anything in the world.&#39;, &#39;I am amazed, but.... &#34;Liza,&#34; I say, &#34;can you imagine that I have not noticed your love?&#39;, &#39;I saw it all, I divined it, but I did not dare to approach you first, because I had an influence over you and was afraid that you would force yourself, from gratitude, to respond to my love, would try to rouse in your heart a feeling which was perhaps absent, and I did not wish that ... because it would be tyranny ... it would be indelicate (in short, I launch off at that point into European, inexplicably lofty subtleties a la George Sand), but now, now you are mine, you are my creation, you are pure, you are good, you are my noble wife.&#39;, &#39; &#39;Into my house come bold and free, Its rightful mistress there to be &#39;.&#34;&#39;, &#39;Then we begin living together, go abroad and so on, and so on.&#39;, &#39;In fact, in the end it seemed vulgar to me myself, and I began putting out my tongue at myself.&#39;, &#39;Besides, they won &#39;t let her out, &#34;the hussy!&#34;&#39;, &#39;I thought.&#39;, &#34;They don&#39;t let them go out very readily, especially in the evening (for some reason I fancied she would come in the evening, and at seven o&#39;clock precisely).&#34;, &#34;Though she did say she was not altogether a slave there yet, and had certain rights; so, h&#39;m!&#34;, &#39;Damn it all, she will come, she is sure to come!&#39;, &#39;It was a good thing, in fact, that Apollon distracted my attention at that time by his rudeness.&#39;, &#39;He drove me beyond all patience!&#39;, &#39;He was the bane of my life, the curse laid upon me by Providence.&#39;, &#39;We had been squabbling continually for years, and I hated him.&#39;, &#39;My God, how I hated him!&#39;, &#39;I believe I had never hated anyone in my life as I hated him, especially at some moments.&#39;, &#39;He was an elderly, dignified man, who worked part of his time as a tailor.&#39;, &#39;But for some unknown reason he despised me beyond all measure, and looked down upon me insufferably.&#39;, &#39;Though, indeed, he looked down upon everyone.&#39;, &#39;Simply to glance at that flaxen, smoothly brushed head, at the tuft of hair he combed up on his forehead and oiled with sunflower oil, at that dignified mouth, compressed into the shape of the letter V, made one feel one was confronting a man who never doubted of himself.&#39;, &#39;He was a pedant, to the most extreme point, the greatest pedant I had met on earth, and with that had a vanity only befitting Alexander of Macedon.&#39;, &#39;He was in love with every button on his coat, every nail on his fingers--absolutely in love with them, and he looked it!&#39;, &#39;In his behaviour to me he was a perfect tyrant, he spoke very little to me, and if he chanced to glance at me he gave me a firm, majestically self-confident and invariably ironical look that drove me sometimes to fury.&#39;, &#39;He did his work with the air of doing me the greatest favour, though he did scarcely anything for me, and did not, indeed, consider himself bound to do anything.&#39;, &#39;There could be no doubt that he looked upon me as the greatest fool on earth, and that &#34;he did not get rid of me&#34; was simply that he could get wages from me every month.&#39;, &#39;He consented to do nothing for me for seven roubles a month.&#39;, &#39;Many sins should be forgiven me for what I suffered from him.&#39;, &#39;My hatred reached such a point that sometimes his very step almost threw me into convulsions.&#39;, &#39;What I loathed particularly was his lisp.&#39;, &#39;His tongue must have been a little too long or something of that sort, for he continually lisped, and seemed to be very proud of it, imagining that it greatly added to his dignity.&#39;, &#39;He spoke in a slow, measured tone, with his hands behind his back and his eyes fixed on the ground.&#39;, &#39;He maddened me particularly when he read aloud the psalms to himself behind his partition.&#39;, &#39;Many a battle I waged over that reading!&#39;, &#39;But he was awfully fond of reading aloud in the evenings, in a slow, even, sing-song voice, as though over the dead.&#39;, &#39;It is interesting that that is how he has ended: he hires himself out to read the psalms over the dead, and at the same time he kills rats and makes blacking.&#39;, &#39;But at that time I could not get rid of him, it was as though he were chemically combined with my existence.&#39;, &#39;Besides, nothing would have induced him to consent to leave me.&#39;, &#39;I could not live in furnished lodgings: my lodging was my private solitude, my shell, my cave, in which I concealed myself from all mankind, and Apollon seemed to me, for some reason, an integral part of that flat, and for seven years I could not turn him away.&#39;, &#39;To be two or three days behind with his wages, for instance, was impossible.&#39;, &#39;He would have made such a fuss, I should not have known where to hide my head.&#39;, &#39;But I was so exasperated with everyone during those days, that I made up my mind for some reason and with some object to PUNISH Apollon and not to pay him for a fortnight the wages that were owing him.&#39;, &#39;I had for a long time--for the last two years--been intending to do this, simply in order to teach him not to give himself airs with me, and to show him that if I liked I could withhold his wages.&#39;, &#39;I purposed to say nothing to him about it, and was purposely silent indeed, in order to score off his pride and force him to be the first to speak of his wages.&#39;, &#39;Then I would take the seven roubles out of a drawer, show him I have the money put aside on purpose, but that I won &#39;t, I won &#39;t, I simply won &#39;t pay him his wages, I won &#39;t just because that is &#34;what I wish,&#34; because &#34;I am master, and it is for me to decide,&#34; because he has been disrespectful, because he has been rude; but if he were to ask respectfully I might be softened and give it to him, otherwise he might wait another fortnight, another three weeks, a whole month....&#39;, &#39;But angry as I was, yet he got the better of me.&#39;, &#39;I could not hold out for four days.&#39;, &#39;He began as he always did begin in such cases, for there had been such cases already, there had been attempts (and it may be observed I knew all this beforehand, I knew his nasty tactics by heart).&#39;, &#39;He would begin by fixing upon me an exceedingly severe stare, keeping it up for several minutes at a time, particularly on meeting me or seeing me out of the house.&#39;, &#39;If I held out and pretended not to notice these stares, he would, still in silence, proceed to further tortures.&#39;, &#39;All at once, A PROPOS of nothing, he would walk softly and smoothly into my room, when I was pacing up and down or reading, stand at the door, one hand behind his back and one foot behind the other, and fix upon me a stare more than severe, utterly contemptuous.&#39;, &#39;If I suddenly asked him what he wanted, he would make me no answer, but continue staring at me persistently for some seconds, then, with a peculiar compression of his lips and a most significant air, deliberately turn round and deliberately go back to his room.&#39;, &#39;Two hours later he would come out again and again present himself before me in the same way.&#39;, &#39;It had happened that in my fury I did not even ask him what he wanted, but simply raised my head sharply and imperiously and began staring back at him.&#39;, &#39;So we stared at one another for two minutes; at last he turned with deliberation and dignity and went back again for two hours.&#39;, &#39;If I were still not brought to reason by all this, but persisted in my revolt, he would suddenly begin sighing while he looked at me, long, deep sighs as though measuring by them the depths of my moral degradation, and, of course, it ended at last by his triumphing completely: I raged and shouted, but still was forced to do what he wanted.&#39;, &#39;This time the usual staring manoeuvres had scarcely begun when I lost my temper and flew at him in a fury.&#39;, &#39;I was irritated beyond endurance apart from him.&#39;, &#39;&#34;Stay,&#34; I cried, in a frenzy, as he was slowly and silently turning, with one hand behind his back, to go to his room.&#39;, &#39;&#34;Stay!&#39;, &#39;Come back, come back, I tell you!&#34;&#39;, &#39;and I must have bawled so unnaturally, that he turned round and even looked at me with some wonder.&#39;, &#39;However, he persisted in saying nothing, and that infuriated me.&#39;, &#39;&#34;How dare you come and look at me like that without being sent for?&#39;, &#39;Answer!&#34;&#39;, &#39;After looking at me calmly for half a minute, he began turning round again.&#39;, &#39;&#34;Stay!&#34;&#39;, &#39;I roared, running up to him, &#34;don &#39;t stir!&#39;, &#39;There.&#39;, &#39;Answer, now: what did you come in to look at?&#34;&#39;, &#39;&#34;If you have any order to give me it &#39;s my duty to carry it out,&#34; he answered, after another silent pause, with a slow, measured lisp, raising his eyebrows and calmly twisting his head from one side to another, all this with exasperating composure.&#39;, &#39;&#34;That &#39;s not what I am asking you about, you torturer!&#34;&#39;, &#39;I shouted, turning crimson with anger.&#39;, &#39;&#34;I &#39;ll tell you why you came here myself: you see, I don &#39;t give you your wages, you are so proud you don &#39;t want to bow down and ask for it, and so you come to punish me with your stupid stares, to worry me and you have no sus-pic-ion how stupid it is--stupid, stupid, stupid, stupid!&#39;, &#39;...&#34; He would have turned round again without a word, but I seized him.&#39;, &#39;&#34;Listen,&#34; I shouted to him.&#39;, &#39;&#34;Here &#39;s the money, do you see, here it is,&#34; (I took it out of the table drawer); &#34;here &#39;s the seven roubles complete, but you are not going to have it, you ... are ... not ... going ... to ... have it until you come respectfully with bowed head to beg my pardon.&#39;, &#39;Do you hear?&#34;&#39;, &#39;&#34;That cannot be,&#34; he answered, with the most unnatural self-confidence.&#39;, &#39;&#34;It shall be so,&#34; I said, &#34;I give you my word of honour, it shall be!&#34;&#39;, &#39;&#34;And there &#39;s nothing for me to beg your pardon for,&#34; he went on, as though he had not noticed my exclamations at all.&#39;, &#39;&#34;Why, besides, you called me a &#39;torturer, &#39; for which I can summon you at the police-station at any time for insulting behaviour.&#34;&#39;, &#39;&#34;Go, summon me,&#34; I roared, &#34;go at once, this very minute, this very second!&#39;, &#39;You are a torturer all the same!&#39;, &#39;a torturer!&#34;&#39;, &#39;But he merely looked at me, then turned, and regardless of my loud calls to him, he walked to his room with an even step and without looking round.&#39;, &#39;&#34;If it had not been for Liza nothing of this would have happened,&#34; I decided inwardly.&#39;, &#39;Then, after waiting a minute, I went myself behind his screen with a dignified and solemn air, though my heart was beating slowly and violently.&#39;, &#39;&#34;Apollon,&#34; I said quietly and emphatically, though I was breathless, &#34;go at once without a minute &#39;s delay and fetch the police-officer.&#34;&#39;, &#39;He had meanwhile settled himself at his table, put on his spectacles and taken up some sewing.&#39;, &#39;But, hearing my order, he burst into a guffaw.&#39;, &#39;&#34;At once, go this minute!&#39;, &#39;Go on, or else you can &#39;t imagine what will happen.&#34;&#39;, &#39;&#34;You are certainly out of your mind,&#34; he observed, without even raising his head, lisping as deliberately as ever and threading his needle.&#39;, &#39;&#34;Whoever heard of a man sending for the police against himself?&#39;, &#39;And as for being frightened--you are upsetting yourself about nothing, for nothing will come of it.&#34;&#39;, &#39;&#34;Go!&#34;&#39;, &#39;I shrieked, clutching him by the shoulder.&#39;, &#39;I felt I should strike him in a minute.&#39;, &#39;But I did not notice the door from the passage softly and slowly open at that instant and a figure come in, stop short, and begin staring at us in perplexity I glanced, nearly swooned with shame, and rushed back to my room.&#39;, &#39;There, clutching at my hair with both hands, I leaned my head against the wall and stood motionless in that position.&#39;, &#34;Two minutes later I heard Apollon&#39;s deliberate footsteps.&#34;, &#39;&#34;There is some woman asking for you,&#34; he said, looking at me with peculiar severity.&#39;, &#39;Then he stood aside and let in Liza.&#39;, &#39;He would not go away, but stared at us sarcastically.&#39;, &#39;&#34;Go away, go away,&#34; I commanded in desperation.&#39;, &#39;At that moment my clock began whirring and wheezing and struck seven.&#39;, &#39;IX &#34;Into my house come bold and free, Its rightful mistress there to be.&#34;&#39;, &#39;I stood before her crushed, crestfallen, revoltingly confused, and I believe I smiled as I did my utmost to wrap myself in the skirts of my ragged wadded dressing-gown--exactly as I had imagined the scene not long before in a fit of depression.&#39;, &#39;After standing over us for a couple of minutes Apollon went away, but that did not make me more at ease.&#39;, &#39;What made it worse was that she, too, was overwhelmed with confusion, more so, in fact, than I should have expected.&#39;, &#39;At the sight of me, of course.&#39;, &#39;&#34;Sit down,&#34; I said mechanically, moving a chair up to the table, and I sat down on the sofa.&#39;, &#39;She obediently sat down at once and gazed at me open-eyed, evidently expecting something from me at once.&#39;, &#39;This naivete of expectation drove me to fury, but I restrained myself.&#39;, &#39;She ought to have tried not to notice, as though everything had been as usual, while instead of that, she ... and I dimly felt that I should make her pay dearly for ALL THIS.&#39;, &#39;&#34;You have found me in a strange position, Liza,&#34; I began, stammering and knowing that this was the wrong way to begin.&#39;, &#39;&#34;No, no, don &#39;t imagine anything,&#34; I cried, seeing that she had suddenly flushed.&#39;, &#39;&#34;I am not ashamed of my poverty.... On the contrary, I look with pride on my poverty.&#39;, &#39;I am poor but honourable.... One can be poor and honourable,&#34; I muttered.&#39;, &#39;&#34;However ... would you like tea?....&#34;&#39;, &#39;&#34;No,&#34; she was beginning.&#39;, &#39;&#34;Wait a minute.&#34;&#39;, &#39;I leapt up and ran to Apollon.&#39;, &#39;I had to get out of the room somehow.&#39;, &#39;&#34;Apollon,&#34; I whispered in feverish haste, flinging down before him the seven roubles which had remained all the time in my clenched fist, &#34;here are your wages, you see I give them to you; but for that you must come to my rescue: bring me tea and a dozen rusks from the restaurant.&#39;, &#34;If you won&#39;t go, you&#39;ll make me a miserable man!&#34;, &#34;You don&#39;t know what this woman is....&#34;, &#39;This is--everything!&#39;, &#39;You may be imagining something....&#39;, &#34;But you don&#39;t know what that woman is!&#34;, &#39;...&#34; Apollon, who had already sat down to his work and put on his spectacles again, at first glanced askance at the money without speaking or putting down his needle; then, without paying the slightest attention to me or making any answer, he went on busying himself with his needle, which he had not yet threaded.&#39;, &#39;I waited before him for three minutes with my arms crossed A LA NAPOLEON.&#39;, &#39;My temples were moist with sweat.&#39;, &#39;I was pale, I felt it.&#39;, &#39;But, thank God, he must have been moved to pity, looking at me.&#39;, &#39;Having threaded his needle he deliberately got up from his seat, deliberately moved back his chair, deliberately took off his spectacles, deliberately counted the money, and finally asking me over his shoulder: &#34;Shall I get a whole portion?&#34;&#39;, &#39;deliberately walked out of the room.&#39;, &#34;As I was going back to Liza, the thought occurred to me on the way: shouldn&#39;t I run away just as I was in my dressing-gown, no matter where, and then let happen what would?&#34;, &#39;I sat down again.&#39;, &#39;She looked at me uneasily.&#39;, &#39;For some minutes we were silent.&#39;, &#39;&#34;I will kill him,&#34; I shouted suddenly, striking the table with my fist so that the ink spurted out of the inkstand.&#39;, &#39;&#34;What are you saying!&#34;&#39;, &#39;she cried, starting.&#39;, &#39;&#34;I will kill him!&#39;, &#39;kill him!&#34;&#39;, &#39;I shrieked, suddenly striking the table in absolute frenzy, and at the same time fully understanding how stupid it was to be in such a frenzy.&#39;, &#39;&#34;You don &#39;t know, Liza, what that torturer is to me.&#39;, &#39;He is my torturer....&#39;, &#39;He has gone now to fetch some rusks; he ...&#34; And suddenly I burst into tears.&#39;, &#39;It was an hysterical attack.&#39;, &#39;How ashamed I felt in the midst of my sobs; but still I could not restrain them.&#39;, &#39;She was frightened.&#39;, &#39;&#34;What is the matter?&#39;, &#39;What is wrong?&#34;&#39;, &#39;she cried, fussing about me.&#39;, &#39;&#34;Water, give me water, over there!&#34;&#39;, &#39;I muttered in a faint voice, though I was inwardly conscious that I could have got on very well without water and without muttering in a faint voice.&#39;, &#39;But I was, what is called, PUTTING IT ON, to save appearances, though the attack was a genuine one.&#39;, &#39;She gave me water, looking at me in bewilderment.&#39;, &#39;At that moment Apollon brought in the tea.&#39;, &#39;It suddenly seemed to me that this commonplace, prosaic tea was horribly undignified and paltry after all that had happened, and I blushed crimson.&#39;, &#39;Liza looked at Apollon with positive alarm.&#39;, &#39;He went out without a glance at either of us.&#39;, &#39;&#34;Liza, do you despise me?&#34;&#39;, &#39;I asked, looking at her fixedly, trembling with impatience to know what she was thinking.&#39;, &#39;She was confused, and did not know what to answer.&#39;, &#39;&#34;Drink your tea,&#34; I said to her angrily.&#39;, &#39;I was angry with myself, but, of course, it was she who would have to pay for it.&#39;, &#39;A horrible spite against her suddenly surged up in my heart; I believe I could have killed her.&#39;, &#39;To revenge myself on her I swore inwardly not to say a word to her all the time.&#39;, &#39;&#34;She is the cause of it all,&#34; I thought.&#39;, &#39;Our silence lasted for five minutes.&#39;, &#39;The tea stood on the table; we did not touch it.&#39;, &#39;I had got to the point of purposely refraining from beginning in order to embarrass her further; it was awkward for her to begin alone.&#39;, &#39;Several times she glanced at me with mournful perplexity.&#39;, &#39;I was obstinately silent.&#39;, &#39;I was, of course, myself the chief sufferer, because I was fully conscious of the disgusting meanness of my spiteful stupidity, and yet at the same time I could not restrain myself.&#39;, &#39;&#34;I want to... get away ... from there altogether,&#34; she began, to break the silence in some way, but, poor girl, that was just what she ought not to have spoken about at such a stupid moment to a man so stupid as I was.&#39;, &#39;My heart positively ached with pity for her tactless and unnecessary straightforwardness.&#39;, &#39;But something hideous at once stifled all compassion in me; it even provoked me to greater venom.&#39;, &#39;I did not care what happened.&#39;, &#39;Another five minutes passed.&#39;, &#39;&#34;Perhaps I am in your way,&#34; she began timidly, hardly audibly, and was getting up.&#39;, &#39;But as soon as I saw this first impulse of wounded dignity I positively trembled with spite, and at once burst out.&#39;, &#39;&#34;Why have you come to me, tell me that, please?&#34;&#39;, &#39;I began, gasping for breath and regardless of logical connection in my words.&#39;, &#39;I longed to have it all out at once, at one burst; I did not even trouble how to begin.&#39;, &#39;&#34;Why have you come?&#39;, &#39;Answer, answer,&#34; I cried, hardly knowing what I was doing.&#39;, &#39;&#34;I &#39;ll tell you, my good girl, why you have come.&#39;, &#34;You&#39;ve come because I talked sentimental stuff to you then.&#34;, &#39;So now you are soft as butter and longing for fine sentiments again.&#39;, &#39;So you may as well know that I was laughing at you then.&#39;, &#39;And I am laughing at you now.&#39;, &#39;Why are you shuddering?&#39;, &#39;Yes, I was laughing at you!&#39;, &#39;I had been insulted just before, at dinner, by the fellows who came that evening before me.&#39;, &#34;I came to you, meaning to thrash one of them, an officer; but I didn&#39;t succeed, I didn&#39;t find him; I had to avenge the insult on someone to get back my own again; you turned up, I vented my spleen on you and laughed at you.&#34;, &#34;I had been humiliated, so I wanted to humiliate; I had been treated like a rag, so I wanted to show my power.... That&#39;s what it was, and you imagined I had come there on purpose to save you.&#34;, &#39;Yes?&#39;, &#39;You imagined that?&#39;, &#39;You imagined that?&#34;&#39;, &#39;I knew that she would perhaps be muddled and not take it all in exactly, but I knew, too, that she would grasp the gist of it, very well indeed.&#39;, &#39;And so, indeed, she did.&#39;, &#39;She turned white as a handkerchief, tried to say something, and her lips worked painfully; but she sank on a chair as though she had been felled by an axe.&#39;, &#39;And all the time afterwards she listened to me with her lips parted and her eyes wide open, shuddering with awful terror.&#39;, &#39;The cynicism, the cynicism of my words overwhelmed her.... &#34;Save you!&#34;&#39;, &#39;I went on, jumping up from my chair and running up and down the room before her.&#39;, &#39;&#34;Save you from what?&#39;, &#39;But perhaps I am worse than you myself.&#39;, &#34;Why didn&#39;t you throw it in my teeth when I was giving you that sermon: &#39;But what did you come here yourself for?&#34;, &#34;was it to read us a sermon?&#39;&#34;, &#39;Power, power was what I wanted then, sport was what I wanted, I wanted to wring out your tears, your humiliation, your hysteria--that was what I wanted then!&#39;, &#34;Of course, I couldn&#39;t keep it up then, because I am a wretched creature, I was frightened, and, the devil knows why, gave you my address in my folly.&#34;, &#39;Afterwards, before I got home, I was cursing and swearing at you because of that address, I hated you already because of the lies I had told you.&#39;, &#39;Because I only like playing with words, only dreaming, but, do you know, what I really want is that you should all go to hell.&#39;, &#39;That is what I want.&#39;, &#34;I want peace; yes, I&#39;d sell the whole world for a farthing, straight off, so long as I was left in peace.&#34;, &#39;Is the world to go to pot, or am I to go without my tea?&#39;, &#39;I say that the world may go to pot for me so long as I always get my tea.&#39;, &#39;Did you know that, or not?&#39;, &#39;Well, anyway, I know that I am a blackguard, a scoundrel, an egoist, a sluggard.&#39;, &#39;Here I have been shuddering for the last three days at the thought of your coming.&#39;, &#39;And do you know what has worried me particularly for these three days?&#39;, &#39;That I posed as such a hero to you, and now you would see me in a wretched torn dressing-gown, beggarly, loathsome.&#39;, &#39;I told you just now that I was not ashamed of my poverty; so you may as well know that I am ashamed of it; I am more ashamed of it than of anything, more afraid of it than of being found out if I were a thief, because I am as vain as though I had been skinned and the very air blowing on me hurt.&#39;, &#39;Surely by now you must realise that I shall never forgive you for having found me in this wretched dressing-gown, just as I was flying at Apollon like a spiteful cur.&#39;, &#39;The saviour, the former hero, was flying like a mangy, unkempt sheep-dog at his lackey, and the lackey was jeering at him!&#39;, &#39;And I shall never forgive you for the tears I could not help shedding before you just now, like some silly woman put to shame!&#39;, &#39;And for what I am confessing to you now, I shall never forgive you either!&#39;, &#39;Yes--you must answer for it all because you turned up like this, because I am a blackguard, because I am the nastiest, stupidest, absurdest and most envious of all the worms on earth, who are not a bit better than I am, but, the devil knows why, are never put to confusion; while I shall always be insulted by every louse, that is my doom!&#39;, &#34;And what is it to me that you don&#39;t understand a word of this!&#34;, &#39;And what do I care, what do I care about you, and whether you go to ruin there or not?&#39;, &#39;Do you understand?&#39;, &#39;How I shall hate you now after saying this, for having been here and listening.&#39;, &#34;Why, it&#39;s not once in a lifetime a man speaks out like this, and then it is in hysterics!&#34;, &#39;... What more do you want?&#39;, &#39;Why do you still stand confronting me, after all this?&#39;, &#39;Why are you worrying me?&#39;, &#39;Why don &#39;t you go?&#34;&#39;, &#39;But at this point a strange thing happened.&#39;, &#39;I was so accustomed to think and imagine everything from books, and to picture everything in the world to myself just as I had made it up in my dreams beforehand, that I could not all at once take in this strange circumstance.&#39;, &#39;What happened was this: Liza, insulted and crushed by me, understood a great deal more than I imagined.&#39;, &#39;She understood from all this what a woman understands first of all, if she feels genuine love, that is, that I was myself unhappy.&#39;, &#39;The frightened and wounded expression on her face was followed first by a look of sorrowful perplexity.&#39;, &#39;When I began calling myself a scoundrel and a blackguard and my tears flowed (the tirade was accompanied throughout by tears) her whole face worked convulsively.&#39;, &#39;She was on the point of getting up and stopping me; when I finished she took no notice of my shouting: &#34;Why are you here, why don &#39;t you go away?&#34;&#39;, &#39;but realised only that it must have been very bitter to me to say all this.&#39;, &#39;Besides, she was so crushed, poor girl; she considered herself infinitely beneath me; how could she feel anger or resentment?&#39;, &#39;She suddenly leapt up from her chair with an irresistible impulse and held out her hands, yearning towards me, though still timid and not daring to stir.... At this point there was a revulsion in my heart too.&#39;, &#39;Then she suddenly rushed to me, threw her arms round me and burst into tears.&#39;, &#39;I, too, could not restrain myself, and sobbed as I never had before.&#39;, &#39;&#34;They won &#39;t let me ...&#39;, &#39;I can &#39;t be good!&#34;&#39;, &#39;I managed to articulate; then I went to the sofa, fell on it face downwards, and sobbed on it for a quarter of an hour in genuine hysterics.&#39;, &#39;She came close to me, put her arms round me and stayed motionless in that position.&#39;, &#39;But the trouble was that the hysterics could not go on for ever, and (I am writing the loathsome truth) lying face downwards on the sofa with my face thrust into my nasty leather pillow, I began by degrees to be aware of a far-away, involuntary but irresistible feeling that it would be awkward now for me to raise my head and look Liza straight in the face.&#39;, &#39;Why was I ashamed?&#39;, &#34;I don&#39;t know, but I was ashamed.&#34;, &#39;The thought, too, came into my overwrought brain that our parts now were completely changed, that she was now the heroine, while I was just a crushed and humiliated creature as she had been before me that night--four days before.... And all this came into my mind during the minutes I was lying on my face on the sofa.&#39;, &#39;My God!&#39;, &#39;surely I was not envious of her then.&#39;, &#34;I don&#39;t know, to this day I cannot decide, and at the time, of course, I was still less able to understand what I was feeling than now.&#34;, &#39;I cannot get on without domineering and tyrannising over someone, but ... there is no explaining anything by reasoning and so it is useless to reason.&#39;, &#39;I conquered myself, however, and raised my head; I had to do so sooner or later ... and I am convinced to this day that it was just because I was ashamed to look at her that another feeling was suddenly kindled and flamed up in my heart ... a feeling of mastery and possession.&#39;, &#39;My eyes gleamed with passion, and I gripped her hands tightly.&#39;, &#39;How I hated her and how I was drawn to her at that minute!&#39;, &#39;The one feeling intensified the other.&#39;, &#39;It was almost like an act of vengeance.&#39;, &#39;At first there was a look of amazement, even of terror on her face, but only for one instant.&#39;, &#39;She warmly and rapturously embraced me.&#39;, &#39;X A quarter of an hour later I was rushing up and down the room in frenzied impatience, from minute to minute I went up to the screen and peeped through the crack at Liza.&#39;, &#39;She was sitting on the ground with her head leaning against the bed, and must have been crying.&#39;, &#39;But she did not go away, and that irritated me.&#39;, &#39;This time she understood it all.&#39;, &#34;I had insulted her finally, but ... there&#39;s no need to describe it.&#34;, &#39;She realised that my outburst of passion had been simply revenge, a fresh humiliation, and that to my earlier, almost causeless hatred was added now a PERSONAL HATRED, born of envy....&#39;, &#39;Though I do not maintain positively that she understood all this distinctly; but she certainly did fully understand that I was a despicable man, and what was worse, incapable of loving her.&#39;, &#39;I know I shall be told that this is incredible--but it is incredible to be as spiteful and stupid as I was; it may be added that it was strange I should not love her, or at any rate, appreciate her love.&#39;, &#39;Why is it strange?&#39;, &#39;In the first place, by then I was incapable of love, for I repeat, with me loving meant tyrannising and showing my moral superiority.&#39;, &#39;I have never in my life been able to imagine any other sort of love, and have nowadays come to the point of sometimes thinking that love really consists in the right--freely given by the beloved object--to tyrannise over her.&#39;, &#39;Even in my underground dreams I did not imagine love except as a struggle.&#39;, &#39;I began it always with hatred and ended it with moral subjugation, and afterwards I never knew what to do with the subjugated object.&#39;, &#39;And what is there to wonder at in that, since I had succeeded in so corrupting myself, since I was so out of touch with &#34;real life,&#34; as to have actually thought of reproaching her, and putting her to shame for having come to me to hear &#34;fine sentiments&#34;; and did not even guess that she had come not to hear fine sentiments, but to love me, because to a woman all reformation, all salvation from any sort of ruin, and all moral renewal is included in love and can only show itself in that form.&#39;, &#39;I did not hate her so much, however, when I was running about the room and peeping through the crack in the screen.&#39;, &#39;I was only insufferably oppressed by her being here.&#39;, &#39;I wanted her to disappear.&#39;, &#39;I wanted &#34;peace,&#34; to be left alone in my underground world.&#39;, &#39;Real life oppressed me with its novelty so much that I could hardly breathe.&#39;, &#39;But several minutes passed and she still remained, without stirring, as though she were unconscious.&#39;, &#39;I had the shamelessness to tap softly at the screen as though to remind her.... She started, sprang up, and flew to seek her kerchief, her hat, her coat, as though making her escape from me.... Two minutes later she came from behind the screen and looked with heavy eyes at me.&#39;, &#39;I gave a spiteful grin, which was forced, however, to KEEP UP APPEARANCES, and I turned away from her eyes.&#39;, &#39;&#34;Good-bye,&#34; she said, going towards the door.&#39;, &#39;I ran up to her, seized her hand, opened it, thrust something in it and closed it again.&#39;, &#39;Then I turned at once and dashed away in haste to the other corner of the room to avoid seeing, anyway....&#39;, &#39;I did mean a moment since to tell a lie--to write that I did this accidentally, not knowing what I was doing through foolishness, through losing my head.&#39;, &#34;But I don&#39;t want to lie, and so I will say straight out that I opened her hand and put the money in it ... from spite.&#34;, &#39;It came into my head to do this while I was running up and down the room and she was sitting behind the screen.&#39;, &#39;But this I can say for certain: though I did that cruel thing purposely, it was not an impulse from the heart, but came from my evil brain.&#39;, &#39;This cruelty was so affected, so purposely made up, so completely a product of the brain, of books, that I could not even keep it up a minute--first I dashed away to avoid seeing her, and then in shame and despair rushed after Liza.&#39;, &#39;I opened the door in the passage and began listening.&#39;, &#39;&#34;Liza!&#39;, &#39;Liza!&#34;&#39;, &#39;I cried on the stairs, but in a low voice, not boldly.&#39;, &#39;There was no answer, but I fancied I heard her footsteps, lower down on the stairs.&#39;, &#39;&#34;Liza!&#34;&#39;, &#39;I cried, more loudly.&#39;, &#39;No answer.&#39;, &#39;But at that minute I heard the stiff outer glass door open heavily with a creak and slam violently; the sound echoed up the stairs.&#39;, &#39;She had gone.&#39;, &#39;I went back to my room in hesitation.&#39;, &#39;I felt horribly oppressed.&#39;, &#39;I stood still at the table, beside the chair on which she had sat and looked aimlessly before me.&#39;, &#39;A minute passed, suddenly I started; straight before me on the table I saw....&#39;, &#39;In short, I saw a crumpled blue five-rouble note, the one I had thrust into her hand a minute before.&#39;, &#39;It was the same note; it could be no other, there was no other in the flat.&#39;, &#39;So she had managed to fling it from her hand on the table at the moment when I had dashed into the further corner.&#39;, &#39;Well!&#39;, &#39;I might have expected that she would do that.&#39;, &#39;Might I have expected it?&#39;, &#39;No, I was such an egoist, I was so lacking in respect for my fellow-creatures that I could not even imagine she would do so.&#39;, &#39;I could not endure it.&#39;, &#39;A minute later I flew like a madman to dress, flinging on what I could at random and ran headlong after her.&#39;, &#39;She could not have got two hundred paces away when I ran out into the street.&#39;, &#39;It was a still night and the snow was coming down in masses and falling almost perpendicularly, covering the pavement and the empty street as though with a pillow.&#39;, &#39;There was no one in the street, no sound was to be heard.&#39;, &#39;The street lamps gave a disconsolate and useless glimmer.&#39;, &#39;I ran two hundred paces to the cross-roads and stopped short.&#39;, &#39;Where had she gone?&#39;, &#39;And why was I running after her?&#39;, &#39;Why?&#39;, &#39;To fall down before her, to sob with remorse, to kiss her feet, to entreat her forgiveness!&#39;, &#39;I longed for that, my whole breast was being rent to pieces, and never, never shall I recall that minute with indifference.&#39;, &#39;But--what for?&#39;, &#39;I thought.&#39;, &#39;Should I not begin to hate her, perhaps, even tomorrow, just because I had kissed her feet today?&#39;, &#39;Should I give her happiness?&#39;, &#39;Had I not recognised that day, for the hundredth time, what I was worth?&#39;, &#39;Should I not torture her?&#39;, &#39;I stood in the snow, gazing into the troubled darkness and pondered this.&#39;, &#39;&#34;And will it not be better?&#34;&#39;, &#39;I mused fantastically, afterwards at home, stifling the living pang of my heart with fantastic dreams.&#39;, &#39;&#34;Will it not be better that she should keep the resentment of the insult for ever?&#39;, &#39;Resentment--why, it is purification; it is a most stinging and painful consciousness!&#39;, &#34;Tomorrow I should have defiled her soul and have exhausted her heart, while now the feeling of insult will never die in her heart, and however loathsome the filth awaiting her--the feeling of insult will elevate and purify her ... by hatred ... h&#39;m!&#34;, &#39;... perhaps, too, by forgiveness.... Will all that make things easier for her though?&#39;, &#39;...&#34; And, indeed, I will ask on my own account here, an idle question: which is better--cheap happiness or exalted sufferings?&#39;, &#39;Well, which is better?&#39;, &#39;So I dreamed as I sat at home that evening, almost dead with the pain in my soul.&#39;, &#39;Never had I endured such suffering and remorse, yet could there have been the faintest doubt when I ran out from my lodging that I should turn back half-way?&#39;, &#39;I never met Liza again and I have heard nothing of her.&#39;, &#39;I will add, too, that I remained for a long time afterwards pleased with the phrase about the benefit from resentment and hatred in spite of the fact that I almost fell ill from misery.&#39;, &#39;* * * * * Even now, so many years later, all this is somehow a very evil memory.&#39;, &#39;I have many evil memories now, but ... hadn &#39;t I better end my &#34;Notes&#34; here?&#39;, &#34;I believe I made a mistake in beginning to write them, anyway I have felt ashamed all the time I&#39;ve been writing this story; so it&#39;s hardly literature so much as a corrective punishment.&#34;, &#39;Why, to tell long stories, showing how I have spoiled my life through morally rotting in my corner, through lack of fitting environment, through divorce from real life, and rankling spite in my underground world, would certainly not be interesting; a novel needs a hero, and all the traits for an anti-hero are EXPRESSLY gathered together here, and what matters most, it all produces an unpleasant impression, for we are all divorced from life, we are all cripples, every one of us, more or less.&#39;, &#39;We are so divorced from it that we feel at once a sort of loathing for real life, and so cannot bear to be reminded of it.&#39;, &#39;Why, we have come almost to looking upon real life as an effort, almost as hard work, and we are all privately agreed that it is better in books.&#39;, &#39;And why do we fuss and fume sometimes?&#39;, &#39;Why are we perverse and ask for something else?&#39;, &#34;We don&#39;t know what ourselves.&#34;, &#39;It would be the worse for us if our petulant prayers were answered.&#39;, &#39;Come, try, give any one of us, for instance, a little more independence, untie our hands, widen the spheres of our activity, relax the control and we ... yes, I assure you ... we should be begging to be under control again at once.&#39;, &#39;I know that you will very likely be angry with me for that, and will begin shouting and stamping.&#39;, &#39;Speak for yourself, you will say, and for your miseries in your underground holes, and don &#39;t dare to say all of us--excuse me, gentlemen, I am not justifying myself with that &#34;all of us.&#34;&#39;, &#34;As for what concerns me in particular I have only in my life carried to an extreme what you have not dared to carry halfway, and what&#39;s more, you have taken your cowardice for good sense, and have found comfort in deceiving yourselves.&#34;, &#39;So that perhaps, after all, there is more life in me than in you.&#39;, &#39;Look into it more carefully!&#39;, &#34;Why, we don&#39;t even know what living means now, what it is, and what it is called?&#34;, &#39;Leave us alone without books and we shall be lost and in confusion at once.&#39;, &#39;We shall not know what to join on to, what to cling to, what to love and what to hate, what to respect and what to despise.&#39;, &#39;We are oppressed at being men--men with a real individual body and blood, we are ashamed of it, we think it a disgrace and try to contrive to be some sort of impossible generalised man.&#39;, &#39;We are stillborn, and for generations past have been begotten, not by living fathers, and that suits us better and better.&#39;, &#39;We are developing a taste for it.&#39;, &#39;Soon we shall contrive to be born somehow from an idea.&#39;, &#39;But enough; I don &#39;t want to write more from &#34;Underground.&#34;&#39;, &#39;[The notes of this paradoxalist do not end here, however.&#39;, &#39;He could not refrain from going on with them, but it seems to us that we may stop here.]&#39;, &#34;End of Project Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky *** END OF THIS PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** ***** This file should be named 600.txt or 600.zip ***** This and all associated files of various formats will be found in: http://www.gutenberg.org/6/0/600/ Produced by Judith Boss.&#34;, &#39;HTML version by Al Haines.&#39;, &#39;Updated editions will replace the previous one--the old editions will be renamed.&#39;, &#39;Creating the works from public domain print editions means that no one owns a United States copyright in these works, so the Foundation (and you!)&#39;, &#39;can copy and distribute it in the United States without permission and without paying copyright royalties.&#39;, &#39;Special rules, set forth in the General Terms of Use part of this license, apply to copying and distributing Project Gutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm concept and trademark.&#39;, &#39;Project Gutenberg is a registered trademark, and may not be used if you charge for the eBooks, unless you receive specific permission.&#39;, &#39;If you do not charge anything for copies of this eBook, complying with the rules is very easy.&#39;, &#39;You may use this eBook for nearly any purpose such as creation of derivative works, reports, performances and research.&#39;, &#39;They may be modified and printed and given away--you may do practically ANYTHING with public domain eBooks.&#39;, &#39;Redistribution is subject to the trademark license, especially commercial redistribution.&#39;, &#39;*** START: FULL LICENSE *** THE FULL PROJECT GUTENBERG LICENSE PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK To protect the Project Gutenberg-tm mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase &#34;Project Gutenberg&#34;), you agree to comply with all the terms of the Full Project Gutenberg-tm License (available with this file or online at http://gutenberg.net/license).&#39;, &#39;Section 1.&#39;, &#39;General Terms of Use and Redistributing Project Gutenberg-tm electronic works 1.A.&#39;, &#39;By reading or using any part of this Project Gutenberg-tm electronic work, you indicate that you have read, understand, agree to and accept all the terms of this license and intellectual property (trademark/copyright) agreement.&#39;, &#39;If you do not agree to abide by all the terms of this agreement, you must cease using and return or destroy all copies of Project Gutenberg-tm electronic works in your possession.&#39;, &#39;If you paid a fee for obtaining a copy of or access to a Project Gutenberg-tm electronic work and you do not agree to be bound by the terms of this agreement, you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph 1.E.8.&#39;, &#39;1.B.&#39;, &#39;&#34;Project Gutenberg&#34; is a registered trademark.&#39;, &#39;It may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement.&#39;, &#39;There are a few things that you can do with most Project Gutenberg-tm electronic works even without complying with the full terms of this agreement.&#39;, &#39;See paragraph 1.C below.&#39;, &#39;There are a lot of things you can do with Project Gutenberg-tm electronic works if you follow the terms of this agreement and help preserve free future access to Project Gutenberg-tm electronic works.&#39;, &#39;See paragraph 1.E below.&#39;, &#39;1.C.&#39;, &#39;The Project Gutenberg Literary Archive Foundation (&#34;the Foundation&#34; or PGLAF), owns a compilation copyright in the collection of Project Gutenberg-tm electronic works.&#39;, &#39;Nearly all the individual works in the collection are in the public domain in the United States.&#39;, &#39;If an individual work is in the public domain in the United States and you are located in the United States, we do not claim a right to prevent you from copying, distributing, performing, displaying or creating derivative works based on the work as long as all references to Project Gutenberg are removed.&#39;, &#39;Of course, we hope that you will support the Project Gutenberg-tm mission of promoting free access to electronic works by freely sharing Project Gutenberg-tm works in compliance with the terms of this agreement for keeping the Project Gutenberg-tm name associated with the work.&#39;, &#39;You can easily comply with the terms of this agreement by keeping this work in the same format with its attached full Project Gutenberg-tm License when you share it without charge with others.&#39;, &#39;1.D.&#39;, &#39;The copyright laws of the place where you are located also govern what you can do with this work.&#39;, &#39;Copyright laws in most countries are in a constant state of change.&#39;, &#39;If you are outside the United States, check the laws of your country in addition to the terms of this agreement before downloading, copying, displaying, performing, distributing or creating derivative works based on this work or any other Project Gutenberg-tm work.&#39;, &#39;The Foundation makes no representations concerning the copyright status of any work in any country outside the United States.&#39;, &#39;1.E.&#39;, &#39;Unless you have removed all references to Project Gutenberg: 1.E.1.&#39;, &#39;The following sentence, with active links to, or other immediate access to, the full Project Gutenberg-tm License must appear prominently whenever any copy of a Project Gutenberg-tm work (any work on which the phrase &#34;Project Gutenberg&#34; appears, or with which the phrase &#34;Project Gutenberg&#34; is associated) is accessed, displayed, performed, viewed, copied or distributed: This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.&#39;, &#39;You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net 1.E.2.&#39;, &#39;If an individual Project Gutenberg-tm electronic work is derived from the public domain (does not contain a notice indicating that it is posted with permission of the copyright holder), the work can be copied and distributed to anyone in the United States without paying any fees or charges.&#39;, &#39;If you are redistributing or providing access to a work with the phrase &#34;Project Gutenberg&#34; associated with or appearing on the work, you must comply either with the requirements of paragraphs 1.E.1 through 1.E.7 or obtain permission for the use of the work and the Project Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or 1.E.9.&#39;, &#39;1.E.3.&#39;, &#39;If an individual Project Gutenberg-tm electronic work is posted with the permission of the copyright holder, your use and distribution must comply with both paragraphs 1.E.1 through 1.E.7 and any additional terms imposed by the copyright holder.&#39;, &#39;Additional terms will be linked to the Project Gutenberg-tm License for all works posted with the permission of the copyright holder found at the beginning of this work.&#39;, &#39;1.E.4.&#39;, &#39;Do not unlink or detach or remove the full Project Gutenberg-tm License terms from this work, or any files containing a part of this work or any other work associated with Project Gutenberg-tm.&#39;, &#39;1.E.5.&#39;, &#39;Do not copy, display, perform, distribute or redistribute this electronic work, or any part of this electronic work, without prominently displaying the sentence set forth in paragraph 1.E.1 with active links or immediate access to the full terms of the Project Gutenberg-tm License.&#39;, &#39;1.E.6.&#39;, &#39;You may convert to and distribute this work in any binary, compressed, marked up, nonproprietary or proprietary form, including any word processing or hypertext form.&#39;, &#39;However, if you provide access to or distribute copies of a Project Gutenberg-tm work in a format other than &#34;Plain Vanilla ASCII&#34; or other format used in the official version posted on the official Project Gutenberg-tm web site (www.gutenberg.net), you must, at no additional cost, fee or expense to the user, provide a copy, a means of exporting a copy, or a means of obtaining a copy upon request, of the work in its original &#34;Plain Vanilla ASCII&#34; or other form.&#39;, &#39;Any alternate format must include the full Project Gutenberg-tm License as specified in paragraph 1.E.1.&#39;, &#39;1.E.7.&#39;, &#39;Do not charge a fee for access to, viewing, displaying, performing, copying or distributing any Project Gutenberg-tm works unless you comply with paragraph 1.E.8 or 1.E.9.&#39;, &#39;1.E.8.&#39;, &#39;You may charge a reasonable fee for copies of or providing access to or distributing Project Gutenberg-tm electronic works provided that - You pay a royalty fee of 20% of the gross profits you derive from the use of Project Gutenberg-tm works calculated using the method you already use to calculate your applicable taxes.&#39;, &#39;The fee is owed to the owner of the Project Gutenberg-tm trademark, but he has agreed to donate royalties under this paragraph to the Project Gutenberg Literary Archive Foundation.&#39;, &#39;Royalty payments must be paid within 60 days following each date on which you prepare (or are legally required to prepare) your periodic tax returns.&#39;, &#39;Royalty payments should be clearly marked as such and sent to the Project Gutenberg Literary Archive Foundation at the address specified in Section 4, &#34;Information about donations to the Project Gutenberg Literary Archive Foundation.&#34;&#39;, &#39;- You provide a full refund of any money paid by a user who notifies you in writing (or by e-mail) within 30 days of receipt that s/he does not agree to the terms of the full Project Gutenberg-tm License.&#39;, &#39;You must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of Project Gutenberg-tm works.&#39;, &#39;- You provide, in accordance with paragraph 1.F.3, a full refund of any money paid for a work or a replacement copy, if a defect in the electronic work is discovered and reported to you within 90 days of receipt of the work.&#39;, &#39;- You comply with all other terms of this agreement for free distribution of Project Gutenberg-tm works.&#39;, &#39;1.E.9.&#39;, &#39;If you wish to charge a fee or distribute a Project Gutenberg-tm electronic work or group of works on different terms than are set forth in this agreement, you must obtain permission in writing from both the Project Gutenberg Literary Archive Foundation and Michael Hart, the owner of the Project Gutenberg-tm trademark.&#39;, &#39;Contact the Foundation as set forth in Section 3 below.&#39;, &#39;1.F.&#39;, &#39;1.F.1.&#39;, &#39;Project Gutenberg volunteers and employees expend considerable effort to identify, do copyright research on, transcribe and proofread public domain works in creating the Project Gutenberg-tm collection.&#39;, &#39;Despite these efforts, Project Gutenberg-tm electronic works, and the medium on which they may be stored, may contain &#34;Defects,&#34; such as, but not limited to, incomplete, inaccurate or corrupt data, transcription errors, a copyright or other intellectual property infringement, a defective or damaged disk or other medium, a computer virus, or computer codes that damage or cannot be read by your equipment.&#39;, &#39;1.F.2.&#39;, &#39;LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the &#34;Right of Replacement or Refund&#34; described in paragraph 1.F.3, the Project Gutenberg Literary Archive Foundation, the owner of the Project Gutenberg-tm trademark, and any other party distributing a Project Gutenberg-tm electronic work under this agreement, disclaim all liability to you for damages, costs and expenses, including legal fees.&#39;, &#39;YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE PROVIDED IN PARAGRAPH F3.&#39;, &#39;YOU AGREE THAT THE FOUNDATION, THE TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGE.&#39;, &#39;1.F.3.&#39;, &#39;LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a defect in this electronic work within 90 days of receiving it, you can receive a refund of the money (if any) you paid for it by sending a written explanation to the person you received the work from.&#39;, &#39;If you received the work on a physical medium, you must return the medium with your written explanation.&#39;, &#39;The person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund.&#39;, &#39;If you received the work electronically, the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund.&#39;, &#39;If the second copy is also defective, you may demand a refund in writing without further opportunities to fix the problem.&#39;, &#39;1.F.4.&#39;, &#34;Except for the limited right of replacement or refund set forth in paragraph 1.F.3, this work is provided to you &#39;AS-IS&#39; WITH NO OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTIBILITY OR FITNESS FOR ANY PURPOSE.&#34;, &#39;1.F.5.&#39;, &#39;Some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages.&#39;, &#39;If any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement, the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law.&#39;, &#39;The invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions.&#39;, &#39;1.F.6.&#39;, &#39;INDEMNITY - You agree to indemnify and hold the Foundation, the trademark owner, any agent or employee of the Foundation, anyone providing copies of Project Gutenberg-tm electronic works in accordance with this agreement, and any volunteers associated with the production, promotion and distribution of Project Gutenberg-tm electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg-tm work, (b) alteration, modification, or additions or deletions to any Project Gutenberg-tm work, and (c) any Defect you cause.&#39;, &#39;Section 2.&#39;, &#39;Information about the Mission of Project Gutenberg-tm Project Gutenberg-tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers.&#39;, &#39;It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life.&#39;, &#34;Volunteers and financial support to provide volunteers with the assistance they need, is critical to reaching Project Gutenberg-tm&#39;s goals and ensuring that the Project Gutenberg-tm collection will remain freely available for generations to come.&#34;, &#39;In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg-tm and future generations.&#39;, &#39;To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation web page at http://www.pglaf.org.&#39;, &#39;Section 3.&#39;, &#39;Information about the Project Gutenberg Literary Archive Foundation The Project Gutenberg Literary Archive Foundation is a non profit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the Internal Revenue Service.&#39;, &#34;The Foundation&#39;s EIN or federal tax identification number is 64-6221541.&#34;, &#39;Its 501(c)(3) letter is posted at http://pglaf.org/fundraising.&#39;, &#34;Contributions to the Project Gutenberg Literary Archive Foundation are tax deductible to the full extent permitted by U.S. federal laws and your state&#39;s laws.&#34;, &#34;The Foundation&#39;s principal office is located at 4557 Melan Dr. S. Fairbanks, AK, 99712., but its volunteers and employees are scattered throughout numerous locations.&#34;, &#39;Its business office is located at 809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887, email business@pglaf.org.&#39;, &#34;Email contact links and up to date contact information can be found at the Foundation&#39;s web site and official page at http://pglaf.org For additional contact information: Dr. Gregory B. Newby Chief Executive and Director gbnewby@pglaf.org Section 4.&#34;, &#39;Information about Donations to the Project Gutenberg Literary Archive Foundation Project Gutenberg-tm depends upon and cannot survive without wide spread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine readable form accessible by the widest array of equipment including outdated equipment.&#39;, &#39;Many small donations ($1 to $5,000) are particularly important to maintaining tax exempt status with the IRS.&#39;, &#39;The Foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the United States.&#39;, &#39;Compliance requirements are not uniform and it takes a considerable effort, much paperwork and many fees to meet and keep up with these requirements.&#39;, &#39;We do not solicit donations in locations where we have not received written confirmation of compliance.&#39;, &#39;To SEND DONATIONS or determine the status of compliance for any particular state visit http://pglaf.org While we cannot and do not solicit contributions from states where we have not met the solicitation requirements, we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate.&#39;, &#39;International donations are gratefully accepted, but we cannot make any statements concerning tax treatment of donations received from outside the United States.&#39;, &#39;U.S. laws alone swamp our small staff.&#39;, &#39;Please check the Project Gutenberg Web pages for current donation methods and addresses.&#39;, &#39;Donations are accepted in a number of other ways including including checks, online payments and credit card donations.&#39;, &#39;To donate, please visit: http://pglaf.org/donate Section 5.&#39;, &#39;General Information About Project Gutenberg-tm electronic works.&#39;, &#39;Professor Michael S. Hart is the originator of the Project Gutenberg-tm concept of a library of electronic works that could be freely shared with anyone.&#39;, &#39;For thirty years, he produced and distributed Project Gutenberg-tm eBooks with only a loose network of volunteer support.&#39;, &#39;Project Gutenberg-tm eBooks are often created from several printed editions, all of which are confirmed as Public Domain in the U.S. unless a copyright notice is included.&#39;, &#39;Thus, we do not necessarily keep eBooks in compliance with any particular paper edition.&#39;, &#39;Most people start at our Web site which has the main PG search facility: http://www.gutenberg.net This Web site includes information about Project Gutenberg-tm, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks.&#39;] . Remove stop words . Stop words are English words which do not add much meaning to a sentence. They can be safely ignored without sacrificing the meaning of the sentence. We already downloaded a file with English stop words in the first section of the notebook. . Here, we will get the list of stop words and store them in stop_word variable. . # get stop words list stop_words = nltk.corpus.stopwords.words(&#39;english&#39;) print(stop_words) . [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &#34;you&#39;re&#34;, &#34;you&#39;ve&#34;, &#34;you&#39;ll&#34;, &#34;you&#39;d&#34;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;, &#39;himself&#39;, &#39;she&#39;, &#34;she&#39;s&#34;, &#39;her&#39;, &#39;hers&#39;, &#39;herself&#39;, &#39;it&#39;, &#34;it&#39;s&#34;, &#39;its&#39;, &#39;itself&#39;, &#39;they&#39;, &#39;them&#39;, &#39;their&#39;, &#39;theirs&#39;, &#39;themselves&#39;, &#39;what&#39;, &#39;which&#39;, &#39;who&#39;, &#39;whom&#39;, &#39;this&#39;, &#39;that&#39;, &#34;that&#39;ll&#34;, &#39;these&#39;, &#39;those&#39;, &#39;am&#39;, &#39;is&#39;, &#39;are&#39;, &#39;was&#39;, &#39;were&#39;, &#39;be&#39;, &#39;been&#39;, &#39;being&#39;, &#39;have&#39;, &#39;has&#39;, &#39;had&#39;, &#39;having&#39;, &#39;do&#39;, &#39;does&#39;, &#39;did&#39;, &#39;doing&#39;, &#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;and&#39;, &#39;but&#39;, &#39;if&#39;, &#39;or&#39;, &#39;because&#39;, &#39;as&#39;, &#39;until&#39;, &#39;while&#39;, &#39;of&#39;, &#39;at&#39;, &#39;by&#39;, &#39;for&#39;, &#39;with&#39;, &#39;about&#39;, &#39;against&#39;, &#39;between&#39;, &#39;into&#39;, &#39;through&#39;, &#39;during&#39;, &#39;before&#39;, &#39;after&#39;, &#39;above&#39;, &#39;below&#39;, &#39;to&#39;, &#39;from&#39;, &#39;up&#39;, &#39;down&#39;, &#39;in&#39;, &#39;out&#39;, &#39;on&#39;, &#39;off&#39;, &#39;over&#39;, &#39;under&#39;, &#39;again&#39;, &#39;further&#39;, &#39;then&#39;, &#39;once&#39;, &#39;here&#39;, &#39;there&#39;, &#39;when&#39;, &#39;where&#39;, &#39;why&#39;, &#39;how&#39;, &#39;all&#39;, &#39;any&#39;, &#39;both&#39;, &#39;each&#39;, &#39;few&#39;, &#39;more&#39;, &#39;most&#39;, &#39;other&#39;, &#39;some&#39;, &#39;such&#39;, &#39;no&#39;, &#39;nor&#39;, &#39;not&#39;, &#39;only&#39;, &#39;own&#39;, &#39;same&#39;, &#39;so&#39;, &#39;than&#39;, &#39;too&#39;, &#39;very&#39;, &#39;s&#39;, &#39;t&#39;, &#39;can&#39;, &#39;will&#39;, &#39;just&#39;, &#39;don&#39;, &#34;don&#39;t&#34;, &#39;should&#39;, &#34;should&#39;ve&#34;, &#39;now&#39;, &#39;d&#39;, &#39;ll&#39;, &#39;m&#39;, &#39;o&#39;, &#39;re&#39;, &#39;ve&#39;, &#39;y&#39;, &#39;ain&#39;, &#39;aren&#39;, &#34;aren&#39;t&#34;, &#39;couldn&#39;, &#34;couldn&#39;t&#34;, &#39;didn&#39;, &#34;didn&#39;t&#34;, &#39;doesn&#39;, &#34;doesn&#39;t&#34;, &#39;hadn&#39;, &#34;hadn&#39;t&#34;, &#39;hasn&#39;, &#34;hasn&#39;t&#34;, &#39;haven&#39;, &#34;haven&#39;t&#34;, &#39;isn&#39;, &#34;isn&#39;t&#34;, &#39;ma&#39;, &#39;mightn&#39;, &#34;mightn&#39;t&#34;, &#39;mustn&#39;, &#34;mustn&#39;t&#34;, &#39;needn&#39;, &#34;needn&#39;t&#34;, &#39;shan&#39;, &#34;shan&#39;t&#34;, &#39;shouldn&#39;, &#34;shouldn&#39;t&#34;, &#39;wasn&#39;, &#34;wasn&#39;t&#34;, &#39;weren&#39;, &#34;weren&#39;t&#34;, &#39;won&#39;, &#34;won&#39;t&#34;, &#39;wouldn&#39;, &#34;wouldn&#39;t&#34;] . Build word histogram . Let&#39;s evaluate the importance of each word based on how many times it appears in the entire text. . We will do so by 1) splitting the words in clean_text, 2) removing the stop words, and then 3) checking the frequency of each word as it appears in the text. . # create an empty dictionary to house the word count word_count = {} # loop through tokenized words, remove stop words and save word count to dictionary for word in nltk.word_tokenize(clean_text): # remove stop words if word not in stop_words: # save word count to dictionary if word not in word_count.keys(): word_count[word] = 1 else: word_count[word] += 1 . Let&#39;s plot the word histogram and see the results. . # plt.figure(figsize=(16,10)) # plt.xticks(rotation = 90) # plt.bar(word_count.keys(), word_count.values()) # plt.show() . def plot_top_words(word_count_dict, show_top_n=20): word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = &#39;index&#39;).rename(columns={0: &#39;score&#39;}) word_count_table.sort_values(by=&#39;score&#39;).tail(show_top_n).plot(kind=&#39;barh&#39;, figsize=(10,10)) plt.show() . plot_top_words(word_count, 20) . sentence_score = {} # loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score for sentence in sentences: # check if word in sentence is in word_count dictionary for word in nltk.word_tokenize(sentence.lower()): if word in word_count.keys(): # only take sentence that has less than 30 words if len(sentence.split(&#39; &#39;)) &lt; 30: # add word score to sentence score if sentence not in sentence_score.keys(): sentence_score[sentence] = word_count[word] else: sentence_score[sentence] += word_count[word] . df_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = &#39;index&#39;).rename(columns={0: &#39;score&#39;}) df_sentence_score.sort_values(by=&#39;score&#39;, ascending = False) . score . Perhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one&#39;s tongue. 914 | . And no one, no one should know what passes between husband and wife if they love one another. 877 | . And one may choose what is contrary to one&#39;s own interests, and sometimes one POSITIVELY OUGHT (that is my idea). 848 | . If he only meant to insult me by that high-official tone, it would not matter, I thought--I could pay him back for it one way or another. 776 | . In fact, I would even have put up with looking base if, at the same time, my face could have been thought strikingly intelligent. 767 | . ... ... | . Where are my foundations? 1 | . &quot;No, Russian.&quot; 1 | . &quot;It is ordained! 1 | . &quot;Tradespeople.&quot; 1 | . &quot;Aha! 1 | . 2138 rows × 1 columns . best_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get) . print(&#39;SUMMARY&#39;) print(&#39;&#39;) for sentence in sentences: if sentence in best_sentences: print (sentence) . SUMMARY And one may choose what is contrary to one&#39;s own interests, and sometimes one POSITIVELY OUGHT (that is my idea). Perhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one&#39;s tongue. And no one, no one should know what passes between husband and wife if they love one another. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/19/TextSummarizer-Dostoyevesky.html",
            "relUrl": "/2020/11/19/TextSummarizer-Dostoyevesky.html",
            "date": " • Nov 19, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Timeseries, Stocks and Altair",
            "content": "toc: true | badges: true | comments: true | sticky_rank: 1 | categories: [Timeseries] | . #collapse-hide import pandas as pd import altair as alt . . #collapse-show sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; . . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=sp500 ).properties( width=1000, height=900 ) . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=1000, height=900 ) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/18/stocks-timeseries-viz-altair.html",
            "relUrl": "/2020/11/18/stocks-timeseries-viz-altair.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Predicting Onset/Diagnosis of Chronic Conditions, Diabetes",
            "content": "toc: true | badges: true | comments: true | sticky_rank: 1 | categories: [Big Data , h2o] | . National Institute of Diabetes and Digestive and Kidney Diseases, https://www.niddk.nih.gov/ . . Credit: code from https://www.kaggle.com/sudalairajkumar/getting-started-with-h2o . import h2o import time import seaborn import itertools import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from h2o.estimators.glm import H2OGeneralizedLinearEstimator from h2o.estimators.gbm import H2OGradientBoostingEstimator from h2o.estimators.random_forest import H2ORandomForestEstimator %matplotlib inline . h2o.init() . Checking whether there is an H2O instance running at http://localhost:54321 . connected. . H2O_cluster_uptime: | 9 mins 06 secs | . H2O_cluster_timezone: | Etc/UTC | . H2O_data_parsing_timezone: | UTC | . H2O_cluster_version: | 3.30.1.3 | . H2O_cluster_version_age: | 1 month and 10 days | . H2O_cluster_name: | H2O_from_python_unknownUser_6cf4p5 | . H2O_cluster_total_nodes: | 1 | . H2O_cluster_free_memory: | 3.179 Gb | . H2O_cluster_total_cores: | 2 | . H2O_cluster_allowed_cores: | 2 | . H2O_cluster_status: | locked, healthy | . H2O_connection_url: | http://localhost:54321 | . H2O_connection_proxy: | {&quot;http&quot;: null, &quot;https&quot;: null} | . H2O_internal_security: | False | . H2O_API_Extensions: | Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 | . Python_version: | 3.6.9 final | . diabetes_df = h2o.import_file(&quot;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv&quot;, destination_frame=&quot;diabetes_df&quot;) . Parse progress: |█████████████████████████████████████████████████████████| 100% . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 763 10 | 101 | 76 | 48 | 180 | 32.9 | 0.171 | 63 | 0 | . 764 2 | 122 | 70 | 27 | 0 | 36.8 | 0.340 | 27 | 0 | . 765 5 | 121 | 72 | 23 | 112 | 26.2 | 0.245 | 30 | 0 | . 766 1 | 126 | 60 | 0 | 0 | 30.1 | 0.349 | 47 | 1 | . 767 1 | 93 | 70 | 31 | 0 | 30.4 | 0.315 | 23 | 0 | . 768 rows × 9 columns . diabetes_df.describe() . Rows:768 Cols:9 . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . type | int | int | int | int | int | real | real | int | int | . mins | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.078 | 21.0 | 0.0 | . mean | 3.845052083333336 | 120.89453125 | 69.10546874999994 | 20.536458333333357 | 79.79947916666666 | 31.99257812500003 | 0.4718763020833334 | 33.240885416666615 | 0.3489583333333333 | . maxs | 17.0 | 199.0 | 122.0 | 99.0 | 846.0 | 67.1 | 2.42 | 81.0 | 1.0 | . sigma | 3.36957806269887 | 31.972618195136224 | 19.355807170644777 | 15.952217567727642 | 115.24400235133803 | 7.884160320375441 | 0.331328595012775 | 11.760231540678689 | 0.47695137724279896 | . zeros | 111 | 5 | 35 | 227 | 374 | 11 | 0 | 0 | 500 | . missing | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 0 | 6.0 | 148.0 | 72.0 | 35.0 | 0.0 | 33.6 | 0.627 | 50.0 | 1.0 | . 1 | 1.0 | 85.0 | 66.0 | 29.0 | 0.0 | 26.6 | 0.351 | 31.0 | 0.0 | . 2 | 8.0 | 183.0 | 64.0 | 0.0 | 0.0 | 23.3 | 0.672 | 32.0 | 1.0 | . 3 | 1.0 | 89.0 | 66.0 | 23.0 | 94.0 | 28.1 | 0.167 | 21.0 | 0.0 | . 4 | 0.0 | 137.0 | 40.0 | 35.0 | 168.0 | 43.1 | 2.288 | 33.0 | 1.0 | . 5 | 5.0 | 116.0 | 74.0 | 0.0 | 0.0 | 25.6 | 0.201 | 30.0 | 0.0 | . 6 | 3.0 | 78.0 | 50.0 | 32.0 | 88.0 | 31.0 | 0.248 | 26.0 | 1.0 | . 7 | 10.0 | 115.0 | 0.0 | 0.0 | 0.0 | 35.3 | 0.134 | 29.0 | 0.0 | . 8 | 2.0 | 197.0 | 70.0 | 45.0 | 543.0 | 30.5 | 0.158 | 53.0 | 1.0 | . 9 | 8.0 | 125.0 | 96.0 | 0.0 | 0.0 | 0.0 | 0.232 | 54.0 | 1.0 | . for col in diabetes_df.columns: diabetes_df[col].hist() . plt.figure(figsize=(10,10)) corr = diabetes_df.cor().as_data_frame() corr.index = diabetes_df.columns sns.heatmap(corr, annot = True, cmap=&#39;RdYlGn&#39;, vmin=-1, vmax=1) plt.title(&quot;Correlation Heatmap&quot;, fontsize=16) plt.show() . train, valid, test = diabetes_df.split_frame(ratios=[0.6,0.2], seed=1234) response = &quot;Outcome&quot; train[response] = train[response].asfactor() valid[response] = valid[response].asfactor() test[response] = test[response].asfactor() print(&quot;Number of rows in train, valid and test set : &quot;, train.shape[0], valid.shape[0], test.shape[0]) . Number of rows in train, valid and test set : 465 148 155 . predictors = diabetes_df.columns[:-1] gbm = H2OGradientBoostingEstimator() gbm.train(x=predictors, y=response, training_frame=train) . gbm Model Build progress: |███████████████████████████████████████████████| 100% . print(gbm) . Model Details ============= H2OGradientBoostingEstimator : Gradient Boosting Machine Model Key: GBM_model_python_1604778858031_53 Model Summary: . number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves . 0 | 50.0 | 50.0 | 11552.0 | 5.0 | 5.0 | 5.0 | 6.0 | 23.0 | 13.68 | . ModelMetricsBinomial: gbm ** Reported on train data. ** MSE: 0.05394537414652564 RMSE: 0.2322614349101582 LogLoss: 0.21164045617145613 Mean Per-Class Error: 0.05419671999032927 AUC: 0.9899262602248459 AUCPR: 0.9845164166436653 Gini: 0.9798525204496917 Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.46220528851885034: . 0 1 Error Rate . 0 0 | 290.0 | 9.0 | 0.0301 | (9.0/299.0) | . 1 1 | 14.0 | 152.0 | 0.0843 | (14.0/166.0) | . 2 Total | 304.0 | 161.0 | 0.0495 | (23.0/465.0) | . Maximum Metrics: Maximum metrics at their respective thresholds . metric threshold value idx . 0 max f1 | 0.462205 | 0.929664 | 150.0 | . 1 max f2 | 0.323826 | 0.944836 | 176.0 | . 2 max f0point5 | 0.548022 | 0.967302 | 132.0 | . 3 max accuracy | 0.506976 | 0.950538 | 142.0 | . 4 max precision | 0.983850 | 1.000000 | 0.0 | . 5 max recall | 0.173383 | 1.000000 | 227.0 | . 6 max specificity | 0.983850 | 1.000000 | 0.0 | . 7 max absolute_mcc | 0.506976 | 0.892066 | 142.0 | . 8 max min_per_class_accuracy | 0.372134 | 0.939799 | 163.0 | . 9 max mean_per_class_accuracy | 0.370458 | 0.945803 | 164.0 | . 10 max tns | 0.983850 | 299.000000 | 0.0 | . 11 max fns | 0.983850 | 165.000000 | 0.0 | . 12 max fps | 0.018713 | 299.000000 | 399.0 | . 13 max tps | 0.173383 | 166.000000 | 227.0 | . 14 max tnr | 0.983850 | 1.000000 | 0.0 | . 15 max fnr | 0.983850 | 0.993976 | 0.0 | . 16 max fpr | 0.018713 | 1.000000 | 399.0 | . 17 max tpr | 0.173383 | 1.000000 | 227.0 | . Gains/Lift Table: Avg response rate: 35.70 %, avg score: 35.71 % . group cumulative_data_fraction lower_threshold lift cumulative_lift response_rate score cumulative_response_rate cumulative_score capture_rate cumulative_capture_rate gain cumulative_gain kolmogorov_smirnov . 0 1 | 0.010753 | 0.964760 | 2.801205 | 2.801205 | 1.000000 | 0.970134 | 1.000000 | 0.970134 | 0.030120 | 0.030120 | 180.120482 | 180.120482 | 0.030120 | . 1 2 | 0.021505 | 0.959842 | 2.801205 | 2.801205 | 1.000000 | 0.963441 | 1.000000 | 0.966788 | 0.030120 | 0.060241 | 180.120482 | 180.120482 | 0.060241 | . 2 3 | 0.030108 | 0.948198 | 2.801205 | 2.801205 | 1.000000 | 0.952824 | 1.000000 | 0.962798 | 0.024096 | 0.084337 | 180.120482 | 180.120482 | 0.084337 | . 3 4 | 0.040860 | 0.944016 | 2.801205 | 2.801205 | 1.000000 | 0.946246 | 1.000000 | 0.958442 | 0.030120 | 0.114458 | 180.120482 | 180.120482 | 0.114458 | . 4 5 | 0.051613 | 0.940243 | 2.801205 | 2.801205 | 1.000000 | 0.942268 | 1.000000 | 0.955073 | 0.030120 | 0.144578 | 180.120482 | 180.120482 | 0.144578 | . 5 6 | 0.101075 | 0.910936 | 2.801205 | 2.801205 | 1.000000 | 0.924669 | 1.000000 | 0.940194 | 0.138554 | 0.283133 | 180.120482 | 180.120482 | 0.283133 | . 6 7 | 0.150538 | 0.868553 | 2.801205 | 2.801205 | 1.000000 | 0.889815 | 1.000000 | 0.923641 | 0.138554 | 0.421687 | 180.120482 | 180.120482 | 0.421687 | . 7 8 | 0.200000 | 0.797054 | 2.801205 | 2.801205 | 1.000000 | 0.834641 | 1.000000 | 0.901630 | 0.138554 | 0.560241 | 180.120482 | 180.120482 | 0.560241 | . 8 9 | 0.301075 | 0.556327 | 2.801205 | 2.801205 | 1.000000 | 0.680179 | 1.000000 | 0.827286 | 0.283133 | 0.843373 | 180.120482 | 180.120482 | 0.843373 | . 9 10 | 0.400000 | 0.326087 | 1.157019 | 2.394578 | 0.413043 | 0.441848 | 0.854839 | 0.731963 | 0.114458 | 0.957831 | 15.701938 | 139.457831 | 0.867530 | . 10 11 | 0.501075 | 0.189518 | 0.357601 | 1.983686 | 0.127660 | 0.257074 | 0.708155 | 0.636170 | 0.036145 | 0.993976 | -64.239938 | 98.368582 | 0.766551 | . 11 12 | 0.600000 | 0.119706 | 0.060896 | 1.666667 | 0.021739 | 0.148143 | 0.594982 | 0.555707 | 0.006024 | 1.000000 | -93.910424 | 66.666667 | 0.622074 | . 12 13 | 0.698925 | 0.083724 | 0.000000 | 1.430769 | 0.000000 | 0.101152 | 0.510769 | 0.491370 | 0.000000 | 1.000000 | -100.000000 | 43.076923 | 0.468227 | . 13 14 | 0.800000 | 0.053134 | 0.000000 | 1.250000 | 0.000000 | 0.067558 | 0.446237 | 0.437824 | 0.000000 | 1.000000 | -100.000000 | 25.000000 | 0.311037 | . 14 15 | 0.898925 | 0.034204 | 0.000000 | 1.112440 | 0.000000 | 0.042448 | 0.397129 | 0.394313 | 0.000000 | 1.000000 | -100.000000 | 11.244019 | 0.157191 | . 15 16 | 1.000000 | 0.018713 | 0.000000 | 1.000000 | 0.000000 | 0.026483 | 0.356989 | 0.357135 | 0.000000 | 1.000000 | -100.000000 | 0.000000 | 0.000000 | . Scoring History: . timestamp duration number_of_trees training_rmse training_logloss training_auc training_pr_auc training_lift training_classification_error . 0 | 2020-11-07 20:03:31 | 0.002 sec | 0.0 | 0.479112 | 0.651666 | 0.500000 | 0.356989 | 1.000000 | 0.643011 | . 1 | 2020-11-07 20:03:31 | 0.050 sec | 1.0 | 0.456602 | 0.606210 | 0.899111 | 0.841991 | 2.801205 | 0.178495 | . 2 | 2020-11-07 20:03:31 | 0.084 sec | 2.0 | 0.437174 | 0.568855 | 0.918050 | 0.868180 | 2.801205 | 0.154839 | . 3 | 2020-11-07 20:03:31 | 0.115 sec | 3.0 | 0.420575 | 0.537898 | 0.919994 | 0.870297 | 2.801205 | 0.156989 | . 4 | 2020-11-07 20:03:31 | 0.141 sec | 4.0 | 0.405987 | 0.511169 | 0.925938 | 0.881996 | 2.801205 | 0.148387 | . 5 | 2020-11-07 20:03:31 | 0.160 sec | 5.0 | 0.392586 | 0.486722 | 0.934239 | 0.894388 | 2.801205 | 0.141935 | . 6 | 2020-11-07 20:03:31 | 0.173 sec | 6.0 | 0.380476 | 0.464792 | 0.937936 | 0.899843 | 2.801205 | 0.141935 | . 7 | 2020-11-07 20:03:31 | 0.188 sec | 7.0 | 0.370456 | 0.446541 | 0.939205 | 0.901852 | 2.801205 | 0.139785 | . 8 | 2020-11-07 20:03:31 | 0.207 sec | 8.0 | 0.362031 | 0.431383 | 0.942207 | 0.906607 | 2.801205 | 0.133333 | . 9 | 2020-11-07 20:03:31 | 0.227 sec | 9.0 | 0.355361 | 0.418882 | 0.941834 | 0.907237 | 2.801205 | 0.133333 | . 10 | 2020-11-07 20:03:31 | 0.249 sec | 10.0 | 0.346859 | 0.403572 | 0.945400 | 0.914144 | 2.801205 | 0.144086 | . 11 | 2020-11-07 20:03:31 | 0.264 sec | 11.0 | 0.340863 | 0.392730 | 0.947445 | 0.918139 | 2.801205 | 0.133333 | . 12 | 2020-11-07 20:03:31 | 0.278 sec | 12.0 | 0.335607 | 0.382817 | 0.949037 | 0.920373 | 2.801205 | 0.131183 | . 13 | 2020-11-07 20:03:31 | 0.294 sec | 13.0 | 0.329409 | 0.371826 | 0.951445 | 0.924512 | 2.801205 | 0.144086 | . 14 | 2020-11-07 20:03:31 | 0.313 sec | 14.0 | 0.324769 | 0.363182 | 0.952946 | 0.927135 | 2.801205 | 0.133333 | . 15 | 2020-11-07 20:03:31 | 0.331 sec | 15.0 | 0.320250 | 0.354856 | 0.954527 | 0.929900 | 2.801205 | 0.122581 | . 16 | 2020-11-07 20:03:31 | 0.349 sec | 16.0 | 0.315399 | 0.346112 | 0.956361 | 0.933066 | 2.801205 | 0.111828 | . 17 | 2020-11-07 20:03:31 | 0.365 sec | 17.0 | 0.310275 | 0.337092 | 0.958123 | 0.935908 | 2.801205 | 0.105376 | . 18 | 2020-11-07 20:03:31 | 0.384 sec | 18.0 | 0.305461 | 0.328962 | 0.960440 | 0.939669 | 2.801205 | 0.103226 | . 19 | 2020-11-07 20:03:31 | 0.399 sec | 19.0 | 0.301918 | 0.322548 | 0.961428 | 0.941032 | 2.801205 | 0.103226 | . See the whole table with table.as_data_frame() Variable Importances: . variable relative_importance scaled_importance percentage . 0 Glucose | 160.889725 | 1.000000 | 0.386115 | . 1 BMI | 107.157028 | 0.666028 | 0.257163 | . 2 DiabetesPedigreeFunction | 49.382526 | 0.306934 | 0.118512 | . 3 Age | 28.789474 | 0.178939 | 0.069091 | . 4 BloodPressure | 24.516136 | 0.152379 | 0.058836 | . 5 Pregnancies | 22.055462 | 0.137084 | 0.052930 | . 6 Insulin | 12.949581 | 0.080487 | 0.031077 | . 7 SkinThickness | 10.949017 | 0.068053 | 0.026276 | . . perf = gbm.model_performance(valid) print(perf) . ModelMetricsBinomial: gbm ** Reported on test data. ** MSE: 0.18115330538099383 RMSE: 0.4256210819273334 LogLoss: 0.5366686568411096 Mean Per-Class Error: 0.25621588841722254 AUC: 0.804932282191227 AUCPR: 0.6470306042290592 Gini: 0.6098645643824541 Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1458393490707833: . 0 1 Error Rate . 0 0 | 53.0 | 44.0 | 0.4536 | (44.0/97.0) | . 1 1 | 3.0 | 48.0 | 0.0588 | (3.0/51.0) | . 2 Total | 56.0 | 92.0 | 0.3176 | (47.0/148.0) | . Maximum Metrics: Maximum metrics at their respective thresholds . metric threshold value idx . 0 max f1 | 0.145839 | 0.671329 | 91.0 | . 1 max f2 | 0.112531 | 0.819936 | 106.0 | . 2 max f0point5 | 0.668426 | 0.642458 | 31.0 | . 3 max accuracy | 0.668426 | 0.750000 | 31.0 | . 4 max precision | 0.980739 | 1.000000 | 0.0 | . 5 max recall | 0.112531 | 1.000000 | 106.0 | . 6 max specificity | 0.980739 | 1.000000 | 0.0 | . 7 max absolute_mcc | 0.145839 | 0.477769 | 91.0 | . 8 max min_per_class_accuracy | 0.328581 | 0.686275 | 64.0 | . 9 max mean_per_class_accuracy | 0.145839 | 0.743784 | 91.0 | . 10 max tns | 0.980739 | 97.000000 | 0.0 | . 11 max fns | 0.980739 | 50.000000 | 0.0 | . 12 max fps | 0.019964 | 97.000000 | 147.0 | . 13 max tps | 0.112531 | 51.000000 | 106.0 | . 14 max tnr | 0.980739 | 1.000000 | 0.0 | . 15 max fnr | 0.980739 | 0.980392 | 0.0 | . 16 max fpr | 0.019964 | 1.000000 | 147.0 | . 17 max tpr | 0.112531 | 1.000000 | 106.0 | . Gains/Lift Table: Avg response rate: 34.46 %, avg score: 36.45 % . group cumulative_data_fraction lower_threshold lift cumulative_lift response_rate score cumulative_response_rate cumulative_score capture_rate cumulative_capture_rate gain cumulative_gain kolmogorov_smirnov . 0 1 | 0.013514 | 0.973098 | 1.450980 | 1.450980 | 0.500000 | 0.977228 | 0.500000 | 0.977228 | 0.019608 | 0.019608 | 45.098039 | 45.098039 | 0.009299 | . 1 2 | 0.020270 | 0.966872 | 2.901961 | 1.934641 | 1.000000 | 0.972400 | 0.666667 | 0.975618 | 0.019608 | 0.039216 | 190.196078 | 93.464052 | 0.028906 | . 2 3 | 0.033784 | 0.952356 | 2.901961 | 2.321569 | 1.000000 | 0.963776 | 0.800000 | 0.970881 | 0.039216 | 0.078431 | 190.196078 | 132.156863 | 0.068122 | . 3 4 | 0.040541 | 0.927994 | 2.901961 | 2.418301 | 1.000000 | 0.939870 | 0.833333 | 0.965713 | 0.019608 | 0.098039 | 190.196078 | 141.830065 | 0.087730 | . 4 5 | 0.054054 | 0.923162 | 1.450980 | 2.176471 | 0.500000 | 0.925415 | 0.750000 | 0.955638 | 0.019608 | 0.117647 | 45.098039 | 117.647059 | 0.097029 | . 5 6 | 0.101351 | 0.892244 | 2.072829 | 2.128105 | 0.714286 | 0.906349 | 0.733333 | 0.932637 | 0.098039 | 0.215686 | 107.282913 | 112.810458 | 0.174449 | . 6 7 | 0.155405 | 0.836001 | 2.176471 | 2.144928 | 0.750000 | 0.861825 | 0.739130 | 0.908006 | 0.117647 | 0.333333 | 117.647059 | 114.492754 | 0.271478 | . 7 8 | 0.202703 | 0.722515 | 1.658263 | 2.031373 | 0.571429 | 0.773092 | 0.700000 | 0.876527 | 0.078431 | 0.411765 | 65.826331 | 103.137255 | 0.318981 | . 8 9 | 0.304054 | 0.517149 | 1.547712 | 1.870153 | 0.533333 | 0.605657 | 0.644444 | 0.786237 | 0.156863 | 0.568627 | 54.771242 | 87.015251 | 0.403679 | . 9 10 | 0.398649 | 0.409838 | 1.036415 | 1.672316 | 0.357143 | 0.461534 | 0.576271 | 0.709188 | 0.098039 | 0.666667 | 3.641457 | 67.231638 | 0.408935 | . 10 11 | 0.500000 | 0.255816 | 0.967320 | 1.529412 | 0.333333 | 0.314072 | 0.527027 | 0.629097 | 0.098039 | 0.764706 | -3.267974 | 52.941176 | 0.403881 | . 11 12 | 0.601351 | 0.146848 | 1.354248 | 1.499890 | 0.466667 | 0.196076 | 0.516854 | 0.556116 | 0.137255 | 0.901961 | 35.424837 | 49.988984 | 0.458662 | . 12 13 | 0.695946 | 0.115818 | 0.621849 | 1.380544 | 0.214286 | 0.134532 | 0.475728 | 0.498813 | 0.058824 | 0.960784 | -37.815126 | 38.054445 | 0.404083 | . 13 14 | 0.797297 | 0.062397 | 0.386928 | 1.254237 | 0.133333 | 0.091193 | 0.432203 | 0.446997 | 0.039216 | 1.000000 | -61.307190 | 25.423729 | 0.309278 | . 14 15 | 0.898649 | 0.039886 | 0.000000 | 1.112782 | 0.000000 | 0.049685 | 0.383459 | 0.402188 | 0.000000 | 1.000000 | -100.000000 | 11.278195 | 0.154639 | . 15 16 | 1.000000 | 0.019964 | 0.000000 | 1.000000 | 0.000000 | 0.030207 | 0.344595 | 0.364487 | 0.000000 | 1.000000 | -100.000000 | 0.000000 | 0.000000 | . . gbm_tune = H2OGradientBoostingEstimator( ntrees = 3000, learn_rate = 0.01, stopping_rounds = 20, stopping_metric = &quot;AUC&quot;, col_sample_rate = 0.7, sample_rate = 0.7, seed = 1234 ) gbm_tune.train(x=predictors, y=response, training_frame=train, validation_frame=valid) . gbm Model Build progress: |███████████████████████████████████████████████| 100% . gbm_tune.model_performance(valid).auc() . 0.8019001414998989 . from h2o.grid.grid_search import H2OGridSearch gbm_grid = H2OGradientBoostingEstimator( ntrees = 3000, learn_rate = 0.01, stopping_rounds = 20, stopping_metric = &quot;AUC&quot;, col_sample_rate = 0.7, sample_rate = 0.7, seed = 1234 ) hyper_params = {&#39;max_depth&#39;:[4,6,8,10,12]} grid = H2OGridSearch(gbm_grid, hyper_params, grid_id=&#39;depth_grid&#39;, search_criteria={&#39;strategy&#39;: &quot;Cartesian&quot;}) #Train grid search grid.train(x=predictors, y=response, training_frame=train, validation_frame=valid) . gbm Grid Build progress: |████████████████████████████████████████████████| 100% . print(grid) . max_depth model_ids logloss 0 10 depth_grid_model_4 0.5610332739005056 1 12 depth_grid_model_5 0.5610431620153586 2 8 depth_grid_model_3 0.5618675574114658 3 6 depth_grid_model_2 0.5688369200105283 4 4 depth_grid_model_1 0.5720338350452505 . sorted_grid = grid.get_grid(sort_by=&#39;auc&#39;,decreasing=True) print(sorted_grid) . max_depth model_ids auc 0 10 depth_grid_model_4 0.8067515666060238 1 12 depth_grid_model_5 0.8067515666060238 2 8 depth_grid_model_3 0.8033151404891854 3 6 depth_grid_model_2 0.8031129977764302 4 4 depth_grid_model_1 0.7984637153830605 . cv_gbm = H2OGradientBoostingEstimator( ntrees = 3000, learn_rate = 0.05, stopping_rounds = 20, stopping_metric = &quot;AUC&quot;, nfolds=4, seed=2018) cv_gbm.train(x = predictors, y = response, training_frame = train, validation_frame=valid) cv_summary = cv_gbm.cross_validation_metrics_summary().as_data_frame() cv_summary . gbm Model Build progress: |███████████████████████████████████████████████| 100% . mean sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid . 0 accuracy | 0.8060614 | 0.028866975 | 0.81512606 | 0.84166664 | 0.775 | 0.7924528 | . 1 auc | 0.83460003 | 0.0253348 | 0.8711854 | 0.82493657 | 0.81303704 | 0.8292411 | . 2 aucpr | 0.7332863 | 0.08590643 | 0.77500844 | 0.60952866 | 0.7449944 | 0.8036139 | . 3 err | 0.19393861 | 0.028866975 | 0.18487395 | 0.15833333 | 0.225 | 0.20754717 | . 4 err_count | 22.5 | 3.3166249 | 22.0 | 19.0 | 27.0 | 22.0 | . 5 f0point5 | 0.7166275 | 0.023856336 | 0.7432432 | 0.6993007 | 0.6938776 | 0.7300885 | . 6 f1 | 0.7359389 | 0.051860016 | 0.8 | 0.6779661 | 0.7157895 | 0.75 | . 7 f2 | 0.75854874 | 0.086101264 | 0.86614174 | 0.65789473 | 0.73913044 | 0.77102804 | . 8 lift_top_group | 2.0913858 | 0.6500798 | 1.2395834 | 1.9354838 | 2.6666667 | 2.5238094 | . 9 logloss | 0.4842181 | 0.03085123 | 0.46077308 | 0.45635447 | 0.52019364 | 0.49955118 | . 10 max_per_class_error | 0.2667725 | 0.061059155 | 0.2535211 | 0.3548387 | 0.24444444 | 0.21428572 | . 11 mcc | 0.58331674 | 0.049473092 | 0.65121645 | 0.5746587 | 0.5324516 | 0.5749401 | . 12 mean_per_class_accuracy | 0.79290384 | 0.027116295 | 0.8315728 | 0.7776368 | 0.77111113 | 0.79129463 | . 13 mean_per_class_error | 0.20709616 | 0.027116295 | 0.16842723 | 0.22236317 | 0.22888888 | 0.20870535 | . 14 mse | 0.1554569 | 0.008760154 | 0.14931631 | 0.14733355 | 0.16610987 | 0.1590679 | . 15 pr_auc | 0.7332863 | 0.08590643 | 0.77500844 | 0.60952866 | 0.7449944 | 0.8036139 | . 16 precision | 0.7053386 | 0.017187094 | 0.7096774 | 0.71428573 | 0.68 | 0.7173913 | . 17 r2 | 0.30923334 | 0.06338601 | 0.37955743 | 0.23102464 | 0.29126453 | 0.33508673 | . 18 recall | 0.7757745 | 0.11168112 | 0.9166667 | 0.6451613 | 0.75555557 | 0.78571427 | . 19 rmse | 0.39416355 | 0.011076028 | 0.38641468 | 0.38384053 | 0.4075658 | 0.39883316 | . 20 specificity | 0.8100332 | 0.070176266 | 0.74647886 | 0.9101124 | 0.7866667 | 0.796875 | . cv_gbm.model_performance(valid).auc() . 0.8059429957550029 . XGBoost: . from h2o.estimators import H2OXGBoostEstimator cv_xgb = H2OXGBoostEstimator( ntrees = 3000, learn_rate = 0.05, stopping_rounds = 20, stopping_metric = &quot;AUC&quot;, nfolds=4, seed=2018) cv_xgb.train(x = predictors, y = response, training_frame = train, validation_frame=valid) cv_xgb.model_performance(valid).auc() . xgboost Model Build progress: |███████████████████████████████████████████| 100% . 0.7982615726703053 . cv_xgb.varimp_plot() . AutoML : Automatic Machine Learning: . From the H2O AutoML page, . from h2o.automl import H2OAutoML aml = H2OAutoML(max_models = 10, max_runtime_secs=100, seed = 1) aml.train(x=predictors, y=response, training_frame=train, validation_frame=valid) . AutoML progress: | 20:03:54.196: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models. ████████████████████████████████████████████████████████| 100% . automl leaderboard: . lb = aml.leaderboard lb . model_id auc logloss aucpr mean_per_class_error rmse mse . StackedEnsemble_BestOfFamily_AutoML_20201107_200354 | 0.838498 | 0.471209 | 0.720251 | 0.225168 | 0.389759 | 0.151912 | . DRF_1_AutoML_20201107_200354 | 0.837813 | 0.673743 | 0.750599 | 0.225158 | 0.390594 | 0.152563 | . StackedEnsemble_AllModels_AutoML_20201107_200354 | 0.836755 | 0.473849 | 0.717937 | 0.233197 | 0.391309 | 0.153122 | . GLM_1_AutoML_20201107_200354 | 0.831638 | 0.488798 | 0.716653 | 0.238204 | 0.395915 | 0.156749 | . GBM_2_AutoML_20201107_200354 | 0.823921 | 0.492342 | 0.730483 | 0.240198 | 0.397899 | 0.158323 | . GBM_1_AutoML_20201107_200354 | 0.823186 | 0.499362 | 0.730256 | 0.23354 | 0.398824 | 0.15906 | . GBM_3_AutoML_20201107_200354 | 0.822944 | 0.491874 | 0.745621 | 0.245578 | 0.396877 | 0.157511 | . XGBoost_2_AutoML_20201107_200354 | 0.822722 | 0.490048 | 0.729844 | 0.237186 | 0.402005 | 0.161608 | . GBM_4_AutoML_20201107_200354 | 0.82235 | 0.492086 | 0.726875 | 0.241578 | 0.398974 | 0.159181 | . GBM_5_AutoML_20201107_200354 | 0.817947 | 0.496638 | 0.68625 | 0.239211 | 0.403748 | 0.163012 | . . metalearner = h2o.get_model(aml.leader.metalearner()[&#39;name&#39;]) metalearner.std_coef_plot() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/17/diab_prediction_with_h2o.html",
            "relUrl": "/2020/11/17/diab_prediction_with_h2o.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "AutoML, Xgboost and H2O",
            "content": "Credit: code from https://www.kaggle.com/paradiselost/tutorial-automl-capabilities-of-h2o-library . . Automated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, the typical stages (and sub-stages) of work are the following: . Data preparation data pre-processing | feature engineering | feature extraction | feature selection | . | Model selection | Hyperparameter optimization (to maximize the performance of the final model) | Many of these steps are often beyond the abilities of non-experts. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. . Some of the notable platforms tackling various stages of AutoML are the following: . auto-sklearn is a Bayesian hyperparameter optimization layer on top of scikit-learn. | TPOT (TeaPOT) is a Python library that automatically creates and optimizes full machine learning pipelines using genetic programming. | TransmogrifAI is a Scala/SparkML library created by Salesforce for automated data cleansing, feature engineering, model selection, and hyperparameter optimization. | H2O AutoML performs (simple) data preprocessing, automates the process of training a large selection of candidate models, tunes hyperparameters of the models and creates stacked ensembles. | H2O Driverless AI is a commercial software package that automates lots of aspects of machine learning applications. It has a strong focus on automatic feature engineering. | . An overview of AutoML capabilities of H2O library is presented in this tutorial. The library can be installed simply by . #!pip install h2o . Let&#39;s import the required packages and call h2o.init(). The specified arguments (nthreads and max_mem_size) are optional. . import sys, os, os.path import warnings warnings.filterwarnings(&#39;ignore&#39;) import numpy as np import pandas as pd import pickle import h2o from h2o.automl import H2OAutoML h2o.init( nthreads=-1, # number of threads when launching a new H2O server max_mem_size=12 # in gigabytes ) . Example 1: a classification task . Let&#39;s apply the power of H2O AutoML to the &quot;Flight delays&quot; competition (it&#39;s a binary classification task) from mlcourse.ai. . train_df = pd.read_csv(&#39;../input/mlcourse/flight_delays_train.csv&#39;) test_df = pd.read_csv(&#39;../input/mlcourse/flight_delays_test.csv&#39;) . print(&#39;train_df cols:&#39;, list(train_df.columns)) print(&#39;test_df cols: &#39;, list(test_df.columns)) train_df.head() . train_df.dtypes . The features Month, DayofMonth, DayOfWeek, DepTime, Distance can be represented as numbers. Let&#39;s convert those features to numerical type (a new feature HourFloat is added): . for df in [train_df, test_df]: df[&#39;Month&#39;] = df[&#39;Month&#39;].apply(lambda s: s.split(&#39;-&#39;)[1]).astype(&#39;int&#39;) df[&#39;DayofMonth&#39;] = df[&#39;DayofMonth&#39;].apply(lambda s: s.split(&#39;-&#39;)[1]).astype(&#39;int&#39;) df[&#39;DayOfWeek&#39;] = df[&#39;DayOfWeek&#39;].apply(lambda s: s.split(&#39;-&#39;)[1]).astype(&#39;int&#39;) df[&#39;HourFloat&#39;] = df[&#39;DepTime&#39;].apply( lambda t: (t // 100) % 24 + ((t % 100) % 60) / 60 ).astype(&#39;float&#39;) . Let&#39;s also introduce a new feature Route that is the concatenation of Origin and Dest: . for df in [train_df, test_df]: df[&#39;Route&#39;] = df[[&#39;Origin&#39;, &#39;Dest&#39;]].apply( lambda pair: &#39;&#39;.join([str(a) for a in pair]), axis=&#39;columns&#39; ).astype(&#39;str&#39;) . We will not use the column DepTime anymore. Split the target column from the features columns in train_df: . target = train_df[&#39;dep_delayed_15min&#39;].map({&#39;Y&#39;: 1, &#39;N&#39;: 0}) feature_cols = [ &#39;Month&#39;, &#39;DayofMonth&#39;, &#39;DayOfWeek&#39;, &#39;HourFloat&#39;, &#39;UniqueCarrier&#39;, &#39;Origin&#39;, &#39;Dest&#39;, &#39;Route&#39;, &#39;Distance&#39;,] train_df_modif = train_df[feature_cols] test_df_modif = test_df[feature_cols] . The features UniqueCarrier, Origin, Dest, Route should be categorical: . N_train = train_df_modif.shape[0] train_test_X = pd.concat([train_df_modif, test_df_modif], axis=&#39;index&#39;) for feat in [&#39;UniqueCarrier&#39;, &#39;Origin&#39;, &#39;Dest&#39;, &#39;Route&#39;]: train_test_X[feat] = train_test_X[feat].astype(&#39;category&#39;) . X_train = train_test_X[:N_train] X_test = train_test_X[N_train:] y_train = target . Pandas DataFrames should be converted to H2O dataframes before calling H2OAutoML(). . Note: if you don&#39;t have to preprocess the data, you can get H2O dataframes directly from the data files by a call like df = h2o.import_file(datafile_path) (where datafile_path is a filesystem path or a URL). . X_y_train_h = h2o.H2OFrame(pd.concat([X_train, y_train], axis=&#39;columns&#39;)) X_y_train_h[&#39;dep_delayed_15min&#39;] = X_y_train_h[&#39;dep_delayed_15min&#39;].asfactor() # ^ the target column should have categorical type for classification tasks # (numerical type for regression tasks) X_test_h = h2o.H2OFrame(X_test) X_y_train_h.describe() . aml = H2OAutoML( max_runtime_secs=(3600 * 8), # 8 hours max_models=None, # no limit seed=17 ) . Among the most important arguments (with their default values) of H2OAutoML() are the following: . nfolds=5 -- number of folds for k-fold cross-validation (nfolds=0 disables cross-validation) | balance_classes=False -- balance training data class counts via over/under-sampling | max_runtime_secs=3600 -- how long the AutoML run will execute (in seconds) | max_models=None -- the maximum number of models to build in an AutoML run (None means no limitation) | include_algos=None -- list of algorithms to restrict to during the model-building phase (cannot be used in combination with exclude_algos parameter; None means that all appropriate H2O algorithms will be used) | exclude_algos=None -- list of algorithms to skip during the model-building phase (None means that all appropriate H2O algorithms will be used) | seed=None -- a random seed for reproducibility (AutoML can only guarantee reproducibility if max_models or early stopping is used because max_runtime_secs is resource limited, meaning that if the resources are not the same between runs, AutoML may be able to train more models on one run vs another) | . H2O AutoML trains and cross-validates: . a default Random Forest (DRF), | an Extremely-Randomized Forest (XRT), | a random grid of Generalized Linear Models (GLM), | a random grid of XGBoost (XGBoost), | a random grid of Gradient Boosting Machines (GBM), | a random grid of Deep Neural Nets (DeepLearning), | and 2 Stacked Ensembles, one of all the models, and one of only the best models of each kind. | . In the cell below, I call aml.train(), save the leaderboard and all individual models. The running time is about 8 hours, so after running it once I saved the output files as a new dataset, connected the dataset to this kernel and commented out the code in the cell. . %%time # aml.train( # x=feature_cols, # y=&#39;dep_delayed_15min&#39;, # training_frame=X_y_train_h # ) # lb = aml.leaderboard # model_ids = list(lb[&#39;model_id&#39;].as_data_frame().iloc[:,0]) # out_path = &quot;.&quot; # for m_id in model_ids: # mdl = h2o.get_model(m_id) # h2o.save_model(model=mdl, path=out_path, force=True) # h2o.export_file(lb, os.path.join(out_path, &#39;aml_leaderboard.h2o&#39;), force=True) . Some of the arguments for H2OAutoML.train() are the following: . training_frame -- the H2OFrame having the columns indicated by x and y | x -- list of feature column names in training_frame | y -- a column name indicating the target | validation_frame -- the H2OFrame with validation data (by default and when nfolds &gt; 1, validation_frame will be ignored) | leaderboard_frame -- the H2OFrame with test data for scoring the leaderboard (optinal; by default (leaderboard_frame=None) the cross-validation metric on training_frame will be used to generate the leaderboard rankings) | . Let&#39;s take a look at the leaderboard: . models_path = &quot;../input/h2o-automl-saved-models-classif/&quot; lb = h2o.import_file(path=os.path.join(models_path, &quot;aml_leaderboard.h2o&quot;)) lb.head(rows=10) #lb.head(rows=lb.nrows) # ^ to see the entire leaderboard . Among the individual models, XGBoost is the leader (auc = 0.749523) for this task. Best individual GBM has auc = 0.741785, best XRT has auc = 0.731317, best DRF has auc = 0.725166, best DNN has auc = 0.706676. . StackedEnsemble_AllModels is usually the leader, StackedEnsemble_BestOfFamily is usually at the 2nd place. Let&#39;s look inside the StackedEnsemble_AllModels. It is an ensemble of all of the individual models in the AutoML run. . se_all = h2o.load_model(os.path.join(models_path, &quot;StackedEnsemble_AllModels_AutoML_20190414_112210&quot;)) # Get the Stacked Ensemble metalearner model metalearner = h2o.get_model(se_all.metalearner()[&#39;name&#39;]) . The AutoML Stacked Ensembles use the GLM with non-negative weights as the default metalearner (combiner) algorithm. Let&#39;s examine the variable importance of the metalearner algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. Intercept represents the constant term in a linear model. . %matplotlib inline metalearner.std_coef_plot(num_of_features=20) # ^ all importance values starting from the 16th are zero #metalearner.coef_norm() # ^ to see the table in the text form . StackedEnsemble_BestOfFamily shows the following: . se_best_of_family = h2o.load_model(os.path.join(models_path, &quot;StackedEnsemble_BestOfFamily_AutoML_20190414_112210&quot;)) # Get the Stacked Ensemble metalearner model metalearner = h2o.get_model(se_best_of_family.metalearner()[&#39;name&#39;]) %matplotlib inline metalearner.std_coef_plot(num_of_features=10) #metalearner.coef_norm() . Let&#39;s reproduce the result (auc) of a few best individual models. . from h2o.estimators.xgboost import H2OXGBoostEstimator model_01 = h2o.load_model(os.path.join(models_path, &quot;XGBoost_grid_1_AutoML_20190414_112210_model_19&quot;)) excluded_params = [&#39;model_id&#39;, &#39;response_column&#39;, &#39;ignored_columns&#39;] model_01_actual_params = {k: v[&#39;actual&#39;] for k, v in model_01.params.items() if k not in excluded_params} reprod_model_01 = H2OXGBoostEstimator(**model_01_actual_params) reprod_model_01.train( x=feature_cols, y=&#39;dep_delayed_15min&#39;, training_frame=X_y_train_h ) reprod_model_01.auc(xval=True) # ^ 0.749453, slightly worse compared to the leaderboard value . from h2o.estimators.gbm import H2OGradientBoostingEstimator model_12 = h2o.load_model(os.path.join(models_path, &quot;GBM_grid_1_AutoML_20190414_112210_model_85&quot;)) excluded_params = [&#39;model_id&#39;, &#39;response_column&#39;, &#39;ignored_columns&#39;] model_12_actual_params = {k: v[&#39;actual&#39;] for k, v in model_12.params.items() if k not in excluded_params} reprod_model_12 = H2OGradientBoostingEstimator(**model_12_actual_params) reprod_model_12.train( x=feature_cols, y=&#39;dep_delayed_15min&#39;, training_frame=X_y_train_h ) reprod_model_12.auc(xval=True) # ^ 0.741785, the same as at the leaderboard . from h2o.estimators.glm import H2OGeneralizedLinearEstimator from h2o.grid.grid_search import H2OGridSearch model_93 = h2o.load_model(os.path.join(models_path, &quot;GLM_grid_1_AutoML_20190414_112210_model_1&quot;)) excluded_params = [&#39;model_id&#39;, &#39;response_column&#39;, &#39;ignored_columns&#39;, &#39;lambda&#39;] model_93_actual_params = {k: v[&#39;actual&#39;] for k, v in model_93.params.items() if k not in excluded_params} reprod_model_93 = H2OGeneralizedLinearEstimator(**model_93_actual_params) reprod_model_93.train( x=feature_cols, y=&#39;dep_delayed_15min&#39;, training_frame=X_y_train_h ) reprod_model_93.auc(xval=True) # ^ 0.699418, the same as at the leaderboard . Let&#39;s train the CatBoostClassifier with the default parameters and compare its results with AutoML run results. . from catboost import Pool, CatBoostClassifier, cv cb_model = CatBoostClassifier( eval_metric=&#39;AUC&#39;, use_best_model=True, random_seed=17 ) cv_data = cv( Pool(X_train, y_train, cat_features=[4,5,6,7]), cb_model.get_params(), fold_count=5, verbose=False ) print(&quot;CatBoostClassifier: the best cv auc is&quot;, np.max(cv_data[&#39;test-AUC-mean&#39;])) . The CatBoostClassifier cross-validation auc result is 0.749009. This value falls between the 2nd (auc = 0.749523) and 3rd (auc = 0.749192) places among the individual models at the leaderboard. . Example 2: a regression task . Let&#39;s consider a regression task from the &quot;New York City Taxi Trip Duration&quot; competition. The challenge is to build a model that predicts the total ride duration of taxi trips in New York City. The features include pickup time, geo-coordinates, number of passengers, and a few other variables. . df_train = pd.read_csv(&#39;../input/nyc-taxi-trip-duration/train.csv&#39;, index_col=0) df_test = pd.read_csv(&#39;../input/nyc-taxi-trip-duration/test.csv&#39;, index_col=0) . We will use only df_train (perform 5-fold cross-validation on it). Convert the date- and time-related features to the datetime format; take the logarithm (log(1 + x)) of the target value (trip duration). After the logarithm transform, the distribution of the target variable is close to normal (see this kernel). . df_train[&#39;pickup_datetime&#39;] = pd.to_datetime(df_train.pickup_datetime) df_train.loc[:, &#39;pickup_date&#39;] = df_train[&#39;pickup_datetime&#39;].dt.date df_train[&#39;dropoff_datetime&#39;] = pd.to_datetime(df_train.dropoff_datetime) df_train[&#39;store_and_fwd_flag&#39;] = 1 * (df_train.store_and_fwd_flag.values == &#39;Y&#39;) df_train[&#39;check_trip_duration&#39;] = (df_train[&#39;dropoff_datetime&#39;] - df_train[&#39;pickup_datetime&#39;]).map( lambda x: x.total_seconds() ) df_train[&#39;log_trip_duration&#39;] = np.log1p(df_train[&#39;trip_duration&#39;].values) cnd = np.abs(df_train[&#39;check_trip_duration&#39;].values - df_train[&#39;trip_duration&#39;].values) &gt; 1 duration_difference = df_train[cnd] if len(duration_difference[[&#39;pickup_datetime&#39;, &#39;dropoff_datetime&#39;, &#39;trip_duration&#39;, &#39;check_trip_duration&#39;]]) == 0: print(&#39;Trip_duration and datetimes are ok.&#39;) else: print(&#39;Ooops.&#39;) . Select the columns common to the train set and test set; convert pd.DataFrame to H2OFrame: . common_cols = [ &#39;vendor_id&#39;, &#39;pickup_datetime&#39;, &#39;passenger_count&#39;, &#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;store_and_fwd_flag&#39;, ] X_y_train_h = h2o.H2OFrame( pd.concat( [df_train[common_cols], df_train[&#39;log_trip_duration&#39;]], axis=&#39;columns&#39; ) ) for ft in [&#39;vendor_id&#39;, &#39;store_and_fwd_flag&#39;]: X_y_train_h[ft] = X_y_train_h[ft].asfactor() X_y_train_h.describe() . I have run the cell below (~8 hours), saved all models and the leaderboard, then commented out the code: . # aml = H2OAutoML( # max_runtime_secs=(3600 * 8), # 8 hours # max_models=None, # no limit # seed=SEED, # ) # aml.train( # x=common_cols, # y=&#39;log_trip_duration&#39;, # training_frame=X_y_train_h # ) # lb = aml.leaderboard # model_ids = list(lb[&#39;model_id&#39;].as_data_frame().iloc[:,0]) # out_path = &quot;.&quot; # for m_id in model_ids: # mdl = h2o.get_model(m_id) # h2o.save_model(model=mdl, path=out_path, force=True) # h2o.export_file(lb, os.path.join(out_path, &#39;aml_leaderboard.h2o&#39;), force=True) . Interestingly, there is only one model at the leaderboard: . models_path = &quot;../input/h2o-automl-saved-models-regress/&quot; lb = h2o.import_file(path=os.path.join(models_path, &quot;aml_leaderboard.h2o&quot;)) lb.head(rows=10) . Let&#39;s compare the result of the model XGBoost_1_AutoML_20190417_212831 with that of the CatBoostRegressor with the default parameters. . from catboost import Pool, CatBoostRegressor, cv cb_model = CatBoostRegressor( eval_metric=&#39;RMSE&#39;, use_best_model=True, random_seed=17 ) cv_data = cv( Pool(df_train[common_cols], df_train[&#39;log_trip_duration&#39;], cat_features=[0,7]), cb_model.get_params(), fold_count=5, verbose=False ) . print(&quot;CatBoostRegressor: the best cv rmse is&quot;, np.min(cv_data[&#39;test-RMSE-mean&#39;])) . Default CatBoost&#39;s RMSE is slightly worse than that of the XGBoost model from the H2O AutoML run. . Conclusion . I think that H2O AutoML is worth a try. And I hope you have found this tutorial useful. . There are extremely useful &quot;H2O AutoML Pro Tips&quot; in the presentation &quot;Scalable Automatic Machine Learning in H2O&quot; mentioned in the References below. . References . H2O.ai | H2O AutoML documentation | AutoML Tutorial: R and Python notebooks | Intro to AutoML + Hands-on Lab: 1 hour video, slides | Scalable Automatic Machine Learning in H2O: 1 hour video, slides | H2O for GPU (H2O4GPU) | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/16/automl-capabilities-of-h2o-library.html",
            "relUrl": "/2020/11/16/automl-capabilities-of-h2o-library.html",
            "date": " • Nov 16, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Adam optimizer",
            "content": "credit: code from https://github.com/enochkan/building-from-scratch/blob/main/adam-optimizer-from-scratch.ipynb https://arxiv.org/abs/1412.6980 . import numpy as np . class AdamOptim(): def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8): self.m_dw, self.v_dw = 0, 0 self.m_db, self.v_db = 0, 0 self.beta1 = beta1 self.beta2 = beta2 self.epsilon = epsilon self.eta = eta def update(self, t, w, b, dw, db): ## dw, db are from current minibatch ## momentum beta 1 # *** weights *** # self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw # *** biases *** # self.m_db = self.beta1*self.m_db + (1-self.beta1)*db ## rms beta 2 # *** weights *** # self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2) # *** biases *** # self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db) ## bias correction m_dw_corr = self.m_dw/(1-self.beta1**t) m_db_corr = self.m_db/(1-self.beta1**t) v_dw_corr = self.v_dw/(1-self.beta2**t) v_db_corr = self.v_db/(1-self.beta2**t) ## update weights and biases w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon)) b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon)) return w, b . ## define loss functions and gradient descent. We don&#39;t really use the loss function here. def loss_function(m): return m**2-2*m+1 ## take derivative def grad_function(m): return 2*m-2 def check_convergence(w0, w1): return (w0 == w1) ## initialize weights and biases, and our optimizer w_0 = 0 b_0 = 0 adam = AdamOptim() t = 1 converged = False while not converged: dw = grad_function(w_0) db = grad_function(b_0) w_0_old = w_0 w_0, b_0 = adam.update(t,w=w_0, b=b_0, dw=dw, db=db) if check_convergence(w_0, w_0_old): print(&#39;converged after &#39;+str(t)+&#39; iterations&#39;) break else: print(&#39;iteration &#39;+str(t)+&#39;: weight=&#39;+str(w_0)) t+=1 . iteration 1: weight=0.009999999950000001 iteration 2: weight=0.01999725400385255 iteration 3: weight=0.029989900621600046 iteration 4: weight=0.039976060276935343 iteration 5: weight=0.049953839711732076 iteration 6: weight=0.05992133621693422 iteration 7: weight=0.06987664190678831 iteration 8: weight=0.07981784795404925 iteration 9: weight=0.08974304875491491 iteration 10: weight=0.0996503459940126 iteration 11: weight=0.10953785258172263 iteration 12: weight=0.11940369643843479 iteration 13: weight=0.12924602410293135 iteration 14: weight=0.13906300414491304 iteration 15: weight=0.14885283036466956 iteration 16: weight=0.15861372476597732 iteration 17: weight=0.1683439402914239 iteration 18: weight=0.17804176331244895 iteration 19: weight=0.1877055158694015 iteration 20: weight=0.19733355765979776 iteration 21: weight=0.2069242877756729 iteration 22: weight=0.21647614619342795 iteration 23: weight=0.22598761502184558 iteration 24: weight=0.23545721951596985 iteration 25: weight=0.24488352886630008 iteration 26: weight=0.25426515677423506 iteration 27: weight=0.26360076182591813 iteration 28: weight=0.2728890476775851 iteration 29: weight=0.2821287630662142 iteration 30: weight=0.2913187016597368 iteration 31: weight=0.3004577017613055 iteration 32: weight=0.30954464588215314 iteration 33: weight=0.3185784601974346 iteration 34: weight=0.32755811389914286 iteration 35: weight=0.3364826184597571 iteration 36: weight=0.3453510268197323 iteration 37: weight=0.3541624325113025 iteration 38: weight=0.36291596873035775 iteration 39: weight=0.3716108073673929 iteration 40: weight=0.38024615800772815 iteration 41: weight=0.3888212669103811 iteration 42: weight=0.3973354159741451 iteration 43: weight=0.405787921698609 iteration 44: weight=0.4141781341470465 iteration 45: weight=0.42250543591732403 iteration 46: weight=0.4307692411262221 iteration 47: weight=0.4389689944118521 iteration 48: weight=0.44710416995817326 iteration 49: weight=0.455174270544982 iteration 50: weight=0.4631788266261575 iteration 51: weight=0.47111739543840325 iteration 52: weight=0.47898956014222716 iteration 53: weight=0.4867949289964492 iteration 54: weight=0.4945331345671171 iteration 55: weight=0.502203832971342 iteration 56: weight=0.5098067031562412 iteration 57: weight=0.5173414462128868 iteration 58: weight=0.5248077847249037 iteration 59: weight=0.5322054621511477 iteration 60: weight=0.5395342422417011 iteration 61: weight=0.5467939084862693 iteration 62: weight=0.5539842635939275 iteration 63: weight=0.5611051290030595 iteration 64: weight=0.568156344420244 iteration 65: weight=0.5751377673867769 iteration 66: weight=0.5820492728714711 iteration 67: weight=0.5888907528883383 iteration 68: weight=0.5956621161377404 iteration 69: weight=0.6023632876695909 iteration 70: weight=0.6089942085671857 iteration 71: weight=0.615554835650261 iteration 72: weight=0.6220451411958915 iteration 73: weight=0.6284651126758695 iteration 74: weight=0.6348147525092407 iteration 75: weight=0.641094077828706 iteration 76: weight=0.6473031202596423 iteration 77: weight=0.653441925710538 iteration 78: weight=0.6595105541736849 iteration 79: weight=0.6655090795350186 iteration 80: weight=0.6714375893920466 iteration 81: weight=0.6772961848788547 iteration 82: weight=0.6830849804972335 iteration 83: weight=0.6888041039530174 iteration 84: weight=0.6944536959967778 iteration 85: weight=0.7000339102680659 iteration 86: weight=0.7055449131424467 iteration 87: weight=0.7109868835806149 iteration 88: weight=0.7163600129789347 iteration 89: weight=0.7216645050207873 iteration 90: weight=0.7269005755281592 iteration 91: weight=0.7320684523129467 iteration 92: weight=0.7371683750274948 iteration 93: weight=0.7422005950139324 iteration 94: weight=0.7471653751519031 iteration 95: weight=0.7520629897043302 iteration 96: weight=0.7568937241608946 iteration 97: weight=0.7616578750789351 iteration 98: weight=0.7663557499215196 iteration 99: weight=0.7709876668924653 iteration 100: weight=0.7755539547681211 iteration 101: weight=0.7800549527257501 iteration 102: weight=0.7844910101683854 iteration 103: weight=0.7888624865460541 iteration 104: weight=0.7931697511732935 iteration 105: weight=0.7974131830429079 iteration 106: weight=0.8015931706359366 iteration 107: weight=0.8057101117278274 iteration 108: weight=0.8097644131908287 iteration 109: weight=0.8137564907926365 iteration 110: weight=0.8176867689913462 iteration 111: weight=0.8215556807267823 iteration 112: weight=0.8253636672082894 iteration 113: weight=0.8291111776990878 iteration 114: weight=0.8327986692973092 iteration 115: weight=0.8364266067138391 iteration 116: weight=0.8399954620471094 iteration 117: weight=0.8435057145549907 iteration 118: weight=0.846957850423946 iteration 119: weight=0.8503523625356184 iteration 120: weight=0.8536897502310298 iteration 121: weight=0.8569705190725782 iteration 122: weight=0.860195180604026 iteration 123: weight=0.8633642521086788 iteration 124: weight=0.8664782563659569 iteration 125: weight=0.8695377214065675 iteration 126: weight=0.8725431802664898 iteration 127: weight=0.8754951707399848 iteration 128: weight=0.8783942351318468 iteration 129: weight=0.8812409200091125 iteration 130: weight=0.8840357759524461 iteration 131: weight=0.8867793573074179 iteration 132: weight=0.8894722219358927 iteration 133: weight=0.8921149309677457 iteration 134: weight=0.8947080485531188 iteration 135: weight=0.8972521416154303 iteration 136: weight=0.8997477796053487 iteration 137: weight=0.9021955342559357 iteration 138: weight=0.9045959793391636 iteration 139: weight=0.9069496904240053 iteration 140: weight=0.9092572446362935 iteration 141: weight=0.9115192204205393 iteration 142: weight=0.9137361973038962 iteration 143: weight=0.9159087556624508 iteration 144: weight=0.918037476490016 iteration 145: weight=0.9201229411695963 iteration 146: weight=0.9221657312476885 iteration 147: weight=0.9241664282115778 iteration 148: weight=0.9261256132697787 iteration 149: weight=0.9280438671357686 iteration 150: weight=0.9299217698151505 iteration 151: weight=0.9317599003963796 iteration 152: weight=0.933558836845177 iteration 153: weight=0.9353191558027519 iteration 154: weight=0.9370414323879417 iteration 155: weight=0.938726240003377 iteration 156: weight=0.9403741501457696 iteration 157: weight=0.9419857322204138 iteration 158: weight=0.9435615533599866 iteration 159: weight=0.9451021782477242 iteration 160: weight=0.946608168945046 iteration 161: weight=0.9480800847236891 iteration 162: weight=0.9495184819024132 iteration 163: weight=0.950923913688324 iteration 164: weight=0.9522969300228624 iteration 165: weight=0.9536380774324956 iteration 166: weight=0.9549478988841421 iteration 167: weight=0.9562269336453583 iteration 168: weight=0.9574757171493036 iteration 169: weight=0.9586947808645 iteration 170: weight=0.9598846521693933 iteration 171: weight=0.9610458542317184 iteration 172: weight=0.9621789058926664 iteration 173: weight=0.9632843215558446 iteration 174: weight=0.9643626110810171 iteration 175: weight=0.9654142796826064 iteration 176: weight=0.9664398278329352 iteration 177: weight=0.9674397511701782 iteration 178: weight=0.9684145404109944 iteration 179: weight=0.9693646812678026 iteration 180: weight=0.9702906543706602 iteration 181: weight=0.9711929351937009 iteration 182: weight=0.9720719939860842 iteration 183: weight=0.9729282957074055 iteration 184: weight=0.9737622999675106 iteration 185: weight=0.9745744609706605 iteration 186: weight=0.9753652274639815 iteration 187: weight=0.9761350426901402 iteration 188: weight=0.9768843443441767 iteration 189: weight=0.9776135645344266 iteration 190: weight=0.9783231297474617 iteration 191: weight=0.9790134608169765 iteration 192: weight=0.9796849728965454 iteration 193: weight=0.9803380754361745 iteration 194: weight=0.9809731721625682 iteration 195: weight=0.9815906610630333 iteration 196: weight=0.9821909343729368 iteration 197: weight=0.9827743785666372 iteration 198: weight=0.9833413743518047 iteration 199: weight=0.9838922966670466 iteration 200: weight=0.984427514682753 iteration 201: weight=0.9849473918050764 iteration 202: weight=0.98545228568296 iteration 203: weight=0.985942548218127 iteration 204: weight=0.9864185255779455 iteration 205: weight=0.9868805582110803 iteration 206: weight=0.9873289808658458 iteration 207: weight=0.9877641226111719 iteration 208: weight=0.9881863068600975 iteration 209: weight=0.988595851395703 iteration 210: weight=0.9889930683993977 iteration 211: weight=0.9893782644814731 iteration 212: weight=0.9897517407138409 iteration 213: weight=0.9901137926648667 iteration 214: weight=0.9904647104362185 iteration 215: weight=0.9908047787016447 iteration 216: weight=0.9911342767475994 iteration 217: weight=0.9914534785156336 iteration 218: weight=0.9917626526464719 iteration 219: weight=0.9920620625256936 iteration 220: weight=0.9923519663309407 iteration 221: weight=0.9926326170805743 iteration 222: weight=0.9929042626837034 iteration 223: weight=0.9931671459915103 iteration 224: weight=0.9934215048497993 iteration 225: weight=0.993667572152694 iteration 226: weight=0.993905575897414 iteration 227: weight=0.9941357392400594 iteration 228: weight=0.9943582805523336 iteration 229: weight=0.9945734134791384 iteration 230: weight=0.9947813469969744 iteration 231: weight=0.9949822854730818 iteration 232: weight=0.9951764287252586 iteration 233: weight=0.995363972082295 iteration 234: weight=0.9955451064449625 iteration 235: weight=0.9957200183474998 iteration 236: weight=0.9958888900195372 iteration 237: weight=0.9960518994484044 iteration 238: weight=0.9962092204417675 iteration 239: weight=0.9963610226905409 iteration 240: weight=0.9965074718320245 iteration 241: weight=0.9966487295132145 iteration 242: weight=0.9967849534542412 iteration 243: weight=0.9969162975118857 iteration 244: weight=0.99704291174313 iteration 245: weight=0.9971649424686971 iteration 246: weight=0.9972825323365392 iteration 247: weight=0.9973958203852304 iteration 248: weight=0.9975049421072285 iteration 249: weight=0.9976100295119631 iteration 250: weight=0.9977112111887169 iteration 251: weight=0.997808612369263 iteration 252: weight=0.9979023549902253 iteration 253: weight=0.997992557755128 iteration 254: weight=0.9980793361961048 iteration 255: weight=0.9981628027352366 iteration 256: weight=0.9982430667454899 iteration 257: weight=0.9983202346112277 iteration 258: weight=0.9983944097882684 iteration 259: weight=0.9984656928634669 iteration 260: weight=0.998534181613794 iteration 261: weight=0.9985999710648938 iteration 262: weight=0.9986631535490955 iteration 263: weight=0.9987238187628614 iteration 264: weight=0.9987820538236523 iteration 265: weight=0.9988379433261909 iteration 266: weight=0.9988915693981099 iteration 267: weight=0.9989430117549652 iteration 268: weight=0.9989923477546039 iteration 269: weight=0.9990396524508698 iteration 270: weight=0.9990849986466379 iteration 271: weight=0.9991284569461627 iteration 272: weight=0.9991700958067327 iteration 273: weight=0.9992099815896199 iteration 274: weight=0.9992481786103167 iteration 275: weight=0.9992847491880505 iteration 276: weight=0.9993197536945726 iteration 277: weight=0.9993532506022106 iteration 278: weight=0.9993852965311832 iteration 279: weight=0.999415946296171 iteration 280: weight=0.9994452529521384 iteration 281: weight=0.9994732678394059 iteration 282: weight=0.999500040627969 iteration 283: weight=0.9995256193610621 iteration 284: weight=0.9995500504979664 iteration 285: weight=0.9995733789560621 iteration 286: weight=0.9995956481521241 iteration 287: weight=0.9996169000428623 iteration 288: weight=0.9996371751647086 iteration 289: weight=0.9996565126728514 iteration 290: weight=0.9996749503795203 iteration 291: weight=0.9996925247915246 iteration 292: weight=0.9997092711470486 iteration 293: weight=0.9997252234517061 iteration 294: weight=0.9997404145138615 iteration 295: weight=0.9997548759792192 iteration 296: weight=0.9997686383646873 iteration 297: weight=0.9997817310915225 iteration 298: weight=0.9997941825177595 iteration 299: weight=0.9998060199699335 iteration 300: weight=0.9998172697740993 iteration 301: weight=0.9998279572861581 iteration 302: weight=0.9998381069214939 iteration 303: weight=0.9998477421839309 iteration 304: weight=0.9998568856940179 iteration 305: weight=0.9998655592166464 iteration 306: weight=0.9998737836880133 iteration 307: weight=0.9998815792419323 iteration 308: weight=0.9998889652355071 iteration 309: weight=0.9998959602741706 iteration 310: weight=0.9999025822361022 iteration 311: weight=0.9999088482960293 iteration 312: weight=0.9999147749484246 iteration 313: weight=0.999920378030106 iteration 314: weight=0.9999256727422497 iteration 315: weight=0.9999306736718255 iteration 316: weight=0.9999353948124632 iteration 317: weight=0.9999398495847598 iteration 318: weight=0.9999440508560381 iteration 319: weight=0.9999480109595638 iteration 320: weight=0.9999517417132329 iteration 321: weight=0.9999552544377384 iteration 322: weight=0.9999585599742249 iteration 323: weight=0.9999616687014424 iteration 324: weight=0.9999645905524075 iteration 325: weight=0.999967335030582 iteration 326: weight=0.9999699112255794 iteration 327: weight=0.9999723278284075 iteration 328: weight=0.9999745931462574 iteration 329: weight=0.9999767151168482 iteration 330: weight=0.9999787013223368 iteration 331: weight=0.9999805590028027 iteration 332: weight=0.9999822950693158 iteration 333: weight=0.9999839161165983 iteration 334: weight=0.9999854284352881 iteration 335: weight=0.9999868380238144 iteration 336: weight=0.9999881505998927 iteration 337: weight=0.9999893716116505 iteration 338: weight=0.9999905062483903 iteration 339: weight=0.9999915594509997 iteration 340: weight=0.9999925359220176 iteration 341: weight=0.9999934401353642 iteration 342: weight=0.9999942763457434 iteration 343: weight=0.9999950485977274 iteration 344: weight=0.9999957607345287 iteration 345: weight=0.999996416406471 iteration 346: weight=0.9999970190791647 iteration 347: weight=0.9999975720413955 iteration 348: weight=0.9999980784127344 iteration 349: weight=0.9999985411508757 iteration 350: weight=0.9999989630587119 iteration 351: weight=0.9999993467911512 iteration 352: weight=0.9999996948616862 iteration 353: weight=1.0000000096487203 iteration 354: weight=1.0000002934016596 iteration 355: weight=1.0000005482467753 iteration 356: weight=1.0000007761928456 iteration 357: weight=1.0000009791365825 iteration 358: weight=1.0000011588678495 iteration 359: weight=1.0000013170746784 iteration 360: weight=1.0000014553480892 iteration 361: weight=1.0000015751867204 iteration 362: weight=1.0000016780012766 iteration 363: weight=1.0000017651187962 iteration 364: weight=1.0000018377867486 iteration 365: weight=1.0000018971769635 iteration 366: weight=1.0000019443894 iteration 367: weight=1.0000019804557585 iteration 368: weight=1.0000020063429436 iteration 369: weight=1.0000020229563793 iteration 370: weight=1.0000020311431854 iteration 371: weight=1.0000020316952167 iteration 372: weight=1.0000020253519715 iteration 373: weight=1.0000020128033733 iteration 374: weight=1.0000019946924315 iteration 375: weight=1.0000019716177821 iteration 376: weight=1.0000019441361176 iteration 377: weight=1.0000019127645048 iteration 378: weight=1.000001877982599 iteration 379: weight=1.0000018402347561 iteration 380: weight=1.0000017999320474 iteration 381: weight=1.000001757454179 iteration 382: weight=1.0000017131513226 iteration 383: weight=1.0000016673458578 iteration 384: weight=1.000001620334032 iteration 385: weight=1.0000015723875386 iteration 386: weight=1.0000015237550193 iteration 387: weight=1.0000014746634915 iteration 388: weight=1.0000014253197047 iteration 389: weight=1.0000013759114286 iteration 390: weight=1.0000013266086762 iteration 391: weight=1.0000012775648637 iteration 392: weight=1.0000012289179103 iteration 393: weight=1.0000011807912805 iteration 394: weight=1.0000011332949712 iteration 395: weight=1.000001086526446 iteration 396: weight=1.000001040571519 iteration 397: weight=1.0000009955051905 iteration 398: weight=1.0000009513924366 iteration 399: weight=1.0000009082889536 iteration 400: weight=1.0000008662418622 iteration 401: weight=1.0000008252903696 iteration 402: weight=1.0000007854663946 iteration 403: weight=1.0000007467951555 iteration 404: weight=1.000000709295723 iteration 405: weight=1.0000006729815407 iteration 406: weight=1.0000006378609128 iteration 407: weight=1.0000006039374625 iteration 408: weight=1.0000005712105615 iteration 409: weight=1.0000005396757328 iteration 410: weight=1.0000005093250262 iteration 411: weight=1.0000004801473712 iteration 412: weight=1.0000004521289048 iteration 413: weight=1.0000004252532784 iteration 414: weight=1.0000003995019433 iteration 415: weight=1.000000374854416 iteration 416: weight=1.0000003512885252 iteration 417: weight=1.000000328780641 iteration 418: weight=1.0000003073058867 iteration 419: weight=1.0000002868383355 iteration 420: weight=1.0000002673511916 iteration 421: weight=1.000000248816957 iteration 422: weight=1.000000231207586 iteration 423: weight=1.0000002144946256 iteration 424: weight=1.0000001986493454 iteration 425: weight=1.0000001836428556 iteration 426: weight=1.0000001694462146 iteration 427: weight=1.0000001560305274 iteration 428: weight=1.0000001433670334 iteration 429: weight=1.0000001314271876 iteration 430: weight=1.0000001201827318 iteration 431: weight=1.0000001096057591 iteration 432: weight=1.000000099668772 iteration 433: weight=1.000000090344732 iteration 434: weight=1.000000081607105 iteration 435: weight=1.0000000734299008 iteration 436: weight=1.0000000657877053 iteration 437: weight=1.0000000586557103 iteration 438: weight=1.0000000520097374 iteration 439: weight=1.0000000458262583 iteration 440: weight=1.0000000400824103 iteration 441: weight=1.000000034756009 iteration 442: weight=1.0000000298255576 iteration 443: weight=1.0000000252702532 iteration 444: weight=1.0000000210699902 iteration 445: weight=1.0000000172053607 iteration 446: weight=1.0000000136576537 iteration 447: weight=1.0000000104088511 iteration 448: weight=1.000000007441623 iteration 449: weight=1.0000000047393205 iteration 450: weight=1.0000000022859659 iteration 451: weight=1.0000000000662446 iteration 452: weight=0.9999999980654928 iteration 453: weight=0.9999999962696857 iteration 454: weight=0.9999999946654243 iteration 455: weight=0.9999999932399216 iteration 456: weight=0.9999999919809877 iteration 457: weight=0.9999999908770149 iteration 458: weight=0.999999989916962 iteration 459: weight=0.9999999890903385 iteration 460: weight=0.9999999883871876 iteration 461: weight=0.9999999877980703 iteration 462: weight=0.9999999873140484 iteration 463: weight=0.9999999869266677 iteration 464: weight=0.9999999866279413 iteration 465: weight=0.9999999864103329 iteration 466: weight=0.9999999862667397 iteration 467: weight=0.9999999861904763 iteration 468: weight=0.9999999861752578 iteration 469: weight=0.9999999862151839 iteration 470: weight=0.9999999863047233 iteration 471: weight=0.9999999864386969 iteration 472: weight=0.9999999866122634 iteration 473: weight=0.9999999868209039 iteration 474: weight=0.9999999870604066 iteration 475: weight=0.9999999873268534 iteration 476: weight=0.9999999876166047 iteration 477: weight=0.9999999879262865 iteration 478: weight=0.9999999882527769 iteration 479: weight=0.999999988593193 iteration 480: weight=0.9999999889448784 iteration 481: weight=0.9999999893053915 iteration 482: weight=0.9999999896724932 iteration 483: weight=0.9999999900441359 iteration 484: weight=0.9999999904184526 iteration 485: weight=0.9999999907937464 iteration 486: weight=0.9999999911684802 iteration 487: weight=0.9999999915412672 iteration 488: weight=0.9999999919108616 iteration 489: weight=0.9999999922761498 iteration 490: weight=0.9999999926361411 iteration 491: weight=0.9999999929899606 iteration 492: weight=0.9999999933368405 iteration 493: weight=0.9999999936761135 iteration 494: weight=0.9999999940072047 iteration 495: weight=0.9999999943296259 iteration 496: weight=0.9999999946429682 iteration 497: weight=0.9999999949468965 iteration 498: weight=0.9999999952411437 iteration 499: weight=0.999999995525505 iteration 500: weight=0.9999999957998327 iteration 501: weight=0.9999999960640314 iteration 502: weight=0.9999999963180537 iteration 503: weight=0.999999996561895 iteration 504: weight=0.9999999967955902 iteration 505: weight=0.9999999970192094 iteration 506: weight=0.9999999972328546 iteration 507: weight=0.9999999974366559 iteration 508: weight=0.9999999976307687 iteration 509: weight=0.9999999978153705 iteration 510: weight=0.9999999979906583 iteration 511: weight=0.9999999981568457 iteration 512: weight=0.999999998314161 iteration 513: weight=0.9999999984628445 iteration 514: weight=0.9999999986031467 iteration 515: weight=0.9999999987353264 iteration 516: weight=0.9999999988596489 iteration 517: weight=0.9999999989763841 iteration 518: weight=0.9999999990858058 iteration 519: weight=0.9999999991881894 iteration 520: weight=0.9999999992838112 iteration 521: weight=0.9999999993729474 iteration 522: weight=0.9999999994558726 iteration 523: weight=0.9999999995328595 iteration 524: weight=0.9999999996041773 iteration 525: weight=0.9999999996700918 iteration 526: weight=0.9999999997308642 iteration 527: weight=0.9999999997867507 iteration 528: weight=0.999999999838002 iteration 529: weight=0.9999999998848629 iteration 530: weight=0.9999999999275718 iteration 531: weight=0.9999999999663607 iteration 532: weight=1.0000000000014544 iteration 533: weight=1.0000000000330709 iteration 534: weight=1.0000000000614209 iteration 535: weight=1.0000000000867075 iteration 536: weight=1.0000000001091267 iteration 537: weight=1.000000000128867 iteration 538: weight=1.0000000001461087 iteration 539: weight=1.0000000001610256 iteration 540: weight=1.0000000001737834 iteration 541: weight=1.0000000001845408 iteration 542: weight=1.0000000001934488 iteration 543: weight=1.0000000002006517 iteration 544: weight=1.0000000002062863 iteration 545: weight=1.0000000002104827 iteration 546: weight=1.0000000002133642 iteration 547: weight=1.0000000002150473 iteration 548: weight=1.0000000002156426 iteration 549: weight=1.0000000002152538 iteration 550: weight=1.0000000002139788 iteration 551: weight=1.0000000002119098 iteration 552: weight=1.0000000002091332 iteration 553: weight=1.0000000002057299 iteration 554: weight=1.0000000002017755 iteration 555: weight=1.0000000001973406 iteration 556: weight=1.000000000192491 iteration 557: weight=1.0000000001872875 iteration 558: weight=1.0000000001817868 iteration 559: weight=1.0000000001760414 iteration 560: weight=1.0000000001700993 iteration 561: weight=1.000000000164005 iteration 562: weight=1.0000000001577996 iteration 563: weight=1.00000000015152 iteration 564: weight=1.0000000001452 iteration 565: weight=1.000000000138871 iteration 566: weight=1.0000000001325604 iteration 567: weight=1.0000000001262936 iteration 568: weight=1.000000000120093 iteration 569: weight=1.0000000001139786 iteration 570: weight=1.000000000107968 iteration 571: weight=1.0000000001020772 iteration 572: weight=1.0000000000963196 iteration 573: weight=1.0000000000907068 iteration 574: weight=1.000000000085249 iteration 575: weight=1.0000000000799543 iteration 576: weight=1.0000000000748297 iteration 577: weight=1.0000000000698805 iteration 578: weight=1.0000000000651112 iteration 579: weight=1.0000000000605247 iteration 580: weight=1.0000000000561229 iteration 581: weight=1.000000000051907 iteration 582: weight=1.000000000047877 iteration 583: weight=1.0000000000440323 iteration 584: weight=1.0000000000403715 iteration 585: weight=1.0000000000368925 iteration 586: weight=1.0000000000335927 iteration 587: weight=1.000000000030469 iteration 588: weight=1.0000000000275175 iteration 589: weight=1.0000000000247347 iteration 590: weight=1.000000000022116 iteration 591: weight=1.0000000000196572 iteration 592: weight=1.000000000017353 iteration 593: weight=1.0000000000151987 iteration 594: weight=1.000000000013189 iteration 595: weight=1.0000000000113185 iteration 596: weight=1.000000000009582 iteration 597: weight=1.0000000000079738 iteration 598: weight=1.0000000000064888 iteration 599: weight=1.0000000000051212 iteration 600: weight=1.0000000000038658 iteration 601: weight=1.0000000000027172 iteration 602: weight=1.0000000000016698 iteration 603: weight=1.0000000000007185 iteration 604: weight=0.9999999999998581 iteration 605: weight=0.9999999999990835 iteration 606: weight=0.9999999999983898 iteration 607: weight=0.9999999999977722 iteration 608: weight=0.999999999997226 iteration 609: weight=0.9999999999967466 iteration 610: weight=0.9999999999963296 iteration 611: weight=0.9999999999959708 iteration 612: weight=0.999999999995666 iteration 613: weight=0.9999999999954114 iteration 614: weight=0.9999999999952033 iteration 615: weight=0.999999999995038 iteration 616: weight=0.999999999994912 iteration 617: weight=0.999999999994822 iteration 618: weight=0.999999999994765 iteration 619: weight=0.9999999999947379 iteration 620: weight=0.999999999994738 iteration 621: weight=0.9999999999947625 iteration 622: weight=0.999999999994809 iteration 623: weight=0.9999999999948752 iteration 624: weight=0.9999999999949587 iteration 625: weight=0.9999999999950575 iteration 626: weight=0.9999999999951696 iteration 627: weight=0.9999999999952933 iteration 628: weight=0.9999999999954268 iteration 629: weight=0.9999999999955685 iteration 630: weight=0.9999999999957171 iteration 631: weight=0.9999999999958711 iteration 632: weight=0.9999999999960292 iteration 633: weight=0.9999999999961904 iteration 634: weight=0.9999999999963536 iteration 635: weight=0.9999999999965179 iteration 636: weight=0.9999999999966823 iteration 637: weight=0.9999999999968462 iteration 638: weight=0.9999999999970088 iteration 639: weight=0.9999999999971695 iteration 640: weight=0.9999999999973277 iteration 641: weight=0.9999999999974829 iteration 642: weight=0.9999999999976348 iteration 643: weight=0.9999999999977829 iteration 644: weight=0.9999999999979269 iteration 645: weight=0.9999999999980665 iteration 646: weight=0.9999999999982015 iteration 647: weight=0.9999999999983318 iteration 648: weight=0.9999999999984571 iteration 649: weight=0.9999999999985775 iteration 650: weight=0.9999999999986927 iteration 651: weight=0.9999999999988028 iteration 652: weight=0.9999999999989078 iteration 653: weight=0.9999999999990076 iteration 654: weight=0.9999999999991023 iteration 655: weight=0.999999999999192 iteration 656: weight=0.9999999999992767 iteration 657: weight=0.9999999999993565 iteration 658: weight=0.9999999999994316 iteration 659: weight=0.9999999999995018 iteration 660: weight=0.9999999999995676 iteration 661: weight=0.999999999999629 iteration 662: weight=0.999999999999686 iteration 663: weight=0.999999999999739 iteration 664: weight=0.999999999999788 iteration 665: weight=0.9999999999998331 iteration 666: weight=0.9999999999998747 iteration 667: weight=0.9999999999999126 iteration 668: weight=0.9999999999999473 iteration 669: weight=0.9999999999999788 iteration 670: weight=1.0000000000000073 iteration 671: weight=1.0000000000000329 iteration 672: weight=1.0000000000000557 iteration 673: weight=1.0000000000000762 iteration 674: weight=1.0000000000000941 iteration 675: weight=1.00000000000011 iteration 676: weight=1.0000000000001235 iteration 677: weight=1.0000000000001352 iteration 678: weight=1.000000000000145 iteration 679: weight=1.0000000000001532 iteration 680: weight=1.0000000000001599 iteration 681: weight=1.000000000000165 iteration 682: weight=1.0000000000001688 iteration 683: weight=1.0000000000001714 iteration 684: weight=1.000000000000173 iteration 685: weight=1.0000000000001734 iteration 686: weight=1.000000000000173 iteration 687: weight=1.0000000000001716 iteration 688: weight=1.0000000000001696 iteration 689: weight=1.000000000000167 iteration 690: weight=1.0000000000001639 iteration 691: weight=1.00000000000016 iteration 692: weight=1.0000000000001559 iteration 693: weight=1.0000000000001514 iteration 694: weight=1.0000000000001465 iteration 695: weight=1.0000000000001414 iteration 696: weight=1.0000000000001361 iteration 697: weight=1.0000000000001306 iteration 698: weight=1.000000000000125 iteration 699: weight=1.0000000000001195 iteration 700: weight=1.0000000000001137 iteration 701: weight=1.000000000000108 iteration 702: weight=1.0000000000001021 iteration 703: weight=1.0000000000000966 iteration 704: weight=1.000000000000091 iteration 705: weight=1.0000000000000855 iteration 706: weight=1.0000000000000802 iteration 707: weight=1.0000000000000748 iteration 708: weight=1.0000000000000697 iteration 709: weight=1.0000000000000648 iteration 710: weight=1.00000000000006 iteration 711: weight=1.0000000000000553 iteration 712: weight=1.0000000000000508 iteration 713: weight=1.0000000000000466 iteration 714: weight=1.0000000000000426 iteration 715: weight=1.0000000000000386 iteration 716: weight=1.0000000000000349 iteration 717: weight=1.0000000000000313 iteration 718: weight=1.000000000000028 iteration 719: weight=1.0000000000000249 iteration 720: weight=1.000000000000022 iteration 721: weight=1.000000000000019 iteration 722: weight=1.0000000000000164 iteration 723: weight=1.000000000000014 iteration 724: weight=1.0000000000000118 iteration 725: weight=1.0000000000000098 iteration 726: weight=1.0000000000000078 iteration 727: weight=1.000000000000006 iteration 728: weight=1.0000000000000044 iteration 729: weight=1.0000000000000029 iteration 730: weight=1.0000000000000016 iteration 731: weight=1.0000000000000002 iteration 732: weight=0.9999999999999991 iteration 733: weight=0.9999999999999981 iteration 734: weight=0.9999999999999972 iteration 735: weight=0.9999999999999964 iteration 736: weight=0.9999999999999958 iteration 737: weight=0.9999999999999952 iteration 738: weight=0.9999999999999947 iteration 739: weight=0.9999999999999942 iteration 740: weight=0.9999999999999939 iteration 741: weight=0.9999999999999936 iteration 742: weight=0.9999999999999933 iteration 743: weight=0.9999999999999931 iteration 744: weight=0.999999999999993 iteration 745: weight=0.9999999999999929 converged after 746 iterations /opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in sqrt .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/15/Adam_optimizer.html",
            "relUrl": "/2020/11/15/Adam_optimizer.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Text Generation - LSTM",
            "content": ". Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning . try: %tensorflow_version 2.x COLAB = True print(&quot;Note: using Google CoLab&quot;) except: print(&quot;Note: not using Google CoLab&quot;) COLAB = False . Note: using Google CoLab . from tensorflow.keras.callbacks import LambdaCallback from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.layers import LSTM from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.utils import get_file import numpy as np import random import sys import io import requests import re . r = requests.get(&quot;https://www.gutenberg.org/cache/epub/600/pg600.txt&quot;) raw_text = r.text print(raw_text[0:1000]) . ﻿Project Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: Notes from the Underground Author: Feodor Dostoevsky Posting Date: September 13, 2008 [EBook #600] Release Date: July, 1996 Language: English *** START OF THIS PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Produced by Judith Boss. HTML version by Al Haines. Notes from the Underground FYODOR DOSTOYEVSKY PART I Underground* *The author of the diary and the diary itself are, of course, imaginary. Nevertheless it is clear that such persons as the writer of these notes not only may, but positively must, exist in our society, when we consider the circumstances i . processed_text = raw_text.lower() processed_text = re.sub(r&#39;[^ x00- x7f]&#39;,r&#39;&#39;, processed_text) . print(&#39;corpus length:&#39;, len(processed_text)) chars = sorted(list(set(processed_text))) print(&#39;total chars:&#39;, len(chars)) char_indices = dict((c, i) for i, c in enumerate(chars)) indices_char = dict((i, c) for i, c in enumerate(chars)) . corpus length: 265582 total chars: 58 . # cut the text in semi-redundant sequences of maxlen characters maxlen = 40 step = 3 sentences = [] next_chars = [] for i in range(0, len(processed_text) - maxlen, step): sentences.append(processed_text[i: i + maxlen]) next_chars.append(processed_text[i + maxlen]) print(&#39;nb sequences:&#39;, len(sentences)) . nb sequences: 88514 . sentences . [&#34;project gutenberg&#39;s notes from the under&#34;, &#34;ject gutenberg&#39;s notes from the undergro&#34;, &#34;t gutenberg&#39;s notes from the underground&#34;, &#34;utenberg&#39;s notes from the underground, b&#34;, &#34;nberg&#39;s notes from the underground, by f&#34;, &#34;rg&#39;s notes from the underground, by feod&#34;, &#39;s notes from the underground, by feodor &#39;, &#39;otes from the underground, by feodor dos&#39;, &#39;s from the underground, by feodor dostoe&#39;, &#39;rom the underground, by feodor dostoevsk&#39;, &#39; the underground, by feodor dostoevsky r n&#39;, &#39;e underground, by feodor dostoevsky r n r nt&#39;, &#39;nderground, by feodor dostoevsky r n r nthis&#39;, &#39;rground, by feodor dostoevsky r n r nthis eb&#39;, &#39;ound, by feodor dostoevsky r n r nthis ebook&#39;, &#39;d, by feodor dostoevsky r n r nthis ebook is&#39;, &#39;by feodor dostoevsky r n r nthis ebook is fo&#39;, &#39;feodor dostoevsky r n r nthis ebook is for t&#39;, &#39;dor dostoevsky r n r nthis ebook is for the &#39;, &#39; dostoevsky r n r nthis ebook is for the use&#39;, &#39;stoevsky r n r nthis ebook is for the use of&#39;, &#39;evsky r n r nthis ebook is for the use of an&#39;, &#39;ky r n r nthis ebook is for the use of anyon&#39;, &#39; n r nthis ebook is for the use of anyone a&#39;, &#39;this ebook is for the use of anyone anyw&#39;, &#39;s ebook is for the use of anyone anywher&#39;, &#39;book is for the use of anyone anywhere a&#39;, &#39;k is for the use of anyone anywhere at n&#39;, &#39;s for the use of anyone anywhere at no c&#39;, &#39;or the use of anyone anywhere at no cost&#39;, &#39;the use of anyone anywhere at no cost an&#39;, &#39; use of anyone anywhere at no cost and w&#39;, &#39;e of anyone anywhere at no cost and with&#39;, &#39;f anyone anywhere at no cost and with r na&#39;, &#39;nyone anywhere at no cost and with r nalmo&#39;, &#39;ne anywhere at no cost and with r nalmost &#39;, &#39;anywhere at no cost and with r nalmost no &#39;, &#39;where at no cost and with r nalmost no res&#39;, &#39;re at no cost and with r nalmost no restri&#39;, &#39;at no cost and with r nalmost no restricti&#39;, &#39;no cost and with r nalmost no restrictions&#39;, &#39;cost and with r nalmost no restrictions wh&#39;, &#39;t and with r nalmost no restrictions whats&#39;, &#39;nd with r nalmost no restrictions whatsoev&#39;, &#39;with r nalmost no restrictions whatsoever.&#39;, &#39;h r nalmost no restrictions whatsoever. y&#39;, &#39;almost no restrictions whatsoever. you &#39;, &#39;ost no restrictions whatsoever. you may&#39;, &#39; no restrictions whatsoever. you may co&#39;, &#39; restrictions whatsoever. you may copy &#39;, &#39;strictions whatsoever. you may copy it,&#39;, &#39;ictions whatsoever. you may copy it, gi&#39;, &#39;ions whatsoever. you may copy it, give &#39;, &#39;s whatsoever. you may copy it, give it &#39;, &#39;hatsoever. you may copy it, give it awa&#39;, &#39;soever. you may copy it, give it away o&#39;, &#39;ver. you may copy it, give it away or r n&#39;, &#39;. you may copy it, give it away or r nre-&#39;, &#39;you may copy it, give it away or r nre-use&#39;, &#39; may copy it, give it away or r nre-use it&#39;, &#39;y copy it, give it away or r nre-use it un&#39;, &#39;opy it, give it away or r nre-use it under&#39;, &#39; it, give it away or r nre-use it under th&#39;, &#39;, give it away or r nre-use it under the t&#39;, &#39;ive it away or r nre-use it under the term&#39;, &#39; it away or r nre-use it under the terms o&#39;, &#39; away or r nre-use it under the terms of t&#39;, &#39;ay or r nre-use it under the terms of the &#39;, &#39;or r nre-use it under the terms of the pro&#39;, &#39; nre-use it under the terms of the projec&#39;, &#39;-use it under the terms of the project g&#39;, &#39;e it under the terms of the project gute&#39;, &#39;t under the terms of the project gutenbe&#39;, &#39;nder the terms of the project gutenberg &#39;, &#39;r the terms of the project gutenberg lic&#39;, &#39;he terms of the project gutenberg licens&#39;, &#39;terms of the project gutenberg license i&#39;, &#39;ms of the project gutenberg license incl&#39;, &#39;of the project gutenberg license include&#39;, &#39;the project gutenberg license included r n&#39;, &#39; project gutenberg license included r nwit&#39;, &#39;oject gutenberg license included r nwith t&#39;, &#39;ct gutenberg license included r nwith this&#39;, &#39;gutenberg license included r nwith this eb&#39;, &#39;enberg license included r nwith this ebook&#39;, &#39;erg license included r nwith this ebook or&#39;, &#39; license included r nwith this ebook or on&#39;, &#39;cense included r nwith this ebook or onlin&#39;, &#39;se included r nwith this ebook or online a&#39;, &#39;included r nwith this ebook or online at w&#39;, &#39;luded r nwith this ebook or online at www.&#39;, &#39;ed r nwith this ebook or online at www.gut&#39;, &#39; nwith this ebook or online at www.gutenb&#39;, &#39;th this ebook or online at www.gutenberg&#39;, &#39;this ebook or online at www.gutenberg.ne&#39;, &#39;s ebook or online at www.gutenberg.net r n&#39;, &#39;book or online at www.gutenberg.net r n r n r&#39;, &#39;k or online at www.gutenberg.net r n r n r nti&#39;, &#39;r online at www.gutenberg.net r n r n r ntitle&#39;, &#39;nline at www.gutenberg.net r n r n r ntitle: n&#39;, &#39;ne at www.gutenberg.net r n r n r ntitle: note&#39;, &#39;at www.gutenberg.net r n r n r ntitle: notes f&#39;, &#39;www.gutenberg.net r n r n r ntitle: notes from&#39;, &#39;.gutenberg.net r n r n r ntitle: notes from th&#39;, &#39;tenberg.net r n r n r ntitle: notes from the u&#39;, &#39;berg.net r n r n r ntitle: notes from the unde&#39;, &#39;g.net r n r n r ntitle: notes from the undergr&#39;, &#39;et r n r n r ntitle: notes from the undergroun&#39;, &#39; n r n r ntitle: notes from the underground r n&#39;, &#39; r ntitle: notes from the underground r n r na&#39;, &#39;itle: notes from the underground r n r nauth&#39;, &#39;e: notes from the underground r n r nauthor:&#39;, &#39;notes from the underground r n r nauthor: fe&#39;, &#39;es from the underground r n r nauthor: feodo&#39;, &#39;from the underground r n r nauthor: feodor d&#39;, &#39;m the underground r n r nauthor: feodor dost&#39;, &#39;he underground r n r nauthor: feodor dostoev&#39;, &#39;underground r n r nauthor: feodor dostoevsky&#39;, &#39;erground r n r nauthor: feodor dostoevsky r n r&#39;, &#39;round r n r nauthor: feodor dostoevsky r n r npo&#39;, &#39;nd r n r nauthor: feodor dostoevsky r n r nposti&#39;, &#39; n r nauthor: feodor dostoevsky r n r nposting &#39;, &#39;author: feodor dostoevsky r n r nposting dat&#39;, &#39;hor: feodor dostoevsky r n r nposting date: &#39;, &#39;: feodor dostoevsky r n r nposting date: sep&#39;, &#39;eodor dostoevsky r n r nposting date: septem&#39;, &#39;or dostoevsky r n r nposting date: september&#39;, &#39;dostoevsky r n r nposting date: september 13&#39;, &#39;toevsky r n r nposting date: september 13, 2&#39;, &#39;vsky r n r nposting date: september 13, 2008&#39;, &#39;y r n r nposting date: september 13, 2008 [e&#39;, &#39; r nposting date: september 13, 2008 [eboo&#39;, &#39;osting date: september 13, 2008 [ebook #&#39;, &#39;ing date: september 13, 2008 [ebook #600&#39;, &#39; date: september 13, 2008 [ebook #600] r n&#39;, &#39;te: september 13, 2008 [ebook #600] r nrel&#39;, &#39; september 13, 2008 [ebook #600] r nreleas&#39;, &#39;ptember 13, 2008 [ebook #600] r nrelease d&#39;, &#39;mber 13, 2008 [ebook #600] r nrelease date&#39;, &#39;r 13, 2008 [ebook #600] r nrelease date: j&#39;, &#39;3, 2008 [ebook #600] r nrelease date: july&#39;, &#39;2008 [ebook #600] r nrelease date: july, 1&#39;, &#39;8 [ebook #600] r nrelease date: july, 1996&#39;, &#39;ebook #600] r nrelease date: july, 1996 r n r&#39;, &#39;ok #600] r nrelease date: july, 1996 r n r nla&#39;, &#39;#600] r nrelease date: july, 1996 r n r nlangu&#39;, &#39;0] r nrelease date: july, 1996 r n r nlanguage&#39;, &#39; nrelease date: july, 1996 r n r nlanguage: e&#39;, &#39;lease date: july, 1996 r n r nlanguage: engl&#39;, &#39;se date: july, 1996 r n r nlanguage: english&#39;, &#39;date: july, 1996 r n r nlanguage: english r n r&#39;, &#39;e: july, 1996 r n r nlanguage: english r n r n r n&#39;, &#39;july, 1996 r n r nlanguage: english r n r n r n***&#39;, &#39;y, 1996 r n r nlanguage: english r n r n r n*** st&#39;, &#39;1996 r n r nlanguage: english r n r n r n*** start&#39;, &#39;6 r n r nlanguage: english r n r n r n*** start of&#39;, &#39; r nlanguage: english r n r n r n*** start of th&#39;, &#39;anguage: english r n r n r n*** start of this &#39;, &#39;uage: english r n r n r n*** start of this pro&#39;, &#39;e: english r n r n r n*** start of this projec&#39;, &#39;english r n r n r n*** start of this project g&#39;, &#39;lish r n r n r n*** start of this project gute&#39;, &#39;h r n r n r n*** start of this project gutenbe&#39;, &#39; r n r n*** start of this project gutenberg &#39;, &#39; n*** start of this project gutenberg ebo&#39;, &#39;* start of this project gutenberg ebook &#39;, &#39;tart of this project gutenberg ebook not&#39;, &#39;t of this project gutenberg ebook notes &#39;, &#39;f this project gutenberg ebook notes fro&#39;, &#39;his project gutenberg ebook notes from t&#39;, &#39; project gutenberg ebook notes from the &#39;, &#39;oject gutenberg ebook notes from the und&#39;, &#39;ct gutenberg ebook notes from the underg&#39;, &#39;gutenberg ebook notes from the undergrou&#39;, &#39;enberg ebook notes from the underground &#39;, &#39;erg ebook notes from the underground ***&#39;, &#39; ebook notes from the underground *** r n r&#39;, &#39;ook notes from the underground *** r n r n r n&#39;, &#39; notes from the underground *** r n r n r n r n r&#39;, &#39;tes from the underground *** r n r n r n r n r npr&#39;, &#39; from the underground *** r n r n r n r n r nprodu&#39;, &#39;om the underground *** r n r n r n r n r nproduced&#39;, &#39;the underground *** r n r n r n r n r nproduced by&#39;, &#39; underground *** r n r n r n r n r nproduced by ju&#39;, &#39;derground *** r n r n r n r n r nproduced by judit&#39;, &#39;ground *** r n r n r n r n r nproduced by judith b&#39;, &#39;und *** r n r n r n r n r nproduced by judith boss&#39;, &#39; *** r n r n r n r n r nproduced by judith boss. &#39;, &#39;* r n r n r n r n r nproduced by judith boss. htm&#39;, &#39; r n r n r n r nproduced by judith boss. html v&#39;, &#39; n r n r nproduced by judith boss. html vers&#39;, &#39; r nproduced by judith boss. html version&#39;, &#39;roduced by judith boss. html version by&#39;, &#39;uced by judith boss. html version by al&#39;, &#39;d by judith boss. html version by al ha&#39;, &#39;y judith boss. html version by al haine&#39;, &#39;udith boss. html version by al haines. r&#39;, &#39;th boss. html version by al haines. r n r n&#39;, &#39;boss. html version by al haines. r n r n r n r&#39;, &#39;s. html version by al haines. r n r n r n r n r n&#39;, &#39; html version by al haines. r n r n r n r n r n r n r&#39;, &#39;ml version by al haines. r n r n r n r n r n r n r n r n&#39;, &#39;version by al haines. r n r n r n r n r n r n r n r n r nn&#39;, &#39;sion by al haines. r n r n r n r n r n r n r n r n r nnote&#39;, &#39;n by al haines. r n r n r n r n r n r n r n r n r nnotes f&#39;, &#39;y al haines. r n r n r n r n r n r n r n r n r nnotes from&#39;, &#39;l haines. r n r n r n r n r n r n r n r n r nnotes from th&#39;, &#39;aines. r n r n r n r n r n r n r n r n r nnotes from the u&#39;, &#39;es. r n r n r n r n r n r n r n r n r nnotes from the unde&#39;, &#39; r n r n r n r n r n r n r n r n r nnotes from the undergr&#39;, &#39; n r n r n r n r n r n r n r nnotes from the undergroun&#39;, &#39; r n r n r n r n r n r nnotes from the underground r n&#39;, &#39; n r n r n r n r nnotes from the underground r n r nf&#39;, &#39; r n r n r nnotes from the underground r n r nfyod&#39;, &#39; n r nnotes from the underground r n r nfyodor &#39;, &#39;notes from the underground r n r nfyodor dos&#39;, &#39;es from the underground r n r nfyodor dostoy&#39;, &#39;from the underground r n r nfyodor dostoyevs&#39;, &#39;m the underground r n r nfyodor dostoyevsky r&#39;, &#39;he underground r n r nfyodor dostoyevsky r n r n&#39;, &#39;underground r n r nfyodor dostoyevsky r n r n r n r&#39;, &#39;erground r n r nfyodor dostoyevsky r n r n r n r n r n&#39;, &#39;round r n r nfyodor dostoyevsky r n r n r n r n r n r np&#39;, &#39;nd r n r nfyodor dostoyevsky r n r n r n r n r n r npart&#39;, &#39; n r nfyodor dostoyevsky r n r n r n r n r n r npart i r&#39;, &#39;fyodor dostoyevsky r n r n r n r n r n r npart i r n r n&#39;, &#39;dor dostoyevsky r n r n r n r n r n r npart i r n r nund&#39;, &#39; dostoyevsky r n r n r n r n r n r npart i r n r nunderg&#39;, &#39;stoyevsky r n r n r n r n r n r npart i r n r nundergrou&#39;, &#39;yevsky r n r n r n r n r n r npart i r n r nunderground*&#39;, &#39;sky r n r n r n r n r n r npart i r n r nunderground* r n r&#39;, &#39; r n r n r n r n r n r npart i r n r nunderground* r n r n &#39;, &#39; n r n r n r n r npart i r n r nunderground* r n r n &#39;, &#39; r n r n r npart i r n r nunderground* r n r n *th&#39;, &#39; n r npart i r n r nunderground* r n r n *the a&#39;, &#39;part i r n r nunderground* r n r n *the auth&#39;, &#39;t i r n r nunderground* r n r n *the author &#39;, &#39; r n r nunderground* r n r n *the author of &#39;, &#39; nunderground* r n r n *the author of the&#39;, &#39;derground* r n r n *the author of the di&#39;, &#39;ground* r n r n *the author of the diary&#39;, &#39;und* r n r n *the author of the diary an&#39;, &#39;* r n r n *the author of the diary and t&#39;, &#39; r n *the author of the diary and the &#39;, &#39; *the author of the diary and the dia&#39;, &#39; *the author of the diary and the diary &#39;, &#39;he author of the diary and the diary its&#39;, &#39;author of the diary and the diary itself&#39;, &#39;hor of the diary and the diary itself r n &#39;, &#39; of the diary and the diary itself r n &#39;, &#39; the diary and the diary itself r n ar&#39;, &#39;e diary and the diary itself r n are, &#39;, &#39;iary and the diary itself r n are, of &#39;, &#39;y and the diary itself r n are, of cou&#39;, &#39;nd the diary itself r n are, of course&#39;, &#39;the diary itself r n are, of course, i&#39;, &#39; diary itself r n are, of course, imag&#39;, &#39;ary itself r n are, of course, imagina&#39;, &#39; itself r n are, of course, imaginary.&#39;, &#39;self r n are, of course, imaginary. n&#39;, &#39;f r n are, of course, imaginary. neve&#39;, &#39; are, of course, imaginary. neverth&#39;, &#39; are, of course, imaginary. neverthele&#39;, &#39;re, of course, imaginary. nevertheless &#39;, &#39; of course, imaginary. nevertheless it &#39;, &#39; course, imaginary. nevertheless it is &#39;, &#39;urse, imaginary. nevertheless it is cle&#39;, &#39;e, imaginary. nevertheless it is clear r&#39;, &#39;imaginary. nevertheless it is clear r n &#39;, &#39;ginary. nevertheless it is clear r n &#39;, &#39;ary. nevertheless it is clear r n tha&#39;, &#39;. nevertheless it is clear r n that s&#39;, &#39;nevertheless it is clear r n that such&#39;, &#39;ertheless it is clear r n that such pe&#39;, &#39;heless it is clear r n that such perso&#39;, &#39;ess it is clear r n that such persons &#39;, &#39; it is clear r n that such persons as &#39;, &#39; is clear r n that such persons as the&#39;, &#39; clear r n that such persons as the wr&#39;, &#39;ear r n that such persons as the write&#39;, &#39; r n that such persons as the writer o&#39;, &#39; that such persons as the writer of t&#39;, &#39; that such persons as the writer of thes&#39;, &#39;at such persons as the writer of these n&#39;, &#39;such persons as the writer of these note&#39;, &#39;h persons as the writer of these notes r n&#39;, &#39;ersons as the writer of these notes r n &#39;, &#39;ons as the writer of these notes r n n&#39;, &#39; as the writer of these notes r n not &#39;, &#39; the writer of these notes r n not onl&#39;, &#39;e writer of these notes r n not only m&#39;, &#39;riter of these notes r n not only may,&#39;, &#39;er of these notes r n not only may, bu&#39;, &#39;of these notes r n not only may, but p&#39;, &#39;these notes r n not only may, but posi&#39;, &#39;se notes r n not only may, but positiv&#39;, &#39;notes r n not only may, but positively&#39;, &#39;es r n not only may, but positively mu&#39;, &#39; n not only may, but positively must,&#39;, &#39; not only may, but positively must, ex&#39;, &#39;not only may, but positively must, exist&#39;, &#39; only may, but positively must, exist in&#39;, &#39;ly may, but positively must, exist in ou&#39;, &#39;may, but positively must, exist in our r n&#39;, &#39;, but positively must, exist in our r n &#39;, &#39;ut positively must, exist in our r n s&#39;, &#39;positively must, exist in our r n soci&#39;, &#39;itively must, exist in our r n society&#39;, &#39;vely must, exist in our r n society, w&#39;, &#39;y must, exist in our r n society, when&#39;, &#39;ust, exist in our r n society, when we&#39;, &#39;, exist in our r n society, when we co&#39;, &#39;xist in our r n society, when we consi&#39;, &#39;t in our r n society, when we consider&#39;, &#39;n our r n society, when we consider th&#39;, &#39;ur r n society, when we consider the c&#39;, &#39; n society, when we consider the circ&#39;, &#39; society, when we consider the circums&#39;, &#39;society, when we consider the circumstan&#39;, &#39;iety, when we consider the circumstances&#39;, &#39;y, when we consider the circumstances in&#39;, &#39;when we consider the circumstances in r n &#39;, &#39;n we consider the circumstances in r n &#39;, &#39;e consider the circumstances in r n th&#39;, &#39;onsider the circumstances in r n the m&#39;, &#39;ider the circumstances in r n the mids&#39;, &#39;r the circumstances in r n the midst o&#39;, &#39;he circumstances in r n the midst of w&#39;, &#39;circumstances in r n the midst of whic&#39;, &#39;cumstances in r n the midst of which o&#39;, &#39;stances in r n the midst of which our &#39;, &#39;nces in r n the midst of which our soc&#39;, &#39;s in r n the midst of which our societ&#39;, &#39;n r n the midst of which our society i&#39;, &#39; the midst of which our society is f&#39;, &#39; the midst of which our society is form&#39;, &#39;he midst of which our society is formed.&#39;, &#39;midst of which our society is formed. i&#39;, &#39;st of which our society is formed. i ha&#39;, &#39;of which our society is formed. i have r&#39;, &#39;which our society is formed. i have r n &#39;, &#39;ch our society is formed. i have r n &#39;, &#39;our society is formed. i have r n tri&#39;, &#39; society is formed. i have r n tried &#39;, &#39;ciety is formed. i have r n tried to &#39;, &#39;ty is formed. i have r n tried to exp&#39;, &#39;is formed. i have r n tried to expose&#39;, &#39;formed. i have r n tried to expose to&#39;, &#39;med. i have r n tried to expose to th&#39;, &#39;. i have r n tried to expose to the v&#39;, &#39;i have r n tried to expose to the view&#39;, &#39;ave r n tried to expose to the view of&#39;, &#39; r n tried to expose to the view of th&#39;, &#39; tried to expose to the view of the p&#39;, &#39; tried to expose to the view of the publ&#39;, &#39;ied to expose to the view of the public &#39;, &#39; to expose to the view of the public mor&#39;, &#39; expose to the view of the public more r n&#39;, &#39;pose to the view of the public more r n &#39;, &#39;e to the view of the public more r n d&#39;, &#39;o the view of the public more r n dist&#39;, &#39;he view of the public more r n distinc&#39;, &#39;view of the public more r n distinctly&#39;, &#39;w of the public more r n distinctly th&#39;, &#39;f the public more r n distinctly than &#39;, &#39;he public more r n distinctly than is &#39;, &#39;public more r n distinctly than is com&#39;, &#39;lic more r n distinctly than is common&#39;, &#39; more r n distinctly than is commonly &#39;, &#39;re r n distinctly than is commonly don&#39;, &#39; n distinctly than is commonly done, &#39;, &#39; distinctly than is commonly done, one&#39;, &#39;distinctly than is commonly done, one of&#39;, &#39;tinctly than is commonly done, one of th&#39;, &#39;ctly than is commonly done, one of the r n&#39;, &#39;y than is commonly done, one of the r n &#39;, &#39;han is commonly done, one of the r n c&#39;, &#39; is commonly done, one of the r n char&#39;, &#39; commonly done, one of the r n charact&#39;, &#39;mmonly done, one of the r n characters&#39;, &#39;nly done, one of the r n characters of&#39;, &#39; done, one of the r n characters of th&#39;, &#39;ne, one of the r n characters of the r&#39;, &#39; one of the r n characters of the rece&#39;, &#39;e of the r n characters of the recent &#39;, &#39;f the r n characters of the recent pas&#39;, &#39;he r n characters of the recent past. &#39;, &#39; n characters of the recent past. he&#39;, &#39; characters of the recent past. he is&#39;, &#39;characters of the recent past. he is on&#39;, &#39;racters of the recent past. he is one o&#39;, &#39;ters of the recent past. he is one of t&#39;, &#39;s of the recent past. he is one of the r&#39;, &#39;f the recent past. he is one of the r n &#39;, &#39;he recent past. he is one of the r n &#39;, &#39;recent past. he is one of the r n rep&#39;, &#39;ent past. he is one of the r n repres&#39;, &#39; past. he is one of the r n represent&#39;, &#39;st. he is one of the r n representati&#39;, &#39; he is one of the r n representatives&#39;, &#39;e is one of the r n representatives of&#39;, &#39;s one of the r n representatives of a &#39;, &#39;ne of the r n representatives of a gen&#39;, &#39;of the r n representatives of a genera&#39;, &#39;the r n representatives of a generatio&#39;, &#39; r n representatives of a generation s&#39;, &#39; representatives of a generation stil&#39;, &#39; representatives of a generation still l&#39;, &#39;presentatives of a generation still livi&#39;, &#39;sentatives of a generation still living.&#39;, &#39;tatives of a generation still living. i&#39;, &#39;ives of a generation still living. in t&#39;, &#39;s of a generation still living. in this&#39;, &#39;f a generation still living. in this r n &#39;, &#39; generation still living. in this r n &#39;, &#39;neration still living. in this r n fr&#39;, &#39;ation still living. in this r n fragm&#39;, &#39;on still living. in this r n fragment&#39;, &#39;still living. in this r n fragment, e&#39;, &#39;ll living. in this r n fragment, enti&#39;, &#39;living. in this r n fragment, entitle&#39;, &#39;ing. in this r n fragment, entitled &#34;&#39;, &#39;. in this r n fragment, entitled &#34;und&#39;, &#39;in this r n fragment, entitled &#34;underg&#39;, &#39;this r n fragment, entitled &#34;undergrou&#39;, &#39;s r n fragment, entitled &#34;underground,&#39;, &#39; fragment, entitled &#34;underground,&#34; t&#39;, &#39; fragment, entitled &#34;underground,&#34; this&#39;, &#39;ragment, entitled &#34;underground,&#34; this pe&#39;, &#39;ment, entitled &#34;underground,&#34; this perso&#39;, &#39;t, entitled &#34;underground,&#34; this person r n&#39;, &#39;entitled &#34;underground,&#34; this person r n &#39;, &#39;itled &#34;underground,&#34; this person r n i&#39;, &#39;ed &#34;underground,&#34; this person r n intr&#39;, &#39;&#34;underground,&#34; this person r n introdu&#39;, &#39;derground,&#34; this person r n introduces&#39;, &#39;ground,&#34; this person r n introduces hi&#39;, &#39;und,&#34; this person r n introduces himse&#39;, &#39;,&#34; this person r n introduces himself &#39;, &#39;this person r n introduces himself and&#39;, &#39;s person r n introduces himself and hi&#39;, &#39;erson r n introduces himself and his v&#39;, &#39;on r n introduces himself and his view&#39;, &#39; n introduces himself and his views, &#39;, &#39; introduces himself and his views, and&#39;, &#39;introduces himself and his views, and, a&#39;, &#39;roduces himself and his views, and, as i&#39;, &#39;uces himself and his views, and, as it w&#39;, &#39;s himself and his views, and, as it were&#39;, &#39;imself and his views, and, as it were, r n&#39;, &#39;elf and his views, and, as it were, r n &#39;, &#39; and his views, and, as it were, r n t&#39;, &#39;d his views, and, as it were, r n trie&#39;, &#39;is views, and, as it were, r n tries t&#39;, &#39;views, and, as it were, r n tries to e&#39;, &#39;ws, and, as it were, r n tries to expl&#39;, &#39; and, as it were, r n tries to explain&#39;, &#39;d, as it were, r n tries to explain th&#39;, &#39;as it were, r n tries to explain the c&#39;, &#39;it were, r n tries to explain the caus&#39;, &#39;were, r n tries to explain the causes &#39;, &#39;e, r n tries to explain the causes owi&#39;, &#39; n tries to explain the causes owing &#39;, &#39; tries to explain the causes owing to &#39;, &#39;tries to explain the causes owing to whi&#39;, &#39;es to explain the causes owing to which &#39;, &#39;to explain the causes owing to which he &#39;, &#39;explain the causes owing to which he has&#39;, &#39;lain the causes owing to which he has r n &#39;, &#39;n the causes owing to which he has r n &#39;, &#39;he causes owing to which he has r n ma&#39;, &#39;causes owing to which he has r n made &#39;, &#39;ses owing to which he has r n made his&#39;, &#39; owing to which he has r n made his ap&#39;, &#39;ing to which he has r n made his appea&#39;, &#39; to which he has r n made his appearan&#39;, &#39; which he has r n made his appearance &#39;, &#39;ich he has r n made his appearance and&#39;, &#39; he has r n made his appearance and wa&#39;, &#39; has r n made his appearance and was b&#39;, &#39;s r n made his appearance and was boun&#39;, &#39; made his appearance and was bound t&#39;, &#39; made his appearance and was bound to m&#39;, &#39;ade his appearance and was bound to make&#39;, &#39; his appearance and was bound to make hi&#39;, &#39;s appearance and was bound to make his r n&#39;, &#39;ppearance and was bound to make his r n &#39;, &#39;arance and was bound to make his r n a&#39;, &#39;nce and was bound to make his r n appe&#39;, &#39; and was bound to make his r n appeara&#39;, &#39;d was bound to make his r n appearance&#39;, &#39;as bound to make his r n appearance in&#39;, &#39;bound to make his r n appearance in ou&#39;, &#39;nd to make his r n appearance in our m&#39;, &#39;to make his r n appearance in our mids&#39;, &#39;make his r n appearance in our midst. &#39;, &#39;e his r n appearance in our midst. in&#39;, &#39;is r n appearance in our midst. in th&#39;, &#39; n appearance in our midst. in the s&#39;, &#39; appearance in our midst. in the seco&#39;, &#39;appearance in our midst. in the second &#39;, &#39;earance in our midst. in the second fra&#39;, &#39;ance in our midst. in the second fragme&#39;, &#39;e in our midst. in the second fragment r&#39;, &#39;n our midst. in the second fragment r n &#39;, &#39;ur midst. in the second fragment r n &#39;, &#39;midst. in the second fragment r n the&#39;, &#39;st. in the second fragment r n there &#39;, &#39; in the second fragment r n there are&#39;, &#39;n the second fragment r n there are ad&#39;, &#39;he second fragment r n there are added&#39;, &#39;second fragment r n there are added th&#39;, &#39;ond fragment r n there are added the a&#39;, &#39; fragment r n there are added the actu&#39;, &#39;agment r n there are added the actual &#39;, &#39;ent r n there are added the actual not&#39;, &#39; r n there are added the actual notes &#39;, &#39; there are added the actual notes of &#39;, &#39; there are added the actual notes of thi&#39;, &#39;ere are added the actual notes of this p&#39;, &#39; are added the actual notes of this pers&#39;, &#39;e added the actual notes of this person r&#39;, &#39;dded the actual notes of this person r n &#39;, &#39;d the actual notes of this person r n &#39;, &#39;he actual notes of this person r n con&#39;, &#39;actual notes of this person r n concer&#39;, &#39;ual notes of this person r n concernin&#39;, &#39; notes of this person r n concerning c&#39;, &#39;tes of this person r n concerning cert&#39;, &#39; of this person r n concerning certain&#39;, &#39; this person r n concerning certain ev&#39;, &#39;is person r n concerning certain event&#39;, &#39;person r n concerning certain events i&#39;, &#39;son r n concerning certain events in h&#39;, &#39; r n concerning certain events in his &#39;, &#39; concerning certain events in his lif&#39;, &#39; concerning certain events in his life.-&#39;, &#39;ncerning certain events in his life.--au&#39;, &#39;rning certain events in his life.--autho&#39;, &#34;ng certain events in his life.--author&#39;s&#34;, &#34;certain events in his life.--author&#39;s no&#34;, &#34;tain events in his life.--author&#39;s note.&#34;, &#34;n events in his life.--author&#39;s note. r n r&#34;, &#34;vents in his life.--author&#39;s note. r n r n r n&#34;, &#34;ts in his life.--author&#39;s note. r n r n r n r ni&#34;, &#34;in his life.--author&#39;s note. r n r n r n r ni r n r&#34;, &#34;his life.--author&#39;s note. r n r n r n r ni r n r ni &#34;, &#34; life.--author&#39;s note. r n r n r n r ni r n r ni am &#34;, &#34;fe.--author&#39;s note. r n r n r n r ni r n r ni am a s&#34;, &#34;--author&#39;s note. r n r n r n r ni r n r ni am a sick&#34;, &#34;uthor&#39;s note. r n r n r n r ni r n r ni am a sick ma&#34;, &#34;or&#39;s note. r n r n r n r ni r n r ni am a sick man..&#34;, &#39;s note. r n r n r n r ni r n r ni am a sick man.... &#39;, &#39;ote. r n r n r n r ni r n r ni am a sick man.... i &#39;, &#39;. r n r n r n r ni r n r ni am a sick man.... i am &#39;, &#39; r n r n r ni r n r ni am a sick man.... i am a s&#39;, &#39; n r ni r n r ni am a sick man.... i am a spit&#39;, &#39;i r n r ni am a sick man.... i am a spitefu&#39;, &#39; r ni am a sick man.... i am a spiteful m&#39;, &#39; am a sick man.... i am a spiteful man.&#39;, &#39; a sick man.... i am a spiteful man. i&#39;, &#39;sick man.... i am a spiteful man. i am&#39;, &#39;k man.... i am a spiteful man. i am an&#39;, &#39;an.... i am a spiteful man. i am an un&#39;, &#39;... i am a spiteful man. i am an unatt&#39;, &#39; i am a spiteful man. i am an unattrac&#39;, &#39; am a spiteful man. i am an unattractiv&#39;, &#39; a spiteful man. i am an unattractive m&#39;, &#39;spiteful man. i am an unattractive man.&#39;, &#39;teful man. i am an unattractive man. i&#39;, &#39;ul man. i am an unattractive man. i r nb&#39;, &#39;man. i am an unattractive man. i r nbeli&#39;, &#39;. i am an unattractive man. i r nbelieve&#39;, &#39;i am an unattractive man. i r nbelieve my&#39;, &#39;m an unattractive man. i r nbelieve my li&#39;, &#39;n unattractive man. i r nbelieve my liver&#39;, &#39;nattractive man. i r nbelieve my liver is&#39;, &#39;tractive man. i r nbelieve my liver is di&#39;, &#39;ctive man. i r nbelieve my liver is disea&#39;, &#39;ve man. i r nbelieve my liver is diseased&#39;, &#39;man. i r nbelieve my liver is diseased. &#39;, &#39;. i r nbelieve my liver is diseased. how&#39;, &#39;i r nbelieve my liver is diseased. howeve&#39;, &#39;believe my liver is diseased. however, &#39;, &#39;ieve my liver is diseased. however, i k&#39;, &#39;e my liver is diseased. however, i know&#39;, &#39;y liver is diseased. however, i know no&#39;, &#39;iver is diseased. however, i know nothi&#39;, &#39;r is diseased. however, i know nothing &#39;, &#39;s diseased. however, i know nothing at &#39;, &#39;iseased. however, i know nothing at all&#39;, &#39;ased. however, i know nothing at all ab&#39;, &#39;d. however, i know nothing at all about&#39;, &#39; however, i know nothing at all about my&#39;, &#39;wever, i know nothing at all about my r nd&#39;, &#39;er, i know nothing at all about my r ndise&#39;, &#39; i know nothing at all about my r ndisease&#39;, &#39;know nothing at all about my r ndisease, a&#39;, &#39;w nothing at all about my r ndisease, and &#39;, &#39;othing at all about my r ndisease, and do &#39;, &#39;ing at all about my r ndisease, and do not&#39;, &#39; at all about my r ndisease, and do not kn&#39;, &#39; all about my r ndisease, and do not know &#39;, &#39;l about my r ndisease, and do not know for&#39;, &#39;bout my r ndisease, and do not know for ce&#39;, &#39;t my r ndisease, and do not know for certa&#39;, &#39;y r ndisease, and do not know for certain &#39;, &#39;disease, and do not know for certain wha&#39;, &#39;ease, and do not know for certain what a&#39;, &#39;e, and do not know for certain what ails&#39;, &#39;and do not know for certain what ails me&#39;, &#39; do not know for certain what ails me. &#39;, &#39; not know for certain what ails me. i d&#39;, &#34;t know for certain what ails me. i don&#39;&#34;, &#34;now for certain what ails me. i don&#39;t c&#34;, &#34; for certain what ails me. i don&#39;t cons&#34;, &#34;r certain what ails me. i don&#39;t consult&#34;, &#34;ertain what ails me. i don&#39;t consult a r&#34;, &#34;ain what ails me. i don&#39;t consult a r ndo&#34;, &#34; what ails me. i don&#39;t consult a r ndocto&#34;, &#34;at ails me. i don&#39;t consult a r ndoctor f&#34;, &#34;ails me. i don&#39;t consult a r ndoctor for &#34;, &#34;s me. i don&#39;t consult a r ndoctor for it,&#34;, &#34;e. i don&#39;t consult a r ndoctor for it, an&#34;, &#34; i don&#39;t consult a r ndoctor for it, and n&#34;, &#34;don&#39;t consult a r ndoctor for it, and neve&#34;, &#34;&#39;t consult a r ndoctor for it, and never h&#34;, &#39;consult a r ndoctor for it, and never have&#39;, &#39;sult a r ndoctor for it, and never have, t&#39;, &#39;t a r ndoctor for it, and never have, thou&#39;, &#39; r ndoctor for it, and never have, though &#39;, &#39;octor for it, and never have, though i h&#39;, &#39;or for it, and never have, though i have&#39;, &#39;for it, and never have, though i have a &#39;, &#39; it, and never have, though i have a res&#39;, &#39;, and never have, though i have a respec&#39;, &#39;nd never have, though i have a respect f&#39;, &#39;never have, though i have a respect for &#39;, &#39;er have, though i have a respect for med&#39;, &#39;have, though i have a respect for medici&#39;, &#39;e, though i have a respect for medicine &#39;, &#39;though i have a respect for medicine and&#39;, &#39;ugh i have a respect for medicine and r nd&#39;, &#39; i have a respect for medicine and r ndoct&#39;, &#39;have a respect for medicine and r ndoctors&#39;, &#39;e a respect for medicine and r ndoctors. b&#39;, &#39; respect for medicine and r ndoctors. besi&#39;, &#39;spect for medicine and r ndoctors. besides&#39;, &#39;ct for medicine and r ndoctors. besides, i&#39;, &#39;for medicine and r ndoctors. besides, i am&#39;, &#39; medicine and r ndoctors. besides, i am ex&#39;, &#39;dicine and r ndoctors. besides, i am extre&#39;, &#39;ine and r ndoctors. besides, i am extremel&#39;, &#39; and r ndoctors. besides, i am extremely s&#39;, &#39;d r ndoctors. besides, i am extremely supe&#39;, &#39;doctors. besides, i am extremely superst&#39;, &#39;tors. besides, i am extremely superstiti&#39;, &#39;s. besides, i am extremely superstitious&#39;, &#39;besides, i am extremely superstitious, s&#39;, &#39;ides, i am extremely superstitious, suff&#39;, &#39;s, i am extremely superstitious, suffici&#39;, &#39;i am extremely superstitious, sufficient&#39;, &#39;m extremely superstitious, sufficiently &#39;, &#39;xtremely superstitious, sufficiently so &#39;, &#39;emely superstitious, sufficiently so to r&#39;, &#39;ly superstitious, sufficiently so to r nre&#39;, &#39;superstitious, sufficiently so to r nrespe&#39;, &#39;erstitious, sufficiently so to r nrespect &#39;, &#39;titious, sufficiently so to r nrespect med&#39;, &#39;ious, sufficiently so to r nrespect medici&#39;, &#39;s, sufficiently so to r nrespect medicine,&#39;, &#39;sufficiently so to r nrespect medicine, an&#39;, &#39;ficiently so to r nrespect medicine, anywa&#39;, &#39;iently so to r nrespect medicine, anyway (&#39;, &#39;tly so to r nrespect medicine, anyway (i a&#39;, &#39; so to r nrespect medicine, anyway (i am w&#39;, &#39; to r nrespect medicine, anyway (i am well&#39;, &#39; r nrespect medicine, anyway (i am well-ed&#39;, &#39;espect medicine, anyway (i am well-educa&#39;, &#39;ect medicine, anyway (i am well-educated&#39;, &#39; medicine, anyway (i am well-educated en&#39;, &#39;dicine, anyway (i am well-educated enoug&#39;, &#39;ine, anyway (i am well-educated enough n&#39;, &#39;, anyway (i am well-educated enough not &#39;, &#39;nyway (i am well-educated enough not to &#39;, &#39;ay (i am well-educated enough not to be r&#39;, &#39;(i am well-educated enough not to be r nsu&#39;, &#39;am well-educated enough not to be r nsuper&#39;, &#39;well-educated enough not to be r nsupersti&#39;, &#39;l-educated enough not to be r nsuperstitio&#39;, &#39;ducated enough not to be r nsuperstitious,&#39;, &#39;ated enough not to be r nsuperstitious, bu&#39;, &#39;d enough not to be r nsuperstitious, but i&#39;, &#39;nough not to be r nsuperstitious, but i am&#39;, &#39;gh not to be r nsuperstitious, but i am su&#39;, &#39;not to be r nsuperstitious, but i am super&#39;, &#39; to be r nsuperstitious, but i am supersti&#39;, &#39; be r nsuperstitious, but i am superstitio&#39;, &#39; r nsuperstitious, but i am superstitious)&#39;, &#39;uperstitious, but i am superstitious). &#39;, &#39;rstitious, but i am superstitious). no,&#39;, &#39;itious, but i am superstitious). no, i &#39;, &#39;ous, but i am superstitious). no, i ref&#39;, &#39;, but i am superstitious). no, i refuse&#39;, &#39;ut i am superstitious). no, i refuse to&#39;, &#39;i am superstitious). no, i refuse to co&#39;, &#39;m superstitious). no, i refuse to consu&#39;, &#39;uperstitious). no, i refuse to consult &#39;, &#39;rstitious). no, i refuse to consult a r n&#39;, &#39;itious). no, i refuse to consult a r ndoc&#39;, &#39;ous). no, i refuse to consult a r ndoctor&#39;, &#39;). no, i refuse to consult a r ndoctor fr&#39;, &#39; no, i refuse to consult a r ndoctor from &#39;, &#39;, i refuse to consult a r ndoctor from spi&#39;, &#39; refuse to consult a r ndoctor from spite.&#39;, &#39;fuse to consult a r ndoctor from spite. t&#39;, &#39;e to consult a r ndoctor from spite. that&#39;, &#39;o consult a r ndoctor from spite. that yo&#39;, &#39;onsult a r ndoctor from spite. that you p&#39;, &#39;ult a r ndoctor from spite. that you prob&#39;, &#39; a r ndoctor from spite. that you probabl&#39;, &#39; ndoctor from spite. that you probably w&#39;, &#39;ctor from spite. that you probably will&#39;, &#39;r from spite. that you probably will no&#39;, &#39;rom spite. that you probably will not u&#39;, &#39; spite. that you probably will not unde&#39;, &#39;ite. that you probably will not underst&#39;, &#39;. that you probably will not understand&#39;, &#39;that you probably will not understand. &#39;, &#39;t you probably will not understand. wel&#39;, &#39;ou probably will not understand. well, &#39;, &#39;probably will not understand. well, i r n&#39;, &#39;bably will not understand. well, i r nund&#39;, &#39;ly will not understand. well, i r nunders&#39;, &#39;will not understand. well, i r nunderstan&#39;, &#39;l not understand. well, i r nunderstand i&#39;, &#39;ot understand. well, i r nunderstand it, &#39;, &#39;understand. well, i r nunderstand it, tho&#39;, &#39;erstand. well, i r nunderstand it, though&#39;, &#39;tand. well, i r nunderstand it, though. &#39;, &#39;d. well, i r nunderstand it, though. of &#39;, &#39; well, i r nunderstand it, though. of cou&#39;, &#39;ll, i r nunderstand it, though. of course&#39;, &#39; i r nunderstand it, though. of course, i&#39;, &#39; nunderstand it, though. of course, i ca&#39;, &#34;derstand it, though. of course, i can&#39;t&#34;, &#34;stand it, though. of course, i can&#39;t ex&#34;, &#34;nd it, though. of course, i can&#39;t expla&#34;, &#34;it, though. of course, i can&#39;t explain &#34;, &#34; though. of course, i can&#39;t explain who&#34;, &#34;ough. of course, i can&#39;t explain who it&#34;, &#34;h. of course, i can&#39;t explain who it is&#34;, &#34; of course, i can&#39;t explain who it is pr&#34;, &#34; course, i can&#39;t explain who it is preci&#34;, &#34;urse, i can&#39;t explain who it is precisel&#34;, &#34;e, i can&#39;t explain who it is precisely r n&#34;, &#34;i can&#39;t explain who it is precisely r ntha&#34;, &#34;an&#39;t explain who it is precisely r nthat i&#34;, &#39;t explain who it is precisely r nthat i am&#39;, &#39;xplain who it is precisely r nthat i am mo&#39;, &#39;ain who it is precisely r nthat i am morti&#39;, &#39; who it is precisely r nthat i am mortifyi&#39;, &#39;o it is precisely r nthat i am mortifying &#39;, &#39;t is precisely r nthat i am mortifying in &#39;, &#39;s precisely r nthat i am mortifying in thi&#39;, &#39;recisely r nthat i am mortifying in this c&#39;, &#39;isely r nthat i am mortifying in this case&#39;, &#39;ly r nthat i am mortifying in this case by&#39;, &#39; nthat i am mortifying in this case by my&#39;, &#39;at i am mortifying in this case by my sp&#39;, &#39;i am mortifying in this case by my spite&#39;, &#39;m mortifying in this case by my spite: i&#39;, &#39;ortifying in this case by my spite: i am&#39;, &#39;ifying in this case by my spite: i am pe&#39;, &#39;ing in this case by my spite: i am perfe&#39;, &#39; in this case by my spite: i am perfectl&#39;, &#39; this case by my spite: i am perfectly w&#39;, &#39;is case by my spite: i am perfectly well&#39;, &#39;case by my spite: i am perfectly well r na&#39;, &#39;e by my spite: i am perfectly well r nawar&#39;, &#39;y my spite: i am perfectly well r naware t&#39;, &#39;y spite: i am perfectly well r naware that&#39;, &#39;pite: i am perfectly well r naware that i &#39;, &#39;e: i am perfectly well r naware that i can&#39;, &#39;i am perfectly well r naware that i cannot&#39;, &#39;m perfectly well r naware that i cannot &#34;p&#39;, &#39;erfectly well r naware that i cannot &#34;pay &#39;, &#39;ectly well r naware that i cannot &#34;pay out&#39;, &#39;ly well r naware that i cannot &#34;pay out&#34; t&#39;, &#39;well r naware that i cannot &#34;pay out&#34; the &#39;, &#39;l r naware that i cannot &#34;pay out&#34; the doc&#39;, &#39;aware that i cannot &#34;pay out&#34; the doctor&#39;, &#39;re that i cannot &#34;pay out&#34; the doctors b&#39;, &#39;that i cannot &#34;pay out&#34; the doctors by n&#39;, &#39;t i cannot &#34;pay out&#34; the doctors by not &#39;, &#39; cannot &#34;pay out&#34; the doctors by not con&#39;, &#39;nnot &#34;pay out&#34; the doctors by not consul&#39;, &#39;t &#34;pay out&#34; the doctors by not consultin&#39;, &#39;pay out&#34; the doctors by not consulting t&#39;, &#39; out&#34; the doctors by not consulting them&#39;, &#39;t&#34; the doctors by not consulting them; i&#39;, &#39;the doctors by not consulting them; i r nk&#39;, &#39; doctors by not consulting them; i r nknow&#39;, &#39;ctors by not consulting them; i r nknow be&#39;, &#39;rs by not consulting them; i r nknow bette&#39;, &#39;by not consulting them; i r nknow better t&#39;, &#39;not consulting them; i r nknow better than&#39;, &#39; consulting them; i r nknow better than an&#39;, &#39;nsulting them; i r nknow better than anyon&#39;, &#39;lting them; i r nknow better than anyone t&#39;, &#39;ng them; i r nknow better than anyone that&#39;, &#39;them; i r nknow better than anyone that by&#39;, &#39;m; i r nknow better than anyone that by al&#39;, &#39;i r nknow better than anyone that by all t&#39;, &#39;know better than anyone that by all this&#39;, &#39;w better than anyone that by all this i &#39;, &#39;etter than anyone that by all this i am &#39;, &#39;er than anyone that by all this i am onl&#39;, &#39;than anyone that by all this i am only i&#39;, &#39;n anyone that by all this i am only inju&#39;, &#39;nyone that by all this i am only injurin&#39;, &#39;ne that by all this i am only injuring m&#39;, &#39;that by all this i am only injuring myse&#39;, &#39;t by all this i am only injuring myself &#39;, &#39;y all this i am only injuring myself and&#39;, &#39;ll this i am only injuring myself and r nn&#39;, &#39;this i am only injuring myself and r nno o&#39;, &#39;s i am only injuring myself and r nno one &#39;, &#39; am only injuring myself and r nno one els&#39;, &#39; only injuring myself and r nno one else. &#39;, &#39;ly injuring myself and r nno one else. bu&#39;, &#39;injuring myself and r nno one else. but s&#39;, &#39;uring myself and r nno one else. but stil&#39;, &#39;ng myself and r nno one else. but still, &#39;, &#39;myself and r nno one else. but still, if &#39;, &#39;elf and r nno one else. but still, if i d&#39;, &#34; and r nno one else. but still, if i don&#39;&#34;, &#34;d r nno one else. but still, if i don&#39;t c&#34;, &#34;no one else. but still, if i don&#39;t cons&#34;, &#34;one else. but still, if i don&#39;t consult&#34;, &#34; else. but still, if i don&#39;t consult a &#34;, &#34;se. but still, if i don&#39;t consult a doc&#34;, &#34; but still, if i don&#39;t consult a doctor&#34;, &#34;ut still, if i don&#39;t consult a doctor it&#34;, &#34;still, if i don&#39;t consult a doctor it is&#34;, &#34;ll, if i don&#39;t consult a doctor it is fr&#34;, &#34; if i don&#39;t consult a doctor it is from &#34;, &#34; i don&#39;t consult a doctor it is from spi&#34;, &#34;don&#39;t consult a doctor it is from spite.&#34;, &#34;&#39;t consult a doctor it is from spite. r nm&#34;, &#39;consult a doctor it is from spite. r nmy l&#39;, &#39;sult a doctor it is from spite. r nmy live&#39;, &#39;t a doctor it is from spite. r nmy liver i&#39;, &#39; doctor it is from spite. r nmy liver is b&#39;, &#39;ctor it is from spite. r nmy liver is bad,&#39;, &#39;r it is from spite. r nmy liver is bad, we&#39;, &#39;t is from spite. r nmy liver is bad, well-&#39;, &#39;s from spite. r nmy liver is bad, well--le&#39;, &#39;rom spite. r nmy liver is bad, well--let i&#39;, &#39; spite. r nmy liver is bad, well--let it g&#39;, &#39;ite. r nmy liver is bad, well--let it get &#39;, &#39;. r nmy liver is bad, well--let it get wor&#39;, &#39;my liver is bad, well--let it get worse!&#39;, &#39;liver is bad, well--let it get worse! r n r&#39;, &#39;er is bad, well--let it get worse! r n r ni &#39;, &#39;is bad, well--let it get worse! r n r ni hav&#39;, &#39;bad, well--let it get worse! r n r ni have b&#39;, &#39;, well--let it get worse! r n r ni have been&#39;, &#39;ell--let it get worse! r n r ni have been go&#39;, &#39;--let it get worse! r n r ni have been going&#39;, &#39;et it get worse! r n r ni have been going on&#39;, &#39;it get worse! r n r ni have been going on li&#39;, &#39;get worse! r n r ni have been going on like &#39;, &#39; worse! r n r ni have been going on like tha&#39;, &#39;rse! r n r ni have been going on like that f&#39;, &#39;! r n r ni have been going on like that for &#39;, &#39; r ni have been going on like that for a l&#39;, &#39; have been going on like that for a long&#39;, &#39;ve been going on like that for a long ti&#39;, &#39;been going on like that for a long time-&#39;, &#39;n going on like that for a long time--tw&#39;, &#39;oing on like that for a long time--twent&#39;, &#39;g on like that for a long time--twenty y&#39;, &#39;n like that for a long time--twenty year&#39;, &#39;ike that for a long time--twenty years. &#39;, &#39; that for a long time--twenty years. no&#39;, &#39;at for a long time--twenty years. now i&#39;, &#39;for a long time--twenty years. now i am&#39;, &#39; a long time--twenty years. now i am r nf&#39;, &#39;long time--twenty years. now i am r nfort&#39;, &#39;g time--twenty years. now i am r nforty. &#39;, &#39;ime--twenty years. now i am r nforty. i &#39;, &#39;--twenty years. now i am r nforty. i use&#39;, &#39;wenty years. now i am r nforty. i used t&#39;, &#39;ty years. now i am r nforty. i used to b&#39;, &#39;years. now i am r nforty. i used to be i&#39;, &#39;rs. now i am r nforty. i used to be in t&#39;, &#39; now i am r nforty. i used to be in the &#39;, &#39;ow i am r nforty. i used to be in the gov&#39;, &#39;i am r nforty. i used to be in the govern&#39;, &#39;m r nforty. i used to be in the governmen&#39;, &#39;forty. i used to be in the government s&#39;, &#39;ty. i used to be in the government serv&#39;, &#39; i used to be in the government service&#39;, &#39; used to be in the government service, b&#39;, &#39;ed to be in the government service, but &#39;, &#39;to be in the government service, but am &#39;, &#39;be in the government service, but am no &#39;, &#39;in the government service, but am no lon&#39;, &#39;the government service, but am no longer&#39;, &#39; government service, but am no longer. &#39;, &#39;vernment service, but am no longer. i r n&#39;, &#39;nment service, but am no longer. i r nwas&#39;, &#39;nt service, but am no longer. i r nwas a &#39;, &#39;service, but am no longer. i r nwas a spi&#39;, &#39;vice, but am no longer. i r nwas a spitef&#39;, &#39;e, but am no longer. i r nwas a spiteful &#39;, &#39;but am no longer. i r nwas a spiteful off&#39;, &#39; am no longer. i r nwas a spiteful offici&#39;, &#39; no longer. i r nwas a spiteful official.&#39;, &#39; longer. i r nwas a spiteful official. i&#39;, &#39;nger. i r nwas a spiteful official. i wa&#39;, &#39;r. i r nwas a spiteful official. i was r&#39;, &#39; i r nwas a spiteful official. i was rude&#39;, &#39; nwas a spiteful official. i was rude an&#39;, &#39;s a spiteful official. i was rude and t&#39;, &#39; spiteful official. i was rude and took&#39;, &#39;iteful official. i was rude and took pl&#39;, &#39;ful official. i was rude and took pleas&#39;, &#39; official. i was rude and took pleasure&#39;, &#39;ficial. i was rude and took pleasure in&#39;, &#39;ial. i was rude and took pleasure in be&#39;, &#39;. i was rude and took pleasure in being&#39;, &#39;i was rude and took pleasure in being so&#39;, &#39;as rude and took pleasure in being so. &#39;, &#39;rude and took pleasure in being so. i r n&#39;, &#39;e and took pleasure in being so. i r ndid&#39;, &#39;nd took pleasure in being so. i r ndid no&#39;, &#39;took pleasure in being so. i r ndid not t&#39;, &#39;k pleasure in being so. i r ndid not take&#39;, &#39;leasure in being so. i r ndid not take br&#39;, &#39;sure in being so. i r ndid not take bribe&#39;, &#39;e in being so. i r ndid not take bribes, &#39;, &#39;n being so. i r ndid not take bribes, you&#39;, &#39;eing so. i r ndid not take bribes, you se&#39;, &#39;g so. i r ndid not take bribes, you see, &#39;, &#39;o. i r ndid not take bribes, you see, so &#39;, &#39; i r ndid not take bribes, you see, so i w&#39;, &#39; ndid not take bribes, you see, so i was &#39;, &#39;d not take bribes, you see, so i was bou&#39;, &#39;ot take bribes, you see, so i was bound &#39;, &#39;take bribes, you see, so i was bound to &#39;, &#39;e bribes, you see, so i was bound to fin&#39;, &#39;ribes, you see, so i was bound to find a&#39;, &#39;es, you see, so i was bound to find a re&#39;, &#39; you see, so i was bound to find a recom&#39;, &#39;u see, so i was bound to find a recompen&#39;, &#39;ee, so i was bound to find a recompense &#39;, &#39; so i was bound to find a recompense in r&#39;, &#39; i was bound to find a recompense in r nth&#39;, &#39;was bound to find a recompense in r nthat,&#39;, &#39; bound to find a recompense in r nthat, at&#39;, &#39;und to find a recompense in r nthat, at le&#39;, &#39; to find a recompense in r nthat, at least&#39;, &#39; find a recompense in r nthat, at least. &#39;, &#39;nd a recompense in r nthat, at least. (a &#39;, &#39;a recompense in r nthat, at least. (a poo&#39;, &#39;ecompense in r nthat, at least. (a poor j&#39;, &#39;mpense in r nthat, at least. (a poor jest&#39;, &#39;nse in r nthat, at least. (a poor jest, b&#39;, &#39; in r nthat, at least. (a poor jest, but &#39;, &#39; r nthat, at least. (a poor jest, but i w&#39;, &#39;hat, at least. (a poor jest, but i will&#39;, &#39;, at least. (a poor jest, but i will no&#39;, &#39;t least. (a poor jest, but i will not s&#39;, &#39;east. (a poor jest, but i will not scra&#39;, &#39;t. (a poor jest, but i will not scratch&#39;, &#39; (a poor jest, but i will not scratch it&#39;, &#39; poor jest, but i will not scratch it ou&#39;, &#39;or jest, but i will not scratch it out. &#39;, &#39;jest, but i will not scratch it out. i &#39;, &#39;t, but i will not scratch it out. i wro&#39;, &#39;but i will not scratch it out. i wrote r&#39;, &#39; i will not scratch it out. i wrote r nit&#39;, &#39;will not scratch it out. i wrote r nit th&#39;, &#39;l not scratch it out. i wrote r nit think&#39;, &#39;ot scratch it out. i wrote r nit thinking&#39;, &#39;scratch it out. i wrote r nit thinking it&#39;, &#39;atch it out. i wrote r nit thinking it wo&#39;, &#39;h it out. i wrote r nit thinking it would&#39;, &#39;t out. i wrote r nit thinking it would so&#39;, &#39;ut. i wrote r nit thinking it would sound&#39;, &#39; i wrote r nit thinking it would sound ve&#39;, &#39; wrote r nit thinking it would sound very &#39;, &#39;ote r nit thinking it would sound very wit&#39;, &#39; r nit thinking it would sound very witty;&#39;, &#39;t thinking it would sound very witty; bu&#39;, &#39;hinking it would sound very witty; but n&#39;, &#39;king it would sound very witty; but now &#39;, &#39;g it would sound very witty; but now tha&#39;, ...] . print(&#39;Vectorization...&#39;) x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) y = np.zeros((len(sentences), len(chars)), dtype=np.bool) for i, sentence in enumerate(sentences): for t, char in enumerate(sentence): x[i, t, char_indices[char]] = 1 y[i, char_indices[next_chars[i]]] = 1 . Vectorization... . x.shape . (88514, 40, 58) . y.shape . (88514, 58) . y[0:10] . array([[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False]]) . # build the model: a single LSTM print(&#39;Build model...&#39;) model = Sequential() model.add(LSTM(128, input_shape=(maxlen, len(chars)))) model.add(Dense(len(chars), activation=&#39;softmax&#39;)) optimizer = RMSprop(lr=0.01) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=optimizer) . Build model... . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 128) 95744 _________________________________________________________________ dense (Dense) (None, 58) 7482 ================================================================= Total params: 103,226 Trainable params: 103,226 Non-trainable params: 0 _________________________________________________________________ . def sample(preds, temperature=1.0): # helper function to sample an index from a probability array preds = np.asarray(preds).astype(&#39;float64&#39;) preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) . def on_epoch_end(epoch, _): # Function invoked at end of each epoch. Prints generated text. print(&quot;******************************************************&quot;) print(&#39;-- Generating text after Epoch: %d&#39; % epoch) start_index = random.randint(0, len(processed_text) - maxlen - 1) for temperature in [0.2, 0.5, 1.0, 1.2]: print(&#39;-- temperature:&#39;, temperature) generated = &#39;&#39; sentence = processed_text[start_index: start_index + maxlen] generated += sentence print(&#39;-- Generating with seed: &quot;&#39; + sentence + &#39;&quot;&#39;) sys.stdout.write(generated) for i in range(400): x_pred = np.zeros((1, maxlen, len(chars))) for t, char in enumerate(sentence): x_pred[0, t, char_indices[char]] = 1. preds = model.predict(x_pred, verbose=0)[0] next_index = sample(preds, temperature) next_char = indices_char[next_index] generated += next_char sentence = sentence[1:] + next_char sys.stdout.write(next_char) sys.stdout.flush() print() . # Ignore useless W0819 warnings generated by TensorFlow 2.0. Hopefully can remove this ignore in the future. # See https://github.com/tensorflow/tensorflow/issues/31308 import logging, os logging.disable(logging.WARNING) os.environ[&quot;TF_CPP_MIN_LOG_LEVEL&quot;] = &quot;3&quot; # Fit the model print_callback = LambdaCallback(on_epoch_end=on_epoch_end) model.fit(x, y, batch_size=128, epochs=60, callbacks=[print_callback]) . Epoch 1/60 692/692 [==============================] - ETA: 0s - loss: 2.1576****************************************************** -- Generating text after Epoch: 0 -- temperature: 0.2 -- Generating with seed: &#34;cried. &#34;but i will make up for it or p&#34; cried. &#34;but i will make up for it or pored the stine the derent of the dook and the stand to the store and the doon the sare the dong the dened to the dook the sare the stare the street of the seres at all the sall the deared the derent of the street in i am in the derent to the stan to the street the stine the sered to the dran that is all the stan the stare the dook the forest of the stan stand of the dook all the stand the streed t -- temperature: 0.5 -- Generating with seed: &#34;cried. &#34;but i will make up for it or p&#34; cried. &#34;but i he know conse the lake the s -- temperature: 1.0 -- Generating with seed: &#34;cried. &#34;but i will make up for it or p&#34; cried. &#34;but i &#34;yes kle the lafine as the prosies netc -- temperature: 1.2 -- Generating with seed: &#34;cried. &#34;but i will make up for it or p&#34; cried. &#34;but i anp, ! 692/692 [==============================] - 155s 224ms/step - loss: 2.1576 Epoch 2/60 572/692 [=======================&gt;......] - ETA: 13s - loss: 1.7386 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/14/Text_Generation_LSTM_Dostoevsky.html",
            "relUrl": "/2020/11/14/Text_Generation_LSTM_Dostoevsky.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Timeseries",
            "content": "Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning . try: %tensorflow_version 2.x COLAB = True print(&quot;Note: using Google CoLab&quot;) except: print(&quot;Note: not using Google CoLab&quot;) COLAB = False . Note: using Google CoLab . # x = [ [32], [41], [39], [20], [15] ] y = [ 1, -1, 0, -1, 1 ] print(x) print(y) . [[32], [41], [39], [20], [15]] [1, -1, 0, -1, 1] . from IPython.display import display, HTML import pandas as pd import numpy as np x = np.array(x) print(x[:,0]) df = pd.DataFrame({&#39;x&#39;:x[:,0], &#39;y&#39;:y}) display(df) . [32 41 39 20 15] . x y . 0 32 | 1 | . 1 41 | -1 | . 2 39 | 0 | . 3 20 | -1 | . 4 15 | 1 | . x = [ [32,1383], [41,2928], [39,8823], [20,1252], [15,1532] ] y = [ 1, -1, 0, -1, 1 ] print(x) print(y) . [[32, 1383], [41, 2928], [39, 8823], [20, 1252], [15, 1532]] [1, -1, 0, -1, 1] . from IPython.display import display, HTML import pandas as pd import numpy as np x = np.array(x) print(x[:,0]) df = pd.DataFrame({&#39;price&#39;:x[:,0], &#39;volume&#39;:x[:,1], &#39;y&#39;:y}) display(df) . [32 41 39 20 15] . price volume y . 0 32 | 1383 | 1 | . 1 41 | 2928 | -1 | . 2 39 | 8823 | 0 | . 3 20 | 1252 | -1 | . 4 15 | 1532 | 1 | . x = [ [[32,1383],[41,2928],[39,8823],[20,1252],[15,1532]], [[35,8272],[32,1383],[41,2928],[39,8823],[20,1252]], [[37,2738],[35,8272],[32,1383],[41,2928],[39,8823]], [[34,2845],[37,2738],[35,8272],[32,1383],[41,2928]], [[32,2345],[34,2845],[37,2738],[35,8272],[32,1383]], ] y = [ 1, -1, 0, -1, 1 ] print(x) print(y) . [[[32, 1383], [41, 2928], [39, 8823], [20, 1252], [15, 1532]], [[35, 8272], [32, 1383], [41, 2928], [39, 8823], [20, 1252]], [[37, 2738], [35, 8272], [32, 1383], [41, 2928], [39, 8823]], [[34, 2845], [37, 2738], [35, 8272], [32, 1383], [41, 2928]], [[32, 2345], [34, 2845], [37, 2738], [35, 8272], [32, 1383]]] [1, -1, 0, -1, 1] . Even if there is only one feature (price), the 3rd dimension must be used: . x = [ [[32],[41],[39],[20],[15]], [[35],[32],[41],[39],[20]], [[37],[35],[32],[41],[39]], [[34],[37],[35],[32],[41]], [[32],[34],[37],[35],[32]], ] y = [ 1, -1, 0, -1, 1 ] print(x) print(y) . [[[32], [41], [39], [20], [15]], [[35], [32], [41], [39], [20]], [[37], [35], [32], [41], [39]], [[34], [37], [35], [32], [41]], [[32], [34], [37], [35], [32]]] [1, -1, 0, -1, 1] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/13/Timeseries.html",
            "relUrl": "/2020/11/13/Timeseries.html",
            "date": " • Nov 13, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Tensorflow and Keras",
            "content": ". Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning . TensorFlow Homepage | TensorFlow GitHib | TensorFlow Google Groups Support | TensorFlow Google Groups Developer Discussion | TensorFlow FAQ | . Deep Learning Tools . TensorFlow - Google&#39;s deep learning API. The focus of this class, along with Keras. | Keras - Also by Google, higher level framework that allows the use of TensorFlow, MXNet and Theano interchangeably. | PyTorch - PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook&#39;s AI Research lab. . | MXNet Apache foundation&#39;s deep learning API. Can be used through Keras. . | Torch is used by Google DeepMind, the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. It has been used for some of the most advanced deep learning projects in the world. However, it requires the LUA)** programming language. It is very advanced, but it is not mainstream. I have not worked with Torch (yet!). | PaddlePaddle - Baidu&#39;s deep learning API. | Deeplearning4J - Java based. Supports all major platforms. GPU support in Java! | Computational Network Toolkit (CNTK) - Microsoft. Support for Windows/Linux, command line only. Bindings for predictions for C#/Python. GPU support. | H2O - Java based. Supports all major platforms. Limited support for computer vision. No GPU support. . | Communicate with TensorFlow using Keras [Cite:franccois2017deep]. . | . try: %tensorflow_version 2.x COLAB = True print(&quot;Note: using Google CoLab&quot;) except: print(&quot;Note: not using Google CoLab&quot;) COLAB = False . Note: using Google CoLab . import tensorflow as tf # Create a Constant op that produces a 1x2 matrix. The op is # added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op. matrix1 = tf.constant([[3., 3.]]) # Create another Constant that produces a 2x1 matrix. matrix2 = tf.constant([[2.],[2.]]) # Create a Matmul op that takes &#39;matrix1&#39; and &#39;matrix2&#39; as inputs. # The returned value, &#39;product&#39;, represents the result of the matrix # multiplication. product = tf.matmul(matrix1, matrix2) print(product) print(float(product)) . tf.Tensor([[12.]], shape=(1, 1), dtype=float32) 12.0 . import tensorflow as tf x = tf.Variable([1.0, 2.0]) a = tf.constant([3.0, 3.0]) # Add an op to subtract &#39;a&#39; from &#39;x&#39;. Run it and print the result sub = tf.subtract(x, a) print(sub) print(sub.numpy()) # ==&gt; [-2. -1.] . tf.Tensor([-2. -1.], shape=(2,), dtype=float32) [-2. -1.] . x.assign([4.0, 6.0]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2,) dtype=float32, numpy=array([4., 6.], dtype=float32)&gt; . sub = tf.subtract(x, a) print(sub) print(sub.numpy()) . tf.Tensor([1. 3.], shape=(2,), dtype=float32) [1. 3.] . # Import libraries for simulation import tensorflow as tf import numpy as np # Imports for visualization import PIL.Image from io import BytesIO from IPython.display import Image, display def DisplayFractal(a, fmt=&#39;jpeg&#39;): &quot;&quot;&quot;Display an array of iteration counts as a colorful picture of a fractal.&quot;&quot;&quot; a_cyclic = (6.28*a/20.0).reshape(list(a.shape)+[1]) img = np.concatenate([10+20*np.cos(a_cyclic), 30+50*np.sin(a_cyclic), 155-80*np.cos(a_cyclic)], 2) img[a==a.max()] = 0 a = img a = np.uint8(np.clip(a, 0, 255)) f = BytesIO() PIL.Image.fromarray(a).save(f, fmt) display(Image(data=f.getvalue())) # Use NumPy to create a 2D array of complex numbers Y, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005] Z = X+1j*Y xs = tf.constant(Z.astype(np.complex64)) zs = tf.Variable(xs) ns = tf.Variable(tf.zeros_like(xs, tf.float32)) # Operation to update the zs and the iteration count. # # Note: We keep computing zs after they diverge! This # is very wasteful! There are better, if a little # less simple, ways to do this. # for i in range(200): # Compute the new values of z: z^2 + x zs_ = zs*zs + xs # Have we diverged with this new value? not_diverged = tf.abs(zs_) &lt; 4 zs.assign(zs_), ns.assign_add(tf.cast(not_diverged, tf.float32)) DisplayFractal(ns.numpy()) . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation import pandas as pd import io import os import requests import numpy as np from sklearn import metrics df = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/auto-mpg.csv&quot;, na_values=[&#39;NA&#39;, &#39;?&#39;]) cars = df[&#39;name&#39;] # Handle missing value df[&#39;horsepower&#39;] = df[&#39;horsepower&#39;].fillna(df[&#39;horsepower&#39;].median()) # Pandas to Numpy x = df[[&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;]].values y = df[&#39;mpg&#39;].values # regression # Build the neural network model = Sequential() model.add(Dense(25, input_dim=x.shape[1], activation=&#39;relu&#39;)) # Hidden 1 model.add(Dense(10, activation=&#39;relu&#39;)) # Hidden 2 model.add(Dense(1)) # Output model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;) model.fit(x,y,verbose=2,epochs=100) . Epoch 1/100 13/13 - 0s - loss: 4580.9326 Epoch 2/100 13/13 - 0s - loss: 1225.3738 Epoch 3/100 13/13 - 0s - loss: 553.5983 Epoch 4/100 13/13 - 0s - loss: 376.0931 Epoch 5/100 13/13 - 0s - loss: 371.2755 Epoch 6/100 13/13 - 0s - loss: 355.6521 Epoch 7/100 13/13 - 0s - loss: 340.7242 Epoch 8/100 13/13 - 0s - loss: 332.5170 Epoch 9/100 13/13 - 0s - loss: 321.3474 Epoch 10/100 13/13 - 0s - loss: 309.0533 Epoch 11/100 13/13 - 0s - loss: 298.6037 Epoch 12/100 13/13 - 0s - loss: 290.2845 Epoch 13/100 13/13 - 0s - loss: 281.7734 Epoch 14/100 13/13 - 0s - loss: 267.6147 Epoch 15/100 13/13 - 0s - loss: 255.6712 Epoch 16/100 13/13 - 0s - loss: 245.0956 Epoch 17/100 13/13 - 0s - loss: 238.7114 Epoch 18/100 13/13 - 0s - loss: 228.2198 Epoch 19/100 13/13 - 0s - loss: 215.6643 Epoch 20/100 13/13 - 0s - loss: 206.8975 Epoch 21/100 13/13 - 0s - loss: 198.3939 Epoch 22/100 13/13 - 0s - loss: 189.5826 Epoch 23/100 13/13 - 0s - loss: 184.5812 Epoch 24/100 13/13 - 0s - loss: 173.4126 Epoch 25/100 13/13 - 0s - loss: 169.0896 Epoch 26/100 13/13 - 0s - loss: 158.8616 Epoch 27/100 13/13 - 0s - loss: 147.4314 Epoch 28/100 13/13 - 0s - loss: 140.2026 Epoch 29/100 13/13 - 0s - loss: 133.2955 Epoch 30/100 13/13 - 0s - loss: 128.1834 Epoch 31/100 13/13 - 0s - loss: 120.1640 Epoch 32/100 13/13 - 0s - loss: 114.6829 Epoch 33/100 13/13 - 0s - loss: 109.8683 Epoch 34/100 13/13 - 0s - loss: 105.6856 Epoch 35/100 13/13 - 0s - loss: 100.0182 Epoch 36/100 13/13 - 0s - loss: 93.3371 Epoch 37/100 13/13 - 0s - loss: 91.3530 Epoch 38/100 13/13 - 0s - loss: 85.7752 Epoch 39/100 13/13 - 0s - loss: 82.4184 Epoch 40/100 13/13 - 0s - loss: 76.0113 Epoch 41/100 13/13 - 0s - loss: 73.7212 Epoch 42/100 13/13 - 0s - loss: 70.6216 Epoch 43/100 13/13 - 0s - loss: 67.4598 Epoch 44/100 13/13 - 0s - loss: 64.0807 Epoch 45/100 13/13 - 0s - loss: 62.0253 Epoch 46/100 13/13 - 0s - loss: 59.2131 Epoch 47/100 13/13 - 0s - loss: 56.5928 Epoch 48/100 13/13 - 0s - loss: 56.3038 Epoch 49/100 13/13 - 0s - loss: 52.7268 Epoch 50/100 13/13 - 0s - loss: 50.3988 Epoch 51/100 13/13 - 0s - loss: 49.0474 Epoch 52/100 13/13 - 0s - loss: 47.9957 Epoch 53/100 13/13 - 0s - loss: 47.7353 Epoch 54/100 13/13 - 0s - loss: 44.7161 Epoch 55/100 13/13 - 0s - loss: 44.8755 Epoch 56/100 13/13 - 0s - loss: 43.3957 Epoch 57/100 13/13 - 0s - loss: 42.0779 Epoch 58/100 13/13 - 0s - loss: 41.9396 Epoch 59/100 13/13 - 0s - loss: 40.3681 Epoch 60/100 13/13 - 0s - loss: 41.1852 Epoch 61/100 13/13 - 0s - loss: 39.7117 Epoch 62/100 13/13 - 0s - loss: 41.0216 Epoch 63/100 13/13 - 0s - loss: 36.1499 Epoch 64/100 13/13 - 0s - loss: 35.3468 Epoch 65/100 13/13 - 0s - loss: 35.1042 Epoch 66/100 13/13 - 0s - loss: 34.2546 Epoch 67/100 13/13 - 0s - loss: 33.6177 Epoch 68/100 13/13 - 0s - loss: 32.9660 Epoch 69/100 13/13 - 0s - loss: 32.6077 Epoch 70/100 13/13 - 0s - loss: 32.2787 Epoch 71/100 13/13 - 0s - loss: 31.4921 Epoch 72/100 13/13 - 0s - loss: 30.9388 Epoch 73/100 13/13 - 0s - loss: 30.7971 Epoch 74/100 13/13 - 0s - loss: 30.1166 Epoch 75/100 13/13 - 0s - loss: 30.3033 Epoch 76/100 13/13 - 0s - loss: 29.7823 Epoch 77/100 13/13 - 0s - loss: 29.0691 Epoch 78/100 13/13 - 0s - loss: 29.2846 Epoch 79/100 13/13 - 0s - loss: 28.1004 Epoch 80/100 13/13 - 0s - loss: 28.0843 Epoch 81/100 13/13 - 0s - loss: 27.5363 Epoch 82/100 13/13 - 0s - loss: 27.3842 Epoch 83/100 13/13 - 0s - loss: 26.8047 Epoch 84/100 13/13 - 0s - loss: 26.2902 Epoch 85/100 13/13 - 0s - loss: 26.4791 Epoch 86/100 13/13 - 0s - loss: 25.9914 Epoch 87/100 13/13 - 0s - loss: 26.4426 Epoch 88/100 13/13 - 0s - loss: 25.3650 Epoch 89/100 13/13 - 0s - loss: 24.7668 Epoch 90/100 13/13 - 0s - loss: 25.0466 Epoch 91/100 13/13 - 0s - loss: 23.9254 Epoch 92/100 13/13 - 0s - loss: 24.7530 Epoch 93/100 13/13 - 0s - loss: 24.3067 Epoch 94/100 13/13 - 0s - loss: 25.2663 Epoch 95/100 13/13 - 0s - loss: 24.8965 Epoch 96/100 13/13 - 0s - loss: 24.3817 Epoch 97/100 13/13 - 0s - loss: 22.7545 Epoch 98/100 13/13 - 0s - loss: 23.2711 Epoch 99/100 13/13 - 0s - loss: 22.9325 Epoch 100/100 13/13 - 0s - loss: 22.9305 . &lt;tensorflow.python.keras.callbacks.History at 0x7efe002f9fd0&gt; . pred = model.predict(x) print(f&quot;Shape: {pred.shape}&quot;) print(pred[0:10]) . Shape: (398, 1) [[14.227755] [13.352877] [13.366304] [14.042093] [13.76317 ] [12.621584] [11.604197] [11.754587] [12.065157] [11.800211]] . # Measure RMSE error. RMSE is common for regression. score = np.sqrt(metrics.mean_squared_error(pred,y)) print(f&quot;Final score (RMSE): {score}&quot;) . Final score (RMSE): 4.71485252486946 . # Sample predictions for i in range(10): print(f&quot;{i+1}. Car name: {cars[i]}, MPG: {y[i]}, &quot; + &quot;predicted MPG: {pred[i]}&quot;) . 1. Car name: chevrolet chevelle malibu, MPG: 18.0, predicted MPG: {pred[i]} 2. Car name: buick skylark 320, MPG: 15.0, predicted MPG: {pred[i]} 3. Car name: plymouth satellite, MPG: 18.0, predicted MPG: {pred[i]} 4. Car name: amc rebel sst, MPG: 16.0, predicted MPG: {pred[i]} 5. Car name: ford torino, MPG: 17.0, predicted MPG: {pred[i]} 6. Car name: ford galaxie 500, MPG: 15.0, predicted MPG: {pred[i]} 7. Car name: chevrolet impala, MPG: 14.0, predicted MPG: {pred[i]} 8. Car name: plymouth fury iii, MPG: 14.0, predicted MPG: {pred[i]} 9. Car name: pontiac catalina, MPG: 14.0, predicted MPG: {pred[i]} 10. Car name: amc ambassador dpl, MPG: 15.0, predicted MPG: {pred[i]} . import pandas as pd import io import requests import numpy as np from sklearn import metrics from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from tensorflow.keras.callbacks import EarlyStopping df = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/iris.csv&quot;, na_values=[&#39;NA&#39;, &#39;?&#39;]) # Convert to numpy - Classification x = df[[&#39;sepal_l&#39;, &#39;sepal_w&#39;, &#39;petal_l&#39;, &#39;petal_w&#39;]].values dummies = pd.get_dummies(df[&#39;species&#39;]) # Classification species = dummies.columns y = dummies.values # Build neural network model = Sequential() model.add(Dense(50, input_dim=x.shape[1], activation=&#39;relu&#39;)) # Hidden 1 model.add(Dense(25, activation=&#39;relu&#39;)) # Hidden 2 model.add(Dense(y.shape[1],activation=&#39;softmax&#39;)) # Output model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) model.fit(x,y,verbose=2,epochs=100) . Epoch 1/100 5/5 - 0s - loss: 1.2044 Epoch 2/100 5/5 - 0s - loss: 1.0163 Epoch 3/100 5/5 - 0s - loss: 0.9398 Epoch 4/100 5/5 - 0s - loss: 0.9153 Epoch 5/100 5/5 - 0s - loss: 0.8853 Epoch 6/100 5/5 - 0s - loss: 0.8544 Epoch 7/100 5/5 - 0s - loss: 0.8274 Epoch 8/100 5/5 - 0s - loss: 0.8119 Epoch 9/100 5/5 - 0s - loss: 0.7936 Epoch 10/100 5/5 - 0s - loss: 0.7705 Epoch 11/100 5/5 - 0s - loss: 0.7500 Epoch 12/100 5/5 - 0s - loss: 0.7247 Epoch 13/100 5/5 - 0s - loss: 0.7005 Epoch 14/100 5/5 - 0s - loss: 0.6765 Epoch 15/100 5/5 - 0s - loss: 0.6503 Epoch 16/100 5/5 - 0s - loss: 0.6312 Epoch 17/100 5/5 - 0s - loss: 0.6034 Epoch 18/100 5/5 - 0s - loss: 0.5799 Epoch 19/100 5/5 - 0s - loss: 0.5612 Epoch 20/100 5/5 - 0s - loss: 0.5416 Epoch 21/100 5/5 - 0s - loss: 0.5222 Epoch 22/100 5/5 - 0s - loss: 0.5048 Epoch 23/100 5/5 - 0s - loss: 0.4886 Epoch 24/100 5/5 - 0s - loss: 0.4717 Epoch 25/100 5/5 - 0s - loss: 0.4600 Epoch 26/100 5/5 - 0s - loss: 0.4461 Epoch 27/100 5/5 - 0s - loss: 0.4323 Epoch 28/100 5/5 - 0s - loss: 0.4218 Epoch 29/100 5/5 - 0s - loss: 0.4096 Epoch 30/100 5/5 - 0s - loss: 0.3990 Epoch 31/100 5/5 - 0s - loss: 0.3883 Epoch 32/100 5/5 - 0s - loss: 0.3787 Epoch 33/100 5/5 - 0s - loss: 0.3684 Epoch 34/100 5/5 - 0s - loss: 0.3594 Epoch 35/100 5/5 - 0s - loss: 0.3510 Epoch 36/100 5/5 - 0s - loss: 0.3413 Epoch 37/100 5/5 - 0s - loss: 0.3341 Epoch 38/100 5/5 - 0s - loss: 0.3243 Epoch 39/100 5/5 - 0s - loss: 0.3164 Epoch 40/100 5/5 - 0s - loss: 0.3128 Epoch 41/100 5/5 - 0s - loss: 0.3011 Epoch 42/100 5/5 - 0s - loss: 0.2978 Epoch 43/100 5/5 - 0s - loss: 0.2863 Epoch 44/100 5/5 - 0s - loss: 0.2832 Epoch 45/100 5/5 - 0s - loss: 0.2735 Epoch 46/100 5/5 - 0s - loss: 0.2705 Epoch 47/100 5/5 - 0s - loss: 0.2627 Epoch 48/100 5/5 - 0s - loss: 0.2563 Epoch 49/100 5/5 - 0s - loss: 0.2499 Epoch 50/100 5/5 - 0s - loss: 0.2448 Epoch 51/100 5/5 - 0s - loss: 0.2381 Epoch 52/100 5/5 - 0s - loss: 0.2348 Epoch 53/100 5/5 - 0s - loss: 0.2301 Epoch 54/100 5/5 - 0s - loss: 0.2236 Epoch 55/100 5/5 - 0s - loss: 0.2187 Epoch 56/100 5/5 - 0s - loss: 0.2144 Epoch 57/100 5/5 - 0s - loss: 0.2082 Epoch 58/100 5/5 - 0s - loss: 0.2033 Epoch 59/100 5/5 - 0s - loss: 0.1978 Epoch 60/100 5/5 - 0s - loss: 0.1949 Epoch 61/100 5/5 - 0s - loss: 0.1858 Epoch 62/100 5/5 - 0s - loss: 0.1832 Epoch 63/100 5/5 - 0s - loss: 0.1779 Epoch 64/100 5/5 - 0s - loss: 0.1736 Epoch 65/100 5/5 - 0s - loss: 0.1689 Epoch 66/100 5/5 - 0s - loss: 0.1657 Epoch 67/100 5/5 - 0s - loss: 0.1655 Epoch 68/100 5/5 - 0s - loss: 0.1607 Epoch 69/100 5/5 - 0s - loss: 0.1587 Epoch 70/100 5/5 - 0s - loss: 0.1523 Epoch 71/100 5/5 - 0s - loss: 0.1498 Epoch 72/100 5/5 - 0s - loss: 0.1473 Epoch 73/100 5/5 - 0s - loss: 0.1452 Epoch 74/100 5/5 - 0s - loss: 0.1455 Epoch 75/100 5/5 - 0s - loss: 0.1390 Epoch 76/100 5/5 - 0s - loss: 0.1415 Epoch 77/100 5/5 - 0s - loss: 0.1332 Epoch 78/100 5/5 - 0s - loss: 0.1379 Epoch 79/100 5/5 - 0s - loss: 0.1297 Epoch 80/100 5/5 - 0s - loss: 0.1287 Epoch 81/100 5/5 - 0s - loss: 0.1269 Epoch 82/100 5/5 - 0s - loss: 0.1243 Epoch 83/100 5/5 - 0s - loss: 0.1234 Epoch 84/100 5/5 - 0s - loss: 0.1204 Epoch 85/100 5/5 - 0s - loss: 0.1196 Epoch 86/100 5/5 - 0s - loss: 0.1177 Epoch 87/100 5/5 - 0s - loss: 0.1153 Epoch 88/100 5/5 - 0s - loss: 0.1154 Epoch 89/100 5/5 - 0s - loss: 0.1138 Epoch 90/100 5/5 - 0s - loss: 0.1118 Epoch 91/100 5/5 - 0s - loss: 0.1101 Epoch 92/100 5/5 - 0s - loss: 0.1094 Epoch 93/100 5/5 - 0s - loss: 0.1087 Epoch 94/100 5/5 - 0s - loss: 0.1057 Epoch 95/100 5/5 - 0s - loss: 0.1066 Epoch 96/100 5/5 - 0s - loss: 0.1035 Epoch 97/100 5/5 - 0s - loss: 0.1027 Epoch 98/100 5/5 - 0s - loss: 0.1037 Epoch 99/100 5/5 - 0s - loss: 0.1016 Epoch 100/100 5/5 - 0s - loss: 0.1036 . &lt;tensorflow.python.keras.callbacks.History at 0x7efe41398b00&gt; . # Print out number of species found: print(species) . Index([&#39;Iris-setosa&#39;, &#39;Iris-versicolor&#39;, &#39;Iris-virginica&#39;], dtype=&#39;object&#39;) . pred = model.predict(x) print(f&quot;Shape: {pred.shape}&quot;) print(pred[0:10]) . Shape: (150, 3) [[9.97697055e-01 2.15794984e-03 1.44956546e-04] [9.94137406e-01 5.50468592e-03 3.57910059e-04] [9.96191502e-01 3.52777750e-03 2.80687527e-04] [9.93117690e-01 6.42941520e-03 4.52894397e-04] [9.97973382e-01 1.89066969e-03 1.36019531e-04] [9.97510195e-01 2.37838831e-03 1.11383706e-04] [9.95967269e-01 3.74595844e-03 2.86734197e-04] [9.96544778e-01 3.24807805e-03 2.07129444e-04] [9.91251111e-01 8.11220054e-03 6.36695651e-04] [9.94891763e-01 4.79313312e-03 3.15106096e-04]] . np.set_printoptions(suppress=True) . print(y[0:10]) . [[1 0 0] [1 0 0] [1 0 0] [1 0 0] [1 0 0] [1 0 0] [1 0 0] [1 0 0] [1 0 0] [1 0 0]] . predict_classes = np.argmax(pred,axis=1) expected_classes = np.argmax(y,axis=1) print(f&quot;Predictions: {predict_classes}&quot;) print(f&quot;Expected: {expected_classes}&quot;) . Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] Expected: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] . print(species[predict_classes[1:10]]) . Index([&#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;, &#39;Iris-setosa&#39;], dtype=&#39;object&#39;) . from sklearn.metrics import accuracy_score correct = accuracy_score(expected_classes,predict_classes) print(f&quot;Accuracy: {correct}&quot;) . Accuracy: 0.98 . sample_flower = np.array( [[5.0,3.0,4.0,2.0]], dtype=float) pred = model.predict(sample_flower) print(pred) pred = np.argmax(pred) print(f&quot;Predict that {sample_flower} is: {species[pred]}&quot;) . [[0.00188068 0.5159181 0.48220128]] Predict that [[5. 3. 4. 2.]] is: Iris-versicolor . sample_flower = np.array( [[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]], dtype=float) pred = model.predict(sample_flower) print(pred) pred = np.argmax(pred,axis=1) print(f&quot;Predict that these two flowers {sample_flower} &quot;) print(f&quot;are: {species[pred]}&quot;) . [[0.00188067 0.51591814 0.4822012 ] [0.9928797 0.00682994 0.0002904 ]] Predict that these two flowers [[5. 3. 4. 2. ] [5.2 3.5 1.5 0.8]] are: Index([&#39;Iris-versicolor&#39;, &#39;Iris-setosa&#39;], dtype=&#39;object&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/12/Tensorflow_and_Keras.html",
            "relUrl": "/2020/11/12/Tensorflow_and_Keras.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Pytorch",
            "content": ". Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import torch import torch.nn as nn import torch.nn.functional as F import numpy as np from torch.autograd import Variable from sklearn import preprocessing class Net(nn.Module): def __init__(self, in_count, out_count): super(Net, self).__init__() self.fc1 = nn.Linear(in_count, 50) self.fc2 = nn.Linear(50, 25) self.fc3 = nn.Linear(25, out_count) self.softmax = nn.Softmax(dim=1) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) x = self.fc3(x) return self.softmax(x) df = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/iris.csv&quot;, na_values=[&#39;NA&#39;, &#39;?&#39;]) le = preprocessing.LabelEncoder() x = df[[&#39;sepal_l&#39;, &#39;sepal_w&#39;, &#39;petal_l&#39;, &#39;petal_w&#39;]].values y = le.fit_transform(df[&#39;species&#39;]) classes = le.classes_ x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=42) x_train = Variable(torch.Tensor(x_train).float()) x_test = Variable(torch.Tensor(x_test).float()) y_train = Variable(torch.Tensor(y_train).long()) y_test = Variable(torch.Tensor(y_test).long()) net = Net(x.shape[1],len(classes)) criterion = nn.CrossEntropyLoss()# cross entropy loss #optimizer = torch.optim.SGD(net.parameters(), lr=0.01) optimizer = torch.optim.Adam(net.parameters(), lr=0.01) for epoch in range(1000): optimizer.zero_grad() out = net(x_train) loss = criterion(out, y_train) loss.backward() optimizer.step() if epoch % 100 == 0: print(f&quot;Epoch {epoch}, loss: {loss.item()}&quot;) pred_prob = net(x_test) _, pred = torch.max(pred_prob, 1) . Epoch 0, loss: 1.0931435823440552 Epoch 100, loss: 0.5726262331008911 Epoch 200, loss: 0.5706601142883301 Epoch 300, loss: 0.5699276328086853 Epoch 400, loss: 0.5696418881416321 Epoch 500, loss: 0.5695095062255859 Epoch 600, loss: 0.5694397687911987 Epoch 700, loss: 0.5693992376327515 Epoch 800, loss: 0.5693740844726562 Epoch 900, loss: 0.5693574547767639 . correct = accuracy_score(y_test,pred) print(f&quot;Accuracy: {correct}&quot;) . Accuracy: 0.9736842105263158 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/11/Pytorch.html",
            "relUrl": "/2020/11/11/Pytorch.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Using Prophet for S&P 500 forcasting",
            "content": "## Prophet docs can be found here: https://facebook.github.io/prophet/docs/quick_start.html . ## This notebooks is published here: https://deepnote.com/publish/e870766e-a2ca-4167-99b6-3cb070a87c2e . import pandas as pd import numpy as np import datetime import quandl import statsmodels.api as sm from fbprophet import Prophet import matplotlib.pyplot as plt . start = datetime.datetime(1960, 1, 1) end = pd.to_datetime(&#39;today&#39;) SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . ... ... | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3365.52 | . 2020-09-30 3363.00 | . 2020-10-01 3374.70 | . 740 rows × 1 columns . df = SP500 df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 740 entries, 1960-01-01 to 2020-10-01 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 Value 740 non-null float64 dtypes: float64(1) memory usage: 11.6 KB . df[&#39;ds&#39;] = df.index df[&#39;y&#39;] = df.Value . df . Value ds y . Date . 1960-01-01 58.03 | 1960-01-01 | 58.03 | . 1960-02-01 55.78 | 1960-02-01 | 55.78 | . 1960-03-01 55.02 | 1960-03-01 | 55.02 | . 1960-04-01 55.73 | 1960-04-01 | 55.73 | . 1960-05-01 55.22 | 1960-05-01 | 55.22 | . ... ... | ... | ... | . 2020-08-01 3391.71 | 2020-08-01 | 3391.71 | . 2020-08-31 3500.31 | 2020-08-31 | 3500.31 | . 2020-09-01 3365.52 | 2020-09-01 | 3365.52 | . 2020-09-30 3363.00 | 2020-09-30 | 3363.00 | . 2020-10-01 3374.70 | 2020-10-01 | 3374.70 | . 740 rows × 3 columns . m = Prophet() m.fit(df) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fac050e3250&gt; . future = m.make_future_dataframe(periods=365) future.tail() . ds . 1100 2021-09-27 | . 1101 2021-09-28 | . 1102 2021-09-29 | . 1103 2021-09-30 | . 1104 2021-10-01 | . forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() . ds yhat yhat_lower yhat_upper . 1100 2021-09-27 | 3206.607077 | 3048.843968 | 3356.838889 | . 1101 2021-09-28 | 3203.268280 | 3056.734891 | 3342.524510 | . 1102 2021-09-29 | 3216.464967 | 3060.886373 | 3365.919004 | . 1103 2021-09-30 | 3212.666864 | 3051.082913 | 3362.695987 | . 1104 2021-10-01 | 3219.847641 | 3063.628335 | 3370.696458 | . fig1 = m.plot(forecast) . fig2 = m.plot_components(forecast) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/11/Prophet-Time-Series.html",
            "relUrl": "/2020/11/11/Prophet-Time-Series.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Dask API for analytics",
            "content": "Credit: code from https://github.com/coiled/data-science-at-scale . from dask.distributed import Client client = Client(n_workers=4) client . /opt/venv/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use. Perhaps you already have a cluster running? Hosting the HTTP server on port 45123 instead http_address[&#34;port&#34;], self.http_server.port . Client . Scheduler: tcp://127.0.0.1:43829 | Dashboard: http://127.0.0.1:45123/status | . | Cluster . Workers: 4 | Cores: 4 | Memory: 5.00 GB | . | . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df import dask.dataframe as dd import aiohttp ddf = dd.read_csv( url, blocksize=&quot;10 MiB&quot;, ).persist() . ddf . Dask DataFrame Structure: Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . npartitions=1 . int64 | object | float64 | float64 | int64 | float64 | int64 | float64 | float64 | float64 | object | object | int64 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: read-csv, 1 tasks # See that we actually have a collection of Pandas DataFrames ddf.map_partitions(type).compute() . 0 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; dtype: object . # View head of Dask DataFrame ddf.head() . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . gdp = ddf.groupby(&#39;province&#39;).gdp.mean() gdp.compute() . province Anhui 3905.870000 Beijing 4673.453333 Chongqing 2477.712500 Fujian 4864.023333 Gansu 1397.832500 Guangdong 15358.781667 Guangxi 2924.104167 Guizhou 1422.010833 Hainan 686.714167 Hebei 6936.825000 Heilongjiang 4041.241667 Henan 7208.966667 Hubei 4772.503333 Hunan 4765.891667 Jiangsu 10761.846667 Jiangxi 2460.782500 Jilin 2274.854167 Liaoning 5231.135000 Ningxia 432.268333 Qinghai 383.099167 Shaanxi 2658.034167 Shandong 12324.002500 Shanghai 6432.454167 Shanxi 2817.210833 Sichuan 5377.790000 Tianjin 2528.665000 Tibet 170.426667 Xinjiang 1828.896667 Yunnan 2604.054167 Zhejiang 9138.151667 Name: gdp, dtype: float64 . gdp.compute().sort_values() . province Tibet 170.426667 Qinghai 383.099167 Ningxia 432.268333 Hainan 686.714167 Gansu 1397.832500 Guizhou 1422.010833 Xinjiang 1828.896667 Jilin 2274.854167 Jiangxi 2460.782500 Chongqing 2477.712500 Tianjin 2528.665000 Yunnan 2604.054167 Shaanxi 2658.034167 Shanxi 2817.210833 Guangxi 2924.104167 Anhui 3905.870000 Heilongjiang 4041.241667 Beijing 4673.453333 Hunan 4765.891667 Hubei 4772.503333 Fujian 4864.023333 Liaoning 5231.135000 Sichuan 5377.790000 Shanghai 6432.454167 Hebei 6936.825000 Henan 7208.966667 Zhejiang 9138.151667 Jiangsu 10761.846667 Shandong 12324.002500 Guangdong 15358.781667 Name: gdp, dtype: float64 . ddf[ddf.reg.str.contains(&#39;East China&#39;)].head() . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . ec = ddf[ddf.reg.str.contains(&#39;East China&#39;)] mean_gdp_prov = ec.groupby(&#39;province&#39;).gdp.agg([&#39;mean&#39;,&#39;count&#39;]) mean_gdp_prov.compute() . mean count . province . Anhui 3905.870000 | 12 | . Fujian 4864.023333 | 12 | . Jiangsu 10761.846667 | 12 | . Jiangxi 2460.782500 | 12 | . Shandong 12324.002500 | 12 | . Shanghai 6432.454167 | 12 | . Zhejiang 9138.151667 | 12 | . mean_gdp_prov.nlargest(5, &#39;mean&#39;).compute() . mean count . province . Shandong 12324.002500 | 12 | . Jiangsu 10761.846667 | 12 | . Zhejiang 9138.151667 | 12 | . Shanghai 6432.454167 | 12 | . Fujian 4864.023333 | 12 | . mean_gdp_prov.to_csv(&#39;mean_gdp-*.csv&#39;) #the * is where the partition number will go . [&#39;/home/jovyan/work/mean_gdp-0.csv&#39;] . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/10/Dask-api-eda.html",
            "relUrl": "/2020/11/10/Dask-api-eda.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Using Dask and dask-sql",
            "content": "Credit: code from https://github.com/nils-braun/dask-sql and https://coiled.io/blog/getting-started-with-dask-and-sql/ . Dask SQL docs https://dask-sql.readthedocs.io/ . dask-sql . import numpy as np import pandas as pd . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.000000 | 0.000000 | 0.000000 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.000000 | 0.000000 | 0.000000 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.000000 | 0.000000 | 0.000000 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.000000 | 0.000000 | 0.000000 | 1601508 | East China | 1499110 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 355 | Zhejiang | 391292.0 | 260313.0 | 2003 | 9705.02 | 498055 | 1.214286 | 0.035714 | 0.035714 | 6217715 | East China | 2261631 | . 356 356 | Zhejiang | 656175.0 | 276652.0 | 2004 | 11648.70 | 668128 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 3162299 | . 357 357 | Zhejiang | 656175.0 | NaN | 2005 | 13417.68 | 772000 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 2370200 | . 358 358 | Zhejiang | 1017303.0 | 394795.0 | 2006 | 15718.47 | 888935 | 1.214286 | 0.035714 | 0.035714 | 11537149 | East China | 2553268 | . 359 359 | Zhejiang | 844647.0 | 0.0 | 2007 | 18753.73 | 1036576 | 0.047619 | 0.000000 | 0.000000 | 16494981 | East China | 2939778 | . 360 rows × 13 columns . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . ddf . Dask DataFrame Structure: Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . npartitions=5 . 0 int64 | object | float64 | float64 | int64 | float64 | int64 | float64 | float64 | float64 | object | object | int64 | . 72 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/21359/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . # client.restart() . Client . Scheduler: inproc://192.168.1.71/21359/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . ddf.groupby(&quot;province&quot;).gdp.mean().compute() . province Anhui 3905.870000 Beijing 4673.453333 Chongqing 2477.712500 Fujian 4864.023333 Gansu 1397.832500 Guangdong 15358.781667 Guangxi 2924.104167 Guizhou 1422.010833 Hainan 686.714167 Hebei 6936.825000 Heilongjiang 4041.241667 Henan 7208.966667 Hubei 4772.503333 Hunan 4765.891667 Jiangsu 10761.846667 Jiangxi 2460.782500 Jilin 2274.854167 Liaoning 5231.135000 Ningxia 432.268333 Qinghai 383.099167 Shaanxi 2658.034167 Shandong 12324.002500 Shanghai 6432.454167 Shanxi 2817.210833 Sichuan 5377.790000 Tianjin 2528.665000 Tibet 170.426667 Xinjiang 1828.896667 Yunnan 2604.054167 Zhejiang 9138.151667 Name: gdp, dtype: float64 . from dask_sql import Context c = Context() . ddf . Dask DataFrame Structure: Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . npartitions=5 . 0 int64 | object | float64 | float64 | int64 | float64 | int64 | float64 | float64 | float64 | object | object | int64 | . 72 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks c.register_dask_table(ddf, &quot;fiscal&quot;) . result = c.sql(&#39;SELECT count(1) FROM fiscal&#39;) . result . Dask DataFrame Structure: COUNT(1) . npartitions=1 . int64 | . ... | . Dask Name: getitem, 22 tasks result.compute() . COUNT(1) . 0 360 | . result = c.sql(&quot;&quot;&quot; SELECT province, gdp, fdi FROM fiscal AS fiscal &quot;&quot;&quot;) . result.compute() . province gdp fdi . 0 Anhui | 2093.30 | 50661 | . 1 Anhui | 2347.32 | 43443 | . 2 Anhui | 2542.96 | 27673 | . 3 Anhui | 2712.34 | 26131 | . 4 Anhui | 2902.09 | 31847 | . ... ... | ... | ... | . 355 Zhejiang | 9705.02 | 498055 | . 356 Zhejiang | 11648.70 | 668128 | . 357 Zhejiang | 13417.68 | 772000 | . 358 Zhejiang | 15718.47 | 888935 | . 359 Zhejiang | 18753.73 | 1036576 | . 360 rows × 3 columns . print(result.compute()) . province gdp fdi 0 Anhui 2093.30 50661 1 Anhui 2347.32 43443 2 Anhui 2542.96 27673 3 Anhui 2712.34 26131 4 Anhui 2902.09 31847 .. ... ... ... 355 Zhejiang 9705.02 498055 356 Zhejiang 11648.70 668128 357 Zhejiang 13417.68 772000 358 Zhejiang 15718.47 888935 359 Zhejiang 18753.73 1036576 [360 rows x 3 columns] . from dask_sql import Context from dask.datasets import timeseries . print(result.gdp.mean().compute()) . 4428.653416666667 . result . Dask DataFrame Structure: province gdp fdi . npartitions=5 . 0 object | float64 | int64 | . 72 ... | ... | ... | . ... ... | ... | ... | . 288 ... | ... | ... | . 359 ... | ... | ... | . Dask Name: getitem, 30 tasks %%time ddf.groupby(&quot;province&quot;).fdi.mean().compute() . CPU times: user 96.7 ms, sys: 1.99 ms, total: 98.7 ms Wall time: 87.4 ms . province Anhui 7.095308e+04 Beijing 2.573693e+05 Chongqing 4.112783e+04 Fujian 3.744664e+05 Gansu 5.295500e+03 Guangdong 1.194950e+06 Guangxi 5.514783e+04 Guizhou 5.812333e+03 Hainan 6.436600e+04 Hebei 1.322308e+05 Heilongjiang 8.271933e+04 Henan 9.442600e+04 Hubei 1.497132e+05 Hunan 1.321102e+05 Jiangsu 8.736957e+05 Jiangxi 1.037352e+05 Jilin 4.122658e+04 Liaoning 2.859253e+05 Ningxia 3.950417e+03 Qinghai 1.098408e+04 Shaanxi 5.089258e+04 Shandong 5.455843e+05 Shanghai 5.082483e+05 Shanxi 3.862883e+04 Sichuan 6.219717e+04 Tianjin 2.501733e+05 Tibet 8.397500e+02 Xinjiang 4.433083e+03 Yunnan 1.704833e+04 Zhejiang 4.259302e+05 Name: fdi, dtype: float64 . %%time c.sql(&#39;SELECT avg(fdi) FROM fiscal GROUP BY province&#39;).compute() . CPU times: user 206 ms, sys: 5.58 ms, total: 212 ms Wall time: 176 ms . AVG(&quot;fiscal&quot;.&quot;fdi&quot;) . 0 70953 | . 1 257369 | . 2 41127 | . 3 374466 | . 4 5295 | . 5 1194950 | . 6 55147 | . 7 5812 | . 8 64366 | . 9 132230 | . 10 82719 | . 11 94426 | . 12 149713 | . 13 132110 | . 14 873695 | . 15 103735 | . 16 41226 | . 17 285925 | . 18 3950 | . 19 10984 | . 20 50892 | . 21 545584 | . 22 508248 | . 23 38628 | . 24 62197 | . 25 250173 | . 26 839 | . 27 4433 | . 28 17048 | . 29 425930 | . dfp = ddf.persist() . import distributed . cached_tasks = distributed.wait(dfp) print(f&#39;cached {len(cached_tasks[0])} results&#39;) . cached 5 results . c.register_dask_table(dfp, &quot;fiscal_cached&quot;) . result = c.sql(&#39;SELECT count(1) FROM fiscal_cached&#39;) result.compute() . COUNT(1) . 0 360 | . %%time c.sql(&#39;SELECT avg(fdi) FROM fiscal GROUP BY province&#39;).compute() . CPU times: user 208 ms, sys: 12.9 ms, total: 221 ms Wall time: 162 ms . AVG(&quot;fiscal&quot;.&quot;fdi&quot;) . 0 70953 | . 1 257369 | . 2 41127 | . 3 374466 | . 4 5295 | . 5 1194950 | . 6 55147 | . 7 5812 | . 8 64366 | . 9 132230 | . 10 82719 | . 11 94426 | . 12 149713 | . 13 132110 | . 14 873695 | . 15 103735 | . 16 41226 | . 17 285925 | . 18 3950 | . 19 10984 | . 20 50892 | . 21 545584 | . 22 508248 | . 23 38628 | . 24 62197 | . 25 250173 | . 26 839 | . 27 4433 | . 28 17048 | . 29 425930 | . c.sql(&#39;SELECT floor(3.14)&#39;).compute() . FLOOR(3.14) . 0 3.0 | . %%time c.sql(&quot;&quot;&quot; SELECT floor(fdi) AS fdi, avg(gdp) as gdp, count(1) as fiscal_count FROM fiscal_cached WHERE fdi &gt; 50 AND gdp &gt;= 0 GROUP BY floor(fdi) &quot;&quot;&quot;).compute() . distributed.utils_perf - WARNING - full garbage collections took 92% CPU time recently (threshold: 10%) . CPU times: user 356 ms, sys: 20.4 ms, total: 376 ms Wall time: 308 ms . fdi gdp fiscal_count . 0 2000 | 1933.98 | 1 | . 1 2342 | 1399.83 | 1 | . 2 2954 | 2277.35 | 1 | . 3 3539 | 1688.49 | 1 | . 4 3864 | 887.67 | 1 | . ... ... | ... | ... | . 354 527776 | 5252.76 | 1 | . 355 668128 | 11648.70 | 1 | . 356 772000 | 13417.68 | 1 | . 357 888935 | 15718.47 | 1 | . 358 1036576 | 18753.73 | 1 | . 359 rows × 3 columns . And now we can run a query and immediately plot a visualization of the result using Pandas plotting syntax! . c.sql(&quot;&quot;&quot; SELECT floor(fdi) AS fdi, avg(gdp) as gdp FROM fiscal_cached WHERE fdi &gt; 50 AND gdp &gt;= 0 GROUP BY floor(fdi) &quot;&quot;&quot;).compute().plot(x=&#39;fdi&#39;, y=&#39;gdp&#39;) . &lt;AxesSubplot:xlabel=&#39;fdi&#39;&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/09/dask-sql.html",
            "relUrl": "/2020/11/09/dask-sql.html",
            "date": " • Nov 9, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "NLP Example bằng tiếng Việt using StackNetClassifier",
            "content": "Credit: Code and Notebooks from https://github.com/ngxbac/aivivn_phanloaisacthaibinhluan . https://www.aivivn.com/ . import pandas as pd import numpy as np from scipy.sparse import hstack, csr_matrix, vstack from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import StratifiedKFold from sklearn.metrics import f1_score from sklearn.ensemble import * from sklearn.linear_model import * from tqdm import * import wordcloud import matplotlib.pyplot as plt import gc import lightgbm as lgb %matplotlib inline . # Load data train_df = pd.read_csv(&quot;./data/train.csv&quot;) test_df = pd.read_csv(&quot;./data/test.csv&quot;) . train_df.head() . id comment label . 0 train_000000 | Dung dc sp tot cam on nshop Đóng gói sản phẩm... | 0 | . 1 train_000001 | Chất lượng sản phẩm tuyệt vời . Son mịn nhưng... | 0 | . 2 train_000002 | Chất lượng sản phẩm tuyệt vời nhưng k có hộp ... | 0 | . 3 train_000003 | :(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ... | 1 | . 4 train_000004 | Lần trước mình mua áo gió màu hồng rất ok mà đ... | 1 | . test_df.head() . id comment . 0 test_000000 | Chưa dùng thử nên chưa biết | . 1 test_000001 | Không đáng tiềnVì ngay đợt sale nên mới mua n... | . 2 test_000002 | Cám ơn shop. Đóng gói sản phẩm rất đẹp và chắc... | . 3 test_000003 | Vải đẹp.phom oki luôn.quá ưng | . 4 test_000004 | Chuẩn hàng đóng gói đẹp | . df = pd.concat([train_df, test_df], axis=0) # del train_df, test_df # gc.collect() . import emoji def extract_emojis(str): return [c for c in str if c in emoji.UNICODE_EMOJI] . good_df = train_df[train_df[&#39;label&#39;] == 0] good_comment = good_df[&#39;comment&#39;].values good_emoji = [] for c in good_comment: good_emoji += extract_emojis(c) good_emoji = np.unique(np.asarray(good_emoji)) . bad_df = train_df[train_df[&#39;label&#39;] == 1] bad_comment = bad_df[&#39;comment&#39;].values bad_emoji = [] for c in bad_comment: bad_emoji += extract_emojis(c) bad_emoji = np.unique(np.asarray(bad_emoji)) . good_emoji . array([&#39;↖&#39;, &#39;↗&#39;, &#39;☀&#39;, &#39;☺&#39;, &#39;♀&#39;, &#39;♥&#39;, &#39;✌&#39;, &#39;✨&#39;, &#39;❌&#39;, &#39;❣&#39;, &#39;❤&#39;, &#39;⭐&#39;, &#39;🆗&#39;, &#39;🌝&#39;, &#39;🌟&#39;, &#39;🌧&#39;, &#39;🌷&#39;, &#39;🌸&#39;, &#39;🌺&#39;, &#39;🌼&#39;, &#39;🍓&#39;, &#39;🎈&#39;, &#39;🎉&#39;, &#39;🏻&#39;, &#39;🏼&#39;, &#39;🏿&#39;, &#39;🐅&#39;, &#39;🐾&#39;, &#39;👉&#39;, &#39;👌&#39;, &#39;👍&#39;, &#39;👏&#39;, &#39;💋&#39;, &#39;💌&#39;, &#39;💐&#39;, &#39;💓&#39;, &#39;💕&#39;, &#39;💖&#39;, &#39;💗&#39;, &#39;💙&#39;, &#39;💚&#39;, &#39;💛&#39;, &#39;💜&#39;, &#39;💞&#39;, &#39;💟&#39;, &#39;💥&#39;, &#39;💪&#39;, &#39;💮&#39;, &#39;💯&#39;, &#39;💰&#39;, &#39;📑&#39;, &#39;🖤&#39;, &#39;😀&#39;, &#39;😁&#39;, &#39;😂&#39;, &#39;😃&#39;, &#39;😄&#39;, &#39;😅&#39;, &#39;😆&#39;, &#39;😇&#39;, &#39;😉&#39;, &#39;😊&#39;, &#39;😋&#39;, &#39;😌&#39;, &#39;😍&#39;, &#39;😎&#39;, &#39;😑&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😖&#39;, &#39;😗&#39;, &#39;😘&#39;, &#39;😙&#39;, &#39;😚&#39;, &#39;😛&#39;, &#39;😜&#39;, &#39;😝&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😡&#39;, &#39;😢&#39;, &#39;😣&#39;, &#39;😥&#39;, &#39;😩&#39;, &#39;😪&#39;, &#39;😫&#39;, &#39;😬&#39;, &#39;😭&#39;, &#39;😯&#39;, &#39;😰&#39;, &#39;😱&#39;, &#39;😲&#39;, &#39;😳&#39;, &#39;😻&#39;, &#39;😿&#39;, &#39;🙁&#39;, &#39;🙂&#39;, &#39;🙃&#39;, &#39;🙄&#39;, &#39;🙆&#39;, &#39;🙌&#39;, &#39;🤑&#39;, &#39;🤔&#39;, &#39;🤗&#39;, &#39;🤙&#39;, &#39;🤝&#39;, &#39;🤣&#39;, &#39;🤤&#39;, &#39;🤨&#39;, &#39;🤪&#39;, &#39;🤭&#39;], dtype=&#39;&lt;U1&#39;) . # Just remove &quot;sad, bad&quot; emoji :D good_emoji_fix = [ &#39;↖&#39;, &#39;↗&#39;, &#39;☀&#39;, &#39;☺&#39;, &#39;♀&#39;, &#39;♥&#39;, &#39;✌&#39;, &#39;✨&#39;, &#39;❣&#39;, &#39;❤&#39;, &#39;⭐&#39;, &#39;🆗&#39;, &#39;🌝&#39;, &#39;🌟&#39;, &#39;🌧&#39;, &#39;🌷&#39;, &#39;🌸&#39;, &#39;🌺&#39;, &#39;🌼&#39;, &#39;🍓&#39;, &#39;🎈&#39;, &#39;🎉&#39;, &#39;🐅&#39;, &#39;🐾&#39;, &#39;👉&#39;, &#39;👌&#39;, &#39;👍&#39;, &#39;👏&#39;, &#39;💋&#39;, &#39;💌&#39;, &#39;💐&#39;, &#39;💓&#39;, &#39;💕&#39;, &#39;💖&#39;, &#39;💗&#39;, &#39;💙&#39;, &#39;💚&#39;, &#39;💛&#39;, &#39;💜&#39;, &#39;💞&#39;, &#39;💟&#39;, &#39;💥&#39;, &#39;💪&#39;, &#39;💮&#39;, &#39;💯&#39;, &#39;💰&#39;, &#39;📑&#39;, &#39;🖤&#39;, &#39;😀&#39;, &#39;😁&#39;, &#39;😂&#39;, &#39;😃&#39;, &#39;😄&#39;, &#39;😅&#39;, &#39;😆&#39;, &#39;😇&#39;, &#39;😉&#39;, &#39;😊&#39;, &#39;😋&#39;, &#39;😌&#39;, &#39;😍&#39;, &#39;😎&#39;, &#39;😑&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😖&#39;, &#39;😗&#39;, &#39;😘&#39;, &#39;😙&#39;, &#39;😚&#39;, &#39;😛&#39;, &#39;😜&#39;, &#39;😝&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😡&#39;, &#39;😯&#39;, &#39;😰&#39;, &#39;😱&#39;, &#39;😲&#39;, &#39;😳&#39;, &#39;😻&#39;, &#39;🙂&#39;, &#39;🙃&#39;, &#39;🙄&#39;, &#39;🙆&#39;, &#39;🙌&#39;, &#39;🤑&#39;, &#39;🤔&#39;, &#39;🤗&#39;, ] . bad_emoji . array([&#39;☹&#39;, &#39;✋&#39;, &#39;❌&#39;, &#39;❓&#39;, &#39;❤&#39;, &#39;⭐&#39;, &#39;🎃&#39;, &#39;🏻&#39;, &#39;🏼&#39;, &#39;🏿&#39;, &#39;👌&#39;, &#39;👍&#39;, &#39;👎&#39;, &#39;👶&#39;, &#39;💀&#39;, &#39;💋&#39;, &#39;😁&#39;, &#39;😂&#39;, &#39;😈&#39;, &#39;😊&#39;, &#39;😌&#39;, &#39;😏&#39;, &#39;😐&#39;, &#39;😑&#39;, &#39;😒&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😖&#39;, &#39;😚&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😠&#39;, &#39;😡&#39;, &#39;😢&#39;, &#39;😣&#39;, &#39;😤&#39;, &#39;😥&#39;, &#39;😧&#39;, &#39;😩&#39;, &#39;😪&#39;, &#39;😫&#39;, &#39;😬&#39;, &#39;😭&#39;, &#39;😳&#39;, &#39;😵&#39;, &#39;😶&#39;, &#39;🙁&#39;, &#39;🙂&#39;, &#39;🙄&#39;, &#39;🤔&#39;, &#39;🤚&#39;, &#39;🤤&#39;], dtype=&#39;&lt;U1&#39;) . # Just remove &quot;good&quot; emoji :D bad_emoji_fix = [ &#39;☹&#39;, &#39;✋&#39;, &#39;❌&#39;, &#39;❓&#39;, &#39;👎&#39;, &#39;👶&#39;, &#39;💀&#39;, &#39;😐&#39;, &#39;😑&#39;, &#39;😒&#39;, &#39;😓&#39;, &#39;😔&#39;, &#39;😞&#39;, &#39;😟&#39;, &#39;😠&#39;, &#39;😡&#39;, &#39;😢&#39;, &#39;😣&#39;, &#39;😤&#39;, &#39;😥&#39;, &#39;😧&#39;, &#39;😩&#39;, &#39;😪&#39;, &#39;😫&#39;, &#39;😬&#39;, &#39;😭&#39;, &#39;😳&#39;, &#39;😵&#39;, &#39;😶&#39;, &#39;🙁&#39;, &#39;🙄&#39;, &#39;🤔&#39;, ] . def count_good_bad_emoji(row): comment = row[&#39;comment&#39;] n_good_emoji = 0 n_bad_emoji = 0 for c in comment: if c in good_emoji_fix: n_good_emoji += 1 if c in bad_emoji_fix: n_bad_emoji += 1 row[&#39;n_good_emoji&#39;] = n_good_emoji row[&#39;n_bad_emoji&#39;] = n_bad_emoji return row . # Some features df[&#39;comment&#39;] = df[&#39;comment&#39;].astype(str).fillna(&#39; &#39;) df[&#39;comment&#39;] = df[&#39;comment&#39;].str.lower() df[&#39;num_words&#39;] = df[&#39;comment&#39;].apply(lambda s: len(s.split())) df[&#39;num_unique_words&#39;] = df[&#39;comment&#39;].apply(lambda s: len(set(w for w in s.split()))) df[&#39;words_vs_unique&#39;] = df[&#39;num_unique_words&#39;] / df[&#39;num_words&#39;] * 100 df = df.apply(count_good_bad_emoji, axis=1) . df[&#39;good_bad_emoji_ratio&#39;] = df[&#39;n_good_emoji&#39;] / df[&#39;n_bad_emoji&#39;] df[&#39;good_bad_emoji_ratio&#39;] = df[&#39;good_bad_emoji_ratio&#39;].replace(np.nan, 0) df[&#39;good_bad_emoji_ratio&#39;] = df[&#39;good_bad_emoji_ratio&#39;].replace(np.inf, 99) df[&#39;good_bad_emoji_diff&#39;] = df[&#39;n_good_emoji&#39;] - df[&#39;n_bad_emoji&#39;] df[&#39;good_bad_emoji_sum&#39;] = df[&#39;n_good_emoji&#39;] + df[&#39;n_bad_emoji&#39;] . train_df = df[~df[&#39;label&#39;].isnull()] test_df = df[df[&#39;label&#39;].isnull()] train_comments = train_df[&#39;comment&#39;].fillna(&quot;none&quot;).values test_comments = test_df[&#39;comment&#39;].fillna(&quot;none&quot;).values y_train = train_df[&#39;label&#39;].values . train_df.head() . id comment label num_words num_unique_words words_vs_unique n_good_emoji n_bad_emoji good_bad_emoji_ratio good_bad_emoji_diff good_bad_emoji_sum . 0 train_000000 | dung dc sp tot cam on nshop đóng gói sản phẩm... | 0.0 | 22 | 20 | 90.909091 | 0 | 0 | 0.0 | 0 | 0 | . 1 train_000001 | chất lượng sản phẩm tuyệt vời . son mịn nhưng... | 0.0 | 18 | 18 | 100.000000 | 0 | 0 | 0.0 | 0 | 0 | . 2 train_000002 | chất lượng sản phẩm tuyệt vời nhưng k có hộp ... | 0.0 | 18 | 14 | 77.777778 | 0 | 0 | 0.0 | 0 | 0 | . 3 train_000003 | :(( mình hơi thất vọng 1 chút vì mình đã kỳ vọ... | 1.0 | 114 | 91 | 79.824561 | 0 | 0 | 0.0 | 0 | 0 | . 4 train_000004 | lần trước mình mua áo gió màu hồng rất ok mà đ... | 1.0 | 26 | 24 | 92.307692 | 0 | 0 | 0.0 | 0 | 0 | . Tạo feature TFIDF đơn giản . tfidf = TfidfVectorizer( min_df = 5, max_df = 0.8, max_features=10000, sublinear_tf=True ) . X_train_tfidf = tfidf.fit_transform(train_comments) X_test_tfidf = tfidf.transform(test_comments) . EXCLUED_COLS = [&#39;id&#39;, &#39;comment&#39;, &#39;label&#39;] static_cols = [c for c in train_df.columns if not c in EXCLUED_COLS] X_train_static = train_df[static_cols].values X_test_static = test_df[static_cols].values . X_train = hstack([X_train_tfidf, csr_matrix(X_train_static)]).tocsr() X_test = hstack([X_test_tfidf, csr_matrix(X_test_static)]).tocsr() # X_train = X_train_tfidf # X_test = X_test_tfidf . X_train.shape, X_test.shape, y_train.shape . ((16087, 2687), (10981, 2687), (16087,)) . Stacking method . models=[ ######## First level ######## [ RandomForestClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), ExtraTreesClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1), LogisticRegression(random_state=1) ], ######## Second level ######## [ RandomForestClassifier (n_estimators=200, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1) ] ] . from pystacknet.pystacknet import StackNetClassifier model = StackNetClassifier( models, metric=&quot;f1&quot;, folds=5, restacking=False, use_retraining=True, use_proba=True, random_state=12345, n_jobs=1, verbose=1 ) model.fit(X_train, y_train) preds=model.predict_proba(X_test) . ====================== Start of Level 0 ====================== Input Dimensionality 2687 at Level 0 4 models included in Level 0 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 1/5 , model 0 , f1===0.789457 Level 0, fold 1/5 , model 1 , f1===0.813472 Level 0, fold 1/5 , model 2 , f1===0.857641 Level 0, fold 1/5 , model 3 , f1===0.865303 =========== end of fold 1 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 2/5 , model 0 , f1===0.812040 Level 0, fold 2/5 , model 1 , f1===0.824688 Level 0, fold 2/5 , model 2 , f1===0.865074 Level 0, fold 2/5 , model 3 , f1===0.872805 =========== end of fold 2 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 3/5 , model 0 , f1===0.804277 Level 0, fold 3/5 , model 1 , f1===0.821736 Level 0, fold 3/5 , model 2 , f1===0.876429 Level 0, fold 3/5 , model 3 , f1===0.876494 =========== end of fold 3 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 4/5 , model 0 , f1===0.800000 Level 0, fold 4/5 , model 1 , f1===0.825202 Level 0, fold 4/5 , model 2 , f1===0.873005 Level 0, fold 4/5 , model 3 , f1===0.863442 =========== end of fold 4 in level 0 =========== . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Level 0, fold 5/5 , model 0 , f1===0.802153 Level 0, fold 5/5 , model 1 , f1===0.813953 Level 0, fold 5/5 , model 2 , f1===0.861043 Level 0, fold 5/5 , model 3 , f1===0.873807 =========== end of fold 5 in level 0 =========== Level 0, model 0 , f1===0.801585 Level 0, model 1 , f1===0.819810 Level 0, model 2 , f1===0.866638 Level 0, model 3 , f1===0.870370 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Output dimensionality of level 0 is 4 ====================== End of Level 0 ====================== level 0 lasted 102.148064 seconds ====================== Start of Level 1 ====================== Input Dimensionality 4 at Level 1 1 models included in Level 1 Level 1, fold 1/5 , model 0 , f1===0.872932 =========== end of fold 1 in level 1 =========== Level 1, fold 2/5 , model 0 , f1===0.874336 =========== end of fold 2 in level 1 =========== Level 1, fold 3/5 , model 0 , f1===0.883098 =========== end of fold 3 in level 1 =========== Level 1, fold 4/5 , model 0 , f1===0.875176 =========== end of fold 4 in level 1 =========== Level 1, fold 5/5 , model 0 , f1===0.878459 =========== end of fold 5 in level 1 =========== Level 1, model 0 , f1===0.876800 Output dimensionality of level 1 is 1 ====================== End of Level 1 ====================== level 1 lasted 18.519124 seconds ====================== End of fit ====================== fit() lasted 120.668525 seconds ====================== Start of Level 0 ====================== 1 estimators included in Level 0 ====================== Start of Level 1 ====================== 1 estimators included in Level 1 . pred_cls = np.argmax(preds, axis=1) . # submission = pd.read_csv(&quot;./data/sample_submission.csv&quot;) # submission[&#39;label&#39;] = pred_cls . # submission.head() . # submission.to_csv(&quot;stack_demo.csv&quot;, index=False) . Ensemble method . from sklearn.model_selection import cross_val_predict models = [ RandomForestClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), ExtraTreesClassifier (n_estimators=100, criterion=&quot;entropy&quot;, max_depth=5, max_features=0.5, random_state=1), GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1), LogisticRegression(random_state=1) ] . def cross_val_and_predict(clf, X, y, X_test, nfolds): kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42) oof_preds = np.zeros((X.shape[0], 2)) sub_preds = np.zeros((X_test.shape[0], 2)) for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)): X_train, y_train = X[train_idx], y[train_idx] X_valid, y_valid = X[valid_idx], y[valid_idx] clf.fit(X_train, y_train) oof_preds[valid_idx] = clf.predict_proba(X_valid) sub_preds += clf.predict_proba(X_test) / kf.n_splits return oof_preds, sub_preds . sub_preds = [] for clf in models: oof_pred, sub_pred = cross_val_and_predict(clf, X_train, y_train, X_test, nfolds=5) oof_pred_cls = oof_pred.argmax(axis=1) oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train) print(clf.__class__) print(f&quot;F1 CV: {oof_f1}&quot;) sub_preds.append(sub_pred) . &lt;class &#39;sklearn.ensemble._forest.RandomForestClassifier&#39;&gt; F1 CV: 0.8036813709933355 &lt;class &#39;sklearn.ensemble._forest.ExtraTreesClassifier&#39;&gt; F1 CV: 0.8206730456291089 &lt;class &#39;sklearn.ensemble._gb.GradientBoostingClassifier&#39;&gt; F1 CV: 0.864902800196505 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . &lt;class &#39;sklearn.linear_model._logistic.LogisticRegression&#39;&gt; F1 CV: 0.8695652173913043 . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . sub_preds = np.asarray(sub_preds) sub_preds = sub_preds.mean(axis=0) sub_pred_cls = sub_preds.argmax(axis=1) . # submission_ensemble = submission.copy() # submission_ensemble[&#39;label&#39;] = sub_pred_cls # submission_ensemble.to_csv(&quot;ensemble.csv&quot;, index=False) . import pandas as pd import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import StratifiedKFold from sklearn.metrics import f1_score import wordcloud import matplotlib.pyplot as plt import gc import lightgbm as lgb %matplotlib inline # Load data train_df = pd.read_csv(&quot;./data/train.csv&quot;) test_df = pd.read_csv(&quot;./data/test.csv&quot;) train_df.head() test_df.head() train_comments = train_df[&#39;comment&#39;].fillna(&quot;none&quot;).values test_comments = test_df[&#39;comment&#39;].fillna(&quot;none&quot;).values y_train = train_df[&#39;label&#39;].values . # Wordcloud of training set cloud = np.array(train_comments).flatten() plt.figure(figsize=(20,10)) word_cloud = wordcloud.WordCloud( max_words=200,background_color =&quot;black&quot;, width=2000,height=1000,mode=&quot;RGB&quot; ).generate(str(cloud)) plt.axis(&quot;off&quot;) plt.imshow(word_cloud) . &lt;matplotlib.image.AxesImage at 0x7f16aaad1f98&gt; . # Wordcloud of test set cloud = np.array(test_comments).flatten() plt.figure(figsize=(20,10)) word_cloud = wordcloud.WordCloud( max_words=100,background_color =&quot;black&quot;, width=2000,height=1000,mode=&quot;RGB&quot; ).generate(str(cloud)) plt.axis(&quot;off&quot;) plt.imshow(word_cloud) . &lt;matplotlib.image.AxesImage at 0x7f16aabc2e48&gt; . tfidf = TfidfVectorizer( min_df=5, max_df= 0.8, max_features=10000, sublinear_tf=True ) X_train = tfidf.fit_transform(train_comments) X_test = tfidf.transform(test_comments) X_train.shape, X_test.shape, y_train.shape def lgb_f1_score(y_hat, data): y_true = data.get_label() y_hat = np.round(y_hat) # scikits f1 doesn&#39;t like probabilities return &#39;f1&#39;, f1_score(y_true, y_hat), True print(&quot;Starting LightGBM. Train shape: {}, test shape: {}&quot;.format(X_train.shape, X_test.shape)) # Cross validation model folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69) # Create arrays and dataframes to store results oof_preds = np.zeros(X_train.shape[0]) sub_preds = np.zeros(X_test.shape[0]) # k-fold for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)): print(&quot;Fold %s&quot; % (n_fold)) train_x, train_y = X_train[train_idx], y_train[train_idx] valid_x, valid_y = X_train[valid_idx], y_train[valid_idx] # set data structure lgb_train = lgb.Dataset(train_x, label=train_y, free_raw_data=False) lgb_test = lgb.Dataset(valid_x, label=valid_y, free_raw_data=False) params = { &#39;objective&#39; :&#39;binary&#39;, &#39;learning_rate&#39; : 0.01, &#39;num_leaves&#39; : 76, &#39;feature_fraction&#39;: 0.64, &#39;bagging_fraction&#39;: 0.8, &#39;bagging_freq&#39;:1, &#39;boosting_type&#39; : &#39;gbdt&#39;, } reg = lgb.train( params, lgb_train, valid_sets=[lgb_train, lgb_test], valid_names=[&#39;train&#39;, &#39;valid&#39;], num_boost_round=10000, verbose_eval=100, early_stopping_rounds=100, feval=lgb_f1_score ) oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration) sub_preds += reg.predict(X_test, num_iteration=reg.best_iteration) / folds.n_splits del reg, train_x, train_y, valid_x, valid_y gc.collect() threshold = 0.5 preds = (sub_preds &gt; threshold).astype(np.uint8) . Starting LightGBM. Train shape: (16087, 2679), test shape: (10981, 2679) Fold 0 [LightGBM] [Info] Number of positive: 5445, number of negative: 7424 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051310 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 52352 [LightGBM] [Info] Number of data points in the train set: 12869, number of used features: 1137 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.423110 -&gt; initscore=-0.310020 [LightGBM] [Info] Start training from score -0.310020 Training until validation scores don&#39;t improve for 100 rounds [100] train&#39;s binary_logloss: 0.393295 train&#39;s f1: 0.874977 valid&#39;s binary_logloss: 0.410023 valid&#39;s f1: 0.851221 [200] train&#39;s binary_logloss: 0.290724 train&#39;s f1: 0.893252 valid&#39;s binary_logloss: 0.323003 valid&#39;s f1: 0.865039 [300] train&#39;s binary_logloss: 0.238792 train&#39;s f1: 0.90528 valid&#39;s binary_logloss: 0.287577 valid&#39;s f1: 0.872143 [400] train&#39;s binary_logloss: 0.205608 train&#39;s f1: 0.919228 valid&#39;s binary_logloss: 0.27182 valid&#39;s f1: 0.872272 [500] train&#39;s binary_logloss: 0.18123 train&#39;s f1: 0.928584 valid&#39;s binary_logloss: 0.265462 valid&#39;s f1: 0.874282 [600] train&#39;s binary_logloss: 0.162145 train&#39;s f1: 0.937332 valid&#39;s binary_logloss: 0.263343 valid&#39;s f1: 0.87482 Early stopping, best iteration is: [522] train&#39;s binary_logloss: 0.176706 train&#39;s f1: 0.930145 valid&#39;s binary_logloss: 0.264713 valid&#39;s f1: 0.875718 Fold 1 [LightGBM] [Info] Number of positive: 5445, number of negative: 7424 [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037747 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 52365 [LightGBM] [Info] Number of data points in the train set: 12869, number of used features: 1135 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.423110 -&gt; initscore=-0.310020 [LightGBM] [Info] Start training from score -0.310020 Training until validation scores don&#39;t improve for 100 rounds [100] train&#39;s binary_logloss: 0.392321 train&#39;s f1: 0.875253 valid&#39;s binary_logloss: 0.410153 valid&#39;s f1: 0.849244 [200] train&#39;s binary_logloss: 0.290396 train&#39;s f1: 0.892278 valid&#39;s binary_logloss: 0.323353 valid&#39;s f1: 0.861736 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/08/nlp_tieng_viet.html",
            "relUrl": "/2020/11/08/nlp_tieng_viet.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Spacy in Python for Natural Language Processing (NLP) Example",
            "content": "Notebook and Code from https://github.com/jeffheaton/t81_558_deep_learning . import urllib.request import csv import codecs import numpy as np url = &quot;https://data.heatonresearch.com/data/t81-558/datasets/sonnet_18.txt&quot; with urllib.request.urlopen(url) as urlstream: for line in codecs.iterdecode(urlstream, &#39;utf-8&#39;): print(line.rstrip()) . Sonnet 18 original text William Shakespeare Shall I compare thee to a summer&#39;s day? Thou art more lovely and more temperate: Rough winds do shake the darling buds of May, And summer&#39;s lease hath all too short a date: Sometime too hot the eye of heaven shines, And often is his gold complexion dimm&#39;d; And every fair from fair sometime declines, By chance or nature&#39;s changing course untrimm&#39;d; But thy eternal summer shall not fade Nor lose possession of that fair thou owest; Nor shall Death brag thou wander&#39;st in his shade, When in eternal lines to time thou growest: So long as men can breathe or eyes can see, So long lives this and this gives life to thee. . import spacy nlp = spacy.load(&#39;en&#39;) doc = nlp(line.rstrip()) for token in doc: print(token.text) . So long lives this and this gives life to thee . . import spacy nlp = spacy.load(&#39;en&#39;) doc = nlp(u&quot;Apple is looking at buying a U.K. startup for $1 billion&quot;) for token in doc: print(token.text) . Apple is looking at buying a U.K. startup for $ 1 billion . You can also obtain the part of speech for each word. Common parts of speech include nouns, verbs, pronouns, and adjectives. . for word in doc: print(word.text, word.pos_) . Apple PROPN is AUX looking VERB at ADP buying VERB a DET U.K. PROPN startup NOUN for ADP $ SYM 1 NUM billion NUM . Spacy includes functions to check if parts of a sentence appear to be numbers, acronyms, or other entities. . for word in doc: print(f&quot;{word} is like number? {word.like_num}&quot;) . Apple is like number? False is is like number? False looking is like number? False at is like number? False buying is like number? False a is like number? False U.K. is like number? False startup is like number? False for is like number? False $ is like number? False 1 is like number? True billion is like number? True . import spacy from spacy import displacy nlp = spacy.load(&#39;en&#39;) doc = nlp(u&quot;This is a sentance&quot;) displacy.serve(doc, style=&quot;dep&quot;) . /home/gao/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you&#39;re calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you&#39;re already running a local web server, so there&#39;s no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization. &#34;__main__&#34;, mod_spec) . &lt;!DOCTYPE html&gt; displaCy . This DET is AUX a DET sentance NOUN nsubj det attr . Using the &#39;dep&#39; visualizer Serving on http://0.0.0.0:5000 ... Shutting down server on port 5000. . Note, you will have to manually stop the above cell . print(doc) . This is a sentance . The following code shows how to reduce words to their stems. Here the sentence words are reduced to their most basic form. For example, &quot;striped&quot; to &quot;stripe.&quot; . import spacy # Initialize spacy &#39;en&#39; model, keeping only tagger # component needed for lemmatization nlp = spacy.load(&#39;en&#39;, disable=[&#39;parser&#39;, &#39;ner&#39;]) sentence = &quot;The striped bats are hanging on their feet for best&quot; # Parse the sentence using the loaded &#39;en&#39; model object `nlp` doc = nlp(sentence) # Extract the lemma for each token and join &quot; &quot;.join([token.lemma_ for token in doc]) . &#39;the stripe bat be hang on -PRON- foot for good&#39; . from spacy.lang.en.stop_words import STOP_WORDS print(STOP_WORDS) . {&#39;moreover&#39;, &#39;does&#39;, &#39;becomes&#39;, &#39;though&#39;, &#39;done&#39;, &#39;often&#39;, &#39;all&#39;, &#39;next&#39;, &#39;sometime&#39;, &#39;show&#39;, &#39;your&#39;, &#39;forty&#39;, &#39;am&#39;, &#39;on&#39;, &#39;however&#39;, &#39;empty&#39;, &#39;’m&#39;, &#39;again&#39;, &#39;have&#39;, &#39;up&#39;, &#39;six&#39;, &#39;any&#39;, &#39;ours&#39;, &#39;may&#39;, &#39;mine&#39;, &#39;not&#39;, &#39;upon&#39;, &#39;top&#39;, &#39;twenty&#39;, &#39;please&#39;, &#39;latter&#39;, &#39;noone&#39;, &#39;this&#39;, &#39;make&#39;, &#39;former&#39;, &#39;wherein&#39;, &#39;hereupon&#39;, &#39;nevertheless&#39;, &#34;&#39;ll&#34;, &#39;less&#39;, &#39;nowhere&#39;, &#39;side&#39;, &#39;via&#39;, &#39;whatever&#39;, &#39;’s&#39;, &#39;becoming&#39;, &#39;onto&#39;, &#39;by&#39;, &#39;being&#39;, &#39;n‘t&#39;, &#39;should&#39;, &#39;themselves&#39;, &#39;almost&#39;, &#39;rather&#39;, &#39;nor&#39;, &#39;once&#39;, &#39;hence&#39;, &#39;few&#39;, &#39;unless&#39;, &#39;along&#39;, &#39;off&#39;, &#39;everyone&#39;, &#39;put&#39;, &#39;fifty&#39;, &#39;one&#39;, &#39;hereby&#39;, &#39;neither&#39;, &#39;anyhow&#39;, &#39;whom&#39;, &#39;‘ve&#39;, &#39;it&#39;, &#39;give&#39;, &#39;seemed&#39;, &#39;‘s&#39;, &#39;or&#39;, &#39;first&#39;, &#39;is&#39;, &#34;&#39;ve&#34;, &#39;everything&#39;, &#39;per&#39;, &#39;front&#39;, &#39;whose&#39;, &#39;whoever&#39;, &#39;three&#39;, &#39;’re&#39;, &#39;just&#39;, &#39;could&#39;, &#39;beyond&#39;, &#39;none&#39;, &#39;below&#39;, &#39;you&#39;, &#39;thereupon&#39;, &#39;wherever&#39;, &#39;full&#39;, &#39;a&#39;, &#39;whereupon&#39;, &#39;go&#39;, &#39;then&#39;, &#39;although&#39;, &#39;has&#39;, &#39;yet&#39;, &#39;we&#39;, &#39;call&#39;, &#39;something&#39;, &#39;ten&#39;, &#39;using&#39;, &#39;anything&#39;, &#39;until&#39;, &#39;two&#39;, &#39;but&#39;, &#39;‘d&#39;, &#39;now&#39;, &#39;amongst&#39;, &#39;serious&#39;, &#39;if&#39;, &#39;already&#39;, &#39;some&#39;, &#39;me&#39;, &#39;their&#39;, &#39;latterly&#39;, &#39;part&#39;, &#39;further&#39;, &#39;between&#39;, &#39;down&#39;, &#39;get&#39;, &#39;namely&#39;, &#39;more&#39;, &#39;nothing&#39;, &#39;do&#39;, &#39;back&#39;, &#39;anywhere&#39;, &#39;hers&#39;, &#39;become&#39;, &#39;there&#39;, &#39;always&#39;, &#39;eight&#39;, &#39;anyway&#39;, &#39;sixty&#39;, &#39;’ll&#39;, &#39;around&#39;, &#39;alone&#39;, &#39;who&#39;, &#39;move&#39;, &#39;over&#39;, &#39;well&#39;, &#39;yourself&#39;, &#39;in&#39;, &#34;&#39;d&#34;, &#39;else&#39;, &#39;about&#39;, &#39;name&#39;, &#39;without&#39;, &#39;therefore&#39;, &#39;thence&#39;, &#39;anyone&#39;, &#39;‘m&#39;, &#39;least&#39;, &#39;had&#39;, &#34;&#39;m&#34;, &#39;see&#39;, &#39;last&#39;, &#39;beside&#39;, &#39;i&#39;, &#39;cannot&#39;, &#39;re&#39;, &#39;she&#39;, &#39;therein&#39;, &#39;made&#39;, &#39;must&#39;, &#39;own&#39;, &#39;they&#39;, &#39;became&#39;, &#39;are&#39;, &#39;other&#39;, &#39;at&#39;, &#39;someone&#39;, &#39;never&#39;, &#39;while&#39;, &#39;here&#39;, &#39;when&#39;, &#39;meanwhile&#39;, &#39;each&#39;, &#39;ever&#39;, &#39;his&#39;, &#39;five&#39;, &#39;thru&#39;, &#39;somewhere&#39;, &#39;itself&#39;, &#39;what&#39;, &#39;only&#39;, &#39;than&#39;, &#39;very&#39;, &#39;under&#39;, &#39;many&#39;, &#39;whole&#39;, &#39;’d&#39;, &#39;say&#39;, &#39;together&#39;, &#39;most&#39;, &#39;seeming&#39;, &#39;ca&#39;, &#39;where&#39;, &#39;‘ll&#39;, &#39;eleven&#39;, &#39;among&#39;, &#39;our&#39;, &#39;otherwise&#39;, &#39;of&#39;, &#39;out&#39;, &#39;myself&#39;, &#39;keep&#39;, &#39;her&#39;, &#39;might&#39;, &#39;really&#39;, &#39;why&#39;, &#39;an&#39;, &#39;against&#39;, &#39;him&#39;, &#39;thereby&#39;, &#39;were&#39;, &#39;twelve&#39;, &#39;towards&#39;, &#34;n&#39;t&#34;, &#39;can&#39;, &#39;so&#39;, &#39;also&#39;, &#39;whither&#39;, &#39;hundred&#39;, &#39;seems&#39;, &#39;thereafter&#39;, &#39;whereby&#39;, &#39;behind&#39;, &#39;whether&#39;, &#39;ourselves&#39;, &#39;formerly&#39;, &#39;either&#39;, &#39;afterwards&#39;, &#39;its&#39;, &#39;various&#39;, &#39;whereafter&#39;, &#39;mostly&#39;, &#39;doing&#39;, &#39;those&#39;, &#39;to&#39;, &#39;nobody&#39;, &#39;perhaps&#39;, &#39;with&#39;, &#39;too&#39;, &#39;these&#39;, &#39;seem&#39;, &#39;toward&#39;, &#39;third&#39;, &#39;into&#39;, &#39;be&#39;, &#39;bottom&#39;, &#39;the&#39;, &#39;enough&#39;, &#39;amount&#39;, &#39;four&#39;, &#39;regarding&#39;, &#39;which&#39;, &#39;even&#39;, &#39;before&#39;, &#39;them&#39;, &#39;same&#39;, &#39;after&#39;, &#39;that&#39;, &#39;will&#39;, &#39;would&#39;, &#39;hereafter&#39;, &#39;elsewhere&#39;, &#39;through&#39;, &#39;how&#39;, &#39;whence&#39;, &#39;‘re&#39;, &#39;above&#39;, &#39;take&#39;, &#39;indeed&#39;, &#39;whereas&#39;, &#39;from&#39;, &#39;himself&#39;, &#39;did&#39;, &#39;quite&#39;, &#39;herein&#39;, &#39;he&#39;, &#39;yours&#39;, &#39;was&#39;, &#39;because&#39;, &#39;herself&#39;, &#39;us&#39;, &#39;thus&#39;, &#39;during&#39;, &#39;everywhere&#39;, &#39;been&#39;, &#34;&#39;re&#34;, &#39;another&#39;, &#39;no&#39;, &#39;several&#39;, &#39;much&#39;, &#39;due&#39;, &#39;throughout&#39;, &#39;within&#39;, &#39;still&#39;, &#39;except&#39;, &#39;n’t&#39;, &#39;as&#39;, &#39;my&#39;, &#39;whenever&#39;, &#39;fifteen&#39;, &#39;besides&#39;, &#39;sometimes&#39;, &#39;used&#39;, &#39;nine&#39;, &#34;&#39;s&#34;, &#39;across&#39;, &#39;somehow&#39;, &#39;yourselves&#39;, &#39;both&#39;, &#39;others&#39;, &#39;for&#39;, &#39;every&#39;, &#39;such&#39;, &#39;and&#39;, &#39;since&#39;, &#39;beforehand&#39;, &#39;’ve&#39;} .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/07/spacy_example.html",
            "relUrl": "/2020/11/07/spacy_example.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Kaggle Submission Example",
            "content": "Notebook and Code from https://github.com/jeffheaton/t81_558_deep_learning . import os import pandas as pd from sklearn.model_selection import train_test_split import tensorflow as tf import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from tensorflow.keras.callbacks import EarlyStopping df_train = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_iris_train.csv&quot;, na_values=[&#39;NA&#39;,&#39;?&#39;]) # Encode feature vector df_train.drop(&#39;id&#39;, axis=1, inplace=True) num_classes = len(df_train.groupby(&#39;species&#39;).species.nunique()) print(&quot;Number of classes: {}&quot;.format(num_classes)) # Convert to numpy - Classification x = df_train[[&#39;sepal_l&#39;, &#39;sepal_w&#39;, &#39;petal_l&#39;, &#39;petal_w&#39;]].values dummies = pd.get_dummies(df_train[&#39;species&#39;]) # Classification species = dummies.columns y = dummies.values # Split into train/test x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=45) # Train, with early stopping model = Sequential() model.add(Dense(50, input_dim=x.shape[1], activation=&#39;relu&#39;)) model.add(Dense(25)) model.add(Dense(y.shape[1],activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;) monitor = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=1e-3, patience=5, verbose=1, mode=&#39;auto&#39;, restore_best_weights=True) model.fit(x_train,y_train,validation_data=(x_test,y_test), callbacks=[monitor],verbose=0,epochs=1000) . Number of classes: 3 Restoring model weights from the end of the best epoch. Epoch 00055: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x178e5493fc8&gt; . Now that we&#39;ve trained the neural network, we can check its log loss. . from sklearn import metrics # Calculate multi log loss error pred = model.predict(x_test) score = metrics.log_loss(y_test, pred) print(&quot;Log loss score: {}&quot;.format(score)) . Log loss score: 0.3136451941728592 . Now we are ready to generate the Kaggle submission file. We will use the iris test data that does not contain a $y$ target value. It is our job to predict this value and submit to Kaggle. . # Generate Kaggle submit file # Encode feature vector df_test = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_iris_test.csv&quot;, na_values=[&#39;NA&#39;,&#39;?&#39;]) # Convert to numpy - Classification ids = df_test[&#39;id&#39;] df_test.drop(&#39;id&#39;, axis=1, inplace=True) x = df_test[[&#39;sepal_l&#39;, &#39;sepal_w&#39;, &#39;petal_l&#39;, &#39;petal_w&#39;]].values y = dummies.values # Generate predictions pred = model.predict(x) #pred # Create submission data set df_submit = pd.DataFrame(pred) df_submit.insert(0,&#39;id&#39;,ids) df_submit.columns = [&#39;id&#39;,&#39;species-0&#39;,&#39;species-1&#39;,&#39;species-2&#39;] # Write submit file locally df_submit.to_csv(&quot;iris_submit.csv&quot;, index=False) print(df_submit) . id species-0 species-1 species-2 0 100 0.022236 0.533230 0.444534 1 101 0.003699 0.394908 0.601393 2 102 0.004600 0.420394 0.575007 3 103 0.956168 0.040161 0.003672 4 104 0.975333 0.022761 0.001906 5 105 0.966681 0.030938 0.002381 6 106 0.992637 0.007049 0.000314 7 107 0.002810 0.358485 0.638705 8 108 0.026152 0.557480 0.416368 9 109 0.001194 0.350682 0.648124 10 110 0.000649 0.268023 0.731328 11 111 0.994907 0.004923 0.000170 12 112 0.072954 0.587299 0.339747 13 113 0.000571 0.258208 0.741221 14 114 0.977138 0.021400 0.001463 15 115 0.004665 0.449740 0.545596 16 116 0.073553 0.567955 0.358493 17 117 0.968778 0.029240 0.001982 18 118 0.983742 0.015341 0.000918 19 119 0.986016 0.013193 0.000792 20 120 0.023752 0.583601 0.392647 21 121 0.032858 0.584882 0.382260 22 122 0.004007 0.395656 0.600338 23 123 0.000885 0.240763 0.758352 24 124 0.000531 0.271212 0.728256 25 125 0.985742 0.013471 0.000787 26 126 0.001298 0.320333 0.678369 27 127 0.001753 0.342856 0.655391 28 128 0.001147 0.317827 0.681026 29 129 0.981223 0.017589 0.001188 30 130 0.036438 0.578421 0.385140 31 131 0.976528 0.021834 0.001638 32 132 0.003681 0.405441 0.590878 33 133 0.024478 0.539376 0.436146 34 134 0.012039 0.466313 0.521649 35 135 0.963704 0.033453 0.002844 36 136 0.000614 0.244336 0.755050 37 137 0.008160 0.490362 0.501478 38 138 0.976859 0.021646 0.001495 39 139 0.003789 0.317224 0.678987 40 140 0.962254 0.034885 0.002861 41 141 0.000792 0.289380 0.709828 42 142 0.000253 0.239028 0.760719 43 143 0.001390 0.298506 0.700104 44 144 0.968422 0.029224 0.002354 45 145 0.029218 0.524128 0.446654 46 146 0.130497 0.579122 0.290381 47 147 0.023003 0.499443 0.477553 48 148 0.022195 0.527769 0.450036 49 149 0.983695 0.015325 0.000980 50 150 0.942703 0.052154 0.005144 . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from sklearn.model_selection import train_test_split from tensorflow.keras.callbacks import EarlyStopping import pandas as pd import io import os import requests import numpy as np from sklearn import metrics save_path = &quot;.&quot; df = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_auto_train.csv&quot;, na_values=[&#39;NA&#39;, &#39;?&#39;]) cars = df[&#39;name&#39;] # Handle missing value df[&#39;horsepower&#39;] = df[&#39;horsepower&#39;].fillna(df[&#39;horsepower&#39;].median()) # Pandas to Numpy x = df[[&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;]].values y = df[&#39;mpg&#39;].values # regression # Split into train/test x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=42) # Build the neural network model = Sequential() model.add(Dense(25, input_dim=x.shape[1], activation=&#39;relu&#39;)) # Hidden 1 model.add(Dense(10, activation=&#39;relu&#39;)) # Hidden 2 model.add(Dense(1)) # Output model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;) monitor = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=1e-3, patience=5, verbose=1, mode=&#39;auto&#39;, restore_best_weights=True) model.fit(x_train,y_train,validation_data=(x_test,y_test), verbose=2,callbacks=[monitor],epochs=1000) # Predict pred = model.predict(x_test) . Train on 261 samples, validate on 88 samples Epoch 1/1000 261/261 - 0s - loss: 382597.1196 - val_loss: 246687.4858 Epoch 2/1000 261/261 - 0s - loss: 192257.0072 - val_loss: 98804.3558 Epoch 3/1000 261/261 - 0s - loss: 67605.7908 - val_loss: 28617.0703 Epoch 4/1000 261/261 - 0s - loss: 15922.8367 - val_loss: 3325.1682 Epoch 5/1000 261/261 - 0s - loss: 1270.3832 - val_loss: 512.5387 Epoch 6/1000 261/261 - 0s - loss: 1118.9636 - val_loss: 1651.5679 Epoch 7/1000 261/261 - 0s - loss: 1703.0441 - val_loss: 1161.2368 Epoch 8/1000 261/261 - 0s - loss: 900.1420 - val_loss: 452.0660 Epoch 9/1000 261/261 - 0s - loss: 355.7248 - val_loss: 304.3305 Epoch 10/1000 261/261 - 0s - loss: 336.1776 - val_loss: 353.2767 Epoch 11/1000 261/261 - 0s - loss: 364.7770 - val_loss: 337.0882 Epoch 12/1000 261/261 - 0s - loss: 334.1086 - val_loss: 301.5655 Epoch 13/1000 261/261 - 0s - loss: 318.2330 - val_loss: 295.2506 Epoch 14/1000 261/261 - 0s - loss: 315.3628 - val_loss: 294.1454 Epoch 15/1000 261/261 - 0s - loss: 313.4151 - val_loss: 292.0427 Epoch 16/1000 261/261 - 0s - loss: 310.5834 - val_loss: 290.4511 Epoch 17/1000 261/261 - 0s - loss: 308.1132 - val_loss: 289.9176 Epoch 18/1000 261/261 - 0s - loss: 307.3153 - val_loss: 287.1054 Epoch 19/1000 261/261 - 0s - loss: 305.2746 - val_loss: 285.1501 Epoch 20/1000 261/261 - 0s - loss: 303.8164 - val_loss: 283.2582 Epoch 21/1000 261/261 - 0s - loss: 302.2492 - val_loss: 281.4607 Epoch 22/1000 261/261 - 0s - loss: 300.0016 - val_loss: 279.4577 Epoch 23/1000 261/261 - 0s - loss: 296.3905 - val_loss: 279.2795 Epoch 24/1000 261/261 - 0s - loss: 296.2508 - val_loss: 278.0922 Epoch 25/1000 261/261 - 0s - loss: 295.3600 - val_loss: 275.6349 Epoch 26/1000 261/261 - 0s - loss: 291.1920 - val_loss: 271.5592 Epoch 27/1000 261/261 - 0s - loss: 293.0040 - val_loss: 270.6060 Epoch 28/1000 261/261 - 0s - loss: 288.8120 - val_loss: 267.5230 Epoch 29/1000 261/261 - 0s - loss: 285.0153 - val_loss: 267.6846 Epoch 30/1000 261/261 - 0s - loss: 284.5063 - val_loss: 267.5903 Epoch 31/1000 261/261 - 0s - loss: 283.2598 - val_loss: 263.2579 Epoch 32/1000 261/261 - 0s - loss: 279.1897 - val_loss: 259.1413 Epoch 33/1000 261/261 - 0s - loss: 278.0727 - val_loss: 257.1468 Epoch 34/1000 261/261 - 0s - loss: 275.0580 - val_loss: 255.3159 Epoch 35/1000 261/261 - 0s - loss: 275.2246 - val_loss: 257.6078 Epoch 36/1000 261/261 - 0s - loss: 273.1009 - val_loss: 253.1600 Epoch 37/1000 261/261 - 0s - loss: 268.6169 - val_loss: 248.6043 Epoch 38/1000 261/261 - 0s - loss: 266.2035 - val_loss: 246.5989 Epoch 39/1000 261/261 - 0s - loss: 263.9700 - val_loss: 245.5532 Epoch 40/1000 261/261 - 0s - loss: 262.1468 - val_loss: 242.2550 Epoch 41/1000 261/261 - 0s - loss: 259.1994 - val_loss: 239.2889 Epoch 42/1000 261/261 - 0s - loss: 258.9926 - val_loss: 237.0006 Epoch 43/1000 261/261 - 0s - loss: 253.8787 - val_loss: 239.7331 Epoch 44/1000 261/261 - 0s - loss: 255.4787 - val_loss: 234.9061 Epoch 45/1000 261/261 - 0s - loss: 251.2081 - val_loss: 231.0518 Epoch 46/1000 261/261 - 0s - loss: 248.3354 - val_loss: 228.7012 Epoch 47/1000 261/261 - 0s - loss: 246.8801 - val_loss: 225.7509 Epoch 48/1000 261/261 - 0s - loss: 243.6159 - val_loss: 224.8320 Epoch 49/1000 261/261 - 0s - loss: 242.0351 - val_loss: 222.3293 Epoch 50/1000 261/261 - 0s - loss: 240.8072 - val_loss: 218.9842 Epoch 51/1000 261/261 - 0s - loss: 237.3082 - val_loss: 216.6910 Epoch 52/1000 261/261 - 0s - loss: 236.4236 - val_loss: 219.1308 Epoch 53/1000 261/261 - 0s - loss: 233.8834 - val_loss: 213.7722 Epoch 54/1000 261/261 - 0s - loss: 229.9621 - val_loss: 209.7647 Epoch 55/1000 261/261 - 0s - loss: 227.2555 - val_loss: 207.4864 Epoch 56/1000 261/261 - 0s - loss: 226.4306 - val_loss: 204.9454 Epoch 57/1000 261/261 - 0s - loss: 223.0296 - val_loss: 204.7334 Epoch 58/1000 261/261 - 0s - loss: 220.8694 - val_loss: 201.1248 Epoch 59/1000 261/261 - 0s - loss: 217.6376 - val_loss: 197.8849 Epoch 60/1000 261/261 - 0s - loss: 216.9886 - val_loss: 196.0564 Epoch 61/1000 261/261 - 0s - loss: 214.6863 - val_loss: 193.1452 Epoch 62/1000 261/261 - 0s - loss: 210.8178 - val_loss: 190.9064 Epoch 63/1000 261/261 - 0s - loss: 208.5358 - val_loss: 189.0982 Epoch 64/1000 261/261 - 0s - loss: 206.8594 - val_loss: 188.4019 Epoch 65/1000 261/261 - 0s - loss: 204.5793 - val_loss: 184.1434 Epoch 66/1000 261/261 - 0s - loss: 202.2459 - val_loss: 182.0629 Epoch 67/1000 261/261 - 0s - loss: 200.4653 - val_loss: 179.7517 Epoch 68/1000 261/261 - 0s - loss: 199.4847 - val_loss: 181.0924 Epoch 69/1000 261/261 - 0s - loss: 196.1007 - val_loss: 176.6571 Epoch 70/1000 261/261 - 0s - loss: 192.8669 - val_loss: 173.5703 Epoch 71/1000 261/261 - 0s - loss: 192.0731 - val_loss: 171.1448 Epoch 72/1000 261/261 - 0s - loss: 188.9124 - val_loss: 169.1036 Epoch 73/1000 261/261 - 0s - loss: 187.2660 - val_loss: 168.4244 Epoch 74/1000 261/261 - 0s - loss: 184.3366 - val_loss: 164.9515 Epoch 75/1000 261/261 - 0s - loss: 182.0560 - val_loss: 162.9232 Epoch 76/1000 261/261 - 0s - loss: 180.9339 - val_loss: 160.5111 Epoch 77/1000 261/261 - 0s - loss: 177.7289 - val_loss: 160.0768 Epoch 78/1000 261/261 - 0s - loss: 177.0166 - val_loss: 157.8780 Epoch 79/1000 261/261 - 0s - loss: 174.2729 - val_loss: 155.0140 Epoch 80/1000 261/261 - 0s - loss: 174.1473 - val_loss: 152.7101 Epoch 81/1000 261/261 - 0s - loss: 170.2462 - val_loss: 150.7686 Epoch 82/1000 261/261 - 0s - loss: 168.1250 - val_loss: 148.6464 Epoch 83/1000 261/261 - 0s - loss: 165.2611 - val_loss: 147.3025 Epoch 84/1000 261/261 - 0s - loss: 163.6456 - val_loss: 144.6445 Epoch 85/1000 261/261 - 0s - loss: 162.0391 - val_loss: 142.6984 Epoch 86/1000 261/261 - 0s - loss: 159.2869 - val_loss: 142.8578 Epoch 87/1000 261/261 - 0s - loss: 158.4979 - val_loss: 140.0451 Epoch 88/1000 261/261 - 0s - loss: 155.8697 - val_loss: 137.2706 Epoch 89/1000 261/261 - 0s - loss: 153.9711 - val_loss: 135.4351 Epoch 90/1000 261/261 - 0s - loss: 154.6780 - val_loss: 135.3691 Epoch 91/1000 261/261 - 0s - loss: 151.5339 - val_loss: 132.4053 Epoch 92/1000 261/261 - 0s - loss: 149.8378 - val_loss: 129.7334 Epoch 93/1000 261/261 - 0s - loss: 146.4563 - val_loss: 128.3390 Epoch 94/1000 261/261 - 0s - loss: 144.4933 - val_loss: 127.0931 Epoch 95/1000 261/261 - 0s - loss: 142.9235 - val_loss: 124.5410 Epoch 96/1000 261/261 - 0s - loss: 141.2332 - val_loss: 122.6840 Epoch 97/1000 261/261 - 0s - loss: 139.6225 - val_loss: 121.8140 Epoch 98/1000 261/261 - 0s - loss: 137.8158 - val_loss: 119.7630 Epoch 99/1000 261/261 - 0s - loss: 136.0081 - val_loss: 118.2237 Epoch 100/1000 261/261 - 0s - loss: 134.2485 - val_loss: 117.2276 Epoch 101/1000 261/261 - 0s - loss: 132.6553 - val_loss: 114.9724 Epoch 102/1000 261/261 - 0s - loss: 130.9867 - val_loss: 113.3426 Epoch 103/1000 261/261 - 0s - loss: 129.7633 - val_loss: 112.5253 Epoch 104/1000 261/261 - 0s - loss: 127.4988 - val_loss: 109.9802 Epoch 105/1000 261/261 - 0s - loss: 126.5202 - val_loss: 108.6993 Epoch 106/1000 261/261 - 0s - loss: 127.0090 - val_loss: 109.9802 Epoch 107/1000 261/261 - 0s - loss: 123.9040 - val_loss: 105.5228 Epoch 108/1000 261/261 - 0s - loss: 122.4337 - val_loss: 106.0400 Epoch 109/1000 261/261 - 0s - loss: 120.6300 - val_loss: 103.0620 Epoch 110/1000 261/261 - 0s - loss: 118.5036 - val_loss: 101.1414 Epoch 111/1000 261/261 - 0s - loss: 119.0572 - val_loss: 100.2416 Epoch 112/1000 261/261 - 0s - loss: 115.5790 - val_loss: 99.5907 Epoch 113/1000 261/261 - 0s - loss: 114.3071 - val_loss: 96.6901 Epoch 114/1000 261/261 - 0s - loss: 112.3629 - val_loss: 95.6015 Epoch 115/1000 261/261 - 0s - loss: 111.1829 - val_loss: 94.8623 Epoch 116/1000 261/261 - 0s - loss: 110.1737 - val_loss: 92.5723 Epoch 117/1000 261/261 - 0s - loss: 108.3667 - val_loss: 92.2069 Epoch 118/1000 261/261 - 0s - loss: 106.8793 - val_loss: 90.0196 Epoch 119/1000 261/261 - 0s - loss: 111.7453 - val_loss: 89.3325 Epoch 120/1000 261/261 - 0s - loss: 108.2630 - val_loss: 93.4876 Epoch 121/1000 261/261 - 0s - loss: 106.3677 - val_loss: 86.3017 Epoch 122/1000 261/261 - 0s - loss: 101.7241 - val_loss: 85.6503 Epoch 123/1000 261/261 - 0s - loss: 100.5858 - val_loss: 83.9417 Epoch 124/1000 261/261 - 0s - loss: 98.9622 - val_loss: 83.3914 Epoch 125/1000 261/261 - 0s - loss: 97.9784 - val_loss: 81.5708 Epoch 126/1000 261/261 - 0s - loss: 96.6995 - val_loss: 80.4465 Epoch 127/1000 261/261 - 0s - loss: 95.5034 - val_loss: 79.5468 Epoch 128/1000 261/261 - 0s - loss: 93.9933 - val_loss: 78.7416 Epoch 129/1000 261/261 - 0s - loss: 93.2547 - val_loss: 77.2559 Epoch 130/1000 261/261 - 0s - loss: 92.0739 - val_loss: 76.4692 Epoch 131/1000 261/261 - 0s - loss: 91.3897 - val_loss: 75.0902 Epoch 132/1000 261/261 - 0s - loss: 89.5802 - val_loss: 74.2796 Epoch 133/1000 261/261 - 0s - loss: 89.2358 - val_loss: 73.7019 Epoch 134/1000 261/261 - 0s - loss: 89.2894 - val_loss: 71.7912 Epoch 135/1000 261/261 - 0s - loss: 86.9927 - val_loss: 70.9630 Epoch 136/1000 261/261 - 0s - loss: 84.9979 - val_loss: 71.5301 Epoch 137/1000 261/261 - 0s - loss: 85.4751 - val_loss: 69.3716 Epoch 138/1000 261/261 - 0s - loss: 84.5646 - val_loss: 69.2690 Epoch 139/1000 261/261 - 0s - loss: 83.6890 - val_loss: 67.7983 Epoch 140/1000 261/261 - 0s - loss: 80.8676 - val_loss: 66.0073 Epoch 141/1000 261/261 - 0s - loss: 79.7220 - val_loss: 65.3198 Epoch 142/1000 261/261 - 0s - loss: 79.1109 - val_loss: 65.2558 Epoch 143/1000 261/261 - 0s - loss: 78.7909 - val_loss: 63.5800 Epoch 144/1000 261/261 - 0s - loss: 77.2276 - val_loss: 62.5765 Epoch 145/1000 261/261 - 0s - loss: 75.8473 - val_loss: 61.7780 Epoch 146/1000 261/261 - 0s - loss: 74.8493 - val_loss: 60.8583 Epoch 147/1000 261/261 - 0s - loss: 74.0530 - val_loss: 59.8856 Epoch 148/1000 261/261 - 0s - loss: 73.0771 - val_loss: 59.4027 Epoch 149/1000 261/261 - 0s - loss: 72.2401 - val_loss: 58.3119 Epoch 150/1000 261/261 - 0s - loss: 72.1309 - val_loss: 57.5037 Epoch 151/1000 261/261 - 0s - loss: 70.7773 - val_loss: 57.7769 Epoch 152/1000 261/261 - 0s - loss: 70.5883 - val_loss: 56.1087 Epoch 153/1000 261/261 - 0s - loss: 68.6020 - val_loss: 55.3935 Epoch 154/1000 261/261 - 0s - loss: 68.0137 - val_loss: 55.3946 Epoch 155/1000 261/261 - 0s - loss: 68.3630 - val_loss: 54.5067 Epoch 156/1000 261/261 - 0s - loss: 68.1104 - val_loss: 53.5928 Epoch 157/1000 261/261 - 0s - loss: 66.8734 - val_loss: 53.7972 Epoch 158/1000 261/261 - 0s - loss: 64.6184 - val_loss: 51.8031 Epoch 159/1000 261/261 - 0s - loss: 64.5744 - val_loss: 51.4003 Epoch 160/1000 261/261 - 0s - loss: 63.6910 - val_loss: 50.6856 Epoch 161/1000 261/261 - 0s - loss: 64.0145 - val_loss: 49.9536 Epoch 162/1000 261/261 - 0s - loss: 61.8386 - val_loss: 49.8901 Epoch 163/1000 261/261 - 0s - loss: 61.9306 - val_loss: 49.2521 Epoch 164/1000 261/261 - 0s - loss: 60.7556 - val_loss: 47.9911 Epoch 165/1000 261/261 - 0s - loss: 60.2802 - val_loss: 47.2594 Epoch 166/1000 261/261 - 0s - loss: 59.2542 - val_loss: 46.9898 Epoch 167/1000 261/261 - 0s - loss: 58.3004 - val_loss: 46.5502 Epoch 168/1000 261/261 - 0s - loss: 57.8545 - val_loss: 45.7245 Epoch 169/1000 261/261 - 0s - loss: 56.9617 - val_loss: 45.0827 Epoch 170/1000 261/261 - 0s - loss: 56.9749 - val_loss: 45.1476 Epoch 171/1000 261/261 - 0s - loss: 55.8050 - val_loss: 44.0151 Epoch 172/1000 261/261 - 0s - loss: 56.0478 - val_loss: 43.5957 Epoch 173/1000 261/261 - 0s - loss: 55.2461 - val_loss: 43.9503 Epoch 174/1000 261/261 - 0s - loss: 54.0493 - val_loss: 42.5281 Epoch 175/1000 261/261 - 0s - loss: 54.2585 - val_loss: 42.0300 Epoch 176/1000 261/261 - 0s - loss: 52.9849 - val_loss: 42.1091 Epoch 177/1000 261/261 - 0s - loss: 52.6699 - val_loss: 41.1280 Epoch 178/1000 261/261 - 0s - loss: 52.5766 - val_loss: 40.6279 Epoch 179/1000 261/261 - 0s - loss: 51.2797 - val_loss: 41.5560 Epoch 180/1000 261/261 - 0s - loss: 51.3167 - val_loss: 39.8998 Epoch 181/1000 261/261 - 0s - loss: 50.6548 - val_loss: 40.3602 Epoch 182/1000 261/261 - 0s - loss: 49.9360 - val_loss: 38.9575 Epoch 183/1000 261/261 - 0s - loss: 49.3195 - val_loss: 38.5161 Epoch 184/1000 261/261 - 0s - loss: 48.8159 - val_loss: 38.2727 Epoch 185/1000 261/261 - 0s - loss: 48.5230 - val_loss: 38.3134 Epoch 186/1000 261/261 - 0s - loss: 48.1472 - val_loss: 37.5338 Epoch 187/1000 261/261 - 0s - loss: 49.0451 - val_loss: 37.0337 Epoch 188/1000 261/261 - 0s - loss: 46.9509 - val_loss: 37.4614 Epoch 189/1000 261/261 - 0s - loss: 46.8951 - val_loss: 36.4360 Epoch 190/1000 261/261 - 0s - loss: 46.1027 - val_loss: 36.2615 Epoch 191/1000 261/261 - 0s - loss: 45.6384 - val_loss: 35.6610 Epoch 192/1000 261/261 - 0s - loss: 46.9916 - val_loss: 35.5148 Epoch 193/1000 261/261 - 0s - loss: 49.5148 - val_loss: 37.5214 Epoch 194/1000 261/261 - 0s - loss: 46.2516 - val_loss: 35.9050 Epoch 195/1000 261/261 - 0s - loss: 45.1961 - val_loss: 35.2473 Epoch 196/1000 261/261 - 0s - loss: 43.8845 - val_loss: 34.1322 Epoch 197/1000 261/261 - 0s - loss: 43.4610 - val_loss: 33.6880 Epoch 198/1000 261/261 - 0s - loss: 42.6286 - val_loss: 33.7127 Epoch 199/1000 261/261 - 0s - loss: 42.4154 - val_loss: 33.2152 Epoch 200/1000 261/261 - 0s - loss: 42.0020 - val_loss: 32.9451 Epoch 201/1000 261/261 - 0s - loss: 41.6191 - val_loss: 32.5093 Epoch 202/1000 261/261 - 0s - loss: 43.7235 - val_loss: 32.3695 Epoch 203/1000 261/261 - 0s - loss: 43.0863 - val_loss: 34.2041 Epoch 204/1000 261/261 - 0s - loss: 41.0544 - val_loss: 32.0973 Epoch 205/1000 261/261 - 0s - loss: 40.7787 - val_loss: 32.5461 Epoch 206/1000 261/261 - 0s - loss: 41.6360 - val_loss: 31.2820 Epoch 207/1000 261/261 - 0s - loss: 40.7417 - val_loss: 32.2974 Epoch 208/1000 261/261 - 0s - loss: 39.9822 - val_loss: 30.7600 Epoch 209/1000 261/261 - 0s - loss: 39.3857 - val_loss: 32.5769 Epoch 210/1000 261/261 - 0s - loss: 39.1410 - val_loss: 30.4246 Epoch 211/1000 261/261 - 0s - loss: 38.7447 - val_loss: 30.0492 Epoch 212/1000 261/261 - 0s - loss: 37.9753 - val_loss: 29.8627 Epoch 213/1000 261/261 - 0s - loss: 38.3355 - val_loss: 29.6306 Epoch 214/1000 261/261 - 0s - loss: 37.3530 - val_loss: 29.6433 Epoch 215/1000 261/261 - 0s - loss: 37.1885 - val_loss: 29.3205 Epoch 216/1000 261/261 - 0s - loss: 36.7803 - val_loss: 29.0165 Epoch 217/1000 261/261 - 0s - loss: 37.1867 - val_loss: 28.8259 Epoch 218/1000 261/261 - 0s - loss: 36.1244 - val_loss: 29.7593 Epoch 219/1000 261/261 - 0s - loss: 37.7266 - val_loss: 28.7380 Epoch 220/1000 261/261 - 0s - loss: 35.7875 - val_loss: 28.4919 Epoch 221/1000 261/261 - 0s - loss: 35.6227 - val_loss: 28.1351 Epoch 222/1000 261/261 - 0s - loss: 35.3527 - val_loss: 27.9021 Epoch 223/1000 261/261 - 0s - loss: 34.9739 - val_loss: 27.9576 Epoch 224/1000 261/261 - 0s - loss: 34.7204 - val_loss: 27.6015 Epoch 225/1000 261/261 - 0s - loss: 34.7849 - val_loss: 27.3595 Epoch 226/1000 261/261 - 0s - loss: 34.6823 - val_loss: 27.3375 Epoch 227/1000 261/261 - 0s - loss: 34.6561 - val_loss: 27.0810 Epoch 228/1000 261/261 - 0s - loss: 34.7526 - val_loss: 26.9909 Epoch 229/1000 261/261 - 0s - loss: 33.2568 - val_loss: 28.1786 Epoch 230/1000 261/261 - 0s - loss: 33.8126 - val_loss: 26.6188 Epoch 231/1000 261/261 - 0s - loss: 33.7432 - val_loss: 26.5590 Epoch 232/1000 261/261 - 0s - loss: 32.8556 - val_loss: 26.4733 Epoch 233/1000 261/261 - 0s - loss: 32.6516 - val_loss: 26.1711 Epoch 234/1000 261/261 - 0s - loss: 32.9949 - val_loss: 25.9887 Epoch 235/1000 261/261 - 0s - loss: 33.1170 - val_loss: 26.3875 Epoch 236/1000 261/261 - 0s - loss: 32.5374 - val_loss: 25.7300 Epoch 237/1000 261/261 - 0s - loss: 32.6500 - val_loss: 25.6121 Epoch 238/1000 261/261 - 0s - loss: 32.4430 - val_loss: 25.6679 Epoch 239/1000 261/261 - 0s - loss: 32.0512 - val_loss: 25.5607 Epoch 240/1000 261/261 - 0s - loss: 31.8485 - val_loss: 25.3327 Epoch 241/1000 261/261 - 0s - loss: 31.3820 - val_loss: 25.6274 Epoch 242/1000 261/261 - 0s - loss: 32.3983 - val_loss: 24.9405 Epoch 243/1000 261/261 - 0s - loss: 30.7282 - val_loss: 24.9071 Epoch 244/1000 261/261 - 0s - loss: 30.4659 - val_loss: 24.7043 Epoch 245/1000 261/261 - 0s - loss: 30.7127 - val_loss: 24.6449 Epoch 246/1000 261/261 - 0s - loss: 29.9609 - val_loss: 24.4984 Epoch 247/1000 261/261 - 0s - loss: 30.2372 - val_loss: 24.3524 Epoch 248/1000 261/261 - 0s - loss: 30.2689 - val_loss: 24.3986 Epoch 249/1000 261/261 - 0s - loss: 30.7721 - val_loss: 24.2271 Epoch 250/1000 261/261 - 0s - loss: 30.6043 - val_loss: 24.0360 Epoch 251/1000 261/261 - 0s - loss: 30.3024 - val_loss: 24.0987 Epoch 252/1000 261/261 - 0s - loss: 28.9162 - val_loss: 23.7909 Epoch 253/1000 261/261 - 0s - loss: 29.2801 - val_loss: 23.8153 Epoch 254/1000 261/261 - 0s - loss: 29.3222 - val_loss: 23.5515 Epoch 255/1000 261/261 - 0s - loss: 28.5132 - val_loss: 23.8399 Epoch 256/1000 261/261 - 0s - loss: 28.9835 - val_loss: 23.3674 Epoch 257/1000 261/261 - 0s - loss: 28.2271 - val_loss: 23.4548 Epoch 258/1000 261/261 - 0s - loss: 27.8565 - val_loss: 23.1535 Epoch 259/1000 261/261 - 0s - loss: 27.8770 - val_loss: 23.1761 Epoch 260/1000 261/261 - 0s - loss: 27.5445 - val_loss: 22.9507 Epoch 261/1000 261/261 - 0s - loss: 27.6223 - val_loss: 22.8882 Epoch 262/1000 261/261 - 0s - loss: 27.3854 - val_loss: 22.9048 Epoch 263/1000 261/261 - 0s - loss: 27.3946 - val_loss: 22.6476 Epoch 264/1000 261/261 - 0s - loss: 27.0089 - val_loss: 22.5546 Epoch 265/1000 261/261 - 0s - loss: 26.9027 - val_loss: 22.4856 Epoch 266/1000 261/261 - 0s - loss: 26.7630 - val_loss: 22.4675 Epoch 267/1000 261/261 - 0s - loss: 27.0150 - val_loss: 22.3077 Epoch 268/1000 261/261 - 0s - loss: 26.3339 - val_loss: 22.1958 Epoch 269/1000 261/261 - 0s - loss: 26.5861 - val_loss: 22.3650 Epoch 270/1000 261/261 - 0s - loss: 26.3245 - val_loss: 22.0337 Epoch 271/1000 261/261 - 0s - loss: 26.0610 - val_loss: 21.9219 Epoch 272/1000 261/261 - 0s - loss: 25.9908 - val_loss: 22.0404 Epoch 273/1000 261/261 - 0s - loss: 25.7291 - val_loss: 22.0628 Epoch 274/1000 261/261 - 0s - loss: 28.5037 - val_loss: 22.0770 Epoch 275/1000 261/261 - 0s - loss: 26.8031 - val_loss: 21.6196 Epoch 276/1000 261/261 - 0s - loss: 26.1467 - val_loss: 21.5624 Epoch 277/1000 261/261 - 0s - loss: 25.3375 - val_loss: 22.5604 Epoch 278/1000 261/261 - 0s - loss: 25.9187 - val_loss: 21.3272 Epoch 279/1000 261/261 - 0s - loss: 25.1155 - val_loss: 21.4415 Epoch 280/1000 261/261 - 0s - loss: 24.9094 - val_loss: 21.2730 Epoch 281/1000 261/261 - 0s - loss: 24.6625 - val_loss: 21.0808 Epoch 282/1000 261/261 - 0s - loss: 24.9405 - val_loss: 21.0107 Epoch 283/1000 261/261 - 0s - loss: 24.4015 - val_loss: 21.2510 Epoch 284/1000 261/261 - 0s - loss: 24.8920 - val_loss: 20.8377 Epoch 285/1000 261/261 - 0s - loss: 24.1691 - val_loss: 20.7580 Epoch 286/1000 261/261 - 0s - loss: 24.4140 - val_loss: 20.6990 Epoch 287/1000 261/261 - 0s - loss: 24.0014 - val_loss: 20.6096 Epoch 288/1000 261/261 - 0s - loss: 23.7787 - val_loss: 20.5134 Epoch 289/1000 261/261 - 0s - loss: 23.7206 - val_loss: 20.6378 Epoch 290/1000 261/261 - 0s - loss: 24.6226 - val_loss: 20.3711 Epoch 291/1000 261/261 - 0s - loss: 23.3452 - val_loss: 20.4992 Epoch 292/1000 261/261 - 0s - loss: 23.6446 - val_loss: 20.4952 Epoch 293/1000 261/261 - 0s - loss: 24.2056 - val_loss: 20.6786 Epoch 294/1000 261/261 - 0s - loss: 25.1895 - val_loss: 20.4868 Epoch 295/1000 Restoring model weights from the end of the best epoch. 261/261 - 0s - loss: 23.6036 - val_loss: 20.3795 Epoch 00295: early stopping . import numpy as np # Measure RMSE error. RMSE is common for regression. score = np.sqrt(metrics.mean_squared_error(pred,y_test)) print(&quot;Final score (RMSE): {}&quot;.format(score)) . Final score (RMSE): 4.5134384517538795 . import pandas as pd # Generate Kaggle submit file # Encode feature vector df_test = pd.read_csv( &quot;https://data.heatonresearch.com/data/t81-558/datasets/&quot;+ &quot;kaggle_auto_test.csv&quot;, na_values=[&#39;NA&#39;,&#39;?&#39;]) # Convert to numpy - regression ids = df_test[&#39;id&#39;] df_test.drop(&#39;id&#39;, axis=1, inplace=True) # Handle missing value df_test[&#39;horsepower&#39;] = df_test[&#39;horsepower&#39;]. fillna(df[&#39;horsepower&#39;].median()) x = df_test[[&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;]].values # Generate predictions pred = model.predict(x) #pred # Create submission data set df_submit = pd.DataFrame(pred) df_submit.insert(0,&#39;id&#39;,ids) df_submit.columns = [&#39;id&#39;,&#39;mpg&#39;] # Write submit file locally df_submit.to_csv(&quot;auto_submit.csv&quot;, index=False) print(df_submit) . id mpg 0 350 29.112602 1 351 27.803200 2 352 27.981804 3 353 30.487831 4 354 27.227440 5 355 26.438324 6 356 27.886986 7 357 29.103935 8 358 26.447609 9 359 30.027260 10 360 30.312553 11 361 30.712151 12 362 23.952263 13 363 24.858467 14 364 23.459129 15 365 22.638985 16 366 26.032127 17 367 26.197884 18 368 28.448906 19 369 28.138954 20 370 27.352821 21 371 27.313377 22 372 26.464119 23 373 26.689583 24 374 26.546562 25 375 27.829781 26 376 27.466354 27 377 30.343369 28 378 29.985909 29 379 27.807251 30 380 28.450882 31 381 26.574844 32 382 28.199501 33 383 29.615051 34 384 29.048317 35 385 29.320534 36 386 29.582710 37 387 24.533165 38 388 24.426888 39 389 24.658607 40 390 21.805504 41 391 26.026482 42 392 24.947670 43 393 26.902489 44 394 26.575218 45 395 33.546684 46 396 24.233910 47 397 28.609993 48 398 28.913261 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/06/example_kaggle_project_submission.html",
            "relUrl": "/2020/11/06/example_kaggle_project_submission.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "World Cup prediction example",
            "content": "Notebook and code from https://www.kaggle.com/agostontorok/soccer-world-cup-2018-winner . Data . FIFA rankings from 1993 to 2018 (courtesy of Tadhg Fitzgerald | International Soccer matches from 1872 to 2018 (courtesy of Mart Jürisoo) | FIFA World Cup 2018 data set (courtesy of Nuggs) | . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from matplotlib import pyplot as plt rankings = pd.read_csv(&#39;fifa_ranking.csv&#39;) rankings = rankings.loc[:,[&#39;rank&#39;, &#39;country_full&#39;, &#39;country_abrv&#39;, &#39;cur_year_avg_weighted&#39;, &#39;rank_date&#39;, &#39;two_year_ago_weighted&#39;, &#39;three_year_ago_weighted&#39;]] rankings = rankings.replace({&quot;IR Iran&quot;: &quot;Iran&quot;}) rankings[&#39;weighted_points&#39;] = rankings[&#39;cur_year_avg_weighted&#39;] + rankings[&#39;two_year_ago_weighted&#39;] + rankings[&#39;three_year_ago_weighted&#39;] rankings[&#39;rank_date&#39;] = pd.to_datetime(rankings[&#39;rank_date&#39;]) . matches = pd.read_csv(&#39;international-football-results-from-1872-to-2017/results.csv&#39;) matches = matches.replace({&#39;Germany DR&#39;: &#39;Germany&#39;, &#39;China&#39;: &#39;China PR&#39;}) matches[&#39;date&#39;] = pd.to_datetime(matches[&#39;date&#39;]) . world_cup = pd.read_csv(&#39;World Cup 2018 Dataset.csv&#39;) world_cup = world_cup.loc[:, [&#39;Team&#39;, &#39;Group&#39;, &#39;First match nagainst&#39;, &#39;Second match n against&#39;, &#39;Third match n against&#39;]] world_cup = world_cup.dropna(how=&#39;all&#39;) world_cup = world_cup.replace({&quot;IRAN&quot;: &quot;Iran&quot;, &quot;Costarica&quot;: &quot;Costa Rica&quot;, &quot;Porugal&quot;: &quot;Portugal&quot;, &quot;Columbia&quot;: &quot;Colombia&quot;, &quot;Korea&quot; : &quot;Korea Republic&quot;}) world_cup = world_cup.set_index(&#39;Team&#39;) . # I want to have the ranks for every day rankings = rankings.set_index([&#39;rank_date&#39;]) .groupby([&#39;country_full&#39;], group_keys=False) .resample(&#39;D&#39;).first() .fillna(method=&#39;ffill&#39;) .reset_index() # join the ranks matches = matches.merge(rankings, left_on=[&#39;date&#39;, &#39;home_team&#39;], right_on=[&#39;rank_date&#39;, &#39;country_full&#39;]) matches = matches.merge(rankings, left_on=[&#39;date&#39;, &#39;away_team&#39;], right_on=[&#39;rank_date&#39;, &#39;country_full&#39;], suffixes=(&#39;_home&#39;, &#39;_away&#39;)) . # feature generation matches[&#39;rank_difference&#39;] = matches[&#39;rank_home&#39;] - matches[&#39;rank_away&#39;] matches[&#39;average_rank&#39;] = (matches[&#39;rank_home&#39;] + matches[&#39;rank_away&#39;])/2 matches[&#39;point_difference&#39;] = matches[&#39;weighted_points_home&#39;] - matches[&#39;weighted_points_away&#39;] matches[&#39;score_difference&#39;] = matches[&#39;home_score&#39;] - matches[&#39;away_score&#39;] matches[&#39;is_won&#39;] = matches[&#39;score_difference&#39;] &gt; 0 # take draw as lost matches[&#39;is_stake&#39;] = matches[&#39;tournament&#39;] != &#39;Friendly&#39; . # I tried earlier the team as well but that did not make a difference either matches[&#39;wc_participant&#39;] = matches[&#39;home_team&#39;] * matches[&#39;home_team&#39;].isin(world_cup.index.tolist()) matches[&#39;wc_participant&#39;] = matches[&#39;wc_participant&#39;].replace({&#39;&#39;:&#39;Other&#39;}) matches = matches.join(pd.get_dummies(matches[&#39;wc_participant&#39;])) . from sklearn import linear_model from sklearn import ensemble from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures X, y = matches.loc[:,[&#39;average_rank&#39;, &#39;rank_difference&#39;, &#39;point_difference&#39;, &#39;is_stake&#39;]], matches[&#39;is_won&#39;] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42) logreg = linear_model.LogisticRegression(C=1e-5) features = PolynomialFeatures(degree=2) model = Pipeline([ (&#39;polynomial_features&#39;, features), (&#39;logistic_regression&#39;, logreg) ]) model = model.fit(X_train, y_train) # figures fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1]) plt.figure(figsize=(15,5)) ax = plt.subplot(1,3,1) ax.plot([0, 1], [0, 1], &#39;k--&#39;) ax.plot(fpr, tpr) ax.set_title(&#39;AUC score is {0:0.2}&#39;.format(roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))) ax.set_aspect(1) ax = plt.subplot(1,3,2) cm = confusion_matrix(y_test, model.predict(X_test)) ax.imshow(cm, cmap=&#39;Blues&#39;, clim = (0, cm.max())) ax.set_xlabel(&#39;Predicted label&#39;) ax.set_title(&#39;Performance on the Test set&#39;) ax = plt.subplot(1,3,3) cm = confusion_matrix(y_train, model.predict(X_train)) ax.imshow(cm, cmap=&#39;Blues&#39;, clim = (0, cm.max())) ax.set_xlabel(&#39;Predicted label&#39;) ax.set_title(&#39;Performance on the Training set&#39;) pass . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . # let&#39;s define the rankings at the time of the World Cup world_cup_rankings = rankings.loc[(rankings[&#39;rank_date&#39;] == rankings[&#39;rank_date&#39;].max()) &amp; rankings[&#39;country_full&#39;].isin(world_cup.index.unique())] world_cup_rankings = world_cup_rankings.set_index([&#39;country_full&#39;]) . import progressbar simulation_results = list() n_simulations = 10000 #bar = progressbar.ProgressBar(max_value=n_simulations) for i in range(n_simulations): #bar.update(i) candidates = [&#39;France&#39;, &#39;Argentina&#39;, &#39;Uruguay&#39;, &#39;Portugal&#39;, &#39;Spain&#39;, &#39;Russia&#39;,&#39;Croatia&#39;, &#39;Denmark&#39;, &#39;Brazil&#39;, &#39;Mexico&#39;, &#39;Belgium&#39;, &#39;Japan&#39;, &#39;Sweden&#39;, &#39;Switzerland&#39;, &#39;Colombia&#39;, &#39;England&#39;] finals = [&#39;round_of_16&#39;, &#39;quarterfinal&#39;, &#39;semifinal&#39;, &#39;final&#39;] for f in finals: iterations = int(len(candidates) / 2) winners = [] for i in range(iterations): home = candidates[i*2] away = candidates[i*2+1] row = pd.DataFrame(np.array([[np.nan, np.nan, np.nan, True]]), columns=X_test.columns) home_rank = world_cup_rankings.loc[home, &#39;rank&#39;] home_points = world_cup_rankings.loc[home, &#39;weighted_points&#39;] opp_rank = world_cup_rankings.loc[away, &#39;rank&#39;] opp_points = world_cup_rankings.loc[away, &#39;weighted_points&#39;] row[&#39;average_rank&#39;] = (home_rank + opp_rank) / 2 row[&#39;rank_difference&#39;] = home_rank - opp_rank row[&#39;point_difference&#39;] = home_points - opp_points home_win_prob = model.predict_proba(row)[:,1][0] # simulation step based on the probability simulated_outcome = np.random.binomial(1, home_win_prob) winners.append(away) if simulated_outcome &lt;= 0.5 else winners.append(home) candidates = winners simulations_results = simulation_results.append(candidates) simulation_results = sum(simulation_results, []) . pd.Series(simulation_results).value_counts().sort_values().divide(n_simulations).plot.barh(figsize=(10,5)) plt.ylabel(&#39;Winning probability&#39;) . Text(0, 0.5, &#39;Winning probability&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/05/soccer-fifa-pred-example.html",
            "relUrl": "/2020/11/05/soccer-fifa-pred-example.html",
            "date": " • Nov 5, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Webscraping Text and Images with BeautifulSoup example",
            "content": "This notebook code is from the app found here: https://github.com/kenichinakanishi/houseplant_classifier/ . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . from urllib.request import Request, urlopen from bs4 import BeautifulSoup def getHTMLContent(link): html = urlopen(link) soup = BeautifulSoup(html, &#39;html.parser&#39;) return soup . req = Request(&#39;https://www.aspca.org/pet-care/animal-poison-control/cats-plant-list&#39;, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}) webpage = urlopen(req).read() # Soupify the webpage soup = BeautifulSoup(webpage, &#39;lxml&#39;) # Search through the parse tree to get all the content from the table content_list = soup.find_all(&#39;span&#39;)[7:-4] # Put it in a dataframe for further processing df_cats = pd.DataFrame(content_list) . /home/gao/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray values = np.array([convert(v) for v in values]) . # Clean up the strings df_cats[0] = df_cats[0].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_cats[4] = df_cats[4].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_cats[1] = df_cats[1].apply(lambda x: str(x).split(&#39;(&#39;)[1][0:-4]) # Get rid of useless columns and rename the columns df_cats = df_cats.drop(columns=[2,3,5,6]).rename(columns = {0:&#39;Name&#39;,1:&#39;Alternative Names&#39;,4:&#39;Scientific Name&#39;,7:&#39;Family&#39;}) # Separate toxic and non-toxic plants df_cats[&#39;Toxic to Cats&#39;] = True first_nontoxic_cats = [index for index in df_cats[df_cats[&#39;Name&#39;].str.startswith(&#39;A&#39;)].index if index&gt;100][0] df_cats.loc[first_nontoxic_cats:,&#39;Toxic to Cats&#39;] = False . df_cats . Name Alternative Names Scientific Name Family Toxic to Cats . 0 Adam-and-Eve | Arum, Lord-and-Ladies, Wake Robin, Starch Root... | Arum maculatum | Araceae | True | . 1 African Wonder Tree | | Ricinus communis | | True | . 2 Alocasia | Elephant&#39;s Ear | Alocasia spp. | Araceae | True | . 3 Aloe | | Aloe vera | Liliaceae | True | . 4 Amaryllis | Many, including: Belladonna lily, Saint Joseph... | Amaryllis spp. | Amaryllidaceae | True | . ... ... | ... | ... | ... | ... | . 980 Yellowrocket | | Barbarea vulgaris | Brassicaceae | False | . 981 Yorba Linda | | Peperomia rotundifolia | Piperaceae | False | . 982 Zebra Haworthia | | Haworthia fasciata | Liliaceae | False | . 983 Zinnia | | Zinnia species | Asteraceae | False | . 984 Zucchini Squash | | Cucurbia pepo cv zucchini | Cucurbitaceae | False | . 985 rows × 5 columns . req = Request(&#39;https://www.aspca.org/pet-care/animal-poison-control/dogs-plant-list&#39;, headers={&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}) webpage = urlopen(req).read() soup = BeautifulSoup(webpage, &#39;lxml&#39;) # soupify the webpage content_list = soup.find_all(&#39;span&#39;)[7:-4] # Get all the content from the table df_dogs = pd.DataFrame(content_list) # Put it in a dataframe for processing . /home/gao/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray values = np.array([convert(v) for v in values]) . # Clean up the strings df_dogs[0] = df_dogs[0].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_dogs[4] = df_dogs[4].apply(lambda x: str(x).split(&#39;&gt;&#39;)[1][:-3]) df_dogs[1] = df_dogs[1].apply(lambda x: str(x).split(&#39;(&#39;)[1][0:-4]) # Get rid of useless columns and rename the columns df_dogs = df_dogs.drop(columns=[2,3,5,6]).rename(columns = {0:&#39;Name&#39;,1:&#39;Alternative Names&#39;,4:&#39;Scientific Name&#39;,7:&#39;Family&#39;}) # Separate toxic and non-toxic plants df_dogs[&#39;Toxic to Dogs&#39;] = True first_nontoxic_dogs = [index for index in df_dogs[df_dogs[&#39;Name&#39;].str.startswith(&#39;A&#39;)].index if index&gt;100][0] df_dogs.loc[first_nontoxic_dogs:,&#39;Toxic to Dogs&#39;] = False . # Merge dataframes into one, outer merge used to retain values that only exist on one side df_catsdogs = df_dogs.merge(df_cats, how=&#39;outer&#39;, on=[&#39;Name&#39;,&#39;Alternative Names&#39;,&#39;Scientific Name&#39;,&#39;Family&#39;]) df_catsdogs = df_catsdogs.fillna(&#39;Unknown&#39;) aspca_df = df_catsdogs.copy() # Assume same toxicity for dogs and cats if unknown aspca_df[&#39;Toxic to Cats&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Dogs&#39;] if (x[&#39;Toxic to Cats&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Cats&#39;], axis=1) aspca_df[&#39;Toxic to Dogs&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Cats&#39;] if (x[&#39;Toxic to Dogs&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Dogs&#39;], axis=1) . # Merge dataframes into one, outer merge used to retain values that only exist on one side df_catsdogs = df_dogs.merge(df_cats, how=&#39;outer&#39;, on=[&#39;Name&#39;,&#39;Alternative Names&#39;,&#39;Scientific Name&#39;,&#39;Family&#39;]) df_catsdogs = df_catsdogs.fillna(&#39;Unknown&#39;) aspca_df = df_catsdogs.copy() # Assume same toxicity for dogs and cats if unknown aspca_df[&#39;Toxic to Cats&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Dogs&#39;] if (x[&#39;Toxic to Cats&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Cats&#39;], axis=1) aspca_df[&#39;Toxic to Dogs&#39;] = aspca_df.apply(lambda x: x[&#39;Toxic to Cats&#39;] if (x[&#39;Toxic to Dogs&#39;] == &#39;Unknown&#39;) else x[&#39;Toxic to Dogs&#39;], axis=1) . aspca_df.sample(10) . Name Alternative Names Scientific Name Family Toxic to Dogs Toxic to Cats . 810 Pink Splash | Flamingo Plant, Polka Dot Plant, Measles Plant... | Hypoestes phyllostachya | Acanthaceae | False | False | . 120 English Ivy | Branching Ivy, Glacier Ivy, Needlepoint Ivy, S... | Hedera helix | Araliaceae | True | True | . 564 Crape Myrtle | Crepe Myrtle | Lagerstroemia indica | Lythraceae | False | False | . 201 Japanese Yew | English Yew, Western Yew, Pacific Yew, Anglo-J... | Taxus sp. | Taxaceae | True | True | . 635 Giant Touch-Me-Not | Buzzy Lizzie, Impatience Plant, Patient Lucy, ... | Impatiens spp. | Balsaminaceae | False | False | . 92 Cowbane | Water Hemlock, Poison Parsnip | Cicuta species | Apiaceae | True | True | . 277 Ornamental Pepper | Natal Cherry, Winter Cherry, Jerusalem Cherry | Solanum pseudocapsicum | Solanaceae | True | True | . 513 Carrot Fern | | Onychium japonica | Polypodiaceae | False | False | . 712 Leather Peperomia | | Peperomia crassifolia | Piperaceae | False | False | . 493 California Pitcher Plant | Cobra Orchid, Cobra Plant, Cobra Lily, Chrysam... | Darlingtonia californica | Sarraceniaceae | False | False | . aspca_df = aspca_df.drop_duplicates(&#39;Scientific Name&#39;) # Get rid of duplicates aspca_df = aspca_df.reset_index(drop=True).sort_index() # Reset and sort index . aspca_df = aspca_df.drop(aspca_df[aspca_df[&#39;Scientific Name&#39;].isin([&#39;&#39;,&#39;NONE LISTED&#39;])].index,axis=0).reset_index(drop=True).sort_index() # Fix mistakes in database . # Ensure proper punctuation for each scientific name. def normalize_capitalization(x): first_word, rest = x.split()[0], x.split()[1:] first_word = [first_word.capitalize()] rest = [word.lower() for word in rest] return &#39; &#39;.join(first_word+rest) # Clean up repeated species that have different names def species_normalizer(word): if word.split()[-1] in [&#39;sp&#39;,&#39;species&#39;,&#39;spp&#39;,&#39;sp.&#39;,&#39;spp.&#39;]: word = &#39;&#39;.join(word.split()[:-1]) return word # Remove cv from names, as it is an outdated way of referring to cultivars def cv_remover(word): if &#39;cv&#39; in word: word = word.replace(&#39; cv &#39;,&#39; &#39;) return word # Remove var. from names def var_remover(word): if &#39;var&#39; in word: word = word.replace(&#39; var. &#39;,&#39; &#39;) return word # Apply each of the functions aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(normalize_capitalization) aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(species_normalizer) aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(cv_remover) aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(var_remover) # Remove special characters aspca_df[&#39;Scientific Name&#39;] = aspca_df[&#39;Scientific Name&#39;].apply(lambda x: &#39;&#39;.join([character for character in x if character.isalnum() or character.isspace()])) # Reset dataframe for further processing aspca_df = aspca_df.sort_values(&#39;Scientific Name&#39;).drop_duplicates(&#39;Scientific Name&#39;) aspca_df = aspca_df.reset_index(drop=True).sort_index() . aspca_df.sample(10) . Name Alternative Names Scientific Name Family Toxic to Dogs Toxic to Cats . 108 American Bittersweet | Bittersweet, Waxwork, Shrubby Bittersweet, Fal... | Celastrus scandens | Celastraceae | True | True | . 530 Pacific Yew | English Yew, Western Yew, Japanese Yew, Anglo-... | Taxus brevifolia | Taxaceae | True | True | . 467 Pie Plant | Rhubarb | Rheum rhabarbarium | Polygonaceae | True | True | . 164 Pheasant Plant | Zebra Plant | Cryptanthus zonatus | Bromeliaceae | False | False | . 452 Primrose | | Primula vulgaris | Primulaceae | True | True | . 506 Jackson Brier | | Smilax lanceolata | Liliaceae | False | False | . 407 Ivy Peperomia | Plantinum Peperomia, Silver leaf Peperomia, Iv... | Peperomia griseoargentea | Piperaceae | False | False | . 147 Poison Hemlock | Poison Parsley, Spotted Hemlock, Winter Fern, ... | Conium maculatum | Umbelliferae | True | True | . 351 Cardinal Flower | Lobelia, Indian Pink | Lobelia cardinalis | Campanulaceae | True | True | . 236 Pink Brocade | | Episcia cultivar | Gesneriaceae | False | False | . use_cols = [&#39;scientificName&#39;,&#39;taxonRank&#39;,&#39;family&#39;,&#39;genus&#39;,&#39;taxonomicStatus&#39;,&#39;taxonID&#39;, &#39;acceptedNameUsageID&#39;] wfo_df = pd.read_csv(&#39;../classification.txt&#39;, sep=&#39; t&#39;, lineterminator=&#39; n&#39;, usecols=use_cols) wfo_df = wfo_df.sort_values(&#39;taxonomicStatus&#39;) . /home/gao/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . wfo_df.sample(10) . taxonID scientificName taxonRank family genus taxonomicStatus acceptedNameUsageID . 796160 wfo-0000798814 | Peridium oblongifolium | SPECIES | Peraceae | Peridium | Synonym | wfo-0000267144 | . 180708 wfo-0000180970 | Cracca smallii | SPECIES | Fabaceae | Cracca | Synonym | wfo-0000178756 | . 911945 wfo-0000914633 | Thinopyrum turcicum | SPECIES | Poaceae | Thinopyrum | Synonym | wfo-0000866236 | . 167159 wfo-0000167369 | Indigofera cinerea | SPECIES | Fabaceae | Indigofera | Synonym | wfo-0000173646 | . 642316 wfo-0000644639 | Diaphanoptera khorasanica | SPECIES | Caryophyllaceae | Diaphanoptera | Accepted | NaN | . 464965 wfo-0000466716 | Phyllocyclus minutiflorus | SPECIES | Gentianaceae | Phyllocyclus | Doubtful | NaN | . 740337 wfo-0000742945 | Daphne pseudomezereum var. koreana | VARIETY | Thymelaeaceae | Daphne | Synonym | wfo-0000637684 | . 868404 wfo-0000871073 | Festuca montis-aurei | SPECIES | Poaceae | Festuca | Synonym | wfo-0000869683 | . 186218 wfo-0000186502 | Lotononis curvicarpa | SPECIES | Fabaceae | Lotononis | Accepted | NaN | . 552490 wfo-0000554468 | Specklinia casualis | SPECIES | Orchidaceae | Specklinia | Synonym | wfo-0000339564 | . # Don&#39;t need this column, we trust the WFO database more aspca_df.drop(&#39;Family&#39;, axis=1, inplace=True) # Merge dataframes together to get trusted info aspca_df = aspca_df.merge(wfo_df, how = &#39;left&#39;, left_on = [&#39;Scientific Name&#39;], right_on = [&#39;scientificName&#39;]) # Sort by taxonomicStatus and drop duplicates keeping the first - keeping accepted names as priority aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) # Fill NaN&#39;s with Unknown aspca_df = aspca_df.fillna(&#39;Unknown&#39;) . # Clean up and deal with scientific names that are unknown, due to misspellings or otherwise. aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) unknown_idx = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;].index print(len(unknown_idx)) . 101 . def get_closest_name(unknown_name, name_df = wfo_df, name_col = &#39;scientificName&#39;, threshold=0.9, verbose=False): &quot;&quot;&quot; Matches an &#39;unknown_name&#39; against accepted names in a &#39;name_df&#39;. Will return names that are above a &#39;threshold&#39; of closeness. Parameters - unknown_name: str Name we want to match against accepted names. name_df: DataFrame DataFrame containing accepted names. name_col: str, name of name_df column DataFrame column containing accepted names. threshold: int How closely does the unknown_name need to match with the accepted name. If above this threshold, the name is added to a dictionary of possible names. verbose: bool Should the function print the entire list of possible names. Returns: - str Closest name to &#39;unknown_name&#39; that was above the given &#39;threshold&#39;. &quot;&quot;&quot; import operator from difflib import SequenceMatcher def similar(a, b): return SequenceMatcher(None, a, b).ratio() poss_names = {} # Only look through entries with the same first letter to save time for true_sciname in name_df[name_df[name_col].str.startswith(unknown_name[0])][name_col].values: similar_score = similar(unknown_name, true_sciname) if similar_score&gt;threshold: poss_names[true_sciname]=similar_score # If the dict is empty if verbose == True: print(poss_names) if not bool(poss_names): print(f&#39;No names close enough to {unknown_name}.&#39;) return &#39;&#39; else: print(f&#39;{unknown_name} is closest to {max(poss_names.items(), key=operator.itemgetter(1))[0]}, with a score of {max(poss_names.items(), key=operator.itemgetter(1))[1]:.2f}&#39;) return max(poss_names.items(), key=operator.itemgetter(1))[0] . def fix_name(unknown_name, true_name): &quot;&quot;&quot; Fixes the aspca_df entries according to the accepted wfo_df entry. Parameters - unknown_name: str Name we want to fix. true_name: DataFrame Accepted name to use. &quot;&quot;&quot; # Get the series we&#39;re looking to change unknown_data = aspca_df[aspca_df[&#39;Scientific Name&#39;] == unknown_name] # Grab accepted data from wfo database based on ID lookup true_data = wfo_df[wfo_df[&#39;scientificName&#39;] == true_name] true_sciname = true_data.loc[:,&#39;scientificName&#39;].values[0] true_family = true_data.loc[:,&#39;family&#39;].values[0] true_genus = true_data.loc[:,&#39;genus&#39;].values[0] true_taxonomicStatus = true_data.loc[:,&#39;taxonomicStatus&#39;].values[0] # Change scientific name, family, genus and taxonomic status to accepted versions aspca_df.iloc[unknown_data.index,2] = true_sciname aspca_df.iloc[unknown_data.index,8] = true_family aspca_df.iloc[unknown_data.index,9] = true_genus aspca_df.iloc[unknown_data.index,10] = true_taxonomicStatus . unknown_idx = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;].index print(f&#39;{len(unknown_idx)} plants currently cannot be matched.&#39;) from tqdm.notebook import tqdm for i in tqdm(unknown_idx): unknown_name = aspca_df.iloc[i,2] closest_name = get_closest_name(unknown_name) if closest_name == &#39;&#39;: continue fix_name(unknown_name,closest_name) . 101 plants currently cannot be matched. Malus sylvestrus is closest to Malus sylvestris, with a score of 0.94 No names close enough to Maranta insignis. No names close enough to Miltonia roezlii alba. No names close enough to Neoregalia. No names close enough to Nephrolepis exalta bostoniensis. Nephrolepsis exalta is closest to Nephrolepis exaltata, with a score of 0.92 No names close enough to Nephrolepsis cordifolia duffii. No names close enough to Lilium orientalis. No names close enough to Nephrolepsis cordifolia plumosa. Nephrolepis exalta is closest to Nephrolepis exaltata, with a score of 0.95 No names close enough to Lilium asiatica. Hosta plataginea is closest to Hosta plantaginea, with a score of 0.97 No names close enough to Lampranthus piquet. Kalmia poliifolia is closest to Kalmia polifolia, with a score of 0.97 Kalmia augustifolia is closest to Kalmia angustifolia, with a score of 0.95 Jasminium is closest to Jasminum, with a score of 0.94 Hoya publcalyx is closest to Hoya pubicalyx, with a score of 0.93 No names close enough to Hoya carnosa krinkle kurl. No names close enough to Hemigraphis exotica. Gynura aurantica is closest to Gynura aurantiaca, with a score of 0.97 No names close enough to Nolina tuberculata. Guzmania lingulata minor is closest to Guzmania lingulata var. minor, with a score of 0.91 Lavendula angustifolia is closest to Lavandula angustifolia, with a score of 0.95 Onychium japonica is closest to Onychium japonicum, with a score of 0.91 No names close enough to Schefflera or brassia actinoplylla. Paeonis officinalis is closest to Paeonia officinalis, with a score of 0.95 No names close enough to Giant dracaena. Taxus canadensus is closest to Taxus canadensis, with a score of 0.94 Stapelia hirsata is closest to Stapelia hirsuta, with a score of 0.94 Sorghum vulgare var sudanesis is closest to Sorghum vulgare var. sudanense, with a score of 0.92 Smilax walteria is closest to Smilax walteri, with a score of 0.97 Secum weinbergii is closest to Sedum weinbergii, with a score of 0.94 No names close enough to Scindapsusphilodendron. Santpaulia confusa is closest to Saintpaulia confusa, with a score of 0.97 Rhipsalis cassutha is closest to Rhipsalis cassytha, with a score of 0.94 Rheum rhabarbarium is closest to Rheum rhabarbarum, with a score of 0.97 Origanum vulgare hirtum is closest to Origanum vulgare var. hirtum, with a score of 0.90 Tolmeia menziesii is closest to Tolmiea menziesii, with a score of 0.94 Podocarpus macrophylla is closest to Podocarpus macrophyllus, with a score of 0.93 Ploystichum munitum is closest to Polystichum munitum, with a score of 0.95 Plectranthus oetendahlii is closest to Plectranthus oertendahlii, with a score of 0.98 Plantanus occidentalis is closest to Platanus occidentalis, with a score of 0.98 Pilea cadieri is closest to Pilea cadierei, with a score of 0.96 No names close enough to Phoenix robellinii. No names close enough to Peperomia serpens variegata. Peperomia prostata is closest to Peperomia prostrata, with a score of 0.97 Peperomia griseoargentea is closest to Peperomia griseoargentia, with a score of 0.96 Pellonia pulchra is closest to Pellionia pulchra, with a score of 0.97 Rhapis flabelliformus is closest to Rhapis flabelliformis, with a score of 0.95 Fuschsia is closest to Fuchsia, with a score of 0.93 No names close enough to Begonia rex peace. Eriogonium umbellatum is closest to Eriogonum umbellatum, with a score of 0.98 Citrus aurantifolia is closest to Citrus aurantiifolia, with a score of 0.97 Cissus dicolor is closest to Cissus discolor, with a score of 0.97 Chlorophytum bichetti is closest to Chlorophytum bichetii, with a score of 0.95 No names close enough to Ceratostigma larpentiae. Cattleya trianaei is closest to Cattleya trianae, with a score of 0.97 Camellia japonica thea japonica is closest to Camellia japonica var. japonica, with a score of 0.90 Caesalpinia gilliessi is closest to Caesalpinia gilliesii, with a score of 0.95 Borage officinalis is closest to Borago officinalis, with a score of 0.94 No names close enough to Bertolonia mosaica. No names close enough to Begonia semperflorens cultivar. Begonia scharfii is closest to Begonia scharffii, with a score of 0.97 Begonia cleopatra is closest to Begonia cleopatrae, with a score of 0.97 No names close enough to Asparagus densiflorus sprengeri. Arum palestinum is closest to Arum palaestinum, with a score of 0.97 Anthurium scherzeranum is closest to Anthurium scherzerianum, with a score of 0.98 Anthirrhinum multiflorum is closest to Antirrhinum multiflorum, with a score of 0.98 Anoectuchilus setaceus is closest to Anoectochilus setaceus, with a score of 0.95 Anethum graveolena is closest to Anethum graveolens, with a score of 0.94 No names close enough to Albiflora. No names close enough to Acantha. Tradescantia flumeninsis is closest to Tradescantia fluminensis, with a score of 0.92 Citrus aurantium is closest to Citrus ×aurantium, with a score of 0.97 Euonymus atropurpurea is closest to Euonymus atropurpureus, with a score of 0.93 Citrus limonia is closest to Citrus ×limonia, with a score of 0.97 Cleome hasserlana is closest to Cleome hassleriana, with a score of 0.91 Eriogonium inflatum is closest to Eriogonum inflatum, with a score of 0.97 No names close enough to Episcia cultivar. Epidendrum atropurpeum is closest to Epidendrum atropurpureum, with a score of 0.96 Eleagnus is closest to Elaeagnus, with a score of 0.94 No names close enough to Echeveria puloliver. Echeveria pulinata is closest to Echeveria pulvinata, with a score of 0.97 No names close enough to Echevaria. No names close enough to Dypsis lutescens chrysalidocarpus lutescens alternate scientific name. No names close enough to Draceana. No names close enough to Daucus carota sativa. Citrus paradisii is closest to Citrus paradisi, with a score of 0.97 No names close enough to Cycasrevolutazamia. No names close enough to Cucurbita maxima turbaniformis. No names close enough to Cucurbita maxima hubbard. No names close enough to Cucurbita maxima butternut. No names close enough to Cucurbita maxima buttercup. No names close enough to Cucurbita maxima banana. No names close enough to Cucurbia pepo zucchini. No names close enough to Cryptanthus bivattus minor. Coleus ampoinicus is closest to Coleus amboinicus, with a score of 0.94 Clivia minata is closest to Clivia miniata, with a score of 0.96 Clintonia umbelluata is closest to Clintonia umbellulata, with a score of 0.98 No names close enough to Cycasandzamia. Veitchia merillii is closest to Veitchia merrillii, with a score of 0.97 . # Scientific names that don&#39;t match anything on record automatically unknown_df = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;] # Synonyms that don&#39;t have a database link to the accepted name aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) unknown_ids = aspca_df[(aspca_df.acceptedNameUsageID == &#39;Unknown&#39;) &amp; (aspca_df.taxonomicStatus == &#39;Synonym&#39;)] len(unknown_ids) + len(unknown_df) . 52 . # Manually fix some scientific names that don&#39;t match anything on record automatically fix_name(&#39;Nephrolepsis cordifolia plumosa&#39;, &#39;Nephrolepis cordifolia&#39;) fix_name(&#39;Nephrolepsis cordifolia duffii&#39;, &#39;Nephrolepis cordifolia&#39;) fix_name(&#39;Nephrolepis exalta bostoniensis&#39;, &#39;Nephrolepis exaltata&#39;) fix_name(&#39;Neoregalia&#39;, &#39;Neoregelia&#39;) fix_name(&#39;Miltonia roezlii alba&#39;, &#39;Miltonia roezlii&#39;) fix_name(&#39;Maranta insignis&#39;, &#39;Calathea insignis&#39;) fix_name(&#39;Lilium orientalis&#39;, &#39;Lilium japonicum&#39;) fix_name(&#39;Lampranthus piquet&#39;, &#39;Lampranthus piquetbergensis&#39;) fix_name(&#39;Hoya carnosa krinkle kurl&#39;, &#39;Hoya carnosa&#39;) fix_name(&#39;Hemigraphis exotica&#39;, &#39;Hemigraphis alternata&#39;) fix_name(&#39;Lilium asiatica&#39;, &#39;Lilium japonicum&#39;) fix_name(&#39;Nolina tuberculata&#39;, &#39;Beaucarnea recurvata&#39;) fix_name(&#39;Giant dracaena&#39;, &#39;Cordyline australis&#39;) fix_name(&#39;Scindapsusphilodendron&#39;, &#39;Philodendron scandens&#39;) fix_name(&#39;Schefflera or brassia actinoplylla&#39;, &#39;Schefflera actinophylla&#39;) fix_name(&#39;Phoenix robellinii&#39;, &#39;Phoenix roebelenii&#39;) fix_name(&#39;Peperomia serpens variegata&#39;, &#39;Peperomia serpens&#39;) fix_name(&#39;Bertolonia mosaica&#39;, &#39;Fittonia albivenis&#39;) fix_name(&#39;Begonia semperflorens cultivar&#39;, &#39;Begonia semperflorens&#39;) fix_name(&#39;Begonia rex peace&#39;, &#39;Begonia rex&#39;) fix_name(&#39;Asparagus densiflorus sprengeri&#39;, &#39;Asparagus densiflorus&#39;) fix_name(&#39;Albiflora&#39;, &#39;Tradescantia zebrina&#39;) fix_name(&#39;Acantha&#39;, &#39;Acanthus&#39;) fix_name(&#39;Episcia cultivar&#39;, &#39;Episcia&#39;) fix_name(&#39;Echevaria&#39;, &#39;Echeveria&#39;) fix_name(&#39;Echeveria puloliver&#39;, &#39;Echeveria harmsii&#39;) fix_name(&#39;Dypsis lutescens chrysalidocarpus lutescens alternate scientific name&#39;, &#39;Dypsis lutescens&#39;) fix_name(&#39;Draceana&#39;, &#39;Dracaena&#39;) fix_name(&#39;Daucus carota sativa&#39;, &#39;Daucus carota&#39;) fix_name(&#39;Ceratostigma larpentiae&#39;, &#39;Ceratostigma plumbaginoides&#39;) fix_name(&#39;Cycasrevolutazamia&#39;, &#39;Cycas revoluta&#39;) fix_name(&#39;Cucurbita maxima turbaniformis&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima hubbard&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima butternut&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima banana&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbita maxima buttercup&#39;, &#39;Cucurbita maxima&#39;) fix_name(&#39;Cucurbia pepo zucchini&#39;, &#39;Cucurbita pepo&#39;) fix_name(&#39;Cryptanthus bivattus minor&#39;, &#39;Cryptanthus bivittatus&#39;) fix_name(&#39;Cycasandzamia&#39;, &#39;Cycas&#39;) . # Manually match up synonyms that don&#39;t have a database link to the accepted name fix_name(&#39;Chlorophytum bichetii&#39;, &#39;Chlorophytum laxum&#39;) fix_name(&#39;Rhapis flabelliformis&#39;, &#39;Rhapis excelsa&#39;) fix_name(&#39;Cleome hassleriana&#39;, &#39;Cleome spinosa&#39;) fix_name(&#39;Pellionia pulchra&#39;, &#39;Pellionia repens&#39;) fix_name(&#39;Cissus discolor&#39;, &#39;Cissus javana&#39;) fix_name(&#39;Miltonia roezlii&#39;, &#39;Miltoniopsis roezlii&#39;) fix_name(&#39;Sorghum vulgare var. sudanense&#39;, &#39;Sorghum bicolor&#39;) fix_name(&#39;Camellia japonica var. japonica&#39;, &#39;Camellia japonica&#39;) fix_name(&#39;Onychium japonicum&#39;, &#39;Onychium japonicum&#39;) fix_name(&#39;Epidendrum atropurpureum&#39;, &#39;Psychilis atropurpurea&#39;) fix_name(&#39;Philodendron scandens&#39;, &#39;Philodendron hederaceum&#39;) fix_name(&#39;Origanum vulgare var. hirtum&#39;, &#39;Origanum vulgare subsp. hirtum&#39;) fix_name(&#39;Guzmania lingulata var. minor&#39;, &#39;Guzmania lingulata var. concolor&#39;) fix_name(&#39;Lavandula angustifolia&#39;, &#39;Lavandula angustifolia&#39;) fix_name(&#39;Begonia semperflorens&#39;, &#39;Begonia cucullata&#39;) fix_name(&#39;Calathea insignis&#39;, &#39;Calathea crotalifera&#39;) fix_name(&#39;Citrus ×limonia&#39;, &#39;Citrus limon&#39;) fix_name(&#39;Coleus amboinicus&#39;, &#39;Plectranthus amboinicus&#39;) fix_name(&#39;Rhipsalis cassytha&#39;, &#39;Rhipsalis dichotoma&#39;) fix_name(&#39;Lycopersicon&#39;, &#39;Solanum lycopersicum&#39;) fix_name(&#39;Lachenalia lilacina&#39;, &#39;Iris domestica&#39;) fix_name(&#39;Cymopterus watsonii&#39;, &#39;Cymopterus terebinthinus&#39;) . # Scientific names that don&#39;t match anything on record automatically unknown_df = aspca_df[aspca_df.taxonomicStatus == &#39;Unknown&#39;] # Synonyms that don&#39;t have a database link to the accepted name aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) unknown_ids = aspca_df[(aspca_df.acceptedNameUsageID == &#39;Unknown&#39;) &amp; (aspca_df.taxonomicStatus == &#39;Synonym&#39;)] len(unknown_ids) + len(unknown_df) . 1 . synonym_idx = aspca_df[aspca_df[&#39;taxonomicStatus&#39;].values == &#39;Synonym&#39;].index print(f&#39;{len(synonym_idx)} entries have a more acceptable synonym&#39;) . 71 entries have a more acceptable synonym . # Work to update the remaining scientific names that are synonyms for their accepted scientific names aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;).reset_index(drop=True) synonym_idx = aspca_df[aspca_df[&#39;taxonomicStatus&#39;].values == &#39;Synonym&#39;].index . for i in synonym_idx: # Get the series we&#39;re looking to change synonym_data = aspca_df.iloc[i,:] synonym_name = synonym_data.loc[&#39;Scientific Name&#39;] # Grab accepted data from wfo database based on ID lookup true_data = wfo_df[wfo_df[&#39;taxonID&#39;] == synonym_data.loc[&#39;acceptedNameUsageID&#39;]] true_sciname = true_data.iloc[:,1].values[0] fix_name(synonym_name,true_sciname) . IndexError Traceback (most recent call last) &lt;ipython-input-36-d42f5603c8fc&gt; in &lt;module&gt; 5 # Grab accepted data from wfo database based on ID lookup 6 true_data = wfo_df[wfo_df[&#39;taxonID&#39;] == synonym_data.loc[&#39;acceptedNameUsageID&#39;]] -&gt; 7 true_sciname = true_data.iloc[:,1].values[0] 8 fix_name(synonym_name,true_sciname) IndexError: index 0 is out of bounds for axis 0 with size 0 . synonym_idx = aspca_df[aspca_df[&#39;taxonomicStatus&#39;].values == &#39;Synonym&#39;].index print(f&#39;{len(synonym_idx)} entries have a more acceptable synonym&#39;) . 31 entries have a more acceptable synonym . # Sort and drop again aspca_df = aspca_df.sort_values(&#39;taxonomicStatus&#39;).drop_duplicates(&#39;Scientific Name&#39;, keep=&#39;first&#39;) aspca_df = aspca_df.sort_values(&#39;Scientific Name&#39;).reset_index(drop=True).sort_index() # Set genus of one-word names to be the name, rather than NaN aspca_df.loc[aspca_df.fillna(&#39;Unknown&#39;)[&#39;genus&#39;]==&#39;Unknown&#39;, &#39;genus&#39;] = aspca_df.loc[aspca_df.fillna(&#39;Unknown&#39;)[&#39;genus&#39;]==&#39;Unknown&#39;, &#39;Scientific Name&#39;] # Drop columns we no longer need aspca_df = aspca_df.drop([&#39;taxonID&#39;, &#39;scientificName&#39;, &#39;taxonomicStatus&#39;, &#39;acceptedNameUsageID&#39;, &#39;taxonRank&#39;], axis=1) # Standardize column names aspca_df.rename(columns = {&#39;genus&#39;:&#39;Genus&#39;, &#39;family&#39;:&#39;Family&#39;}, inplace=True) # Reorder columns cols = [&#39;Name&#39;, &#39;Scientific Name&#39;, &#39;Genus&#39;, &#39;Family&#39;, &#39;Alternative Names&#39;, &#39;Toxic to Dogs&#39;, &#39;Toxic to Cats&#39;] aspca_df = aspca_df[cols] . aspca_df.to_csv(&#39;Plant Toxicity - v6.csv&#39;) aspca_df.sample(10) . Name Scientific Name Genus Family Alternative Names Toxic to Dogs Toxic to Cats . 102 Celosia Globosa | Celosia globosa | Celosia | Amaranthaceae | Globe Amarantha, Perpetua | False | False | . 18 Alocasia | Alocasia | Alocasia | Araceae | Elephant&#39;s Ear | True | True | . 386 Variegated Philodendron | Philodendron hederaceum | Philodendron | Araceae | | True | True | . 411 American Mandrake | Podophyllum peltatum | Podophyllum | Berberidaceae | Mayapple, Indian Apple Root, Umbrella Leaf, Wi... | True | True | . 94 Chestnut | Castanea dentata | Castanea | Fagaceae | American Chestnut | False | False | . 291 Butterfly Iris | Iris spuria | Iris | Iridaceae | Spuria Iris | True | True | . 243 Climbing Lily | Gloriosa superba | Gloriosa | Colchicaceae | Gloriosa Lily, Glory Lily, Superb Lily | True | True | . 4 Measles Plant | Acanthus | Acanthus | Acanthaceae | Polka Dot Plant, Flamingo Plant, Baby’s Tears,... | False | False | . 246 Orange Star | Guzmania lingulata var. concolor | Guzmania | Bromeliaceae | | False | False | . 420 Algaroba | Prosopis limensis | Prosopis | Fabaceae | Kiawe, Mesquite | False | False | . aspca_df.head() . Name Scientific Name Genus Family Alternative Names Toxic to Dogs Toxic to Cats . 0 Sand Verbena | Abronia fragrans | Abronia | Nyctaginaceae | Prairie Snowball, Wild Lantana | False | False | . 1 Prayer Bean | Abrus precatorius | Abrus | Fabaceae | Rosary Pea, Buddhist Rosary Bead, Indian Bead,... | True | True | . 2 Copperleaf | Acalypha godseffiana | Acalypha | Euphorbiaceae | Lance Copperleaf | False | False | . 3 Chenille Plant | Acalypha hispida | Acalypha | Euphorbiaceae | Philippine Medusa, Foxtail, Red-hot Cat Tail | False | False | . 4 Measles Plant | Acanthus | Acanthus | Acanthaceae | Polka Dot Plant, Flamingo Plant, Baby’s Tears,... | False | False | . aspca_df[aspca_df[&#39;Toxic to Dogs&#39;] != aspca_df[&#39;Toxic to Cats&#39;]] . Name Scientific Name Genus Family Alternative Names Toxic to Dogs Toxic to Cats . 262 Day Lilies (many varieties) | Hemerocallis | Hemerocallis | Xanthorrhoeaceae | | False | True | . 263 Orange Day Lily | Hemerocallis graminea | Hemerocallis | Xanthorrhoeaceae | | False | True | . 296 Black Walnut | Juglans nigra | Juglans | Juglandaceae | | True | False | . 317 Lily | Lilium | Lilium | Liliaceae | | False | True | . 319 Tiger Lily | Lilium lancifolium | Lilium | Liliaceae | | False | True | . 320 Easter Lily | Lilium longiflorum | Lilium | Liliaceae | | False | True | . 321 Red Lily | Lilium philadelphicum | Lilium | Liliaceae | | False | True | . 322 Japanese Show Lily | Lilium speciosum | Lilium | Liliaceae | | False | True | . aspca_df[[&#39;Family&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Family&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[70:80] . Toxic to Cats Toxic to Dogs . Family . Lauraceae 0.500000 | 0.500000 | . Proteaceae 0.500000 | 0.500000 | . Convolvulaceae 0.500000 | 0.500000 | . Commelinaceae 0.500000 | 0.500000 | . Euphorbiaceae 0.600000 | 0.600000 | . Fabaceae 0.600000 | 0.600000 | . Berberidaceae 0.666667 | 0.666667 | . Polygonaceae 0.666667 | 0.666667 | . Apiaceae 0.666667 | 0.666667 | . Moraceae 0.666667 | 0.666667 | . # How many Families have mixed toxicity len(aspca_df[[&#39;Family&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Family&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[aspca_df[[&#39;Family&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Family&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[&#39;Toxic to Dogs&#39;].apply(lambda x: 0&lt;x&lt;1)]) . 33 . # How many Families len(aspca_df[&#39;Family&#39;].unique()) . 111 . aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[208:218] . Toxic to Cats Toxic to Dogs . Genus . Schefflera 0.666667 | 0.666667 | . Cordyline 0.666667 | 0.666667 | . Iris 0.666667 | 0.666667 | . Aloe 0.666667 | 0.666667 | . Dracaena 0.800000 | 0.800000 | . Aralia 1.000000 | 1.000000 | . Ficus 1.000000 | 1.000000 | . Apocynum 1.000000 | 1.000000 | . Sansevieria 1.000000 | 1.000000 | . Rumex 1.000000 | 1.000000 | . # How many Genuses have mixed toxicity len(aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)[&#39;Toxic to Dogs&#39;].apply(lambda x: 0&lt;x&lt;1)]) . 9 . # How many Genuses len(aspca_df[[&#39;Genus&#39;,&#39;Toxic to Dogs&#39;,&#39;Toxic to Cats&#39;]].pivot_table(index = &#39;Genus&#39;).sort_values(by=&#39;Toxic to Dogs&#39;)) . 346 . # If running in Colabs !pip install selenium -q !apt-get update # to update ubuntu to correctly run apt install !apt install chromium-chromedriver -q !cp /usr/lib/chromium-browser/chromedriver /usr/bin . WARNING: You are using pip version 20.2.3; however, version 20.2.4 is available. You should consider upgrading via the &#39;/home/gao/anaconda3/bin/python -m pip install --upgrade pip&#39; command. Reading package lists... Done E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied) E: Unable to lock directory /var/lib/apt/lists/ W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied) W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied) E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root? cp: cannot stat &#39;/usr/lib/chromium-browser/chromedriver&#39;: No such file or directory . import sys sys.path.insert(0,&#39;/usr/lib/chromium-browser/chromedriver&#39;) # Import and setup the Selenium webdriver from selenium import webdriver chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;--headless&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;) wd = webdriver.Chrome(&#39;chromedriver&#39;,chrome_options=chrome_options) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/04/Webscraping_Example.html",
            "relUrl": "/2020/11/04/Webscraping_Example.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Setting up pyspark locally, for development.",
            "content": "Welcome to ____ __ / __/__ ___ _____/ /__ _ / _ / _ `/ __/ &#39;_/ /__ / .__/ _,_/_/ /_/ _ version 2.4.4 /_/ Using Python version 3.7.3 (default, Mar 27 2019 22:11:17) SparkSession available as &#39;spark&#39;. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/big%20data/2020/11/03/pyspark-shell.html",
            "relUrl": "/big%20data/2020/11/03/pyspark-shell.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "MCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis",
            "content": "Metropolis-Hastings . This post includes code and notes from : https://www.tweag.io/blog/2019-10-25-mcmc-intro1/ . https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo . | https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm . | . %matplotlib notebook %matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [10, 6] np.random.seed(42) . state_space = (&quot;sunny&quot;, &quot;cloudy&quot;, &quot;rainy&quot;) . transition_matrix = np.array(((0.6, 0.3, 0.1), (0.3, 0.4, 0.3), (0.2, 0.3, 0.5))) . n_steps = 20000 states = [0] for i in range(n_steps): states.append(np.random.choice((0, 1, 2), p=transition_matrix[states[-1]])) states = np.array(states) . def despine(ax, spines=(&#39;top&#39;, &#39;left&#39;, &#39;right&#39;)): for spine in spines: ax.spines[spine].set_visible(False) fig, ax = plt.subplots() width = 1000 offsets = range(1, n_steps, 5) for i, label in enumerate(state_space): ax.plot(offsets, [np.sum(states[:offset] == i) / offset for offset in offsets], label=label) ax.set_xlabel(&quot;number of steps&quot;) ax.set_ylabel(&quot;likelihood&quot;) ax.legend(frameon=False) despine(ax, (&#39;top&#39;, &#39;right&#39;)) plt.show() . def log_prob(x): return -0.5 * np.sum(x ** 2) . def proposal(x, stepsize): return np.random.uniform(low=x - 0.5 * stepsize, high=x + 0.5 * stepsize, size=x.shape) . def p_acc_MH(x_new, x_old, log_prob): return min(1, np.exp(log_prob(x_new) - log_prob(x_old))) . def sample_MH(x_old, log_prob, stepsize): x_new = proposal(x_old, stepsize) # here we determine whether we accept the new state or not: # we draw a random number uniformly from [0,1] and compare # it with the acceptance probability accept = np.random.random() &lt; p_acc_MH(x_new, x_old, log_prob) if accept: return accept, x_new else: return accept, x_old . def build_MH_chain(init, stepsize, n_total, log_prob): n_accepted = 0 chain = [init] for _ in range(n_total): accept, state = sample_MH(chain[-1], log_prob, stepsize) chain.append(state) n_accepted += accept acceptance_rate = n_accepted / float(n_total) return chain, acceptance_rate . chain, acceptance_rate = build_MH_chain(np.array([2.0]), 3.0, 10000, log_prob) . chain = [state for state, in chain] . print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) last_states = &quot;, &quot;.join(&quot;{:.5f}&quot;.format(state) for state in chain[-10:]) print(&quot;Last ten states of chain: &quot; + last_states) . Acceptance rate: 0.720 Last ten states of chain: 1.05847, 1.59966, 0.14389, -1.13281, 0.24131, -0.77448, -0.59703, 0.67707, 1.47065, 1.27361 . def plot_samples(chain, log_prob, ax, orientation=&#39;vertical&#39;, normalize=True, xlims=(-5, 5), legend=True): from scipy.integrate import quad ax.hist(chain, bins=50, density=True, label=&quot;MCMC samples&quot;, orientation=orientation) # we numerically calculate the normalization constant of our PDF if normalize: Z, _ = quad(lambda x: np.exp(log_prob(x)), -np.inf, np.inf) else: Z = 1.0 xses = np.linspace(xlims[0], xlims[1], 1000) yses = [np.exp(log_prob(x)) / Z for x in xses] if orientation == &#39;horizontal&#39;: (yses, xses) = (xses, yses) ax.plot(xses, yses, label=&quot;true distribution&quot;) if legend: ax.legend(frameon=False) fig, ax = plt.subplots() plot_samples(chain[500:], log_prob, ax) despine(ax) ax.set_yticks(()) plt.show() . def sample_and_display(init_state, stepsize, n_total, n_burnin, log_prob): chain, acceptance_rate = build_MH_chain(init_state, stepsize, n_total, log_prob) print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) fig, ax = plt.subplots() plot_samples([state for state, in chain[n_burnin:]], log_prob, ax) despine(ax) ax.set_yticks(()) plt.show() sample_and_display(np.array([2.0]), 30, 10000, 500, log_prob) . Acceptance rate: 0.104 . sample_and_display(np.array([2.0]), 0.1, 10000, 500, log_prob) . Acceptance rate: 0.985 . sample_and_display(np.array([2.0]), 0.1, 500000, 25000, log_prob) . Acceptance rate: 0.990 . %matplotlib notebook %matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [10, 6] np.random.seed(42) def log_gaussian(x, mu, sigma): # The np.sum() is for compatibility with sample_MH return - 0.5 * np.sum((x - mu) ** 2) / sigma ** 2 - np.log(np.sqrt(2 * np.pi * sigma ** 2)) class BivariateNormal(object): n_variates = 2 def __init__(self, mu1, mu2, sigma1, sigma2): self.mu1, self.mu2 = mu1, mu2 self.sigma1, self.sigma2 = sigma1, sigma2 def log_p_x(self, x): return log_gaussian(x, self.mu1, self.sigma1) def log_p_y(self, x): return log_gaussian(x, self.mu2, self.sigma2) def log_prob(self, x): cov_matrix = np.array([[self.sigma1 ** 2, 0], [0, self.sigma2 ** 2]]) inv_cov_matrix = np.linalg.inv(cov_matrix) kernel = -0.5 * (x - self.mu1) @ inv_cov_matrix @ (x - self.mu2).T normalization = np.log(np.sqrt((2 * np.pi) ** self.n_variates * np.linalg.det(cov_matrix))) return kernel - normalization bivariate_normal = BivariateNormal(mu1=0.0, mu2=0.0, sigma1=1.0, sigma2=0.15) . from mpl_toolkits.axes_grid1 import make_axes_locatable fig, ax = plt.subplots() xses = np.linspace(-2, 2, 200) yses = np.linspace(-0.5, 0.5, 200) log_density_values = [[bivariate_normal.log_prob(np.array((x, y))) for x in xses] for y in yses] dx = (xses[1] - xses[0]) / 2 dy = (yses[1] - yses[0]) / 2 extent = [xses[0] - dx, xses[-1] + dx, yses[0] - dy, yses[-1] + dy] im = ax.imshow(np.exp(log_density_values), extent=extent) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) divider = make_axes_locatable(ax) cax = divider.append_axes(&#39;right&#39;, size=&#39;5%&#39;, pad=0.05) cb = fig.colorbar(im, cax=cax) cb.set_label(&#39;probability density&#39;) plt.show() . def sample_gibbs(old_state, bivariate_dist, stepsizes): &quot;&quot;&quot;Draws a single sample using the systematic Gibbs sampling transition kernel Arguments: - old_state: the old (two-dimensional) state of a Markov chain (a list containing two floats) - bivariate_dist: an object representing a bivariate distribution (in our case, an instance of BivariateNormal) - stepsizes: a list of step sizes &quot;&quot;&quot; x_old, y_old = old_state # for compatibility with sample_MH, change floats to one-dimensional # numpy arrays of length one x_old = np.array([x_old]) y_old = np.array([y_old]) # draw new x conditioned on y p_x_y = bivariate_dist.log_p_x accept_x, x_new = sample_MH(x_old, p_x_y, stepsizes[0]) # draw new y conditioned on x p_y_x = bivariate_dist.log_p_y accept_y, y_new = sample_MH(y_old, p_y_x, stepsizes[1]) # Don&#39;t forget to turn the one-dimensional numpy arrays x_new, y_new # of length one back into floats return (accept_x, accept_y), (x_new[0], y_new[0]) . def build_gibbs_chain(init, stepsizes, n_total, bivariate_dist): &quot;&quot;&quot;Builds a Markov chain by performing repeated transitions using the systematic Gibbs sampling transition kernel Arguments: - init: an initial (two-dimensional) state for the Markov chain (a list containing two floats) - stepsizes: a list of step sizes of type float - n_total: the total length of the Markov chain - bivariate_dist: an object representing a bivariate distribution (in our case, an instance of BivariateNormal) &quot;&quot;&quot; init_x, init_k = init chain = [init] acceptances = [] for _ in range(n_total): accept, new_state = sample_gibbs(chain[-1], bivariate_dist, stepsizes) chain.append(new_state) acceptances.append(accept) acceptance_rates = np.mean(acceptances, 0) print(&quot;Acceptance rates: x: {:.3f}, y: {:.3f}&quot;.format(acceptance_rates[0], acceptance_rates[1])) return chain stepsizes = (6.5, 1.0) initial_state = [2.0, -1.0] chain = build_gibbs_chain(initial_state, stepsizes, 100000, bivariate_normal) chain = np.array(chain) . Acceptance rates: x: 0.462, y: 0.456 . def plot_samples_2D(chain, path_length, burnin, ax, xlims=(-3, 3), ylims=(-0.5, 0.5)): chain = np.array(chain) bins = [np.linspace(xlims[0], xlims[1], 100), np.linspace(ylims[0], ylims[1], 100)] ax.hist2d(*chain[burnin:].T, bins=bins) ax.plot(*chain[:path_length].T, marker=&#39;o&#39;, c=&#39;w&#39;, lw=0.4, markersize=1, alpha=0.75) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_xlim(xlims[0], xlims[1]) ax.set_ylim(ylims[0], ylims[1]) def plot_bivariate_samples(chain, burnin, pdf): fig = plt.figure(figsize=(12,7)) ax_c = plt.subplot2grid((4, 4), (1, 0), rowspan=1, colspan=3) plot_samples_2D(chain, 100, burnin, ax_c) ax_t = plt.subplot2grid((4, 4), (0, 0), rowspan=1, colspan=3, sharex=ax_c) plot_samples(chain[:,0], pdf.log_p_x, ax_t, normalize=False) plt.setp(ax_t.get_xticklabels(), visible=False) ax_t.set_yticks(()) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax_t.spines[spine].set_visible(False) ax_r = plt.subplot2grid((4, 4), (1, 3), rowspan=1, colspan=1, sharey=ax_c) plot_samples(chain[:,1], pdf.log_p_y, ax_r, orientation=&#39;horizontal&#39;, normalize=False, legend=False) plt.setp(ax_r.get_yticklabels(), visible=False) ax_r.set_xticks(()) for spine in (&#39;top&#39;, &#39;bottom&#39;, &#39;right&#39;): ax_r.spines[spine].set_visible(False) plt.show() plot_bivariate_samples(chain, burnin=200, pdf=bivariate_normal) . mix_params = dict(mu1=1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7) . fig, ax = plt.subplots() xspace = np.linspace(-0.5, 3, 200) # densities of both components first_component = [np.exp(log_gaussian(x, mix_params[&#39;mu1&#39;], mix_params[&#39;sigma1&#39;])) for x in xspace] second_component = [np.exp(log_gaussian(x, mix_params[&#39;mu2&#39;], mix_params[&#39;sigma2&#39;])) for x in xspace] # apply component weights first_component = mix_params[&#39;w1&#39;] * np.array(first_component) second_component = mix_params[&#39;w2&#39;] * np.array(second_component) ax.plot(xspace, first_component, color=&#39;black&#39;) ax.fill_between(xspace, first_component, alpha=0.6, label=&quot;1st component&quot;) ax.plot(xspace, second_component, color=&#39;black&#39;) ax.fill_between(xspace, second_component, alpha=0.6, label=&quot;2nd component&quot;) ax.set_xlabel(&#39;x&#39;) ax.set_yticks(()) ax.legend(frameon=False) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax.spines[spine].set_visible(False) plt.show() . class GaussianMixture(object): def __init__(self, mu1, mu2, sigma1, sigma2, w1, w2): self.mu1, self.mu2 = mu1, mu2 self.sigma1, self.sigma2 = sigma1, sigma2 self.w1, self.w2 = w1, w2 def log_prob(self, x): return np.logaddexp(np.log(self.w1) + log_gaussian(x, self.mu1, self.sigma1), np.log(self.w2) + log_gaussian(x, self.mu2, self.sigma2)) def log_p_x_k(self, x, k): # logarithm of p(x|k) mu = (self.mu1, self.mu2)[k] sigma = (self.sigma1, self.sigma2)[k] return log_gaussian(x, mu, sigma) def p_k_x(self, k, x): # p(k|x) using Bayes&#39; theorem mu = (self.mu1, self.mu2)[k] sigma = (self.sigma1, self.sigma2)[k] weight = (self.w1, self.w2)[k] log_normalization = self.log_prob(x) return np.exp(log_gaussian(x, mu, sigma) + np.log(weight) - log_normalization) . def sample_gibbs(old_state, mixture, stepsize): &quot;&quot;&quot;Draws a single sample using the systematic Gibbs sampling transition kernel Arguments: - old_state: the old (two-dimensional) state of a Markov chain (a list containing a float and an integer representing the initial mixture component) - mixture: an object representing a mixture of densities (in our case, an instance of GaussianMixture) - stepsize: a step size of type float &quot;&quot;&quot; x_old, k_old = old_state # for compatibility with sample_MH, change floats to one-dimensional # numpy arrays of length one x_old = np.array([x_old]) # draw new x conditioned on k x_pdf = lambda x: mixture.log_p_x_k(x, k_old) accept, x_new = sample_MH(x_old, x_pdf, stepsize) # ... turn the one-dimensional numpy arrays of length one back # into floats x_new = x_new[0] # draw new k conditioned on x k_probabilities = (mixture.p_k_x(0, x_new), mixture.p_k_x(1, x_new)) jump_probability = k_probabilities[1 - k_old] k_new = np.random.choice((0,1), p=k_probabilities) return accept, jump_probability, (x_new, k_new) def build_gibbs_chain(init, stepsize, n_total, mixture): &quot;&quot;&quot;Builds a Markov chain by performing repeated transitions using the systematic Gibbs sampling transition kernel Arguments: - init: an initial (two-dimensional) state of a Markov chain (a list containing a one-dimensional numpy array of length one and an integer representing the initial mixture component) - stepsize: a step size of type float - n_total: the total length of the Markov chain - mixture: an object representing a mixture of densities (in our case, an instance of GaussianMixture) &quot;&quot;&quot; init_x, init_k = init chain = [init] acceptances = [] jump_probabilities = [] for _ in range(n_total): accept, jump_probability, new_state = sample_gibbs(chain[-1], mixture, stepsize) chain.append(new_state) jump_probabilities.append(jump_probability) acceptances.append(accept) acceptance_rates = np.mean(acceptances) print(&quot;Acceptance rate: x: {:.3f}&quot;.format(acceptance_rates)) print(&quot;Average probability to change mode: {}&quot;.format(np.mean(jump_probabilities))) return chain mixture = GaussianMixture(**mix_params) stepsize = 1.0 initial_state = [2.0, 1] chain = build_gibbs_chain(initial_state, stepsize, 10000, mixture) burnin = 1000 x_states = [state[0] for state in chain[burnin:]] . Acceptance rate: x: 0.631 Average probability to change mode: 0.08629295966662387 . fig, ax = plt.subplots() plot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-1,2.5)) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax.spines[spine].set_visible(False) ax.set_yticks(()) ax.set_xlabel(&#39;x&#39;) plt.show() . mixture = GaussianMixture(mu1=-1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7) stepsize = 1.0 initial_state = [2.0, 1] chain = build_gibbs_chain(initial_state, stepsize, 100000, mixture) burnin = 10000 x_states = [state[0] for state in chain[burnin:]] . Acceptance rate: x: 0.558 Average probability to change mode: 6.139534006013391e-06 . fig, ax = plt.subplots() plot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-2,2.5)) for spine in (&#39;top&#39;, &#39;left&#39;, &#39;right&#39;): ax.spines[spine].set_visible(False) ax.set_yticks(()) ax.set_xlabel(&#39;x&#39;) plt.show() . Hamiltonian Monte Carlo . import numpy as np import matplotlib.pyplot as plt np.random.seed(42) xspace = np.linspace(-2, 2, 100) unnormalized_probs = np.exp(-0.5 * xspace ** 2) energies = 0.5 * xspace ** 2 fig, ax = plt.subplots(dpi=80) ax.plot(xspace, unnormalized_probs, label=r&quot;$p(x)$&quot;) ax.plot(xspace, energies, label=r&quot;$E(x)=- log p(x)$&quot;) prop = dict(arrowstyle=&quot;-|&gt;,head_width=0.4,head_length=0.8&quot;, shrinkA=0,shrinkB=0) x_index1 = 75 ax.scatter((xspace[x_index1],), (energies[x_index1],), color=&quot;k&quot;) a_start1 = np.array((xspace[x_index1], energies[x_index1])) a_end1 = np.array((xspace[x_index1] - xspace[x_index1], energies[x_index1])) ax.annotate(&quot;&quot;,a_end1, a_start1, arrowprops=prop) text_pos1 = (a_start1[0] + 0.5 * (a_end1[0] - a_start1[0]), a_end1[1] + 0.075) ax.text(*text_pos1, r&quot;force&quot;, horizontalalignment=&quot;center&quot;) x_index2 = 38 ax.scatter((xspace[x_index2],), (energies[x_index2],), color=&quot;k&quot;) a_start2 = np.array((xspace[x_index2], energies[x_index2])) a_end2 = np.array((xspace[x_index2] - xspace[x_index2], energies[x_index2])) ax.annotate(&quot;&quot;,a_end2, a_start2, arrowprops=prop) text_pos2 = (a_start2[0] + 0.5 * (a_end2[0] - a_start2[0]), a_end2[1] + 0.075, ) ax.text(*text_pos2, r&quot;force&quot;, horizontalalignment=&quot;center&quot;) ax.set_xlabel(&quot;x&quot;) ax.set_yticks(()) for spine in (&#39;top&#39;, &#39;right&#39;, &#39;left&#39;): ax.spines[spine].set_visible(False) ax.legend(frameon=False) plt.show() . def leapfrog(x, v, gradient, timestep, trajectory_length): v -= 0.5 * timestep * gradient(x) for _ in range(trajectory_length - 1): x += timestep * v v -= timestep * gradient(x) x += timestep * v v -= 0.5 * timestep * gradient(x) return x, v . def sample_HMC(x_old, log_prob, log_prob_gradient, timestep, trajectory_length): # switch to physics mode! def E(x): return -log_prob(x) def gradient(x): return -log_prob_gradient(x) def K(v): return 0.5 * np.sum(v ** 2) def H(x, v): return K(v) + E(x) # Metropolis acceptance probability, implemented in &quot;logarithmic space&quot; # for numerical stability: def log_p_acc(x_new, v_new, x_old, v_old): return min(0, -(H(x_new, v_new) - H(x_old, v_old))) # give a random kick to particle by drawing its momentum from p(v) v_old = np.random.normal(size=x_old.shape) # approximately calculate position x_new and momentum v_new after # time trajectory_length * timestep x_new, v_new = leapfrog(x_old.copy(), v_old.copy(), gradient, timestep, trajectory_length) # accept / reject based on Metropolis criterion accept = np.log(np.random.random()) &lt; log_p_acc(x_new, v_new, x_old, v_old) # we consider only the position x (meaning, we marginalize out v) if accept: return accept, x_new else: return accept, x_old . def build_HMC_chain(init, timestep, trajectory_length, n_total, log_prob, gradient): n_accepted = 0 chain = [init] for _ in range(n_total): accept, state = sample_HMC(chain[-1].copy(), log_prob, gradient, timestep, trajectory_length) chain.append(state) n_accepted += accept acceptance_rate = n_accepted / float(n_total) return chain, acceptance_rate . def log_prob(x): return -0.5 * np.sum(x ** 2) . def log_prob_gradient(x): return -x . chain, acceptance_rate = build_HMC_chain(np.array([5.0, 1.0]), 1.5, 10, 10000, log_prob, log_prob_gradient) print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) . Acceptance rate: 0.622 . fig, ax = plt.subplots(dpi=80) plot_samples_2D(chain, 100, 200, ax, xlims=(-5.5, 5.5), ylims=(-5.5, 5.5)) plt.show() . chain, acceptance_rate = build_MH_chain(np.array([5.0, 1.0]), 2.6, 10000, log_prob) print(&quot;Acceptance rate: {:.3f}&quot;.format(acceptance_rate)) . Acceptance rate: 0.623 . fig, ax = plt.subplots(dpi=80) plot_samples_2D(chain, 100, 200, ax, xlims=(-5.5, 5.5), ylims=(-5.5, 5.5)) plt.show() . Replica Exchange . mix_params = dict(mu1=-1.5, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7) mixture = GaussianMixture(**mix_params) temperatures = [0.1, 0.4, 0.6, 0.8, 1.0] . from scipy.integrate import quad def plot_tempered_distributions(log_prob, temperatures, axes, xlim=(-4, 4)): xspace = np.linspace(*xlim, 1000) for i, (temp, ax) in enumerate(zip(temperatures, axes)): pdf = lambda x: np.exp(temp * log_prob(x)) Z = quad(pdf, -1000, 1000)[0] ax.plot(xspace, np.array(list(map(pdf, xspace))) / Z) ax.text(0.8, 0.3, r&#39;$ beta={}$&#39;.format(temp), transform=ax.transAxes) ax.text(0.05, 0.3, &#39;replica {}&#39;.format(len(temperatures) - i - 1), transform=ax.transAxes) ax.set_yticks(()) plt.show() fig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True, figsize=(8, 7)) plot_tempered_distributions(mixture.log_prob, temperatures, axes) plt.show() . def handle_left_border(leftmost_old_state, leftmost_temperature, leftmost_stepsize, log_prob, new_multistate): accepted, state = sample_MH(leftmost_old_state, lambda x: leftmost_temperature * log_prob(x), leftmost_stepsize) new_multistate = [state] + new_multistate return new_multistate, accepted def handle_right_border(rightmost_old_state, rightmost_temperature, rightmost_stepsize, log_prob, new_multistate): accepted, state = sample_MH(rightmost_old_state, lambda x: rightmost_temperature * log_prob(x), rightmost_stepsize) new_multistate = new_multistate + [state] return new_multistate, accepted def build_RE_chain(init, stepsizes, n_total, temperatures, swap_interval, log_prob): from itertools import cycle n_replicas = len(temperatures) # a bunch of arrays in which we will store how many # Metropolis-Hastings / swap moves were accepted # and how many there were performed in total accepted_MH_moves = np.zeros(n_replicas) total_MH_moves = np.zeros(n_replicas) accepted_swap_moves = np.zeros(n_replicas - 1) total_swap_moves = np.zeros(n_replicas - 1) cycler = cycle((True, False)) chain = [init] for k in range(n_total): new_multistate = [] if k &gt; 0 and k % swap_interval == 0: # perform RE swap # First, determine the swap partners if next(cycler): # swap (0,1), (2,3), ... partners = [(j-1, j) for j in range(1, n_replicas, 2)] else: # swap (1,2), (3,4), ... partners = [(j-1, j) for j in range(2, len(temperatures), 2)] # Now, for each pair of replicas, attempt an exchange for (i,j) in partners: bi, bj = temperatures[i], temperatures[j] lpi, lpj = log_prob(chain[-1][i]), log_prob(chain[-1][j]) log_p_acc = min(0, bi * lpj - bi * lpi + bj * lpi - bj * lpj) if np.log(np.random.uniform()) &lt; log_p_acc: new_multistate += [chain[-1][j], chain[-1][i]] accepted_swap_moves[i] += 1 else: new_multistate += [chain[-1][i], chain[-1][j]] total_swap_moves[i] += 1 # We might have border cases: if left- / rightmost replicas don&#39;t participate # in swaps, have them draw a sample if partners[0][0] != 0: new_multistate, accepted = handle_left_border(chain[-1][0], temperatures[0], stepsizes[0], log_prob, new_multistate) accepted_MH_moves[0] += accepted total_MH_moves[0] += 1 if partners[-1][1] != len(temperatures) - 1: new_multistate, accepted = handle_right_border(chain[-1][-1], temperatures[-1], stepsizes[-1], log_prob, new_multistate) accepted_MH_moves[-1] += accepted total_MH_moves[-1] += 1 else: # perform sampling in single chains for j, temp in enumerate(temperatures): accepted, state = sample_MH(chain[-1][j], lambda x: temp * log_prob(x), stepsizes[j]) accepted_MH_moves[j] += accepted total_MH_moves[j] += 1 new_multistate.append(state) chain.append(new_multistate) # calculate acceptance rates MH_acceptance_rates = accepted_MH_moves / total_MH_moves # safe division in case of zero total swap moves swap_acceptance_rates = np.divide(accepted_swap_moves, total_swap_moves, out=np.zeros(n_replicas - 1), where=total_swap_moves != 0) return MH_acceptance_rates, swap_acceptance_rates, np.array(chain) . stepsizes = [2.75, 2.5, 2.0, 1.75, 1.6] . def print_MH_acceptance_rates(mh_acceptance_rates): print(&quot;MH acceptance rates: &quot; + &quot;&quot;.join([&quot;{}: {:.3f} &quot;.format(i, x) for i, x in enumerate(mh_acceptance_rates)])) mh_acc_rates, swap_acc_rates, chains = build_RE_chain(np.random.uniform(low=-3, high=3, size=len(temperatures)), stepsizes, 10000, temperatures, 500000000, mixture.log_prob) print_MH_acceptance_rates(mh_acc_rates) . MH acceptance rates: 0: 0.790 1: 0.551 2: 0.415 3: 0.552 4: 0.705 . def plot_RE_samples(chains, axes, bins=np.linspace(-4, 4, 50)): for i, (chain, ax) in enumerate(zip(chains, axes)): ax.hist(chain, bins, density=True, label=&quot;MCMC samples&quot;) fig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True, figsize=(8, 7)) plot_RE_samples(chains[100:].T, axes) plot_tempered_distributions(mixture.log_prob, temperatures, axes) plt.show() . init = np.random.uniform(low=-4, high=4, size=len(temperatures)) mh_acc_rates, swap_acc_rates, chains = build_RE_chain(init, stepsizes, 10000, temperatures, 5, mixture.log_prob) print_MH_acceptance_rates(mh_acc_rates) swap_rate_string = &quot;&quot;.join([&quot;{}&lt;-&gt;{}: {:.3f}, &quot;.format(i, i+1, x) for i, x in enumerate(swap_acc_rates)])[:-2] print(&quot;Swap acceptance rates:&quot;, swap_rate_string) . MH acceptance rates: 0: 0.797 1: 0.552 2: 0.539 3: 0.499 4: 0.466 Swap acceptance rates: 0&lt;-&gt;1: 0.585, 1&lt;-&gt;2: 0.846, 2&lt;-&gt;3: 0.829, 3&lt;-&gt;4: 0.876 . fig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True, figsize=(8, 7)) plot_RE_samples(chains[100:].T, axes) plot_tempered_distributions(mixture.log_prob, temperatures, axes) plt.show() . # Detect swaps. This method works only under the assumption that # when performing local MCMC moves, starting from two different # initial states, you cannot end up with the same state swaps = {} # for each pair of chains... for i in range(len(chains) - 1): # shift one chain by one state to the left. # Where states from both chains match up, a successful exchange # was performed matches = np.where(chains[i, :-1] == chains[i+1, 1:])[0] if len(matches) &gt; 0: swaps[i] = matches # Reconstruct trajectories of single states through the temperature # ladder def reconstruct_trajectory(start_index, chains): res = [] current_ens = start_index for i in range(len(chains)): res.append(current_ens) if i in swaps: if current_ens in swaps[i]: current_ens += 1 elif current_ens in swaps[i] + 1: current_ens -= 1 return np.array(res) def plot_state_trajectories(trajectories, ax, max_samples=300): for trajectory in trajectories: ax.plot(-trajectory[:max_samples] - 1, lw=2) ax.set_xlabel(&quot;# of MCMC samples&quot;) ax.set_ylabel(r&quot;inverse temperature $ beta$&quot;) # make order of temperatures appear as above - whatever it takes... ax.set_yticks(range(-len(temperatures), 0)) ax.set_yticklabels(temperatures[::-1]) # which states to follow start_state_indices = (4, 0) fig, ax = plt.subplots(figsize=(8, 6)) trajectories = np.array([reconstruct_trajectory(i, chains) for i in start_state_indices]) plot_state_trajectories(trajectories, ax) plt.show() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/03/MCMC.html",
            "relUrl": "/2020/11/03/MCMC.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "T-Tests The Purpose, Assumptions, How it Works, and Corrections.",
            "content": ". Purpose: . The purpose of the T-test is to compare if there are mean differences between two groups of interest. . Assumptions: . Normal distribution of the dependant variable | Independent and identically distributed (i.i.d) variables | . How it works: . T-test comparisons use the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term is known, where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student’s t distribution. This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal. . Corrections: . When we are interested in comparing statistical differences between more than two groups, we may use ANOVA, but if we want to compare differences between more than a single pairwise comparison, we can conduct multiple t-tests. In doing so, we will end up increasing the likelihood of a false positive (type I error) where we are incorrectly rejecting the null hypothesis that there are no statistical differences between groups. One way to address this is to use the Bonferroni correction. . Bonferroni Correction . The Bonferroni correction, the namesake of Carlo Emilio Bonferroni, accounts for what we lose in a p-hacking quest in the experimentation, which is the justification for taking p-values at face value. By intuition, when we go searching for significant differences everywhere, the chance of seeing an apparent significant difference by chance anywhere increases. Using the Bonferroni correction, if the starting alpha/significance level is .05 and we are testing 10 hypotheses, then the corrected alpha/significance level we should use would be .005. Understanding the lack of an incentive to make such an adjustment is straightforward. Another way to address this is to first use ANOVA to detect statistical differences between all groups before deciding whether to use t tests to look for pairwise comparisons between groups. . python ttest.py .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/statistics/2020/11/02/statistics-ttests-post.html",
            "relUrl": "/statistics/2020/11/02/statistics-ttests-post.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Pandas profiling and Shap values for European Soccer Match Data",
            "content": "This post includes code and notes from this gist and this post. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . %load_ext sql . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . tables = pd.read_sql(&quot;&quot;&quot;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&quot;&quot;&quot;, connection) tables . type name tbl_name rootpage sql . 0 table | sqlite_sequence | sqlite_sequence | 4 | CREATE TABLE sqlite_sequence(name,seq) | . 1 table | Player_Attributes | Player_Attributes | 11 | CREATE TABLE &quot;Player_Attributes&quot; ( n t`id` tIN... | . 2 table | Player | Player | 14 | CREATE TABLE `Player` ( n t`id` tINTEGER PRIMA... | . 3 table | Match | Match | 18 | CREATE TABLE `Match` ( n t`id` tINTEGER PRIMAR... | . 4 table | League | League | 24 | CREATE TABLE `League` ( n t`id` tINTEGER PRIMA... | . 5 table | Country | Country | 26 | CREATE TABLE `Country` ( n t`id` tINTEGER PRIM... | . 6 table | Team | Team | 29 | CREATE TABLE &quot;Team&quot; ( n t`id` tINTEGER PRIMARY... | . 7 table | Team_Attributes | Team_Attributes | 2 | CREATE TABLE `Team_Attributes` ( n t`id` tINTE... | . 8 table | Match_df | Match_df | 3 | CREATE TABLE Match_df( n id INT, n country_n... | . 9 table | Match_Wins | Match_Wins | 308451 | CREATE TABLE Match_Wins( n id INT, n country... | . %%sql SELECT * FROM Match LIMIT 3; . Environment variable $DATABASE_URL not set, and no connect string given. Connection info needed in SQLAlchemy format, example: postgresql://username:password@hostname/dbname or an existing connection: dict_keys([]) . connection . &lt;sqlalchemy.engine.base.Connection at 0x7f785f788c50&gt; . match_wins = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Match_Wins;&quot;&quot;&quot;, connection) . # sql_query = %sql SELECT * FROM Match_Wins # df = sql_query.DataFrame() # df . match_wins . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | 0 | . 25975 25946 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | 1 | . 25976 25947 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | 0 | . 25977 25948 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | 0 | . 25978 25949 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | 1 | . 25979 rows × 11 columns . import matplotlib.pyplot as plt from pandas_profiling import ProfileReport profile = ProfileReport(match_wins, title=&#39;Pandas Profiling Report&#39;) . profile.to_widgets() . . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . 5 24614 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | AC Bellinzona | Neuchâtel Xamax | 1 | 2 | 0 | . 6 24615 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Zürich | FC Luzern | 1 | 0 | 1 | . 7 24616 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Sion | BSC Young Boys | 2 | 1 | 1 | . 8 24617 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Vaduz | FC Aarau | 0 | 2 | 0 | . 9 24668 | Switzerland | Switzerland Super League | 2008/2009 | 3 | 2008-07-26 00:00:00 | FC Basel | AC Bellinzona | 2 | 0 | 1 | . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 25969 25940 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | Grasshopper Club Zürich | FC Thun | 0 | 0 | 0 | . 25970 25941 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | FC Sion | FC Zürich | 2 | 2 | 0 | . 25971 25942 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | FC Vaduz | Lugano | 0 | 0 | 0 | . 25972 25943 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | BSC Young Boys | FC Basel | 2 | 3 | 0 | . 25973 25944 | Switzerland | Switzerland Super League | 2015/2016 | 35 | 2016-05-22 00:00:00 | FC St. Gallen | FC Luzern | 1 | 4 | 0 | . 25974 25945 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | 0 | . 25975 25946 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | 1 | . 25976 25947 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | 0 | . 25977 25948 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | 0 | . 25978 25949 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | 1 | . profile.to_notebook_iframe() . . profile.to_file(output_file=&quot;pandas_profiling.html&quot;) . . match_wins.head() . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . cols = match_wins.columns colours = [&#39;darkblue&#39;, &#39;red&#39;] sns.heatmap(match_wins[cols].isnull(), cmap=sns.color_palette(colours)) . &lt;AxesSubplot:&gt; . # top = match_wins[&quot;home_team_win&quot;].describe()[&#39;top&#39;] # impute with the most frequent value. # match_wins[&quot;home_team_win&quot;] = match_wins[&quot;home_team_win&quot;].fillna(top) . pct_list = [] for col in match_wins.columns: pct_missing = np.mean(match_wins[col].isnull()) if round(pct_missing*100) &gt;0: pct_list.append([col, round(pct_missing*100)]) print(&#39;{} - {}%&#39;.format(col, round(pct_missing*100))) . id - 0% country_name - 0% league_name - 0% season - 0% stage - 0% date - 0% home_team - 0% away_team - 0% home_team_goal - 0% away_team_goal - 0% home_team_win - 0% . match_wins.country_name . 0 Switzerland 1 Switzerland 2 Switzerland 3 Switzerland 4 Switzerland ... 25974 Switzerland 25975 Switzerland 25976 Switzerland 25977 Switzerland 25978 Switzerland Name: country_name, Length: 25979, dtype: object . # # extracting the titles from the names: # Title = [] # for name in match_wins.country_name: # Title.append(name.split(&quot;,&quot;)[1].split(&quot;.&quot;)[0]) # match_wins[&quot;Team&quot;] = Title . match_wins.groupby([&quot;home_team&quot;, &#39;season&#39;])[&#39;home_team_win&#39;].agg([&#39;sum&#39;]).round(0) . sum . home_team season . 1. FC Kaiserslautern 2010/2011 6 | . 2011/2012 2 | . 1. FC Köln 2008/2009 4 | . 2009/2010 3 | . 2010/2011 11 | . ... ... ... | . Śląsk Wrocław 2011/2012 9 | . 2012/2013 9 | . 2013/2014 5 | . 2014/2015 9 | . 2015/2016 5 | . 1478 rows × 1 columns . df = df.drop(columns = [&quot;Name&quot;]) df = df.drop(columns = [&quot;PassengerId&quot;]) df = df.drop(columns = [&quot;Ticket&quot;]) . match_wins.dtypes . id int64 country_name int8 league_name int8 season int8 stage int64 date int16 home_team int16 away_team int16 home_team_goal int64 away_team_goal int64 home_team_win int64 dtype: object . match_wins.country_name = pd.Categorical(match_wins.country_name) match_wins.league_name = pd.Categorical(match_wins.league_name) match_wins.season = pd.Categorical(match_wins.season) match_wins.date = pd.Categorical(match_wins.date) . match_wins[&quot;country_name&quot;] = match_wins.country_name.cat.codes . match_wins[&quot;league_name&quot;] = match_wins.league_name.cat.codes match_wins[&quot;season&quot;] = match_wins.season.cat.codes match_wins[&quot;date&quot;] = match_wins.date.cat.codes . match_wins.home_team = pd.Categorical(match_wins.home_team) . match_wins.away_team = pd.Categorical(match_wins.away_team) . match_wins[&quot;away_team&quot;] = match_wins.away_team.cat.codes . match_wins[&quot;home_team&quot;] = match_wins.home_team.cat.codes . match_wins[&quot;home_team&quot;] . 0 24 1 72 2 84 3 173 4 76 ... 25974 76 25975 160 25976 84 25977 95 25978 100 Name: home_team, Length: 25979, dtype: int16 . match_wins.date = pd.Categorical(match_wins.date) . match_wins[&quot;date&quot;] = match_wins.date.cat.codes . match_wins . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | 10 | 10 | 0 | 1 | 0 | 24 | 76 | 1 | 2 | 0 | . 1 24560 | 10 | 10 | 0 | 1 | 1 | 72 | 91 | 3 | 1 | 1 | . 2 24561 | 10 | 10 | 0 | 1 | 2 | 84 | 98 | 1 | 2 | 0 | . 3 24562 | 10 | 10 | 0 | 1 | 2 | 173 | 100 | 1 | 2 | 0 | . 4 24613 | 10 | 10 | 0 | 2 | 3 | 76 | 117 | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | 10 | 10 | 7 | 36 | 1693 | 76 | 117 | 0 | 1 | 0 | . 25975 25946 | 10 | 10 | 7 | 36 | 1693 | 160 | 93 | 3 | 0 | 1 | . 25976 25947 | 10 | 10 | 7 | 36 | 1693 | 84 | 91 | 2 | 2 | 0 | . 25977 25948 | 10 | 10 | 7 | 36 | 1693 | 95 | 24 | 0 | 3 | 0 | . 25978 25949 | 10 | 10 | 7 | 36 | 1693 | 100 | 98 | 3 | 1 | 1 | . 25979 rows × 11 columns . match_wins.dtypes . id int64 country_name int8 league_name int8 season int8 stage int64 date int16 home_team int16 away_team int16 home_team_goal int64 away_team_goal int64 home_team_win int64 dtype: object . #match_wins = match_wins.drop(columns = [&quot;Title&quot;]) target = match_wins.home_team_win.values match_wins = match_wins.drop(columns =[&quot;home_team_win&quot;]) . match_wins . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 24559 | 10 | 10 | 0 | 1 | 0 | 24 | 76 | 1 | 2 | . 1 24560 | 10 | 10 | 0 | 1 | 1 | 72 | 91 | 3 | 1 | . 2 24561 | 10 | 10 | 0 | 1 | 2 | 84 | 98 | 1 | 2 | . 3 24562 | 10 | 10 | 0 | 1 | 2 | 173 | 100 | 1 | 2 | . 4 24613 | 10 | 10 | 0 | 2 | 3 | 76 | 117 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | 10 | 10 | 7 | 36 | 1693 | 76 | 117 | 0 | 1 | . 25975 25946 | 10 | 10 | 7 | 36 | 1693 | 160 | 93 | 3 | 0 | . 25976 25947 | 10 | 10 | 7 | 36 | 1693 | 84 | 91 | 2 | 2 | . 25977 25948 | 10 | 10 | 7 | 36 | 1693 | 95 | 24 | 0 | 3 | . 25978 25949 | 10 | 10 | 7 | 36 | 1693 | 100 | 98 | 3 | 1 | . 25979 rows × 10 columns . target . array([0, 1, 0, ..., 0, 0, 1]) . from sklearn.model_selection import train_test_split . x_train, x_test, y_train, y_test = train_test_split(match_wins, target, test_size=0.2, random_state=0) . from sklearn.linear_model import LogisticRegression . LR = LogisticRegression() LR.fit(x_train, y_train) . /home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression() . LR.score(x_test, y_test) . 0.9736335642802155 . import shap explainer = shap.LinearExplainer(LR, x_train, feature_perturbation=&quot;interventional&quot;) shap_values = explainer.shap_values(x_test) shap.summary_plot(shap_values, x_test) . shap.dependence_plot(&quot;home_team&quot;, shap_values, x_test) . shap.summary_plot(shap_values, x_train, plot_type=&quot;bar&quot;) . shap.initjs() shap.force_plot(explainer.expected_value, shap_values, x_test, link=&quot;logit&quot;) . shap.plots.force is slow for many thousands of rows, try subsampling your data. . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() shap.force_plot(explainer.expected_value, shap_values[0,:], x_test.iloc[0,:], link=&quot;logit&quot;) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. shap.initjs() shap.force_plot(explainer.expected_value, shap_values[3,:], x_test.iloc[3,:], link=&quot;logit&quot;) . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written.",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/02/pandas_prof_shap_values.html",
            "relUrl": "/2020/11/02/pandas_prof_shap_values.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Data Science Content",
            "content": "Machine Learning . Pyspark Content . Pyspark Style Guide . | Spark Joins . | . Style Guides . Python Style Guide | . Reinforcement Learning . Reinforcement Learning Project | . Principal Component Analysis (PCA) . Principal Component Analysis (PCA) with Scikit-learn . Deep Learning . https://www.fast.ai/ https://udemy.com/course/deeplearning/ https://udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp https://github.com/jeffheaton/t81_558_deep_learning . Data Engineering Resources . Data Engineering Resources | . NLP . BERT Using BERT for Unlabeled Data | | . | . Docker . Using Docker for Data Science | | . Github Roadmaps . VS Code Roadmap | . APIs . News API | . Startups . Startup Financial Modeling | . Content . Explainable Machine Learning . LIME explanations for classification results | . Mapping with Python . Hex Maps for Spatial Data . | Choropleth using the Plot.ly . | . Organization . Organizing Folders with Python . Economics and Data Science . An Economist’s Value in Data Science Covers Econometrics. Economists ‘have a unique set of skills that allows them to apply statistics in a very similar fashion to data scientists, while also being able to evaluate bias, externalities, and comprehend the math behind algorithms. Additionally, their experience analyzing businesses provide an ability to question and discuss the value of models relative to business goals.’ . Statistics .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/11/01/ds-content.html",
            "relUrl": "/data%20science%20content/2020/11/01/ds-content.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . Find the quandl api documentation here - . from sklearn.datasets import fetch_california_housing california = fetch_california_housing() X = california.data y = california.target * 100000 print(f&#39;Data shape is {X.shape}&#39;) print(f&#39;Target shape is {y.shape}&#39;) . Data shape is (20640, 8) Target shape is (20640,) . import pandas as pd import numpy as np import matplotlib.pyplot as plt import quandl quandl.ApiConfig.api_key = &#39;&#39; %matplotlib inline . quandl_call = ( &quot;ZILLOW/{category}{code}_{indicator}&quot; ) def download_data(category, code, indicator): &quot;&quot;&quot; Reads in a single dataset from Zillow Quandl API Parameters - category : &quot;Chicago_Area&quot; or &quot;Evanston&quot; code : &quot;Evanston&quot; or &quot;Chicago&quot; indicator : &quot;Sales_Price&quot; or &quot;other&quot; Returns - DataFrame &quot;&quot;&quot; AREA_CATEGORY_dict = {&quot;Evanston&quot;: &quot;C&quot;, &quot;Chicago_Area&quot;: &quot;C&quot;} AREA_CODE_dict = {&quot;Evanston&quot;: &quot;64604&quot;, &quot;Chicago&quot;: &quot;36156&quot;} INDICATOR_CODE_dict = {&quot;Sales_Price&quot;: &quot;SP&quot;} category = AREA_CATEGORY_dict[category] code = AREA_CODE_dict[code] indicator = INDICATOR_CODE_dict[indicator] return quandl.get(quandl_call.format(category=category, code=code, indicator=indicator)) . # data = quandl.get_table(&quot;ZILLOW/REGIONS&quot;, paginate=True) . # col = &#39;region&#39; # mask = np.column_stack([data[col].str.contains(r&quot;Boston&quot;, na=False) for col in data]) # data.loc[mask.any(axis=1)] . # col = &#39;region&#39; # mask = np.column_stack([data[col].str.contains(r&quot;Evanston&quot;, na=False) for col in data]) # df=data.loc[mask.any(axis=1)] #df[&#39;region&#39;] . Chicago and Evanston Home Sale Prices . EV_SP = download_data(&#39;Chicago_Area&#39;, &#39;Evanston&#39;, &#39;Sales_Price&#39;) CH_SP = download_data(&#39;Chicago_Area&#39;, &#39;Chicago&#39;, &#39;Sales_Price&#39;) . CH_SP.query(&quot;Value &gt; 270000&quot;) . Value . Date . 2008-03-31 325100.0 | . 2008-04-30 314800.0 | . 2008-05-31 286900.0 | . 2008-06-30 274600.0 | . 2019-03-31 290800.0 | . 2019-04-30 292000.0 | . 2019-05-31 276000.0 | . 2019-06-30 271500.0 | . 2020-01-31 281400.0 | . 2020-02-29 302900.0 | . 2020-03-31 309200.0 | . from pandasql import sqldf pysqldf = lambda q: sqldf(q, globals()) q = &quot;&quot;&quot;SELECT date ,Value FROM CH_SP WHERE Value &gt; 270000 LIMIT 10;&quot;&quot;&quot; values = pysqldf(q) values . Date Value . 0 2008-03-31 00:00:00.000000 | 325100.0 | . 1 2008-04-30 00:00:00.000000 | 314800.0 | . 2 2008-05-31 00:00:00.000000 | 286900.0 | . 3 2008-06-30 00:00:00.000000 | 274600.0 | . 4 2019-03-31 00:00:00.000000 | 290800.0 | . 5 2019-04-30 00:00:00.000000 | 292000.0 | . 6 2019-05-31 00:00:00.000000 | 276000.0 | . 7 2019-06-30 00:00:00.000000 | 271500.0 | . 8 2020-01-31 00:00:00.000000 | 281400.0 | . 9 2020-02-29 00:00:00.000000 | 302900.0 | . fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Value&#39;) EV_SP[&#39;Value&#39;].plot(label=&#39;Evanston&#39;) CH_SP[&#39;Value&#39;].plot(label=&#39;Chicago&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f3bf2e42c18&gt; . import seaborn as sns from scipy.stats import norm sns.distplot(CH_SP[&#39;Value&#39;], fit=norm); . sns.distplot(EV_SP[&#39;Value&#39;], fit=norm); .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/11/01/Housing_Prediction.html",
            "relUrl": "/2020/11/01/Housing_Prediction.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Data Sources",
            "content": "Fiscal Transfer Data . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . Stroke Data . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . Diabetes . import pandas as pd url = &#39;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv&#39; df = pd.read_csv(url, error_bad_lines=False) df . df = h2o.import_file(&quot;https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv&quot;, destination_frame=&quot;df&quot;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20sources/2020/11/01/Data-Sources.html",
            "relUrl": "/data%20sources/2020/11/01/Data-Sources.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
            "content": "This post includes code and notes from data-analysis-using-sql. . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . %load_ext sql . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . connection . &lt;sqlalchemy.engine.base.Connection at 0x7f70c27cc0b8&gt; . tables = pd.read_sql(&quot;&quot;&quot;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&quot;&quot;&quot;, connection) tables . type name tbl_name rootpage sql . 0 table | sqlite_sequence | sqlite_sequence | 4 | CREATE TABLE sqlite_sequence(name,seq) | . 1 table | Player_Attributes | Player_Attributes | 11 | CREATE TABLE &quot;Player_Attributes&quot; ( n t`id` tIN... | . 2 table | Player | Player | 14 | CREATE TABLE `Player` ( n t`id` tINTEGER PRIMA... | . 3 table | Match | Match | 18 | CREATE TABLE `Match` ( n t`id` tINTEGER PRIMAR... | . 4 table | League | League | 24 | CREATE TABLE `League` ( n t`id` tINTEGER PRIMA... | . 5 table | Country | Country | 26 | CREATE TABLE `Country` ( n t`id` tINTEGER PRIM... | . 6 table | Team | Team | 29 | CREATE TABLE &quot;Team&quot; ( n t`id` tINTEGER PRIMARY... | . 7 table | Team_Attributes | Team_Attributes | 2 | CREATE TABLE `Team_Attributes` ( n t`id` tINTE... | . # %%sql # SELECT * # FROM sqlite_master # WHERE type=&#39;table&#39; # ; . engine.execute(&quot;SELECT * FROM Country LIMIT 10&quot;).fetchall() . [(1, &#39;Belgium&#39;), (1729, &#39;England&#39;), (4769, &#39;France&#39;), (7809, &#39;Germany&#39;), (10257, &#39;Italy&#39;), (13274, &#39;Netherlands&#39;), (15722, &#39;Poland&#39;), (17642, &#39;Portugal&#39;), (19694, &#39;Scotland&#39;), (21518, &#39;Spain&#39;)] . %%sql SELECT * FROM Match LIMIT 3; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . 2 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492474 | 10000 | 9994 | 0 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.95 | 3.2 | 3.6 | 1.8 | 3.3 | 3.95 | 1.9 | 3.2 | 3.5 | 1.9 | 3.2 | 3.5 | None | None | None | 1.83 | 3.3 | 3.6 | 1.95 | 3.3 | 3.8 | 2 | 3.25 | 3.25 | 1.85 | 3.25 | 3.75 | 1.91 | 3.25 | 3.6 | . 3 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492475 | 9984 | 8635 | 0 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.38 | 3.3 | 2.75 | 2.4 | 3.3 | 2.55 | 2.6 | 3.1 | 2.3 | 2.5 | 3.2 | 2.5 | None | None | None | 2.5 | 3.25 | 2.4 | 2.63 | 3.3 | 2.5 | 2.35 | 3.25 | 2.65 | 2.5 | 3.2 | 2.5 | 2.3 | 3.2 | 2.75 | . # %%sql # DROP TABLE IF EXISTS Team_table # CREATE TABLE Team_table AS # SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . [] . sql_query = %sql SELECT * FROM Team_table LIMIT 10 df = sql_query.DataFrame() df . * sqlite:///database.sqlite Done. . countries = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Country;&quot;&quot;&quot;, connection) countries.style.highlight_max() . id name . 0 1 | Belgium | . 1 1729 | England | . 2 4769 | France | . 3 7809 | Germany | . 4 10257 | Italy | . 5 13274 | Netherlands | . 6 15722 | Poland | . 7 17642 | Portugal | . 8 19694 | Scotland | . 9 21518 | Spain | . 10 24558 | Switzerland | . # leagues = pd.read_sql(&quot;&quot;&quot;SELECT * # FROM League # JOIN Country ON Country.id = League.country_id;&quot;&quot;&quot;, connection) # leagues . %%sql DROP TABLE IF EXISTS Match_Table; . * sqlite:///database.sqlite Done. . [] . %sql SELECT * FROM Match LIMIT 1; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . %sql SELECT * FROM Country LIMIT 1; . * sqlite:///database.sqlite Done. . id name . 1 | Belgium | . %sql SELECT * FROM League LIMIT 1; . * sqlite:///database.sqlite Done. . id country_id name . 1 | 1 | Belgium Jupiler League | . %%sql DROP TABLE IF EXISTS Match_df; CREATE TABLE Match_df AS SELECT Match.id, Country.name AS country_name, League.name AS league_name, season, stage, date, HT.team_long_name AS home_team, AT.team_long_name AS away_team, home_team_goal, away_team_goal FROM Match JOIN Country on Country.id = Match.country_id JOIN League on League.id = Match.league_id LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id ORDER by date ; . * sqlite:///database.sqlite Done. Done. . [] . %%sql SELECT COUNT(*) FROM Match_df; . * sqlite:///database.sqlite Done. . COUNT(*) . 25979 | . sql_query = %sql SELECT * FROM Match_df LIMIT 10 df = sql_query.DataFrame() df . * sqlite:///database.sqlite Done. . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | . 5 24614 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | AC Bellinzona | Neuchâtel Xamax | 1 | 2 | . 6 24615 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Zürich | FC Luzern | 1 | 0 | . 7 24616 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Sion | BSC Young Boys | 2 | 1 | . 8 24617 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-24 00:00:00 | FC Vaduz | FC Aarau | 0 | 2 | . 9 24668 | Switzerland | Switzerland Super League | 2008/2009 | 3 | 2008-07-26 00:00:00 | FC Basel | AC Bellinzona | 2 | 0 | . Build home team win label for classification . %%sql DROP TABLE IF EXISTS Match_Wins; CREATE TABLE Match_Wins AS SELECT * , CASE WHEN home_team_goal &gt; away_team_goal THEN 1 ELSE 0 END AS home_team_win FROM Match_df ; . * sqlite:///database.sqlite Done. Done. . [] . sql_query = %sql SELECT * FROM Match_Wins df = sql_query.DataFrame() df . * sqlite:///database.sqlite Done. . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 25945 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | 0 | . 25975 25946 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | 1 | . 25976 25947 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | 0 | . 25977 25948 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | 0 | . 25978 25949 | Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | 1 | . 25979 rows × 11 columns . from dask import dataframe as dd ddf = dd.from_pandas(df, npartitions=5) . ddf.head() . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal home_team_win . 0 24559 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | 0 | . 1 24560 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | 1 | . 2 24561 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | 0 | . 3 24562 | Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | 0 | . 4 24613 | Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | 1 | . df[&quot;home_team_goal&quot;] = df[&quot;home_team_goal&quot;].astype(float) df[&quot;away_team_goal&quot;] = df[&quot;away_team_goal&quot;].astype(float) df[&quot;stage&quot;] = df[&quot;stage&quot;].astype(float) . feat_list = [ &quot;home_team_goal&quot; ,&quot;away_team_goal&quot; ] . target = [&#39;home_team_win&#39;] . X_train = ddf[feat_list].persist() y_train = ddf[target].persist() . X_train.count().compute() . home_team_goal 25979 away_team_goal 25979 dtype: int64 . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/12521/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask_ml.xgboost import XGBRegressor XGBR = XGBRegressor() XGBR_model = XGBR.fit(X_train,y_train) . # from dask_ml.xgboost import XGBClassifier # XGBC = XGBClassifier() # XGBC_model = XGBC.fit(X_train,y_train) . client.close() . X, y = df.iloc[:, 1:10], df[&quot;home_team_win&quot;] X . country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-18 00:00:00 | BSC Young Boys | FC Basel | 1 | 2 | . 1 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-19 00:00:00 | FC Aarau | FC Sion | 3 | 1 | . 2 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | FC Luzern | FC Vaduz | 1 | 2 | . 3 Switzerland | Switzerland Super League | 2008/2009 | 1 | 2008-07-20 00:00:00 | Neuchâtel Xamax | FC Zürich | 1 | 2 | . 4 Switzerland | Switzerland Super League | 2008/2009 | 2 | 2008-07-23 00:00:00 | FC Basel | Grasshopper Club Zürich | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25974 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Basel | Grasshopper Club Zürich | 0 | 1 | . 25975 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | Lugano | FC St. Gallen | 3 | 0 | . 25976 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Luzern | FC Sion | 2 | 2 | . 25977 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Thun | BSC Young Boys | 0 | 3 | . 25978 Switzerland | Switzerland Super League | 2015/2016 | 36 | 2016-05-25 00:00:00 | FC Zürich | FC Vaduz | 3 | 1 | . 25979 rows × 9 columns . X, y = ddf.iloc[:, 1:10], df[&quot;home_team_win&quot;] X . Dask DataFrame Structure: country_name league_name season stage date home_team away_team home_team_goal away_team_goal . npartitions=5 . 0 object | object | object | int64 | object | object | object | int64 | int64 | . 5196 ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20784 ... | ... | ... | ... | ... | ... | ... | ... | ... | . 25978 ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: getitem, 10 tasks y . 0 0 1 1 2 0 3 0 4 1 5 0 6 1 7 1 8 0 9 1 Name: home_team_win, dtype: int64 . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . # import xgboost # dtrain = xgboost.DMatrix(X_train, y_train) # dtest = xgboost.DMatrix(X_test, y_test) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/31/sql_calls_in_jupyter_Soccer_explore.html",
            "relUrl": "/2020/10/31/sql_calls_in_jupyter_Soccer_explore.html",
            "date": " • Oct 31, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . Find the quandl api documentation here - . import pandas as pd import numpy as np import matplotlib.pyplot as plt import quandl %matplotlib inline . quandl_call = ( &quot;ZILLOW/{category}{code}_{indicator}&quot; ) def download_data(category, code, indicator): &quot;&quot;&quot; Reads in a single dataset from the John Hopkins GitHub repo as a DataFrame Parameters - category : &quot;Chicago_Area&quot; or &quot;Evanston&quot; code : &quot;Evanston&quot; or &quot;Chicago&quot; indicator : &quot;Sales_Price&quot; or &quot;other&quot; Returns - DataFrame &quot;&quot;&quot; AREA_CATEGORY_dict = {&quot;Evanston&quot;: &quot;C&quot;, &quot;Chicago_Area&quot;: &quot;C&quot;} AREA_CODE_dict = {&quot;Evanston&quot;: &quot;64604&quot;, &quot;Chicago&quot;: &quot;36156&quot;} INDICATOR_CODE_dict = {&quot;Sales_Price&quot;: &quot;SP&quot;} category = AREA_CATEGORY_dict[category] code = AREA_CODE_dict[code] indicator = INDICATOR_CODE_dict[indicator] return quandl.get(quandl_call.format(category=category, code=code, indicator=indicator)) . df = download_data(&#39;Chicago_Area&#39;, &#39;Evanston&#39;, &#39;Sales_Price&#39;) . df.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bb6db9ba8&gt; . df[&#39;Value&#39;].plot(label=&#39;Evanston House Prices&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bb4c88cf8&gt; . timeseries = df[&#39;Value&#39;] timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.rolling(12).std().plot(label=&#39;12 Month Rolling Std&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f4bb4a00eb8&gt; . timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f4bb4b0aa90&gt; . from statsmodels.tsa.seasonal import seasonal_decompose decomposition = seasonal_decompose(df[&#39;Value&#39;], freq=12) fig = plt.figure() fig = decomposition.plot() fig.set_size_inches(15, 8) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . &lt;Figure size 432x288 with 0 Axes&gt; . from statsmodels.tsa.arima_model import ARIMA import statsmodels.api as sm . model = sm.tsa.statespace.SARIMAX(df[&#39;Value&#39;],order=(0,1,0), seasonal_order=(1,1,1,12)) results = model.fit() print(results.summary()) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency M will be used. % freq, ValueWarning) . Statespace Model Results ========================================================================================== Dep. Variable: Value No. Observations: 138 Model: SARIMAX(0, 1, 0)x(1, 1, 1, 12) Log Likelihood -1441.360 Date: Mon, 26 Oct 2020 AIC 2888.719 Time: 06:57:44 BIC 2897.204 Sample: 03-31-2008 HQIC 2892.166 - 08-31-2019 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.S.L12 0.3461 0.072 4.832 0.000 0.206 0.487 ma.S.L12 -0.8710 0.113 -7.736 0.000 -1.092 -0.650 sigma2 6.479e+08 1.03e-11 6.3e+19 0.000 6.48e+08 6.48e+08 =================================================================================== Ljung-Box (Q): 120.48 Jarque-Bera (JB): 9.48 Prob(Q): 0.00 Prob(JB): 0.01 Heteroskedasticity (H): 0.66 Skew: -0.60 Prob(H) (two-sided): 0.19 Kurtosis: 3.62 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). [2] Covariance matrix is singular or near-singular, with condition number 5.31e+35. Standard errors may be unstable. . results.resid.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd2947be0&gt; . results.resid.plot(kind=&#39;kde&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd28ffac8&gt; . df[&#39;forecast&#39;] = results.predict(start = 1, end= 200, dynamic= True) df[[&#39;Value&#39;,&#39;forecast&#39;]].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd2689f60&gt; . from pandas.tseries.offsets import DateOffset future_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ] . future_dates . [Timestamp(&#39;2019-08-31 00:00:00&#39;), Timestamp(&#39;2019-09-30 00:00:00&#39;), Timestamp(&#39;2019-10-31 00:00:00&#39;), Timestamp(&#39;2019-11-30 00:00:00&#39;), Timestamp(&#39;2019-12-31 00:00:00&#39;), Timestamp(&#39;2020-01-31 00:00:00&#39;), Timestamp(&#39;2020-02-29 00:00:00&#39;), Timestamp(&#39;2020-03-31 00:00:00&#39;), Timestamp(&#39;2020-04-30 00:00:00&#39;), Timestamp(&#39;2020-05-31 00:00:00&#39;), Timestamp(&#39;2020-06-30 00:00:00&#39;), Timestamp(&#39;2020-07-31 00:00:00&#39;), Timestamp(&#39;2020-08-31 00:00:00&#39;), Timestamp(&#39;2020-09-30 00:00:00&#39;), Timestamp(&#39;2020-10-31 00:00:00&#39;), Timestamp(&#39;2020-11-30 00:00:00&#39;), Timestamp(&#39;2020-12-31 00:00:00&#39;), Timestamp(&#39;2021-01-31 00:00:00&#39;), Timestamp(&#39;2021-02-28 00:00:00&#39;), Timestamp(&#39;2021-03-31 00:00:00&#39;), Timestamp(&#39;2021-04-30 00:00:00&#39;), Timestamp(&#39;2021-05-31 00:00:00&#39;), Timestamp(&#39;2021-06-30 00:00:00&#39;), Timestamp(&#39;2021-07-31 00:00:00&#39;)] . future_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns) . future_df = pd.concat([df,future_dates_df]) . future_df.head() . Value forecast . 2008-03-31 370900.0 | NaN | . 2008-04-30 389600.0 | 370900.0 | . 2008-05-31 367100.0 | 370900.0 | . 2008-06-30 365600.0 | 370900.0 | . 2008-07-31 339000.0 | 370900.0 | . future_df.tail() . Value forecast . 2021-03-31 NaN | NaN | . 2021-04-30 NaN | NaN | . 2021-05-31 NaN | NaN | . 2021-06-30 NaN | NaN | . 2021-07-31 NaN | NaN | . future_df[&#39;forecast&#39;] = results.predict(start = 1, end = 720, dynamic= True) future_df[[&#39;Value&#39;, &#39;forecast&#39;]].plot(figsize=(12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e3573c8&gt; . Exponentially-weighted moving averages . df[&#39;6-month-SMA&#39;]=df[&#39;Value&#39;].rolling(window=6).mean() df[&#39;12-month-SMA&#39;]=df[&#39;Value&#39;].rolling(window=12).mean() . df[&#39;EWMA12&#39;] = df[&#39;Value&#39;].ewm(span=12).mean() . df[[&#39;Value&#39;,&#39;EWMA12&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e350860&gt; . Getting at the trend by removing the cyclical elements of Housing Prices . # Tuple unpacking df_cycle, df_trend = sm.tsa.filters.hpfilter(df.Value) . df_cycle . Date 2008-03-31 1942.830509 2008-04-30 24797.247533 2008-05-31 6450.450288 2008-06-30 9085.726225 2008-07-31 -13417.668736 ... 2019-04-30 25876.617108 2019-05-31 6082.075050 2019-06-30 789.779248 2019-07-31 -4001.523626 2019-08-31 1206.419488 Name: Value, Length: 138, dtype: float64 . df[&quot;trend&quot;] = df_trend . df[[&#39;trend&#39;,&#39;Value&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e275358&gt; . df[[&#39;trend&#39;,&#39;Value&#39;]][&quot;2010-01-31&quot;:].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e20be10&gt; . Chicago and Evanston Home Sale Prices . EV_SP = download_data(&#39;Chicago_Area&#39;, &#39;Evanston&#39;, &#39;Sales_Price&#39;) CH_SP = download_data(&#39;Chicago_Area&#39;, &#39;Chicago&#39;, &#39;Sales_Price&#39;) . fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Value&#39;) EV_SP[&#39;Value&#39;].plot(label=&#39;Evanston&#39;) CH_SP[&#39;Value&#39;].plot(label=&#39;Chicago&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f4b9e0d8828&gt; . CH_SP.plot(figsize=(12,6)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e056470&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/30/Function-for-zillow-data-quandl-api.html",
            "relUrl": "/2020/10/30/Function-for-zillow-data-quandl-api.html",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Creating E-Books (.epub) in python using ebooklib",
            "content": "This post includes code and notes from ebooklib and make-an-ebook. . import os import requests from ebooklib import epub from pyquery import PyQuery as pq . # coding=utf-8 url = &quot;http://example.com/%s.html&quot; build_dir = &quot;build/&quot; if not os.path.exists(build_dir): os.makedirs(build_dir) source_urls = [url % i for i in range(1,2)] urls = [ (build_dir + &quot;%s.html&quot; % i, url % i) for i in range(1,2) ] for filename, url in urls: print(&quot;Getting &quot;, url) response = requests.get(url) with open(filename, &#39;wb&#39;) as f: f.write(response.content) . Getting http://example.com/1.html . if __name__ == &#39;__main__&#39;: book = epub.EpubBook() # add metadata book.set_identifier(&#39;sample123456&#39;) book.set_title(&#39;Sample book&#39;) book.set_language(&#39;en&#39;) book.add_author(&#39;Example Author&#39;) # intro chapter c1 = epub.EpubHtml(title=&#39;Introduction&#39;, file_name=&#39;intro.xhtml&#39;, lang=&#39;en&#39;) c1.content=u&#39;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Introduction&lt;/h1&gt;&lt;p&gt;Introduction paragraph here.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#39; # about chapter c2 = epub.EpubHtml(title=&#39;About this book&#39;, file_name=&#39;about.xhtml&#39;) c2.content=&#39;&lt;h1&gt;About this book&lt;/h1&gt;&lt;p&gt;Text about his book.&lt;/p&gt;&#39; # add chapters to the book book.add_item(c1) book.add_item(c2) # create table of contents # - add section # - add auto created links to chapters book.toc = (epub.Link(&#39;intro.xhtml&#39;, &#39;Introduction&#39;, &#39;intro&#39;), (epub.Section(&#39;Languages&#39;), (c1, c2)) ) # add navigation files book.add_item(epub.EpubNcx()) book.add_item(epub.EpubNav()) # define css style style = &#39;&#39;&#39; @namespace epub &quot;http://www.idpf.org/2007/ops&quot;; body { font-family: Cambria, Liberation Serif, Bitstream Vera Serif, Georgia, Times, Times New Roman, serif; } h2 { text-align: left; text-transform: uppercase; font-weight: 200; } ol { list-style-type: none; } ol &gt; li:first-child { margin-top: 0.3em; } nav[epub|type~=&#39;toc&#39;] &gt; ol &gt; li &gt; ol { list-style-type:square; } nav[epub|type~=&#39;toc&#39;] &gt; ol &gt; li &gt; ol &gt; li { margin-top: 0.3em; } &#39;&#39;&#39; # add css file nav_css = epub.EpubItem(uid=&quot;style_nav&quot;, file_name=&quot;style/nav.css&quot;, media_type=&quot;text/css&quot;, content=style) book.add_item(nav_css) # create spine book.spine = [&#39;nav&#39;, c1, c2] # create epub file epub.write_epub(&#39;test.epub&#39;, book, {}) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/29/pyquery_for_ebooks_python.html",
            "relUrl": "/2020/10/29/pyquery_for_ebooks_python.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Principal component analysis with sklearn",
            "content": "import seaborn as sns from sklearn import datasets import pandas as pd . iris = datasets.load_iris() . target_names = iris.target_names . X = pd.DataFrame(iris.data) . X.columns = [&#39;Sepal Length&#39;, &#39;Sepal Width&#39;, &#39;Petal Length&#39;, &#39;Petal Width&#39;] . for name in X.columns: X[name] = (X[name]-X[name].mean())/X[name].std() . sns.heatmap(X.corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;2.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f026c877780&gt; . import numpy as np . # calculate the Covariance matrix Q = X.cov().to_numpy()# find the eigenvalue and eigenvector of the Covariance matrix D, V = np.linalg.eigh(Q) # print the results np.set_printoptions(precision=2) print(&#39;principal components:&#39;) . principal components: . for i in range(1, len(D)): print(&#39;Feature %d : %2.3f&#39;%(i, D[i])) . Feature 1 : 0.147 Feature 2 : 0.914 Feature 3 : 2.918 . # perform the linear transformation X_new = X.dot(V)# define the columns names to the X_new X_new.columns = [&quot;Feature %d&quot;%i for i in range(1,5)]# The correlation between different features disappear! sns.heatmap(X_new.corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;2.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f023c032940&gt; . # truncation: dimensional reduction X_reduced = X.copy() . V_trun = V[:,2:4] . X_reduced = X_reduced.dot(V_trun) . X_reduced[&#39;Species&#39;] = iris.target . X_reduced.columns = [&#39;Feature 3&#39;, &#39;Feature 4&#39;, &#39;Species&#39;] . for i, t in enumerate(target_names): X_reduced[&#39;Species&#39;].replace(i, t, inplace=True) . sns.scatterplot(data=X_reduced, x=&#39;Feature 3&#39;, y=&#39;Feature 4&#39;, hue=&#39;Species&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f023bf7f400&gt; . from sklearn.decomposition import PCA . X3 = X.to_numpy() pca = PCA(n_components=3, random_state=0) X3_reduced = pca.fit(X3).transform(X3) X3_reduced = pd.DataFrame(X3_reduced) X3_reduced[&#39;species&#39;] = iris.target# plot the results in 3D scatter plot . from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt . fig = plt.figure(figsize=(6,6)) ax = Axes3D(fig) for i in range(3): idx = X3_reduced[&#39;species&#39;]==i ax.scatter(X3_reduced[0][idx], X3_reduced[1][idx], X3_reduced[2][idx], label=target_names[i]) plt.legend() ax.view_init(20,75) plt.xlabel(&#39;Feature 2&#39;) plt.ylabel(&#39;Feature 3&#39;) ax.set_zlabel(&#39;Feature 4&#39;) . Text(0.5, 0, &#39;Feature 4&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/28/PCA-with-sklearn.html",
            "relUrl": "/2020/10/28/PCA-with-sklearn.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
            "content": "This post includes code and notes from data-analysis-using-sql. . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . connection . &lt;sqlalchemy.engine.base.Connection at 0x7fb9c7c07080&gt; . engine.execute(&quot;SELECT * FROM Country LIMIT 10&quot;).fetchall() . [(1, &#39;Belgium&#39;), (1729, &#39;England&#39;), (4769, &#39;France&#39;), (7809, &#39;Germany&#39;), (10257, &#39;Italy&#39;), (13274, &#39;Netherlands&#39;), (15722, &#39;Poland&#39;), (17642, &#39;Portugal&#39;), (19694, &#39;Scotland&#39;), (21518, &#39;Spain&#39;)] . %load_ext sql . %sql sqlite:///database.sqlite . %%sql SELECT * FROM Country LIMIT 10 . * sqlite:///database.sqlite Done. . id name . 1 | Belgium | . 1729 | England | . 4769 | France | . 7809 | Germany | . 10257 | Italy | . 13274 | Netherlands | . 15722 | Poland | . 17642 | Portugal | . 19694 | Scotland | . 21518 | Spain | . %%sql SELECT id ,name FROM Country WHERE name = &quot;England&quot; . * sqlite:///database.sqlite Done. . id name . 1729 | England | . %%sql SELECT * FROM League LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id name . 1 | 1 | Belgium Jupiler League | . 1729 | 1729 | England Premier League | . 4769 | 4769 | France Ligue 1 | . 7809 | 7809 | Germany 1. Bundesliga | . 10257 | 10257 | Italy Serie A | . 13274 | 13274 | Netherlands Eredivisie | . 15722 | 15722 | Poland Ekstraklasa | . 17642 | 17642 | Portugal Liga ZON Sagres | . 19694 | 19694 | Scotland Premier League | . 21518 | 21518 | Spain LIGA BBVA | . %%sql SELECT * FROM Match LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . 2 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492474 | 10000 | 9994 | 0 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.95 | 3.2 | 3.6 | 1.8 | 3.3 | 3.95 | 1.9 | 3.2 | 3.5 | 1.9 | 3.2 | 3.5 | None | None | None | 1.83 | 3.3 | 3.6 | 1.95 | 3.3 | 3.8 | 2 | 3.25 | 3.25 | 1.85 | 3.25 | 3.75 | 1.91 | 3.25 | 3.6 | . 3 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492475 | 9984 | 8635 | 0 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.38 | 3.3 | 2.75 | 2.4 | 3.3 | 2.55 | 2.6 | 3.1 | 2.3 | 2.5 | 3.2 | 2.5 | None | None | None | 2.5 | 3.25 | 2.4 | 2.63 | 3.3 | 2.5 | 2.35 | 3.25 | 2.65 | 2.5 | 3.2 | 2.5 | 2.3 | 3.2 | 2.75 | . 4 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492476 | 9991 | 9998 | 5 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.44 | 3.75 | 7.5 | 1.4 | 4 | 6.8 | 1.4 | 3.9 | 6 | 1.44 | 3.6 | 6.5 | None | None | None | 1.44 | 3.75 | 6 | 1.44 | 4 | 7.5 | 1.45 | 3.75 | 6.5 | 1.5 | 3.75 | 5.5 | 1.44 | 3.75 | 6.5 | . 5 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492477 | 7947 | 9985 | 1 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 5 | 3.5 | 1.65 | 5 | 3.5 | 1.6 | 4 | 3.3 | 1.7 | 4 | 3.4 | 1.72 | None | None | None | 4.2 | 3.4 | 1.7 | 4.5 | 3.5 | 1.73 | 4.5 | 3.4 | 1.65 | 4.5 | 3.5 | 1.65 | 4.75 | 3.3 | 1.67 | . 6 | 1 | 1 | 2008/2009 | 1 | 2008-09-24 00:00:00 | 492478 | 8203 | 8342 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 4.75 | 3.4 | 1.67 | 4.85 | 3.4 | 1.65 | 3.7 | 3.2 | 1.8 | 5 | 3.25 | 1.62 | None | None | None | 4.2 | 3.4 | 1.7 | 5.5 | 3.75 | 1.67 | 4.35 | 3.4 | 1.7 | 4.5 | 3.4 | 1.7 | None | None | None | . 7 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492479 | 9999 | 8571 | 2 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.1 | 3.2 | 3.3 | 2.05 | 3.25 | 3.15 | 1.85 | 3.2 | 3.5 | 1.83 | 3.3 | 3.6 | None | None | None | 1.83 | 3.3 | 3.6 | 1.91 | 3.4 | 3.6 | 2.1 | 3.25 | 3 | 1.85 | 3.25 | 3.75 | 2.1 | 3.25 | 3.1 | . 8 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492480 | 4049 | 9996 | 1 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 3.2 | 3.4 | 2.2 | 2.55 | 3.3 | 2.4 | 2.4 | 3.2 | 2.4 | 2.5 | 3.2 | 2.5 | None | None | None | 2.7 | 3.25 | 2.25 | 2.6 | 3.4 | 2.4 | 2.8 | 3.25 | 2.25 | 2.8 | 3.2 | 2.25 | 2.88 | 3.25 | 2.2 | . 9 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492481 | 10001 | 9986 | 1 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.25 | 3.25 | 2.88 | 2.3 | 3.25 | 2.7 | 2.1 | 3.1 | 3 | 2.25 | 3.2 | 2.75 | None | None | None | 2.2 | 3.25 | 2.75 | 2.2 | 3.3 | 3.1 | 2.25 | 3.25 | 2.8 | 2.2 | 3.3 | 2.8 | 2.25 | 3.2 | 2.8 | . 10 | 1 | 1 | 2008/2009 | 10 | 2008-11-01 00:00:00 | 492564 | 8342 | 8571 | 4 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.3 | 5.25 | 9.5 | 1.25 | 5 | 10 | 1.3 | 4.2 | 8 | 1.25 | 4.5 | 10 | None | None | None | 1.35 | 4.2 | 7 | 1.27 | 5 | 10 | 1.3 | 4.35 | 8.5 | 1.25 | 5 | 10 | 1.29 | 4.5 | 9 | . %%sql SELECT * FROM Player LIMIT 10; . * sqlite:///database.sqlite Done. . id player_api_id player_name player_fifa_api_id birthday height weight . 1 | 505942 | Aaron Appindangoye | 218353 | 1992-02-29 00:00:00 | 182.88 | 187 | . 2 | 155782 | Aaron Cresswell | 189615 | 1989-12-15 00:00:00 | 170.18 | 146 | . 3 | 162549 | Aaron Doran | 186170 | 1991-05-13 00:00:00 | 170.18 | 163 | . 4 | 30572 | Aaron Galindo | 140161 | 1982-05-08 00:00:00 | 182.88 | 198 | . 5 | 23780 | Aaron Hughes | 17725 | 1979-11-08 00:00:00 | 182.88 | 154 | . 6 | 27316 | Aaron Hunt | 158138 | 1986-09-04 00:00:00 | 182.88 | 161 | . 7 | 564793 | Aaron Kuhl | 221280 | 1996-01-30 00:00:00 | 172.72 | 146 | . 8 | 30895 | Aaron Lennon | 152747 | 1987-04-16 00:00:00 | 165.1 | 139 | . 9 | 528212 | Aaron Lennox | 206592 | 1993-02-19 00:00:00 | 190.5 | 181 | . 10 | 101042 | Aaron Meijers | 188621 | 1987-10-28 00:00:00 | 175.26 | 170 | . %%sql SELECT * FROM Player_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id player_fifa_api_id player_api_id date overall_rating potential preferred_foot attacking_work_rate defensive_work_rate crossing finishing heading_accuracy short_passing volleys dribbling curve free_kick_accuracy long_passing ball_control acceleration sprint_speed agility reactions balance shot_power jumping stamina strength long_shots aggression interceptions positioning vision penalties marking standing_tackle sliding_tackle gk_diving gk_handling gk_kicking gk_positioning gk_reflexes . 1 | 218353 | 505942 | 2016-02-18 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 2 | 218353 | 505942 | 2015-11-19 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 3 | 218353 | 505942 | 2015-09-21 00:00:00 | 62 | 66 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 63 | 41 | 45 | 54 | 48 | 65 | 66 | 69 | 6 | 11 | 10 | 8 | 8 | . 4 | 218353 | 505942 | 2015-03-20 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 5 | 218353 | 505942 | 2007-02-22 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 6 | 189615 | 155782 | 2016-04-21 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 40 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 62 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 7 | 189615 | 155782 | 2016-04-07 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 32 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 60 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 8 | 189615 | 155782 | 2016-01-07 00:00:00 | 73 | 75 | left | high | medium | 79 | 52 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 59 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 9 | 189615 | 155782 | 2015-12-24 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 10 | 189615 | 155782 | 2015-12-17 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . %%sql SELECT * FROM Team LIMIT 10; . * sqlite:///database.sqlite Done. . id team_api_id team_fifa_api_id team_long_name team_short_name . 1 | 9987 | 673 | KRC Genk | GEN | . 2 | 9993 | 675 | Beerschot AC | BAC | . 3 | 10000 | 15005 | SV Zulte-Waregem | ZUL | . 4 | 9994 | 2007 | Sporting Lokeren | LOK | . 5 | 9984 | 1750 | KSV Cercle Brugge | CEB | . 6 | 8635 | 229 | RSC Anderlecht | AND | . 7 | 9991 | 674 | KAA Gent | GEN | . 8 | 9998 | 1747 | RAEC Mons | MON | . 9 | 7947 | None | FCV Dender EH | DEN | . 10 | 9985 | 232 | Standard de Liège | STL | . %%sql SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id team_fifa_api_id team_api_id date buildUpPlaySpeed buildUpPlaySpeedClass buildUpPlayDribbling buildUpPlayDribblingClass buildUpPlayPassing buildUpPlayPassingClass buildUpPlayPositioningClass chanceCreationPassing chanceCreationPassingClass chanceCreationCrossing chanceCreationCrossingClass chanceCreationShooting chanceCreationShootingClass chanceCreationPositioningClass defencePressure defencePressureClass defenceAggression defenceAggressionClass defenceTeamWidth defenceTeamWidthClass defenceDefenderLineClass . 1 | 434 | 9930 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 50 | Mixed | Organised | 60 | Normal | 65 | Normal | 55 | Normal | Organised | 50 | Medium | 55 | Press | 45 | Normal | Cover | . 2 | 434 | 9930 | 2014-09-19 00:00:00 | 52 | Balanced | 48 | Normal | 56 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 3 | 434 | 9930 | 2015-09-10 00:00:00 | 47 | Balanced | 41 | Normal | 54 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 4 | 77 | 8485 | 2010-02-22 00:00:00 | 70 | Fast | None | Little | 70 | Long | Organised | 70 | Risky | 70 | Lots | 70 | Lots | Organised | 60 | Medium | 70 | Double | 70 | Wide | Cover | . 5 | 77 | 8485 | 2011-02-22 00:00:00 | 47 | Balanced | None | Little | 52 | Mixed | Organised | 53 | Normal | 48 | Normal | 52 | Normal | Organised | 47 | Medium | 47 | Press | 52 | Normal | Cover | . 6 | 77 | 8485 | 2012-02-22 00:00:00 | 58 | Balanced | None | Little | 62 | Mixed | Organised | 45 | Normal | 70 | Lots | 55 | Normal | Organised | 40 | Medium | 40 | Press | 60 | Normal | Cover | . 7 | 77 | 8485 | 2013-09-20 00:00:00 | 62 | Balanced | None | Little | 45 | Mixed | Organised | 40 | Normal | 50 | Normal | 55 | Normal | Organised | 42 | Medium | 42 | Press | 60 | Normal | Cover | . 8 | 77 | 8485 | 2014-09-19 00:00:00 | 58 | Balanced | 64 | Normal | 62 | Mixed | Organised | 56 | Normal | 68 | Lots | 57 | Normal | Organised | 41 | Medium | 42 | Press | 60 | Normal | Cover | . 9 | 77 | 8485 | 2015-09-10 00:00:00 | 59 | Balanced | 64 | Normal | 53 | Mixed | Organised | 51 | Normal | 72 | Lots | 63 | Normal | Free Form | 49 | Medium | 45 | Press | 63 | Normal | Cover | . 10 | 614 | 8576 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 40 | Mixed | Organised | 45 | Normal | 35 | Normal | 55 | Normal | Organised | 30 | Deep | 70 | Double | 30 | Narrow | Offside Trap | . %%sql CREATE TABLE Team_table AS SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . [] . %%sql DROP TABLE IF EXISTS Team_table . * sqlite:///database.sqlite Done. . [] . sql_query = %sql SELECT * FROM Team LIMIT 10 df = sql_query.DataFrame() . * sqlite:///database.sqlite Done. . df . id team_api_id team_fifa_api_id team_long_name team_short_name . 0 1 | 9987 | 673.0 | KRC Genk | GEN | . 1 2 | 9993 | 675.0 | Beerschot AC | BAC | . 2 3 | 10000 | 15005.0 | SV Zulte-Waregem | ZUL | . 3 4 | 9994 | 2007.0 | Sporting Lokeren | LOK | . 4 5 | 9984 | 1750.0 | KSV Cercle Brugge | CEB | . 5 6 | 8635 | 229.0 | RSC Anderlecht | AND | . 6 7 | 9991 | 674.0 | KAA Gent | GEN | . 7 8 | 9998 | 1747.0 | RAEC Mons | MON | . 8 9 | 7947 | NaN | FCV Dender EH | DEN | . 9 10 | 9985 | 232.0 | Standard de Liège | STL | . import matplotlib.pyplot as plt plt.figure(figsize=(20,7)) plot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name plot.bar(); . * sqlite:///database.sqlite Done. . plot.pie(); . type(plot) . sql.run.ResultSet . # #Imports # import numpy as np # linear algebra # import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import sqlite3 # import matplotlib.pyplot as plt # # Input data files are available in the &quot;../input/&quot; directory. # # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory # path = &quot;../input/&quot; #Insert path here # database = path + &#39;database.sqlite&#39; . First we will create the connection to the DB, and see what tables we have . tables = pd.read_sql(&quot;&quot;&quot;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&quot;&quot;&quot;, connection) tables . type name tbl_name rootpage sql . 0 table | sqlite_sequence | sqlite_sequence | 4 | CREATE TABLE sqlite_sequence(name,seq) | . 1 table | Player_Attributes | Player_Attributes | 11 | CREATE TABLE &quot;Player_Attributes&quot; ( n t`id` tIN... | . 2 table | Player | Player | 14 | CREATE TABLE `Player` ( n t`id` tINTEGER PRIMA... | . 3 table | Match | Match | 18 | CREATE TABLE `Match` ( n t`id` tINTEGER PRIMAR... | . 4 table | League | League | 24 | CREATE TABLE `League` ( n t`id` tINTEGER PRIMA... | . 5 table | Country | Country | 26 | CREATE TABLE `Country` ( n t`id` tINTEGER PRIM... | . 6 table | Team | Team | 29 | CREATE TABLE &quot;Team&quot; ( n t`id` tINTEGER PRIMARY... | . 7 table | Team_Attributes | Team_Attributes | 2 | CREATE TABLE `Team_Attributes` ( n t`id` tINTE... | . countries = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Country;&quot;&quot;&quot;, connection) countries . id name . 0 1 | Belgium | . 1 1729 | England | . 2 4769 | France | . 3 7809 | Germany | . 4 10257 | Italy | . 5 13274 | Netherlands | . 6 15722 | Poland | . 7 17642 | Portugal | . 8 19694 | Scotland | . 9 21518 | Spain | . 10 24558 | Switzerland | . leagues = pd.read_sql(&quot;&quot;&quot;SELECT * FROM League JOIN Country ON Country.id = League.country_id;&quot;&quot;&quot;, connection) leagues . id country_id name id name . 0 1 | 1 | Belgium Jupiler League | 1 | Belgium | . 1 1729 | 1729 | England Premier League | 1729 | England | . 2 4769 | 4769 | France Ligue 1 | 4769 | France | . 3 7809 | 7809 | Germany 1. Bundesliga | 7809 | Germany | . 4 10257 | 10257 | Italy Serie A | 10257 | Italy | . 5 13274 | 13274 | Netherlands Eredivisie | 13274 | Netherlands | . 6 15722 | 15722 | Poland Ekstraklasa | 15722 | Poland | . 7 17642 | 17642 | Portugal Liga ZON Sagres | 17642 | Portugal | . 8 19694 | 19694 | Scotland Premier League | 19694 | Scotland | . 9 21518 | 21518 | Spain LIGA BBVA | 21518 | Spain | . 10 24558 | 24558 | Switzerland Super League | 24558 | Switzerland | . teams = pd.read_sql(&quot;&quot;&quot;SELECT * FROM Team ORDER BY team_long_name LIMIT 10;&quot;&quot;&quot;, connection) teams . id team_api_id team_fifa_api_id team_long_name team_short_name . 0 16848 | 8350 | 29 | 1. FC Kaiserslautern | KAI | . 1 15624 | 8722 | 31 | 1. FC Köln | FCK | . 2 16239 | 8165 | 171 | 1. FC Nürnberg | NUR | . 3 16243 | 9905 | 169 | 1. FSV Mainz 05 | MAI | . 4 11817 | 8576 | 614 | AC Ajaccio | AJA | . 5 11074 | 108893 | 111989 | AC Arles-Avignon | ARL | . 6 49116 | 6493 | 1714 | AC Bellinzona | BEL | . 7 26560 | 10217 | 650 | ADO Den Haag | HAA | . 8 9537 | 8583 | 57 | AJ Auxerre | AUX | . 9 9547 | 9829 | 69 | AS Monaco | MON | . detailed_matches = pd.read_sql(&quot;&quot;&quot;SELECT Match.id, Country.name AS country_name, League.name AS league_name, season, stage, date, HT.team_long_name AS home_team, AT.team_long_name AS away_team, home_team_goal, away_team_goal FROM Match JOIN Country on Country.id = Match.country_id JOIN League on League.id = Match.league_id LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id WHERE country_name = &#39;Spain&#39; ORDER by date LIMIT 10;&quot;&quot;&quot;, connection) detailed_matches . id country_name league_name season stage date home_team away_team home_team_goal away_team_goal . 0 21518 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-30 00:00:00 | Valencia CF | RCD Mallorca | 3 | 0 | . 1 21525 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-30 00:00:00 | RCD Espanyol | Real Valladolid | 1 | 0 | . 2 21519 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | CA Osasuna | Villarreal CF | 1 | 1 | . 3 21520 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | RC Deportivo de La Coruña | Real Madrid CF | 2 | 1 | . 4 21521 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | CD Numancia | FC Barcelona | 1 | 0 | . 5 21522 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Racing Santander | Sevilla FC | 1 | 1 | . 6 21523 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Real Sporting de Gijón | Getafe CF | 1 | 2 | . 7 21524 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Real Betis Balompié | RC Recreativo | 0 | 1 | . 8 21526 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Athletic Club de Bilbao | UD Almería | 1 | 3 | . 9 21527 | Spain | Spain LIGA BBVA | 2008/2009 | 1 | 2008-08-31 00:00:00 | Atlético Madrid | Málaga CF | 4 | 0 | . # Star with Spain Real Madrid CF, barcelonat . leages_by_season = pd.read_sql(&quot;&quot;&quot;SELECT Country.name AS country_name, League.name AS league_name, season, count(distinct stage) AS number_of_stages, count(distinct HT.team_long_name) AS number_of_teams, avg(home_team_goal) AS avg_home_team_scors, avg(away_team_goal) AS avg_away_team_goals, avg(home_team_goal-away_team_goal) AS avg_goal_dif, avg(home_team_goal+away_team_goal) AS avg_goals, sum(home_team_goal+away_team_goal) AS total_goals FROM Match JOIN Country on Country.id = Match.country_id JOIN League on League.id = Match.league_id LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id WHERE country_name in (&#39;Spain&#39;, &#39;Germany&#39;, &#39;France&#39;, &#39;Italy&#39;, &#39;England&#39;) GROUP BY Country.name, League.name, season HAVING count(distinct stage) &gt; 10 ORDER BY Country.name, League.name, season DESC ;&quot;&quot;&quot;, connection) leages_by_season . country_name league_name season number_of_stages number_of_teams avg_home_team_scors avg_away_team_goals avg_goal_dif avg_goals total_goals . 0 England | England Premier League | 2015/2016 | 38 | 20 | 1.492105 | 1.207895 | 0.284211 | 2.700000 | 1026 | . 1 England | England Premier League | 2014/2015 | 38 | 20 | 1.473684 | 1.092105 | 0.381579 | 2.565789 | 975 | . 2 England | England Premier League | 2013/2014 | 38 | 20 | 1.573684 | 1.194737 | 0.378947 | 2.768421 | 1052 | . 3 England | England Premier League | 2012/2013 | 38 | 20 | 1.557895 | 1.239474 | 0.318421 | 2.797368 | 1063 | . 4 England | England Premier League | 2011/2012 | 38 | 20 | 1.589474 | 1.215789 | 0.373684 | 2.805263 | 1066 | . 5 England | England Premier League | 2010/2011 | 38 | 20 | 1.623684 | 1.173684 | 0.450000 | 2.797368 | 1063 | . 6 England | England Premier League | 2009/2010 | 38 | 20 | 1.697368 | 1.073684 | 0.623684 | 2.771053 | 1053 | . 7 England | England Premier League | 2008/2009 | 38 | 20 | 1.400000 | 1.078947 | 0.321053 | 2.478947 | 942 | . 8 France | France Ligue 1 | 2015/2016 | 38 | 20 | 1.436842 | 1.089474 | 0.347368 | 2.526316 | 960 | . 9 France | France Ligue 1 | 2014/2015 | 38 | 20 | 1.410526 | 1.081579 | 0.328947 | 2.492105 | 947 | . 10 France | France Ligue 1 | 2013/2014 | 38 | 20 | 1.415789 | 1.039474 | 0.376316 | 2.455263 | 933 | . 11 France | France Ligue 1 | 2012/2013 | 38 | 20 | 1.468421 | 1.076316 | 0.392105 | 2.544737 | 967 | . 12 France | France Ligue 1 | 2011/2012 | 38 | 20 | 1.473684 | 1.042105 | 0.431579 | 2.515789 | 956 | . 13 France | France Ligue 1 | 2010/2011 | 38 | 20 | 1.342105 | 1.000000 | 0.342105 | 2.342105 | 890 | . 14 France | France Ligue 1 | 2009/2010 | 38 | 20 | 1.389474 | 1.021053 | 0.368421 | 2.410526 | 916 | . 15 France | France Ligue 1 | 2008/2009 | 38 | 20 | 1.286842 | 0.971053 | 0.315789 | 2.257895 | 858 | . 16 Germany | Germany 1. Bundesliga | 2015/2016 | 34 | 18 | 1.565359 | 1.264706 | 0.300654 | 2.830065 | 866 | . 17 Germany | Germany 1. Bundesliga | 2014/2015 | 34 | 18 | 1.588235 | 1.166667 | 0.421569 | 2.754902 | 843 | . 18 Germany | Germany 1. Bundesliga | 2013/2014 | 34 | 18 | 1.748366 | 1.411765 | 0.336601 | 3.160131 | 967 | . 19 Germany | Germany 1. Bundesliga | 2012/2013 | 34 | 18 | 1.591503 | 1.343137 | 0.248366 | 2.934641 | 898 | . 20 Germany | Germany 1. Bundesliga | 2011/2012 | 34 | 18 | 1.660131 | 1.199346 | 0.460784 | 2.859477 | 875 | . 21 Germany | Germany 1. Bundesliga | 2010/2011 | 34 | 18 | 1.647059 | 1.274510 | 0.372549 | 2.921569 | 894 | . 22 Germany | Germany 1. Bundesliga | 2009/2010 | 34 | 18 | 1.513072 | 1.316993 | 0.196078 | 2.830065 | 866 | . 23 Germany | Germany 1. Bundesliga | 2008/2009 | 34 | 18 | 1.699346 | 1.222222 | 0.477124 | 2.921569 | 894 | . 24 Italy | Italy Serie A | 2015/2016 | 38 | 20 | 1.471053 | 1.105263 | 0.365789 | 2.576316 | 979 | . 25 Italy | Italy Serie A | 2014/2015 | 38 | 20 | 1.498681 | 1.187335 | 0.311346 | 2.686016 | 1018 | . 26 Italy | Italy Serie A | 2013/2014 | 38 | 20 | 1.536842 | 1.186842 | 0.350000 | 2.723684 | 1035 | . 27 Italy | Italy Serie A | 2012/2013 | 38 | 20 | 1.494737 | 1.144737 | 0.350000 | 2.639474 | 1003 | . 28 Italy | Italy Serie A | 2011/2012 | 38 | 20 | 1.511173 | 1.072626 | 0.438547 | 2.583799 | 925 | . 29 Italy | Italy Serie A | 2010/2011 | 38 | 20 | 1.431579 | 1.081579 | 0.350000 | 2.513158 | 955 | . 30 Italy | Italy Serie A | 2009/2010 | 38 | 20 | 1.542105 | 1.068421 | 0.473684 | 2.610526 | 992 | . 31 Italy | Italy Serie A | 2008/2009 | 38 | 20 | 1.521053 | 1.078947 | 0.442105 | 2.600000 | 988 | . 32 Spain | Spain LIGA BBVA | 2015/2016 | 38 | 20 | 1.618421 | 1.126316 | 0.492105 | 2.744737 | 1043 | . 33 Spain | Spain LIGA BBVA | 2014/2015 | 38 | 20 | 1.536842 | 1.118421 | 0.418421 | 2.655263 | 1009 | . 34 Spain | Spain LIGA BBVA | 2013/2014 | 38 | 20 | 1.631579 | 1.118421 | 0.513158 | 2.750000 | 1045 | . 35 Spain | Spain LIGA BBVA | 2012/2013 | 38 | 20 | 1.686842 | 1.184211 | 0.502632 | 2.871053 | 1091 | . 36 Spain | Spain LIGA BBVA | 2011/2012 | 38 | 20 | 1.678947 | 1.084211 | 0.594737 | 2.763158 | 1050 | . 37 Spain | Spain LIGA BBVA | 2010/2011 | 38 | 20 | 1.636842 | 1.105263 | 0.531579 | 2.742105 | 1042 | . 38 Spain | Spain LIGA BBVA | 2009/2010 | 38 | 20 | 1.600000 | 1.113158 | 0.486842 | 2.713158 | 1031 | . 39 Spain | Spain LIGA BBVA | 2008/2009 | 38 | 20 | 1.660526 | 1.236842 | 0.423684 | 2.897368 | 1101 | . df = pd.DataFrame(index=np.sort(leages_by_season[&#39;season&#39;].unique()), columns=leages_by_season[&#39;country_name&#39;].unique()) df.loc[:,&#39;Germany&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Germany&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;Spain&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Spain&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;France&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;France&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;Italy&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Italy&#39;,&#39;avg_goals&#39;]) df.loc[:,&#39;England&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;England&#39;,&#39;avg_goals&#39;]) df.plot(figsize=(12,5),title=&#39;Average Goals per Game Over Time&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb9c3770780&gt; . df = pd.DataFrame(index=np.sort(leages_by_season[&#39;season&#39;].unique()), columns=leages_by_season[&#39;country_name&#39;].unique()) df.loc[:,&#39;Germany&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Germany&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;Spain&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Spain&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;France&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;France&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;Italy&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;Italy&#39;,&#39;avg_goal_dif&#39;]) df.loc[:,&#39;England&#39;] = list(leages_by_season.loc[leages_by_season[&#39;country_name&#39;]==&#39;England&#39;,&#39;avg_goal_dif&#39;]) df.plot(figsize=(12,5),title=&#39;Average Goals Difference Home vs Out&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb9c39687f0&gt; . players_height = pd.read_sql(&quot;&quot;&quot;SELECT CASE WHEN ROUND(height)&lt;165 then 165 WHEN ROUND(height)&gt;195 then 195 ELSE ROUND(height) END AS calc_height, COUNT(height) AS distribution, (avg(PA_Grouped.avg_overall_rating)) AS avg_overall_rating, (avg(PA_Grouped.avg_potential)) AS avg_potential, AVG(weight) AS avg_weight FROM PLAYER LEFT JOIN (SELECT Player_Attributes.player_api_id, avg(Player_Attributes.overall_rating) AS avg_overall_rating, avg(Player_Attributes.potential) AS avg_potential FROM Player_Attributes GROUP BY Player_Attributes.player_api_id) AS PA_Grouped ON PLAYER.player_api_id = PA_Grouped.player_api_id GROUP BY calc_height ORDER BY calc_height ;&quot;&quot;&quot;, connection) players_height . calc_height distribution avg_overall_rating avg_potential avg_weight . 0 165.0 | 74 | 67.365543 | 73.327754 | 139.459459 | . 1 168.0 | 118 | 67.500518 | 73.124182 | 144.127119 | . 2 170.0 | 403 | 67.726903 | 73.379056 | 147.799007 | . 3 173.0 | 530 | 66.980272 | 72.848746 | 152.824528 | . 4 175.0 | 1188 | 66.805204 | 72.258774 | 156.111953 | . 5 178.0 | 1489 | 66.367212 | 71.943339 | 160.665547 | . 6 180.0 | 1388 | 66.419053 | 71.846394 | 165.261527 | . 7 183.0 | 1954 | 66.634380 | 71.754555 | 170.167861 | . 8 185.0 | 1278 | 66.928964 | 71.833475 | 174.636933 | . 9 188.0 | 1305 | 67.094253 | 72.151949 | 179.278161 | . 10 191.0 | 652 | 66.997649 | 71.846159 | 184.791411 | . 11 193.0 | 470 | 67.485141 | 72.459225 | 188.795745 | . 12 195.0 | 211 | 67.425619 | 72.615373 | 196.464455 | . players_height.calc_height . 0 165.0 1 168.0 2 170.0 3 173.0 4 175.0 5 178.0 6 180.0 7 183.0 8 185.0 9 188.0 10 191.0 11 193.0 12 195.0 Name: calc_height, dtype: float64 . # players_height.plot(x=[&#39;calc_height&#39;],y=[&#39;avg_overall_rating&#39;],figsize=(12,5),title=&#39;Potential vs Height&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/27/sql_calls_in_jupyter_Soccer_Pred.html",
            "relUrl": "/2020/10/27/sql_calls_in_jupyter_Soccer_Pred.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Data Science Content",
            "content": "On the borders of Statistics and Machine learning: Discussion of conscious v. unconscious processes and Statistics and Machine learning, causal inference v. pattern recognition. | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/10/26/links.html",
            "relUrl": "/data%20science%20content/2020/10/26/links.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Working with sqlite databases in Jupyter for European Soccer Match Data",
            "content": "This post includes code and notes from data-analysis-using-sql. . import sqlalchemy as db import sqlite3 import pandas as pd import numpy as np . engine = db.create_engine(&#39;sqlite:///database.sqlite&#39;) connection = engine.connect() metadata = db.MetaData() . connection . &lt;sqlalchemy.engine.base.Connection at 0x7fb9c3356780&gt; . engine.execute(&quot;SELECT * FROM Country LIMIT 10&quot;).fetchall() . [(1, &#39;Belgium&#39;), (1729, &#39;England&#39;), (4769, &#39;France&#39;), (7809, &#39;Germany&#39;), (10257, &#39;Italy&#39;), (13274, &#39;Netherlands&#39;), (15722, &#39;Poland&#39;), (17642, &#39;Portugal&#39;), (19694, &#39;Scotland&#39;), (21518, &#39;Spain&#39;)] . %load_ext sql . The sql extension is already loaded. To reload it, use: %reload_ext sql . %sql sqlite:///database.sqlite . %%sql SELECT * FROM Country LIMIT 10 . * sqlite:///database.sqlite Done. . id name . 1 | Belgium | . 1729 | England | . 4769 | France | . 7809 | Germany | . 10257 | Italy | . 13274 | Netherlands | . 15722 | Poland | . 17642 | Portugal | . 19694 | Scotland | . 21518 | Spain | . %%sql SELECT id ,name FROM Country WHERE name = &quot;England&quot; . * sqlite:///database.sqlite Done. . id name . 1729 | England | . %%sql SELECT * FROM League LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id name . 1 | 1 | Belgium Jupiler League | . 1729 | 1729 | England Premier League | . 4769 | 4769 | France Ligue 1 | . 7809 | 7809 | Germany 1. Bundesliga | . 10257 | 10257 | Italy Serie A | . 13274 | 13274 | Netherlands Eredivisie | . 15722 | 15722 | Poland Ekstraklasa | . 17642 | 17642 | Portugal Liga ZON Sagres | . 19694 | 19694 | Scotland Premier League | . 21518 | 21518 | Spain LIGA BBVA | . %%sql SELECT * FROM Match LIMIT 10; . * sqlite:///database.sqlite Done. . id country_id league_id season stage date match_api_id home_team_api_id away_team_api_id home_team_goal away_team_goal home_player_X1 home_player_X2 home_player_X3 home_player_X4 home_player_X5 home_player_X6 home_player_X7 home_player_X8 home_player_X9 home_player_X10 home_player_X11 away_player_X1 away_player_X2 away_player_X3 away_player_X4 away_player_X5 away_player_X6 away_player_X7 away_player_X8 away_player_X9 away_player_X10 away_player_X11 home_player_Y1 home_player_Y2 home_player_Y3 home_player_Y4 home_player_Y5 home_player_Y6 home_player_Y7 home_player_Y8 home_player_Y9 home_player_Y10 home_player_Y11 away_player_Y1 away_player_Y2 away_player_Y3 away_player_Y4 away_player_Y5 away_player_Y6 away_player_Y7 away_player_Y8 away_player_Y9 away_player_Y10 away_player_Y11 home_player_1 home_player_2 home_player_3 home_player_4 home_player_5 home_player_6 home_player_7 home_player_8 home_player_9 home_player_10 home_player_11 away_player_1 away_player_2 away_player_3 away_player_4 away_player_5 away_player_6 away_player_7 away_player_8 away_player_9 away_player_10 away_player_11 goal shoton shotoff foulcommit card cross corner possession B365H B365D B365A BWH BWD BWA IWH IWD IWA LBH LBD LBA PSH PSD PSA WHH WHD WHA SJH SJD SJA VCH VCD VCA GBH GBD GBA BSH BSD BSA . 1 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492473 | 9987 | 9993 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.73 | 3.4 | 5 | 1.75 | 3.35 | 4.2 | 1.85 | 3.2 | 3.5 | 1.8 | 3.3 | 3.75 | None | None | None | 1.7 | 3.3 | 4.33 | 1.9 | 3.3 | 4 | 1.65 | 3.4 | 4.5 | 1.78 | 3.25 | 4 | 1.73 | 3.4 | 4.2 | . 2 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492474 | 10000 | 9994 | 0 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.95 | 3.2 | 3.6 | 1.8 | 3.3 | 3.95 | 1.9 | 3.2 | 3.5 | 1.9 | 3.2 | 3.5 | None | None | None | 1.83 | 3.3 | 3.6 | 1.95 | 3.3 | 3.8 | 2 | 3.25 | 3.25 | 1.85 | 3.25 | 3.75 | 1.91 | 3.25 | 3.6 | . 3 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492475 | 9984 | 8635 | 0 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.38 | 3.3 | 2.75 | 2.4 | 3.3 | 2.55 | 2.6 | 3.1 | 2.3 | 2.5 | 3.2 | 2.5 | None | None | None | 2.5 | 3.25 | 2.4 | 2.63 | 3.3 | 2.5 | 2.35 | 3.25 | 2.65 | 2.5 | 3.2 | 2.5 | 2.3 | 3.2 | 2.75 | . 4 | 1 | 1 | 2008/2009 | 1 | 2008-08-17 00:00:00 | 492476 | 9991 | 9998 | 5 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.44 | 3.75 | 7.5 | 1.4 | 4 | 6.8 | 1.4 | 3.9 | 6 | 1.44 | 3.6 | 6.5 | None | None | None | 1.44 | 3.75 | 6 | 1.44 | 4 | 7.5 | 1.45 | 3.75 | 6.5 | 1.5 | 3.75 | 5.5 | 1.44 | 3.75 | 6.5 | . 5 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492477 | 7947 | 9985 | 1 | 3 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 5 | 3.5 | 1.65 | 5 | 3.5 | 1.6 | 4 | 3.3 | 1.7 | 4 | 3.4 | 1.72 | None | None | None | 4.2 | 3.4 | 1.7 | 4.5 | 3.5 | 1.73 | 4.5 | 3.4 | 1.65 | 4.5 | 3.5 | 1.65 | 4.75 | 3.3 | 1.67 | . 6 | 1 | 1 | 2008/2009 | 1 | 2008-09-24 00:00:00 | 492478 | 8203 | 8342 | 1 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 4.75 | 3.4 | 1.67 | 4.85 | 3.4 | 1.65 | 3.7 | 3.2 | 1.8 | 5 | 3.25 | 1.62 | None | None | None | 4.2 | 3.4 | 1.7 | 5.5 | 3.75 | 1.67 | 4.35 | 3.4 | 1.7 | 4.5 | 3.4 | 1.7 | None | None | None | . 7 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492479 | 9999 | 8571 | 2 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.1 | 3.2 | 3.3 | 2.05 | 3.25 | 3.15 | 1.85 | 3.2 | 3.5 | 1.83 | 3.3 | 3.6 | None | None | None | 1.83 | 3.3 | 3.6 | 1.91 | 3.4 | 3.6 | 2.1 | 3.25 | 3 | 1.85 | 3.25 | 3.75 | 2.1 | 3.25 | 3.1 | . 8 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492480 | 4049 | 9996 | 1 | 2 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 3.2 | 3.4 | 2.2 | 2.55 | 3.3 | 2.4 | 2.4 | 3.2 | 2.4 | 2.5 | 3.2 | 2.5 | None | None | None | 2.7 | 3.25 | 2.25 | 2.6 | 3.4 | 2.4 | 2.8 | 3.25 | 2.25 | 2.8 | 3.2 | 2.25 | 2.88 | 3.25 | 2.2 | . 9 | 1 | 1 | 2008/2009 | 1 | 2008-08-16 00:00:00 | 492481 | 10001 | 9986 | 1 | 0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 2.25 | 3.25 | 2.88 | 2.3 | 3.25 | 2.7 | 2.1 | 3.1 | 3 | 2.25 | 3.2 | 2.75 | None | None | None | 2.2 | 3.25 | 2.75 | 2.2 | 3.3 | 3.1 | 2.25 | 3.25 | 2.8 | 2.2 | 3.3 | 2.8 | 2.25 | 3.2 | 2.8 | . 10 | 1 | 1 | 2008/2009 | 10 | 2008-11-01 00:00:00 | 492564 | 8342 | 8571 | 4 | 1 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | 1.3 | 5.25 | 9.5 | 1.25 | 5 | 10 | 1.3 | 4.2 | 8 | 1.25 | 4.5 | 10 | None | None | None | 1.35 | 4.2 | 7 | 1.27 | 5 | 10 | 1.3 | 4.35 | 8.5 | 1.25 | 5 | 10 | 1.29 | 4.5 | 9 | . %%sql SELECT * FROM Player LIMIT 10; . * sqlite:///database.sqlite Done. . id player_api_id player_name player_fifa_api_id birthday height weight . 1 | 505942 | Aaron Appindangoye | 218353 | 1992-02-29 00:00:00 | 182.88 | 187 | . 2 | 155782 | Aaron Cresswell | 189615 | 1989-12-15 00:00:00 | 170.18 | 146 | . 3 | 162549 | Aaron Doran | 186170 | 1991-05-13 00:00:00 | 170.18 | 163 | . 4 | 30572 | Aaron Galindo | 140161 | 1982-05-08 00:00:00 | 182.88 | 198 | . 5 | 23780 | Aaron Hughes | 17725 | 1979-11-08 00:00:00 | 182.88 | 154 | . 6 | 27316 | Aaron Hunt | 158138 | 1986-09-04 00:00:00 | 182.88 | 161 | . 7 | 564793 | Aaron Kuhl | 221280 | 1996-01-30 00:00:00 | 172.72 | 146 | . 8 | 30895 | Aaron Lennon | 152747 | 1987-04-16 00:00:00 | 165.1 | 139 | . 9 | 528212 | Aaron Lennox | 206592 | 1993-02-19 00:00:00 | 190.5 | 181 | . 10 | 101042 | Aaron Meijers | 188621 | 1987-10-28 00:00:00 | 175.26 | 170 | . %%sql SELECT * FROM Player_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id player_fifa_api_id player_api_id date overall_rating potential preferred_foot attacking_work_rate defensive_work_rate crossing finishing heading_accuracy short_passing volleys dribbling curve free_kick_accuracy long_passing ball_control acceleration sprint_speed agility reactions balance shot_power jumping stamina strength long_shots aggression interceptions positioning vision penalties marking standing_tackle sliding_tackle gk_diving gk_handling gk_kicking gk_positioning gk_reflexes . 1 | 218353 | 505942 | 2016-02-18 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 2 | 218353 | 505942 | 2015-11-19 00:00:00 | 67 | 71 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 71 | 70 | 45 | 54 | 48 | 65 | 69 | 69 | 6 | 11 | 10 | 8 | 8 | . 3 | 218353 | 505942 | 2015-09-21 00:00:00 | 62 | 66 | right | medium | medium | 49 | 44 | 71 | 61 | 44 | 51 | 45 | 39 | 64 | 49 | 60 | 64 | 59 | 47 | 65 | 55 | 58 | 54 | 76 | 35 | 63 | 41 | 45 | 54 | 48 | 65 | 66 | 69 | 6 | 11 | 10 | 8 | 8 | . 4 | 218353 | 505942 | 2015-03-20 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 5 | 218353 | 505942 | 2007-02-22 00:00:00 | 61 | 65 | right | medium | medium | 48 | 43 | 70 | 60 | 43 | 50 | 44 | 38 | 63 | 48 | 60 | 64 | 59 | 46 | 65 | 54 | 58 | 54 | 76 | 34 | 62 | 40 | 44 | 53 | 47 | 62 | 63 | 66 | 5 | 10 | 9 | 7 | 7 | . 6 | 189615 | 155782 | 2016-04-21 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 40 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 62 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 7 | 189615 | 155782 | 2016-04-07 00:00:00 | 74 | 76 | left | high | medium | 80 | 53 | 58 | 71 | 32 | 73 | 70 | 69 | 68 | 71 | 79 | 78 | 78 | 67 | 90 | 71 | 85 | 79 | 56 | 60 | 68 | 67 | 60 | 66 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 8 | 189615 | 155782 | 2016-01-07 00:00:00 | 73 | 75 | left | high | medium | 79 | 52 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 59 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 9 | 189615 | 155782 | 2015-12-24 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . 10 | 189615 | 155782 | 2015-12-17 00:00:00 | 73 | 75 | left | high | medium | 79 | 51 | 57 | 70 | 29 | 71 | 68 | 69 | 68 | 70 | 79 | 78 | 78 | 67 | 90 | 71 | 84 | 79 | 56 | 58 | 67 | 66 | 58 | 65 | 59 | 76 | 75 | 78 | 14 | 7 | 9 | 9 | 12 | . %%sql SELECT * FROM Team LIMIT 10; . * sqlite:///database.sqlite Done. . id team_api_id team_fifa_api_id team_long_name team_short_name . 1 | 9987 | 673 | KRC Genk | GEN | . 2 | 9993 | 675 | Beerschot AC | BAC | . 3 | 10000 | 15005 | SV Zulte-Waregem | ZUL | . 4 | 9994 | 2007 | Sporting Lokeren | LOK | . 5 | 9984 | 1750 | KSV Cercle Brugge | CEB | . 6 | 8635 | 229 | RSC Anderlecht | AND | . 7 | 9991 | 674 | KAA Gent | GEN | . 8 | 9998 | 1747 | RAEC Mons | MON | . 9 | 7947 | None | FCV Dender EH | DEN | . 10 | 9985 | 232 | Standard de Liège | STL | . %%sql SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . id team_fifa_api_id team_api_id date buildUpPlaySpeed buildUpPlaySpeedClass buildUpPlayDribbling buildUpPlayDribblingClass buildUpPlayPassing buildUpPlayPassingClass buildUpPlayPositioningClass chanceCreationPassing chanceCreationPassingClass chanceCreationCrossing chanceCreationCrossingClass chanceCreationShooting chanceCreationShootingClass chanceCreationPositioningClass defencePressure defencePressureClass defenceAggression defenceAggressionClass defenceTeamWidth defenceTeamWidthClass defenceDefenderLineClass . 1 | 434 | 9930 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 50 | Mixed | Organised | 60 | Normal | 65 | Normal | 55 | Normal | Organised | 50 | Medium | 55 | Press | 45 | Normal | Cover | . 2 | 434 | 9930 | 2014-09-19 00:00:00 | 52 | Balanced | 48 | Normal | 56 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 3 | 434 | 9930 | 2015-09-10 00:00:00 | 47 | Balanced | 41 | Normal | 54 | Mixed | Organised | 54 | Normal | 63 | Normal | 64 | Normal | Organised | 47 | Medium | 44 | Press | 54 | Normal | Cover | . 4 | 77 | 8485 | 2010-02-22 00:00:00 | 70 | Fast | None | Little | 70 | Long | Organised | 70 | Risky | 70 | Lots | 70 | Lots | Organised | 60 | Medium | 70 | Double | 70 | Wide | Cover | . 5 | 77 | 8485 | 2011-02-22 00:00:00 | 47 | Balanced | None | Little | 52 | Mixed | Organised | 53 | Normal | 48 | Normal | 52 | Normal | Organised | 47 | Medium | 47 | Press | 52 | Normal | Cover | . 6 | 77 | 8485 | 2012-02-22 00:00:00 | 58 | Balanced | None | Little | 62 | Mixed | Organised | 45 | Normal | 70 | Lots | 55 | Normal | Organised | 40 | Medium | 40 | Press | 60 | Normal | Cover | . 7 | 77 | 8485 | 2013-09-20 00:00:00 | 62 | Balanced | None | Little | 45 | Mixed | Organised | 40 | Normal | 50 | Normal | 55 | Normal | Organised | 42 | Medium | 42 | Press | 60 | Normal | Cover | . 8 | 77 | 8485 | 2014-09-19 00:00:00 | 58 | Balanced | 64 | Normal | 62 | Mixed | Organised | 56 | Normal | 68 | Lots | 57 | Normal | Organised | 41 | Medium | 42 | Press | 60 | Normal | Cover | . 9 | 77 | 8485 | 2015-09-10 00:00:00 | 59 | Balanced | 64 | Normal | 53 | Mixed | Organised | 51 | Normal | 72 | Lots | 63 | Normal | Free Form | 49 | Medium | 45 | Press | 63 | Normal | Cover | . 10 | 614 | 8576 | 2010-02-22 00:00:00 | 60 | Balanced | None | Little | 40 | Mixed | Organised | 45 | Normal | 35 | Normal | 55 | Normal | Organised | 30 | Deep | 70 | Double | 30 | Narrow | Offside Trap | . %%sql CREATE TABLE Team_table AS SELECT * FROM Team_Attributes LIMIT 10; . * sqlite:///database.sqlite Done. . [] . %%sql DROP TABLE IF EXISTS Team_table . * sqlite:///database.sqlite Done. . [] . sql_query = %sql SELECT * FROM Team LIMIT 10 df = sql_query.DataFrame() . * sqlite:///database.sqlite Done. . df . id team_api_id team_fifa_api_id team_long_name team_short_name . 0 1 | 9987 | 673.0 | KRC Genk | GEN | . 1 2 | 9993 | 675.0 | Beerschot AC | BAC | . 2 3 | 10000 | 15005.0 | SV Zulte-Waregem | ZUL | . 3 4 | 9994 | 2007.0 | Sporting Lokeren | LOK | . 4 5 | 9984 | 1750.0 | KSV Cercle Brugge | CEB | . 5 6 | 8635 | 229.0 | RSC Anderlecht | AND | . 6 7 | 9991 | 674.0 | KAA Gent | GEN | . 7 8 | 9998 | 1747.0 | RAEC Mons | MON | . 8 9 | 7947 | NaN | FCV Dender EH | DEN | . 9 10 | 9985 | 232.0 | Standard de Liège | STL | . import matplotlib.pyplot as plt plt.figure(figsize=(20,7)) plot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name plot.bar(); . * sqlite:///database.sqlite Done. . plot.pie(); . type(plot) . sql.run.ResultSet .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/26/Working_with_sqlite3_dbs_in_jupyter_Soccer_Pred.html",
            "relUrl": "/2020/10/26/Working_with_sqlite3_dbs_in_jupyter_Soccer_Pred.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader import datetime import pandas_datareader.data as web import statsmodels.api as sm import quandl . start = datetime.datetime(1960, 1, 1) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . ... ... | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . 740 rows × 1 columns . df = SP500 . df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.tail() . Value . Date . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 2020-09-30 3363.00 | . 2020-10-01 3380.80 | . df.columns = [&#39;Value&#39;] df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . df.describe().transpose() . count mean std min 25% 50% 75% max . Value 740.0 | 761.732932 | 834.566138 | 53.73 | 100.9 | 349.425 | 1239.415 | 3526.65 | . Step 2: Visualize the Data . Let&#39;s visualize this data with a few methods. . df.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8873251b38&gt; . timeseries = df[&#39;Value&#39;] . timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.rolling(12).std().plot(label=&#39;12 Month Rolling Std&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f88731e22e8&gt; . timeseries.rolling(12).mean().plot(label=&#39;12 Month Rolling Mean&#39;) timeseries.plot() plt.legend() . &lt;matplotlib.legend.Legend at 0x7f887315f630&gt; . Decomposition . ETS decomposition allows us to see the individual parts! . from statsmodels.tsa.seasonal import seasonal_decompose decomposition = seasonal_decompose(df[&#39;Value&#39;], freq=12) fig = plt.figure() fig = decomposition.plot() fig.set_size_inches(15, 8) . &lt;Figure size 432x288 with 0 Axes&gt; . Testing for Stationarity . df.head() . Value . Date . 1960-01-01 58.03 | . 1960-02-01 55.78 | . 1960-03-01 55.02 | . 1960-04-01 55.73 | . 1960-05-01 55.22 | . from statsmodels.tsa.stattools import adfuller . result = adfuller(df[&#39;Value&#39;]) . print(&#39;Augmented Dickey-Fuller Test:&#39;) labels = [&#39;ADF Test Statistic&#39;,&#39;p-value&#39;,&#39;#Lags Used&#39;,&#39;Number of Observations Used&#39;] for value,label in zip(result,labels): print(label+&#39; : &#39;+str(value) ) if result[1] &lt;= 0.05: print(&quot;strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary&quot;) else: print(&quot;weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary &quot;) . Augmented Dickey-Fuller Test: ADF Test Statistic : 1.7247353245135 p-value : 0.9981874531215522 #Lags Used : 20 Number of Observations Used : 719 weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary . def adf_check(time_series): &quot;&quot;&quot; Pass in a time series, returns ADF report &quot;&quot;&quot; result = adfuller(time_series) print(&#39;Augmented Dickey-Fuller Test:&#39;) labels = [&#39;ADF Test Statistic&#39;,&#39;p-value&#39;,&#39;#Lags Used&#39;,&#39;Number of Observations Used&#39;] for value,label in zip(result,labels): print(label+&#39; : &#39;+str(value) ) if result[1] &lt;= 0.05: print(&quot;strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary&quot;) else: print(&quot;weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary &quot;) . First Difference . df[&#39;Value First Difference&#39;] = df[&#39;Value&#39;] - df[&#39;Value&#39;].shift(1) . adf_check(df[&#39;Value First Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -4.267790128581322 p-value : 0.0005048563860225925 #Lags Used : 20 Number of Observations Used : 718 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . df[&#39;Value First Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f886d63feb8&gt; . Second Difference . df[&#39;Value Second Difference&#39;] = df[&#39;Value First Difference&#39;] - df[&#39;Value First Difference&#39;].shift(1) . adf_check(df[&#39;Value Second Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -12.29955077642857 p-value : 7.504260735615441e-23 #Lags Used : 18 Number of Observations Used : 719 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . df[&#39;Value Second Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8877f726d8&gt; . Seasonal Difference . df[&#39;Seasonal Difference&#39;] = df[&#39;Value&#39;] - df[&#39;Value&#39;].shift(12) df[&#39;Seasonal Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8877fb79e8&gt; . adf_check(df[&#39;Seasonal Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -5.239903673260254 p-value : 7.284266188346342e-06 #Lags Used : 20 Number of Observations Used : 707 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . Seasonal First Difference . df[&#39;Seasonal First Difference&#39;] = df[&#39;Value First Difference&#39;] - df[&#39;Value First Difference&#39;].shift(12) df[&#39;Seasonal First Difference&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f88acb0c2b0&gt; . adf_check(df[&#39;Seasonal First Difference&#39;].dropna()) . Augmented Dickey-Fuller Test: ADF Test Statistic : -6.196739887980032 p-value : 5.940155101037563e-08 #Lags Used : 20 Number of Observations Used : 706 strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary . from statsmodels.graphics.tsaplots import plot_acf,plot_pacf . # Check out: https://stackoverflow.com/questions/21788593/statsmodels-duplicate-charts # https://github.com/statsmodels/statsmodels/issues/1265 fig_first = plot_acf(df[&quot;Value First Difference&quot;].dropna()) . fig_seasonal_first = plot_acf(df[&quot;Seasonal First Difference&quot;].dropna()) . Pandas also has this functionality built in, but only for ACF, not PACF. So I recommend using statsmodels, as ACF and PACF is more core to its functionality than it is to pandas&#39; functionality. . from pandas.plotting import autocorrelation_plot autocorrelation_plot(df[&#39;Seasonal First Difference&#39;].dropna()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f886d660358&gt; . We can then plot this relationship: . result = plot_pacf(df[&quot;Seasonal First Difference&quot;].dropna()) . fig = plt.figure(figsize=(12,8)) ax1 = fig.add_subplot(211) fig = sm.graphics.tsa.plot_acf(df[&#39;Seasonal First Difference&#39;].iloc[13:], lags=40, ax=ax1) ax2 = fig.add_subplot(212) fig = sm.graphics.tsa.plot_pacf(df[&#39;Seasonal First Difference&#39;].iloc[13:], lags=40, ax=ax2) . from statsmodels.tsa.arima_model import ARIMA . help(ARIMA) . Help on class ARIMA in module statsmodels.tsa.arima_model: class ARIMA(ARMA) | ARIMA(endog, order, exog=None, dates=None, freq=None, missing=&#39;none&#39;) | | Autoregressive Integrated Moving Average ARIMA(p,d,q) Model | | Parameters | - | endog : array-like | The endogenous variable. | order : iterable | The (p,d,q) order of the model for the number of AR parameters, | differences, and MA parameters to use. | exog : array-like, optional | An optional array of exogenous variables. This should *not* include a | constant or trend. You can specify this in the `fit` method. | dates : array-like of datetime, optional | An array-like object of datetime objects. If a pandas object is given | for endog or exog, it is assumed to have a DateIndex. | freq : str, optional | The frequency of the time-series. A Pandas offset or &#39;B&#39;, &#39;D&#39;, &#39;W&#39;, | &#39;M&#39;, &#39;A&#39;, or &#39;Q&#39;. This is optional if dates are given. | | | Notes | -- | If exogenous variables are given, then the model that is fit is | | .. math:: | | phi(L)(y_t - X_t beta) = theta(L) epsilon_t | | where :math:` phi` and :math:` theta` are polynomials in the lag | operator, :math:`L`. This is the regression model with ARMA errors, | or ARMAX model. This specification is used, whether or not the model | is fit using conditional sum of square or maximum-likelihood, using | the `method` argument in | :meth:`statsmodels.tsa.arima_model.ARIMA.fit`. Therefore, for | now, `css` and `mle` refer to estimation methods only. This may | change for the case of the `css` model in future versions. | | Method resolution order: | ARIMA | ARMA | statsmodels.tsa.base.tsa_model.TimeSeriesModel | statsmodels.base.model.LikelihoodModel | statsmodels.base.model.Model | builtins.object | | Methods defined here: | | __getnewargs__(self) | | __init__(self, endog, order, exog=None, dates=None, freq=None, missing=&#39;none&#39;) | Initialize self. See help(type(self)) for accurate signature. | | fit(self, start_params=None, trend=&#39;c&#39;, method=&#39;css-mle&#39;, transparams=True, solver=&#39;lbfgs&#39;, maxiter=500, full_output=1, disp=5, callback=None, start_ar_lags=None, **kwargs) | Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter. | | Parameters | - | start_params : array-like, optional | Starting parameters for ARMA(p,q). If None, the default is given | by ARMA._fit_start_params. See there for more information. | transparams : bool, optional | Whehter or not to transform the parameters to ensure stationarity. | Uses the transformation suggested in Jones (1980). If False, | no checking for stationarity or invertibility is done. | method : str {&#39;css-mle&#39;,&#39;mle&#39;,&#39;css&#39;} | This is the loglikelihood to maximize. If &#34;css-mle&#34;, the | conditional sum of squares likelihood is maximized and its values | are used as starting values for the computation of the exact | likelihood via the Kalman filter. If &#34;mle&#34;, the exact likelihood | is maximized via the Kalman Filter. If &#34;css&#34; the conditional sum | of squares likelihood is maximized. All three methods use | `start_params` as starting parameters. See above for more | information. | trend : str {&#39;c&#39;,&#39;nc&#39;} | Whether to include a constant or not. &#39;c&#39; includes constant, | &#39;nc&#39; no constant. | solver : str or None, optional | Solver to be used. The default is &#39;lbfgs&#39; (limited memory | Broyden-Fletcher-Goldfarb-Shanno). Other choices are &#39;bfgs&#39;, | &#39;newton&#39; (Newton-Raphson), &#39;nm&#39; (Nelder-Mead), &#39;cg&#39; - | (conjugate gradient), &#39;ncg&#39; (non-conjugate gradient), and | &#39;powell&#39;. By default, the limited memory BFGS uses m=12 to | approximate the Hessian, projected gradient tolerance of 1e-8 and | factr = 1e2. You can change these by using kwargs. | maxiter : int, optional | The maximum number of function evaluations. Default is 500. | tol : float | The convergence tolerance. Default is 1e-08. | full_output : bool, optional | If True, all output from solver will be available in | the Results object&#39;s mle_retvals attribute. Output is dependent | on the solver. See Notes for more information. | disp : int, optional | If True, convergence information is printed. For the default | l_bfgs_b solver, disp controls the frequency of the output during | the iterations. disp &lt; 0 means no output in this case. | callback : function, optional | Called after each iteration as callback(xk) where xk is the current | parameter vector. | start_ar_lags : int, optional | Parameter for fitting start_params. When fitting start_params, | residuals are obtained from an AR fit, then an ARMA(p,q) model is | fit via OLS using these residuals. If start_ar_lags is None, fit | an AR process according to best BIC. If start_ar_lags is not None, | fits an AR process with a lag length equal to start_ar_lags. | See ARMA._fit_start_params_hr for more information. | kwargs | See Notes for keyword arguments that can be passed to fit. | | Returns | - | `statsmodels.tsa.arima.ARIMAResults` class | | See Also | -- | statsmodels.base.model.LikelihoodModel.fit : for more information | on using the solvers. | ARIMAResults : results class returned by fit | | Notes | -- | If fit by &#39;mle&#39;, it is assumed for the Kalman Filter that the initial | unknown state is zero, and that the initial variance is | P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel(&#39;F&#39;)).reshape(r, | r, order = &#39;F&#39;) | | predict(self, params, start=None, end=None, exog=None, typ=&#39;linear&#39;, dynamic=False) | ARIMA model in-sample and out-of-sample prediction | | Parameters | - | params : array-like | The fitted parameters of the model. | start : int, str, or datetime | Zero-indexed observation number at which to start forecasting, ie., | the first forecast is start. Can also be a date string to | parse or a datetime type. | end : int, str, or datetime | Zero-indexed observation number at which to end forecasting, ie., | the first forecast is start. Can also be a date string to | parse or a datetime type. However, if the dates index does not | have a fixed frequency, end must be an integer index if you | want out of sample prediction. | exog : array-like, optional | If the model is an ARMAX and out-of-sample forecasting is | requested, exog must be given. Note that you&#39;ll need to pass | `k_ar` additional lags for any exogenous variables. E.g., if you | fit an ARMAX(2, q) model and want to predict 5 steps, you need 7 | observations to do this. | dynamic : bool, optional | The `dynamic` keyword affects in-sample prediction. If dynamic | is False, then the in-sample lagged values are used for | prediction. If `dynamic` is True, then in-sample forecasts are | used in place of lagged dependent variables. The first forecasted | value is `start`. | typ : str {&#39;linear&#39;, &#39;levels&#39;} | | - &#39;linear&#39; : Linear prediction in terms of the differenced | endogenous variables. | - &#39;levels&#39; : Predict the levels of the original endogenous | variables. | | | Returns | - | predict : array | The predicted values. | | | | Notes | -- | Use the results predict method instead. | | - | Static methods defined here: | | __new__(cls, endog, order, exog=None, dates=None, freq=None, missing=&#39;none&#39;) | Create and return a new object. See help(type) for accurate signature. | | - | Methods inherited from ARMA: | | geterrors(self, params) | Get the errors of the ARMA process. | | Parameters | - | params : array-like | The fitted ARMA parameters | order : array-like | 3 item iterable, with the number of AR, MA, and exogenous | parameters, including the trend | | hessian(self, params) | Compute the Hessian at params, | | Notes | -- | This is a numerical approximation. | | loglike(self, params, set_sigma2=True) | Compute the log-likelihood for ARMA(p,q) model | | Notes | -- | Likelihood used depends on the method set in fit | | loglike_css(self, params, set_sigma2=True) | Conditional Sum of Squares likelihood function. | | loglike_kalman(self, params, set_sigma2=True) | Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter. | | score(self, params) | Compute the score function at params. | | Notes | -- | This is a numerical approximation. | | - | Class methods inherited from ARMA: | | from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type | Create a Model from a formula and dataframe. | | Parameters | - | formula : str or generic Formula object | The formula specifying the model | data : array-like | The data for the model. See Notes. | subset : array-like | An array-like object of booleans, integers, or index values that | indicate the subset of df to use in the model. Assumes df is a | `pandas.DataFrame` | drop_cols : array-like | Columns to drop from the design matrix. Cannot be used to | drop terms involving categoricals. | args : extra arguments | These are passed to the model | kwargs : extra keyword arguments | These are passed to the model with one exception. The | ``eval_env`` keyword is passed to patsy. It can be either a | :class:`patsy:patsy.EvalEnvironment` object or an integer | indicating the depth of the namespace to use. For example, the | default ``eval_env=0`` uses the calling namespace. If you wish | to use a &#34;clean&#34; environment set ``eval_env=-1``. | | Returns | - | model : Model instance | | Notes | -- | data must define __getitem__ with the keys in the formula terms | args and kwargs are passed on to the model instantiation. E.g., | a numpy structured or rec array, a dictionary, or a pandas DataFrame. | | - | Data descriptors inherited from statsmodels.tsa.base.tsa_model.TimeSeriesModel: | | exog_names | | - | Methods inherited from statsmodels.base.model.LikelihoodModel: | | information(self, params) | Fisher information matrix of model | | Returns -Hessian of loglike evaluated at params. | | initialize(self) | Initialize (possibly re-initialize) a Model instance. For | instance, the design matrix of a linear model may change | and some things must be recomputed. | | - | Data descriptors inherited from statsmodels.base.model.Model: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | endog_names | Names of endogenous variables . model = sm.tsa.statespace.SARIMAX(df[&#39;Value&#39;],order=(0,1,0), seasonal_order=(1,1,1,12)) results = model.fit() print(results.summary()) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:219: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting. &#39; ignored when e.g. forecasting.&#39;, ValueWarning) . Statespace Model Results ========================================================================================== Dep. Variable: Value No. Observations: 740 Model: SARIMAX(0, 1, 0)x(1, 1, 1, 12) Log Likelihood -3719.108 Date: Fri, 23 Oct 2020 AIC 7444.215 Time: 09:03:22 BIC 7457.982 Sample: 0 HQIC 7449.528 - 740 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.S.L12 0.0043 0.021 0.201 0.840 -0.037 0.046 ma.S.L12 -0.9513 0.018 -53.297 0.000 -0.986 -0.916 sigma2 1563.8216 30.663 51.001 0.000 1503.724 1623.919 =================================================================================== Ljung-Box (Q): 116.83 Jarque-Bera (JB): 9251.13 Prob(Q): 0.00 Prob(JB): 0.00 Heteroskedasticity (H): 410.23 Skew: -1.86 Prob(H) (two-sided): 0.00 Kurtosis: 20.08 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . results.resid.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872daf940&gt; . results.resid.plot(kind=&#39;kde&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872d863c8&gt; . Prediction of Future Values . Firts we can get an idea of how well our model performs by just predicting for values that we actually already know: . df[&#39;forecast&#39;] = results.predict(start = 1, end= 720, dynamic= True) df[[&#39;Value&#39;,&#39;forecast&#39;]].plot(figsize=(12,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f88ace0d7b8&gt; . Forecasting . df.tail() . Value Value First Difference Value Second Difference Seasonal Difference Seasonal First Difference forecast . Date . 2020-08-01 3391.71 | 120.59 | 57.09 | 166.67 | 173.75 | NaN | . 2020-08-31 3500.31 | 108.60 | -11.99 | 223.00 | 56.33 | NaN | . 2020-09-01 3526.65 | 26.34 | -82.26 | 571.84 | 348.84 | NaN | . 2020-09-30 3363.00 | -163.65 | -189.99 | 710.61 | 138.77 | NaN | . 2020-10-01 3380.80 | 17.80 | 181.45 | 796.21 | 85.60 | NaN | . from pandas.tseries.offsets import DateOffset . future_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ] . future_dates . [Timestamp(&#39;2020-10-01 00:00:00&#39;), Timestamp(&#39;2020-11-01 00:00:00&#39;), Timestamp(&#39;2020-12-01 00:00:00&#39;), Timestamp(&#39;2021-01-01 00:00:00&#39;), Timestamp(&#39;2021-02-01 00:00:00&#39;), Timestamp(&#39;2021-03-01 00:00:00&#39;), Timestamp(&#39;2021-04-01 00:00:00&#39;), Timestamp(&#39;2021-05-01 00:00:00&#39;), Timestamp(&#39;2021-06-01 00:00:00&#39;), Timestamp(&#39;2021-07-01 00:00:00&#39;), Timestamp(&#39;2021-08-01 00:00:00&#39;), Timestamp(&#39;2021-09-01 00:00:00&#39;), Timestamp(&#39;2021-10-01 00:00:00&#39;), Timestamp(&#39;2021-11-01 00:00:00&#39;), Timestamp(&#39;2021-12-01 00:00:00&#39;), Timestamp(&#39;2022-01-01 00:00:00&#39;), Timestamp(&#39;2022-02-01 00:00:00&#39;), Timestamp(&#39;2022-03-01 00:00:00&#39;), Timestamp(&#39;2022-04-01 00:00:00&#39;), Timestamp(&#39;2022-05-01 00:00:00&#39;), Timestamp(&#39;2022-06-01 00:00:00&#39;), Timestamp(&#39;2022-07-01 00:00:00&#39;), Timestamp(&#39;2022-08-01 00:00:00&#39;), Timestamp(&#39;2022-09-01 00:00:00&#39;)] . future_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns) . future_df = pd.concat([df,future_dates_df]) . future_df.head() . Value Value First Difference Value Second Difference Seasonal Difference Seasonal First Difference forecast . 1960-01-01 58.03 | NaN | NaN | NaN | NaN | NaN | . 1960-02-01 55.78 | -2.25 | NaN | NaN | NaN | 58.03 | . 1960-03-01 55.02 | -0.76 | 1.49 | NaN | NaN | 58.03 | . 1960-04-01 55.73 | 0.71 | 1.47 | NaN | NaN | 58.03 | . 1960-05-01 55.22 | -0.51 | -1.22 | NaN | NaN | 58.03 | . future_df.tail() . Value Value First Difference Value Second Difference Seasonal Difference Seasonal First Difference forecast . 2022-05-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-06-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-07-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-08-01 NaN | NaN | NaN | NaN | NaN | NaN | . 2022-09-01 NaN | NaN | NaN | NaN | NaN | NaN | . future_df[&#39;forecast&#39;] = results.predict(start = 1, end = 720, dynamic= True) future_df[[&#39;Value&#39;, &#39;forecast&#39;]].plot(figsize=(12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872997fd0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/25/ARIMA-ADF-FORECASTING.html",
            "relUrl": "/2020/10/25/ARIMA-ADF-FORECASTING.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post46": {
            "title": "Machine Learning in Healthcare notes",
            "content": "Machine Learning for Healthcare . +Machine Learning for Healthcare . Mycin system from the 1970s: ‘MYCIN was an early backward chaining expert system that used artificial intelligence to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for patient’s body weight — the name derived from the antibiotics themselves, as many antibiotics have the suffix “-mycin”. The’ . | Internist-I: For primary care. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/10/23/ML-for-HC-notes.html",
            "relUrl": "/data%20science%20content/2020/10/23/ML-for-HC-notes.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post47": {
            "title": "Stock Market Analysis of Microsoft, Zoom, and Snowflake",
            "content": "Daily Return and Cumulative Returns . This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . import pandas_datareader import datetime . import pandas_datareader.data as web . start = datetime.datetime(2019, 1, 1) end = datetime.datetime(2021, 1, 1) #start = datetime.datetime(2012, 1, 1) #end = datetime.datetime(2017, 1, 1) #tesla = web.DataReader(&quot;TSLA&quot;, &#39;yahoo&#39;, start, end) . MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() TSLA_stock = web.DataReader(&#39;TSLA&#39;, &#39;yahoo&#39;, start, end) TSLA_stock.head() . High Low Open Close Volume Adj Close . Date . 2019-01-02 63.026001 | 59.759998 | 61.220001 | 62.023998 | 58293000.0 | 62.023998 | . 2019-01-03 61.880001 | 59.476002 | 61.400002 | 60.071999 | 34826000.0 | 60.071999 | . 2019-01-04 63.599998 | 60.546001 | 61.200001 | 63.537998 | 36970500.0 | 63.537998 | . 2019-01-07 67.348000 | 63.549999 | 64.344002 | 66.991997 | 37756000.0 | 66.991997 | . 2019-01-08 68.802002 | 65.403999 | 68.391998 | 67.070000 | 35042500.0 | 67.070000 | . MSFT_stock[&#39;Open&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8),title=&#39;Open Price&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;ZOOM_stock&#39;) TSLA_stock[&#39;Open&#39;].plot(label=&#39;TSLA_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fabf2074c90&gt; . . MSFT_stock[&#39;Volume&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8),title=&#39;Volume Traded&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;ZOOM_stock&#39;) TSLA_stock[&#39;Volume&#39;].plot(label=&#39;TSLA_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fabf1feab90&gt; . MSFT_stock[&#39;Volume&#39;].argmax() . 291 . MSFT_stock[&#39;Total Traded&#39;] = MSFT_stock[&#39;Open&#39;]*MSFT_stock[&#39;Volume&#39;] TSLA_stock[&#39;Total Traded&#39;] = TSLA_stock[&#39;Open&#39;]*TSLA_stock[&#39;Volume&#39;] ZOOM_stock[&#39;Total Traded&#39;] = ZOOM_stock[&#39;Open&#39;]*ZOOM_stock[&#39;Volume&#39;] . MSFT_stock[&#39;Total Traded&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8)) ZOOM_stock[&#39;Total Traded&#39;].plot(label=&#39;ZOOM_stock&#39;) TSLA_stock[&#39;Total Traded&#39;].plot(label=&#39;TSLA_stock&#39;) plt.legend() plt.ylabel(&#39;Total Traded&#39;) . Text(0, 0.5, &#39;Total Traded&#39;) . ZOOM_stock[&#39;Total Traded&#39;].argmax() . 346 . MA (Moving Averages) . ZOOM_stock[&#39;MA50&#39;] = ZOOM_stock[&#39;Open&#39;].rolling(50).mean() ZOOM_stock[&#39;MA200&#39;] = ZOOM_stock[&#39;Open&#39;].rolling(200).mean() ZOOM_stock[[&#39;Open&#39;,&#39;MA50&#39;,&#39;MA200&#39;]].plot(label=&#39;ZOOM_stock&#39;,figsize=(16,8)) . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . from pandas.plotting import scatter_matrix car_comp = pd.concat([MSFT_stock[&#39;Open&#39;],ZOOM_stock[&#39;Open&#39;],SNOW_stock[&#39;Open&#39;]],axis=1) car_comp.columns = [&#39;MSFT_stock Open&#39;,&#39;ZOOM_stock Open&#39;,&#39;SNOW_stock Open&#39;] . # You can use a semi-colon to remove the axes print outs scatter_matrix(car_comp,figsize=(8,8),alpha=0.2,hist_kwds={&#39;bins&#39;:50}); . from mpl_finance import candlestick_ohlc from matplotlib.dates import DateFormatter, date2num, WeekdayLocator, DayLocator, MONDAY . # Rest the index to get a column of January Dates MSFT_stock_reset = MSFT_stock.loc[&#39;2019-01&#39;:&#39;2019-01&#39;].reset_index() . # Create a new column of numerical &quot;date&quot; values for matplotlib to use MSFT_stock_reset[&#39;date_ax&#39;] = MSFT_stock_reset[&#39;Date&#39;].apply(lambda date: date2num(date)) MSFT_stock_values = [tuple(vals) for vals in MSFT_stock_reset[[&#39;date_ax&#39;, &#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]].values] . mondays = WeekdayLocator(MONDAY) # major ticks on the mondays alldays = DayLocator() # minor ticks on the days weekFormatter = DateFormatter(&#39;%b %d&#39;) # e.g., Jan 12 dayFormatter = DateFormatter(&#39;%d&#39;) # e.g., 12 . fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) ax.xaxis.set_major_locator(mondays) ax.xaxis.set_minor_locator(alldays) ax.xaxis.set_major_formatter(weekFormatter) candlestick_ohlc(ax, MSFT_stock_values, width=0.6, colorup=&#39;g&#39;,colordown=&#39;r&#39;); . # Method 1: Using shift MSFT_stock[&#39;returns&#39;] = (MSFT_stock[&#39;Close&#39;] / MSFT_stock[&#39;Close&#39;].shift(1) ) - 1 MSFT_stock.head() MSFT_stock[&#39;returns&#39;] = MSFT_stock[&#39;Close&#39;].pct_change(1) MSFT_stock.head() . High Low Open Close Volume Adj Close Total Traded returns . Date . 2019-01-02 101.750000 | 98.940002 | 99.550003 | 101.120003 | 35329300.0 | 98.860214 | 3.517032e+09 | NaN | . 2019-01-03 100.190002 | 97.199997 | 100.099998 | 97.400002 | 42579100.0 | 95.223351 | 4.262168e+09 | -0.036788 | . 2019-01-04 102.510002 | 98.930000 | 99.720001 | 101.930000 | 44060600.0 | 99.652115 | 4.393723e+09 | 0.046509 | . 2019-01-07 103.269997 | 100.980003 | 101.639999 | 102.059998 | 35656100.0 | 99.779205 | 3.624086e+09 | 0.001275 | . 2019-01-08 103.970001 | 101.709999 | 103.040001 | 102.800003 | 31514400.0 | 100.502670 | 3.247244e+09 | 0.007251 | . TSLA_stock[&#39;returns&#39;] = TSLA_stock[&#39;Close&#39;].pct_change(1) ZOOM_stock[&#39;returns&#39;] = ZOOM_stock[&#39;Close&#39;].pct_change(1) . TSLA_stock.head() . High Low Open Close Volume Adj Close Total Traded returns . Date . 2019-01-02 63.026001 | 59.759998 | 61.220001 | 62.023998 | 58293000.0 | 62.023998 | 3.568698e+09 | NaN | . 2019-01-03 61.880001 | 59.476002 | 61.400002 | 60.071999 | 34826000.0 | 60.071999 | 2.138316e+09 | -0.031472 | . 2019-01-04 63.599998 | 60.546001 | 61.200001 | 63.537998 | 36970500.0 | 63.537998 | 2.262595e+09 | 0.057697 | . 2019-01-07 67.348000 | 63.549999 | 64.344002 | 66.991997 | 37756000.0 | 66.991997 | 2.429372e+09 | 0.054361 | . 2019-01-08 68.802002 | 65.403999 | 68.391998 | 67.070000 | 35042500.0 | 67.070000 | 2.396627e+09 | 0.001164 | . ZOOM_stock.head() . High Low Open Close Volume Adj Close Total Traded MA50 MA200 returns . Date . 2019-04-18 66.000000 | 60.320999 | 65.000000 | 62.000000 | 25764700 | 62.000000 | 1.674706e+09 | NaN | NaN | NaN | . 2019-04-22 68.900002 | 59.939999 | 61.000000 | 65.699997 | 9949700 | 65.699997 | 6.069317e+08 | NaN | NaN | 0.059677 | . 2019-04-23 74.168999 | 65.550003 | 66.870003 | 69.000000 | 6786500 | 69.000000 | 4.538133e+08 | NaN | NaN | 0.050228 | . 2019-04-24 71.500000 | 63.160000 | 71.400002 | 63.200001 | 4973500 | 63.200001 | 3.551079e+08 | NaN | NaN | -0.084058 | . 2019-04-25 66.849998 | 62.599998 | 64.739998 | 65.000000 | 3863300 | 65.000000 | 2.501100e+08 | NaN | NaN | 0.028481 | . TSLA_stock[&#39;returns&#39;].hist(bins=50) . &lt;AxesSubplot:&gt; . MSFT_stock[&#39;returns&#39;].hist(bins=50) . &lt;AxesSubplot:&gt; . ZOOM_stock[&#39;returns&#39;].hist(bins=50) . &lt;AxesSubplot:&gt; . MSFT_stock[&#39;returns&#39;].hist(bins=100,label=&#39;MSFT_stock&#39;,figsize=(10,8),alpha=0.5) ZOOM_stock[&#39;returns&#39;].hist(bins=100,label=&#39;ZOOM_stock&#39;,alpha=0.5) TSLA_stock[&#39;returns&#39;].hist(bins=100,label=&#39;TSLA_stock&#39;,alpha=0.5) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fabf1902d90&gt; . MSFT_stock[&#39;returns&#39;].plot(kind=&#39;kde&#39;,label=&#39;MSFT_stock&#39;,figsize=(12,6)) ZOOM_stock[&#39;returns&#39;].plot(kind=&#39;kde&#39;,label=&#39;ZOOM_stock&#39;) TSLA_stock[&#39;returns&#39;].plot(kind=&#39;kde&#39;,label=&#39;TSLA_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fabf11a3e50&gt; . box_df = pd.concat([MSFT_stock[&#39;returns&#39;],ZOOM_stock[&#39;returns&#39;],TSLA_stock[&#39;returns&#39;]],axis=1) box_df.columns = [&#39;MSFT_stock Returns&#39;,&#39; ZOOM_stock Returns&#39;,&#39;TSLA_stock Returns&#39;] box_df.plot(kind=&#39;box&#39;,figsize=(8,11),colormap=&#39;jet&#39;) . &lt;AxesSubplot:&gt; . scatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={&#39;bins&#39;:50}); . scatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={&#39;bins&#39;:50}); . box_df.plot(kind=&#39;scatter&#39;,x=&#39; ZOOM_stock Returns&#39;,y=&#39;MSFT_stock Returns&#39;,alpha=0.4,figsize=(10,8)) . &lt;AxesSubplot:xlabel=&#39; ZOOM_stock Returns&#39;, ylabel=&#39;MSFT_stock Returns&#39;&gt; . Daily Return and Cumulative Return . MSFT_stock[&#39;Cumulative Return&#39;] = (1 + MSFT_stock[&#39;returns&#39;]).cumprod() MSFT_stock.head() . High Low Open Close Volume Adj Close Total Traded returns Cumulative Return . Date . 2019-01-02 101.750000 | 98.940002 | 99.550003 | 101.120003 | 35329300.0 | 98.860214 | 3.517032e+09 | NaN | NaN | . 2019-01-03 100.190002 | 97.199997 | 100.099998 | 97.400002 | 42579100.0 | 95.223351 | 4.262168e+09 | -0.036788 | 0.963212 | . 2019-01-04 102.510002 | 98.930000 | 99.720001 | 101.930000 | 44060600.0 | 99.652115 | 4.393723e+09 | 0.046509 | 1.008010 | . 2019-01-07 103.269997 | 100.980003 | 101.639999 | 102.059998 | 35656100.0 | 99.779205 | 3.624086e+09 | 0.001275 | 1.009296 | . 2019-01-08 103.970001 | 101.709999 | 103.040001 | 102.800003 | 31514400.0 | 100.502670 | 3.247244e+09 | 0.007251 | 1.016614 | . TSLA_stock[&#39;Cumulative Return&#39;] = (1 + TSLA_stock[&#39;returns&#39;]).cumprod() ZOOM_stock[&#39;Cumulative Return&#39;] = (1 + ZOOM_stock[&#39;returns&#39;]).cumprod() . MSFT_stock[&#39;Cumulative Return&#39;].plot(label=&#39;MSFT_stock&#39;,figsize=(16,8),title=&#39;Cumulative Return&#39;) TSLA_stock[&#39;Cumulative Return&#39;].plot(label=&#39;TSLA_stock&#39;) ZOOM_stock[&#39;Cumulative Return&#39;].plot(label=&#39;ZOOM_stock&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fabf0c54650&gt; . from fbprophet import Prophet . MSFT_stock[&#39;ds&#39;] = MSFT_stock.index MSFT_stock[&#39;y&#39;] = MSFT_stock.Open m = Prophet() m.fit(MSFT_stock) . INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fabf0b5e7d0&gt; . future = m.make_future_dataframe(periods=365) future.tail() . ds . 828 2021-11-02 | . 829 2021-11-03 | . 830 2021-11-04 | . 831 2021-11-05 | . 832 2021-11-06 | . forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() . ds yhat yhat_lower yhat_upper . 828 2021-11-02 | 264.480553 | 180.297752 | 351.490936 | . 829 2021-11-03 | 264.454392 | 181.887667 | 348.682850 | . 830 2021-11-04 | 264.470521 | 183.308874 | 351.080864 | . 831 2021-11-05 | 264.680107 | 182.084709 | 351.905134 | . 832 2021-11-06 | 263.754254 | 180.125064 | 351.617704 | . fig1 = m.plot(forecast) . fig2 = m.plot_components(forecast) . MSFT_stock[&#39;Open&#39;].plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . import statsmodels.api as sm . MSFT_stock_cycle, MSFT_stock_trend = sm.tsa.filters.hpfilter(MSFT_stock[&#39;Open&#39;]) . MSFT_stock_cycle . Date 2019-01-02 -1.311918 2019-01-03 -1.097494 2019-01-04 -1.812243 2019-01-07 -0.224671 2019-01-08 0.847869 ... 2020-11-02 -8.431408 2020-11-03 -8.845880 2020-11-04 1.239842 2020-11-05 9.198111 2020-11-06 9.350548 Name: Open_cycle, Length: 468, dtype: float64 . MSFT_stock[&#39;trend&#39;] = MSFT_stock_trend . MSFT_stock[&#39;trend&#39;] . Date 2019-01-02 100.861921 2019-01-03 101.197493 2019-01-04 101.532244 2019-01-07 101.864670 2019-01-08 102.192132 ... 2020-11-02 212.721402 2020-11-03 212.735880 2020-11-04 212.780163 2020-11-05 212.841883 2020-11-06 212.909447 Name: trend, Length: 468, dtype: float64 . MSFT_stock[[&quot;trend&quot;, &quot;Open&quot;]].plot(figsize=(16,6)) . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . TSLA_stock[&#39;ds&#39;] = TSLA_stock.index TSLA_stock[&#39;y&#39;] = TSLA_stock.Open m = Prophet() m.fit(TSLA_stock) . INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fabeaebca10&gt; . # Python TSLA_stock[&#39;cap&#39;] = 8.5 m = Prophet(growth=&#39;logistic&#39;) m.fit(TSLA_stock) . INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fabf081d190&gt; . # future = m.make_future_dataframe(periods=1826) # future[&#39;cap&#39;] = 600 # fcst = m.predict(future) # fig = m.plot(fcst) . future = m.make_future_dataframe(periods=365) future.tail() . ds . 828 2021-11-02 | . 829 2021-11-03 | . 830 2021-11-04 | . 831 2021-11-05 | . 832 2021-11-06 | . forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() . ds yhat yhat_lower yhat_upper . 828 2021-11-02 | 1136.958087 | 1029.660624 | 1241.408457 | . 829 2021-11-03 | 1138.386277 | 1029.960322 | 1245.559028 | . 830 2021-11-04 | 1139.054611 | 1027.658532 | 1245.938576 | . 831 2021-11-05 | 1140.980461 | 1031.883548 | 1250.009090 | . 832 2021-11-06 | 1142.201501 | 1037.157758 | 1245.925892 | . fig1 = m.plot(forecast) . fig2 = m.plot_components(forecast) . ZOOM_stock[&#39;ds&#39;] = ZOOM_stock.index ZOOM_stock[&#39;y&#39;] = ZOOM_stock.Open m = Prophet() m.fit(ZOOM_stock) . INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fabf0b64790&gt; . future = m.make_future_dataframe(periods=365) future.tail() . ds . 754 2021-11-02 | . 755 2021-11-03 | . 756 2021-11-04 | . 757 2021-11-05 | . 758 2021-11-06 | . forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() . ds yhat yhat_lower yhat_upper . 754 2021-11-02 | 1498.560150 | 1318.618163 | 1677.021462 | . 755 2021-11-03 | 1500.430463 | 1324.946607 | 1679.133120 | . 756 2021-11-04 | 1502.503460 | 1323.082402 | 1683.102389 | . 757 2021-11-05 | 1505.218307 | 1323.789633 | 1683.789366 | . 758 2021-11-06 | 1499.986193 | 1323.049103 | 1680.953619 | . fig1 = m.plot(forecast) . fig2 = m.plot_components(forecast) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/22/stock-market-returns.html",
            "relUrl": "/2020/10/22/stock-market-returns.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post48": {
            "title": "Using the Quandl API and Pandas Datareader API to call Microsoft, Apple, Zoom, Snowflake stocks and other finance data",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader.data as web import datetime . start = datetime.datetime(2020, 1, 1) end = pd.to_datetime(&#39;today&#39;) . AAPL_stock = web.DataReader(&#39;AAPL&#39;, &#39;yahoo&#39;, start, end) AAPL_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) AAPL_stock[&#39;Open&#39;].plot(label=&#39;Apple&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) AAPL_stock[&#39;Volume&#39;].plot(label=&#39;Apple&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f3e5dc577f0&gt; . import pandas_datareader.data as web import datetime gdp = web.DataReader(&quot;GDP&quot;, &quot;fred&quot;, start, end) . gdp.head() . GDP . DATE . 2020-01-01 21561.139 | . 2020-04-01 19520.114 | . 2020-07-01 21170.252 | . import quandl . #quandl.ApiConfig.api_key = &#39;&#39; . mydata = quandl.get(&quot;EIA/PET_RWTC_D&quot;) mydata.head() . LimitExceededError Traceback (most recent call last) &lt;ipython-input-10-68997e6ce84c&gt; in &lt;module&gt; -&gt; 1 mydata = quandl.get(&#34;EIA/PET_RWTC_D&#34;) 2 3 mydata.head() ~/anaconda3/lib/python3.8/site-packages/quandl/get.py in get(dataset, **kwargs) 46 if dataset_args[&#39;column_index&#39;] is not None: 47 kwargs.update({&#39;column_index&#39;: dataset_args[&#39;column_index&#39;]}) &gt; 48 data = Dataset(dataset_args[&#39;code&#39;]).data(params=kwargs, handle_column_not_found=True) 49 # Array 50 elif isinstance(dataset, list): ~/anaconda3/lib/python3.8/site-packages/quandl/model/dataset.py in data(self, **options) 45 updated_options = Util.merge_options(&#39;params&#39;, params, **options) 46 try: &gt; 47 return Data.all(**updated_options) 48 except NotFoundError: 49 if handle_not_found_error: ~/anaconda3/lib/python3.8/site-packages/quandl/operations/list.py in all(cls, **options) 13 options[&#39;params&#39;] = {} 14 path = Util.constructed_path(cls.list_path(), options[&#39;params&#39;]) &gt; 15 r = Connection.request(&#39;get&#39;, path, **options) 16 response_data = r.json() 17 Util.convert_to_dates(response_data) ~/anaconda3/lib/python3.8/site-packages/quandl/connection.py in request(cls, http_verb, url, **options) 36 abs_url = &#39;%s/%s&#39; % (ApiConfig.api_base, url) 37 &gt; 38 return cls.execute_request(http_verb, abs_url, **options) 39 40 @classmethod ~/anaconda3/lib/python3.8/site-packages/quandl/connection.py in execute_request(cls, http_verb, url, **options) 48 **options) 49 if response.status_code &lt; 200 or response.status_code &gt;= 300: &gt; 50 cls.handle_api_error(response) 51 else: 52 return response ~/anaconda3/lib/python3.8/site-packages/quandl/connection.py in handle_api_error(cls, resp) 112 klass = d_klass.get(code_letter, QuandlError) 113 --&gt; 114 raise klass(message, resp.status_code, resp.text, resp.headers, code) LimitExceededError: (Status 429) (Quandl Error QELx01) You have exceeded the anonymous user limit of 50 calls per day. To make more calls today, please register for a free Quandl account and then include your API key with your requests. . mydata.plot(figsize=(12,6)) . #mydata = quandl.get(&quot;EIA/PET_RWTC_D&quot;, returns=&quot;numpy&quot;,start_date=start,end_date=end) . mydata = quandl.get(&quot;FRED/GDP&quot;,start_date=start,end_date=end) . mydata.head() . mydata = quandl.get([&quot;NSE/OIL.1&quot;, &quot;WIKI/AAPL.4&quot;],start_date=start,end_date=end) . mydata.head() . NSE/OIL - Open WIKI/AAPL - Close . mydata = quandl.get(&quot;FRED/GDP&quot;) . mydata = quandl.get(&#39;WIKI/FB&#39;,start_date=start,end_date=end) . mydata.head() . Open High Low Close Volume Ex-Dividend Split Ratio Adj. Open Adj. High Adj. Low Adj. Close Adj. Volume . Date . mydata = quandl.get(&#39;WIKI/FB.1&#39;,start_date=start,end_date=end) mydata.head() . Open . Date . mydata = quandl.get(&#39;WIKI/FB.7&#39;,start_date=start,end_date=end) mydata.head() . Split Ratio . Date . # Homes . houses = quandl.get(&#39;ZILLOW/M11_ZRIAH&#39;,start_date=start,end_date=end) . houses.head() . Value . Date . 2020-01-31 3342.0 | . 2020-02-29 3358.0 | . houses.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f900d58fef0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/21/quant_data_datareader_quandl.html",
            "relUrl": "/2020/10/21/quant_data_datareader_quandl.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post49": {
            "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
            "content": "The purpose of the T-test is to compare if there are mean differences between two groups of interest. When we are interested in comparing statistical differences between more than two groups, and we conduct multiple t-tests, we will end up increasing the likelihood of a false positive (type I error) where we are incorrectly rejecting the null hypothesis that there are no statistical differences between groups. One way to address this is to use the Bonferroni correction. The Bonferroni correction, the namesake of Carlo Emilio Bonferroni, accounts for what we lose in a p-hacking quest in the experimentation, which is the justification for taking p-values at face value. By intuition, when we go searching for significant differences everywhere, the chance of seeing an apparent significant difference by chance anywhere increases. Using the Bonferroni correction, if the starting alpha/significance level is .05 and we are testing 10 hypotheses, then the corrected alpha/significance level we should use would be .005. Understanding the lack of an incentive to make such an adjustment is straightforward. Another way to address this is to first use ANOVA to detect statistical differences between all groups before deciding whether to use t tests to look for pairwise comparisons between groups. . T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student&#39;s t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd import seaborn as sns import numpy as np import pandas as pd from statsmodels.stats.power import NormalIndPower, TTestIndPower from scipy.stats import ttest_ind_from_stats import numpy as np import scipy . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;]] df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;] . df=df_subset df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . # Add distributions by region import matplotlib.pyplot as plt #fig, axes = plt.subplots(nrows=3, ncols=3) test_cells = [&#39;East China&#39;, &#39;North China&#39;] metrics = [&#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;] for test_cell in test_cells: for metric in metrics: df.loc[df[&quot;region&quot;] == test_cell].hist(column=[metric], bins=60) print(test_cell) print(metric) . East China gdp East China fdi East China it North China gdp North China fdi North China it . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec7cbdd8&gt;]], dtype=object) . Distributions of Dependant Variables . Right skew . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec6e00f0&gt;]], dtype=object) . sns.distplot(df[&#39;gdp&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec5d6a90&gt; . sns.distplot(df[&#39;fdi&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4a4d30&gt; . sns.distplot(df[&#39;it&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4df278&gt; . sns.distplot(df[&#39;specific&#39;].dropna()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec3e09e8&gt; . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec31ccc0&gt;]], dtype=object) . Removal of GDP value outliers more than 3 standard deviations away from the mean . outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean . import scipy.stats as stats . df[&#39;gdp_zscore&#39;] = stats.zscore(df[&#39;gdp&#39;]) . these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped . df[abs(df[&#39;gdp_zscore&#39;])&gt;3].hist(column = [&#39;gdp&#39;]) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec873208&gt;]], dtype=object) . df_no_gdp_outliers=df[abs(df[&#39;gdp_zscore&#39;])&lt;3] . df_no_gdp_outliers . year region province gdp fdi it specific gdp_zscore . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | -0.521466 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | -0.464746 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | -0.421061 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | -0.383239 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | -0.340870 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 354 2002 | East China | Zhejiang | 8003.67 | 307610 | 1962633 | 365437.0 | 0.798274 | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | 1.178172 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | 1.612181 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | 2.007180 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | 2.520929 | . 350 rows × 8 columns . df_no_gdp_outliers.hist(column=[&#39;gdp&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec95e4e0&gt;]], dtype=object) . counts_fiscal=df.groupby(&#39;region&#39;).count() counts_fiscal . year province gdp fdi it specific gdp_zscore . region . East China 84 | 84 | 84 | 84 | 84 | 84 | 84 | . North China 48 | 48 | 48 | 48 | 48 | 47 | 48 | . Northeast China 36 | 36 | 36 | 36 | 36 | 36 | 36 | . Northwest China 60 | 60 | 60 | 60 | 60 | 60 | 60 | . South Central China 72 | 72 | 72 | 72 | 72 | 72 | 72 | . Southwest China 60 | 60 | 60 | 60 | 60 | 57 | 60 | . counts_fiscal=df.groupby(&#39;province&#39;).count() counts_fiscal . year region gdp fdi it specific gdp_zscore . province . Anhui 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Beijing 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Chongqing 12 | 12 | 12 | 12 | 12 | 9 | 12 | . Fujian 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Gansu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangdong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guizhou 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hainan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hebei 12 | 12 | 12 | 12 | 12 | 11 | 12 | . Heilongjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Henan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hubei 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hunan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangsu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jilin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Liaoning 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Ningxia 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Qinghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shaanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shandong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Sichuan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tianjin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tibet 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Xinjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Yunnan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Zhejiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . #df_no_gdp_outliers.pivot_table(index=&#39;grouping column 1&#39;, columns=&#39;grouping column 2&#39;, values=&#39;aggregating column&#39;, aggfunc=&#39;sum&#39;) . #pd.crosstab(df_no_gdp_outliers, &#39;year&#39;) . df_no_gdp_outliers_subset = df_no_gdp_outliers[[&#39;region&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;]] df_no_gdp_outliers_subset . region gdp fdi it . 0 East China | 2093.30 | 50661 | 631930 | . 1 East China | 2347.32 | 43443 | 657860 | . 2 East China | 2542.96 | 27673 | 889463 | . 3 East China | 2712.34 | 26131 | 1227364 | . 4 East China | 2902.09 | 31847 | 1499110 | . ... ... | ... | ... | ... | . 354 East China | 8003.67 | 307610 | 1962633 | . 355 East China | 9705.02 | 498055 | 2261631 | . 356 East China | 11648.70 | 668128 | 3162299 | . 357 East China | 13417.68 | 772000 | 2370200 | . 358 East China | 15718.47 | 888935 | 2553268 | . 350 rows × 4 columns . def aggregate_and_ttest(dataset, groupby_feature=&#39;region&#39;, alpha=.05, test_cells = [0, 1]): #Imports from tqdm import tqdm from scipy.stats import ttest_ind_from_stats metrics = [&#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;] feature_size = &#39;size&#39; feature_mean = &#39;mean&#39; feature_std = &#39;std&#39; for metric in tqdm(metrics): #print(metric) crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg([&#39;size&#39;, &#39;mean&#39;, &#39;std&#39;]) print(crosstab) treatment = crosstab.index[test_cells[0]] control = crosstab.index[test_cells[1]] counts_control = crosstab.loc[control, feature_size] counts_treatment = crosstab.loc[treatment, feature_size] mean_control = crosstab.loc[control, feature_mean] mean_treatment = crosstab.loc[treatment, feature_mean] standard_deviation_control = crosstab.loc[control, feature_std] standard_deviation_treatment = crosstab.loc[treatment, feature_std] t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control) #fstring to print the p value and t statistic print(f&quot;The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} is {t_statistic} and the p value is {p_value}.&quot;) #f string to say of the comparison is significant at a given alpha level if p_value &lt; alpha: print(f&#39;The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}&#39;) else: print(f&#39;The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}&#39;) . aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1]) . 100%|██████████| 3/3 [00:00&lt;00:00, 115.78it/s] . size mean std region East China 78 6070.604231 3500.372702 North China 48 4239.038542 2866.705149 Northeast China 36 3849.076944 1948.531835 Northwest China 60 1340.026167 1174.399739 South Central China 68 4835.540882 3697.129915 Southwest China 60 2410.398833 2144.589994 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 355577.897436 275635.866746 North China 48 169600.583333 127011.475909 Northeast China 36 136623.750000 142734.495232 Northwest China 60 15111.133333 22954.193559 South Central China 68 218931.426471 339981.399823 Southwest China 60 25405.083333 31171.373876 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 1.775615e+06 1.153030e+06 North China 48 1.733719e+06 1.548794e+06 Northeast China 36 2.665148e+06 1.768442e+06 Northwest China 60 1.703538e+06 1.446408e+06 South Central China 68 2.500962e+06 2.196436e+06 Southwest China 60 2.424971e+06 2.002198e+06 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372. The comparison between East China and North China is not statistically significant at the threshold of 0.05 . . from tqdm import tqdm for i in tqdm(range(10000)): ... . 100%|██████████| 10000/10000 [00:00&lt;00:00, 2169617.21it/s] . EastvNorth=pd.DataFrame() EastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1]) EastvNorth . 100%|██████████| 3/3 [00:00&lt;00:00, 135.00it/s] . size mean std region East China 78 6070.604231 3500.372702 North China 48 4239.038542 2866.705149 Northeast China 36 3849.076944 1948.531835 Northwest China 60 1340.026167 1174.399739 South Central China 68 4835.540882 3697.129915 Southwest China 60 2410.398833 2144.589994 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 355577.897436 275635.866746 North China 48 169600.583333 127011.475909 Northeast China 36 136623.750000 142734.495232 Northwest China 60 15111.133333 22954.193559 South Central China 68 218931.426471 339981.399823 Southwest China 60 25405.083333 31171.373876 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05. The comparison between East China and North China is statistically significant at the threshold of 0.05 size mean std region East China 78 1.775615e+06 1.153030e+06 North China 48 1.733719e+06 1.548794e+06 Northeast China 36 2.665148e+06 1.768442e+06 Northwest China 60 1.703538e+06 1.446408e+06 South Central China 68 2.500962e+06 2.196436e+06 Southwest China 60 2.424971e+06 2.002198e+06 The t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372. The comparison between East China and North China is not statistically significant at the threshold of 0.05 . . Genearate an experimental_crosstab to be used in statistical tests . experimental_crosstab = df_no_gdp_outliers_subset.groupby(&#39;region&#39;).agg([&#39;size&#39;, &#39;mean&#39;, &#39;std&#39;]) . experimental_crosstab.index . Index([&#39;East China&#39;, &#39;North China&#39;, &#39;Northeast China&#39;, &#39;Northwest China&#39;, &#39;South Central China&#39;, &#39;Southwest China&#39;], dtype=&#39;object&#39;, name=&#39;region&#39;) . df = experimental_crosstab.T df . region East China North China Northeast China Northwest China South Central China Southwest China . gdp size 7.800000e+01 | 4.800000e+01 | 3.600000e+01 | 6.000000e+01 | 6.800000e+01 | 6.000000e+01 | . mean 6.070604e+03 | 4.239039e+03 | 3.849077e+03 | 1.340026e+03 | 4.835541e+03 | 2.410399e+03 | . std 3.500373e+03 | 2.866705e+03 | 1.948532e+03 | 1.174400e+03 | 3.697130e+03 | 2.144590e+03 | . fdi size 7.800000e+01 | 4.800000e+01 | 3.600000e+01 | 6.000000e+01 | 6.800000e+01 | 6.000000e+01 | . mean 3.555779e+05 | 1.696006e+05 | 1.366238e+05 | 1.511113e+04 | 2.189314e+05 | 2.540508e+04 | . std 2.756359e+05 | 1.270115e+05 | 1.427345e+05 | 2.295419e+04 | 3.399814e+05 | 3.117137e+04 | . it size 7.800000e+01 | 4.800000e+01 | 3.600000e+01 | 6.000000e+01 | 6.800000e+01 | 6.000000e+01 | . mean 1.775615e+06 | 1.733719e+06 | 2.665148e+06 | 1.703538e+06 | 2.500962e+06 | 2.424971e+06 | . std 1.153030e+06 | 1.548794e+06 | 1.768442e+06 | 1.446408e+06 | 2.196436e+06 | 2.002198e+06 | . #experimental_crosstab.reset_index().unstack() . experimental_crosstab.iloc[0,1] . 6070.604230769231 . experimental_crosstab.index . Index([&#39;East China&#39;, &#39;North China&#39;, &#39;Northeast China&#39;, &#39;Northwest China&#39;, &#39;South Central China&#39;, &#39;Southwest China&#39;], dtype=&#39;object&#39;, name=&#39;region&#39;) . experimental_crosstab . gdp fdi it . size mean std size mean std size mean std . region . East China 78 | 6070.604231 | 3500.372702 | 78 | 355577.897436 | 275635.866746 | 78 | 1.775615e+06 | 1.153030e+06 | . North China 48 | 4239.038542 | 2866.705149 | 48 | 169600.583333 | 127011.475909 | 48 | 1.733719e+06 | 1.548794e+06 | . Northeast China 36 | 3849.076944 | 1948.531835 | 36 | 136623.750000 | 142734.495232 | 36 | 2.665148e+06 | 1.768442e+06 | . Northwest China 60 | 1340.026167 | 1174.399739 | 60 | 15111.133333 | 22954.193559 | 60 | 1.703538e+06 | 1.446408e+06 | . South Central China 68 | 4835.540882 | 3697.129915 | 68 | 218931.426471 | 339981.399823 | 68 | 2.500962e+06 | 2.196436e+06 | . Southwest China 60 | 2410.398833 | 2144.589994 | 60 | 25405.083333 | 31171.373876 | 60 | 2.424971e+06 | 2.002198e+06 | . experimental_crosstab.columns = [&#39;_&#39;.join(col) for col in experimental_crosstab.columns.values] . experimental_crosstab . gdp_size gdp_mean gdp_std fdi_size fdi_mean fdi_std it_size it_mean it_std . region . East China 78 | 6070.604231 | 3500.372702 | 78 | 355577.897436 | 275635.866746 | 78 | 1.775615e+06 | 1.153030e+06 | . North China 48 | 4239.038542 | 2866.705149 | 48 | 169600.583333 | 127011.475909 | 48 | 1.733719e+06 | 1.548794e+06 | . Northeast China 36 | 3849.076944 | 1948.531835 | 36 | 136623.750000 | 142734.495232 | 36 | 2.665148e+06 | 1.768442e+06 | . Northwest China 60 | 1340.026167 | 1174.399739 | 60 | 15111.133333 | 22954.193559 | 60 | 1.703538e+06 | 1.446408e+06 | . South Central China 68 | 4835.540882 | 3697.129915 | 68 | 218931.426471 | 339981.399823 | 68 | 2.500962e+06 | 2.196436e+06 | . Southwest China 60 | 2410.398833 | 2144.589994 | 60 | 25405.083333 | 31171.373876 | 60 | 2.424971e+06 | 2.002198e+06 | . experimental_crosstab.loc[&#39;East China&#39;, &#39;gdp_size&#39;] . 78 . experimental_crosstab.to_csv(&#39;fiscal_experimental_crosstab.csv&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/19/ttests-distributions-crosstabs-functions.html",
            "relUrl": "/2020/10/19/ttests-distributions-crosstabs-functions.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post50": {
            "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
            "content": "import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd import seaborn as sns . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;]] df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;] . df=df_subset df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . Distributions of Dependant Variables . Right skew . sns.distplot(df[&#39;gdp&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1043c20588&gt; . sns.distplot(df[&#39;fdi&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f10437f74a8&gt; . sns.distplot(df[&#39;it&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1043a09ef0&gt; . sns.distplot(df[&#39;specific&#39;].dropna()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f10439b7a20&gt; . df.hist(column=[&#39;fdi&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f10439a19e8&gt;]], dtype=object) . Removal of GDP value outliers more than 3 standard deviations away from the mean . outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean . import scipy.stats as stats . df[&#39;gdp_zscore&#39;] = stats.zscore(df[&#39;gdp&#39;]) . these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped . df[abs(df[&#39;gdp_zscore&#39;])&gt;3].hist(column = [&#39;gdp&#39;]) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f104364e0f0&gt;]], dtype=object) . df_no_gdp_outliers=df[abs(df[&#39;gdp_zscore&#39;])&lt;3] . df_no_gdp_outliers . year region province gdp fdi it specific gdp_zscore . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | -0.521466 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | -0.464746 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | -0.421061 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | -0.383239 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | -0.340870 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 354 2002 | East China | Zhejiang | 8003.67 | 307610 | 1962633 | 365437.0 | 0.798274 | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | 1.178172 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | 1.612181 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | 2.007180 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | 2.520929 | . 350 rows × 8 columns . df_no_gdp_outliers.hist(column=[&#39;gdp&#39;], bins=60) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f10429f5ba8&gt;]], dtype=object) . counts_fiscal=df.groupby(&#39;region&#39;).count() counts_fiscal . year province gdp fdi it specific gdp_zscore . region . East China 84 | 84 | 84 | 84 | 84 | 84 | 84 | . North China 48 | 48 | 48 | 48 | 48 | 47 | 48 | . Northeast China 36 | 36 | 36 | 36 | 36 | 36 | 36 | . Northwest China 60 | 60 | 60 | 60 | 60 | 60 | 60 | . South Central China 72 | 72 | 72 | 72 | 72 | 72 | 72 | . Southwest China 60 | 60 | 60 | 60 | 60 | 57 | 60 | . counts_fiscal=df.groupby(&#39;province&#39;).count() counts_fiscal . year region gdp fdi it specific gdp_zscore . province . Anhui 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Beijing 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Chongqing 12 | 12 | 12 | 12 | 12 | 9 | 12 | . Fujian 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Gansu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangdong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Guizhou 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hainan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hebei 12 | 12 | 12 | 12 | 12 | 11 | 12 | . Heilongjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Henan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hubei 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Hunan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangsu 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jiangxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Jilin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Liaoning 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Ningxia 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Qinghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shaanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shandong 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanghai 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Shanxi 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Sichuan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tianjin 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Tibet 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Xinjiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Yunnan 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Zhejiang 12 | 12 | 12 | 12 | 12 | 12 | 12 | . Subset by needed columns . df_no_gdp_outliers.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;, &#39;gdp_zscore&#39;], dtype=&#39;object&#39;) . df_no_gdp_outliers_subset = df_no_gdp_outliers[[&#39;region&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;]] df_no_gdp_outliers_subset . region gdp fdi it . 0 East China | 2093.30 | 50661 | 631930 | . 1 East China | 2347.32 | 43443 | 657860 | . 2 East China | 2542.96 | 27673 | 889463 | . 3 East China | 2712.34 | 26131 | 1227364 | . 4 East China | 2902.09 | 31847 | 1499110 | . ... ... | ... | ... | ... | . 354 East China | 8003.67 | 307610 | 1962633 | . 355 East China | 9705.02 | 498055 | 2261631 | . 356 East China | 11648.70 | 668128 | 3162299 | . 357 East China | 13417.68 | 772000 | 2370200 | . 358 East China | 15718.47 | 888935 | 2553268 | . 350 rows × 4 columns . Genearate an experimental_crosstab to be used in statistical tests . experimental_crosstab = df_no_gdp_outliers_subset.groupby(&#39;region&#39;).agg([&#39;size&#39;, &#39;mean&#39;, &#39;std&#39;]) . experimental_crosstab.index . Index([&#39;East China&#39;, &#39;North China&#39;, &#39;Northeast China&#39;, &#39;Northwest China&#39;, &#39;South Central China&#39;, &#39;Southwest China&#39;], dtype=&#39;object&#39;, name=&#39;region&#39;) . experimental_crosstab = experimental_crosstab.reset_index() . experimental_crosstab . region gdp fdi it . size mean std size mean std size mean std . 0 East China | 78 | 6070.604231 | 3500.372702 | 78 | 355577.897436 | 275635.866746 | 78 | 1.775615e+06 | 1.153030e+06 | . 1 North China | 48 | 4239.038542 | 2866.705149 | 48 | 169600.583333 | 127011.475909 | 48 | 1.733719e+06 | 1.548794e+06 | . 2 Northeast China | 36 | 3849.076944 | 1948.531835 | 36 | 136623.750000 | 142734.495232 | 36 | 2.665148e+06 | 1.768442e+06 | . 3 Northwest China | 60 | 1340.026167 | 1174.399739 | 60 | 15111.133333 | 22954.193559 | 60 | 1.703538e+06 | 1.446408e+06 | . 4 South Central China | 68 | 4835.540882 | 3697.129915 | 68 | 218931.426471 | 339981.399823 | 68 | 2.500962e+06 | 2.196436e+06 | . 5 Southwest China | 60 | 2410.398833 | 2144.589994 | 60 | 25405.083333 | 31171.373876 | 60 | 2.424971e+06 | 2.002198e+06 | . experimental_crosstab.to_csv(&#39;fiscal_experimental_crosstab.csv&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/18/ttests-distributions-crosstabs.html",
            "relUrl": "/2020/10/18/ttests-distributions-crosstabs.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post51": {
            "title": "Using Dask with dask.bag and regex to parse Notes from the Underground from project gutenberg",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import dask.bag as db import re . book_bag = db.from_url(&#39;https://www.gutenberg.org/cache/epub/600/pg600.txt&#39;) . book_bag.take(5) . (b&#34; xef xbb xbfProject Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky r n&#34;, b&#39; r n&#39;, b&#39;This eBook is for the use of anyone anywhere at no cost and with r n&#39;, b&#39;almost no restrictions whatsoever. You may copy it, give it away or r n&#39;, b&#39;re-use it under the terms of the Project Gutenberg License included r n&#39;) . remove_spaces = book_bag.map(lambda x:x.strip()) . remove_spaces.take(10) . (b&#34; xef xbb xbfProject Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky&#34;, b&#39;&#39;, b&#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, b&#39;almost no restrictions whatsoever. You may copy it, give it away or&#39;, b&#39;re-use it under the terms of the Project Gutenberg License included&#39;, b&#39;with this eBook or online at www.gutenberg.net&#39;, b&#39;&#39;, b&#39;&#39;, b&#39;Title: Notes from the Underground&#39;, b&#39;&#39;) . def decode_to_ascii(x): return x.decode(&quot;ascii&quot;,&quot;ignore&quot;) . ascii_text = remove_spaces.map(decode_to_ascii) . ascii_text.take(10) . (&#34;Project Gutenberg&#39;s Notes from the Underground, by Feodor Dostoevsky&#34;, &#39;&#39;, &#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever. You may copy it, give it away or&#39;, &#39;re-use it under the terms of the Project Gutenberg License included&#39;, &#39;with this eBook or online at www.gutenberg.net&#39;, &#39;&#39;, &#39;&#39;, &#39;Title: Notes from the Underground&#39;, &#39;&#39;) . def remove_punctuation(x): return re.sub(r&#39;[^ w s]&#39;,&#39;&#39;,x) . remove_punctuation = ascii_text.map(remove_punctuation) . remove_punctuation.take(10) . (&#39;Project Gutenbergs Notes from the Underground by Feodor Dostoevsky&#39;, &#39;&#39;, &#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever You may copy it give it away or&#39;, &#39;reuse it under the terms of the Project Gutenberg License included&#39;, &#39;with this eBook or online at wwwgutenbergnet&#39;, &#39;&#39;, &#39;&#39;, &#39;Title Notes from the Underground&#39;, &#39;&#39;) . lower_text = remove_punctuation.map(str.lower) . lower_text.take(10) . (&#39;project gutenbergs notes from the underground by feodor dostoevsky&#39;, &#39;&#39;, &#39;this ebook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever you may copy it give it away or&#39;, &#39;reuse it under the terms of the project gutenberg license included&#39;, &#39;with this ebook or online at wwwgutenbergnet&#39;, &#39;&#39;, &#39;&#39;, &#39;title notes from the underground&#39;, &#39;&#39;) . split_word_list = lower_text.map(lambda x: x.split(&#39; &#39;)) . split_word_list.take(10) . ([&#39;project&#39;, &#39;gutenbergs&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;, &#39;by&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;], [&#39;&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;], [&#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;], [&#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;, &#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;], [&#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;, &#39;wwwgutenbergnet&#39;], [&#39;&#39;], [&#39;&#39;], [&#39;title&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;], [&#39;&#39;]) . def remove_empty_words(word_list): return list(filter(lambda a: a != &#39;&#39;, word_list)) non_empty_words = split_word_list.filter(remove_empty_words) . non_empty_words.take(10) . ([&#39;project&#39;, &#39;gutenbergs&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;, &#39;by&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;], [&#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;], [&#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;, &#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;], [&#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;, &#39;wwwgutenbergnet&#39;], [&#39;title&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;], [&#39;author&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;], [&#39;posting&#39;, &#39;date&#39;, &#39;september&#39;, &#39;13&#39;, &#39;2008&#39;, &#39;ebook&#39;, &#39;600&#39;], [&#39;release&#39;, &#39;date&#39;, &#39;july&#39;, &#39;1996&#39;], [&#39;language&#39;, &#39;english&#39;]) . all_words = non_empty_words.flatten() . type(all_words) . dask.bag.core.Bag . all_words.take(30) . (&#39;project&#39;, &#39;gutenbergs&#39;, &#39;notes&#39;, &#39;from&#39;, &#39;the&#39;, &#39;underground&#39;, &#39;by&#39;, &#39;feodor&#39;, &#39;dostoevsky&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;&#39;, &#39;you&#39;, &#39;may&#39;) . change_to_key_value = all_words.map(lambda x: (x, 1)) . change_to_key_value.take(4) . ((&#39;project&#39;, 1), (&#39;gutenbergs&#39;, 1), (&#39;notes&#39;, 1), (&#39;from&#39;, 1)) . grouped_words = all_words.groupby(lambda x:x) . grouped_words.take(1) . ((&#39;project&#39;, [&#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;, &#39;project&#39;]),) . word_count = grouped_words.map(lambda x: (x[0], len(x[1]))) . word_count.take(10) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;from&#39;, 186), (&#39;the&#39;, 1555), (&#39;underground&#39;, 26), (&#39;by&#39;, 153), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;this&#39;, 237)) . change_to_key_value.take(10) . ((&#39;project&#39;, 1), (&#39;gutenbergs&#39;, 1), (&#39;notes&#39;, 1), (&#39;from&#39;, 1), (&#39;the&#39;, 1), (&#39;underground&#39;, 1), (&#39;by&#39;, 1), (&#39;feodor&#39;, 1), (&#39;dostoevsky&#39;, 1), (&#39;this&#39;, 1)) . # Take a running count of a word # In this case, the default value of # count needs to be provided def add_bin_op(count, x): return count + x[1] # Take the output from multiple bin_op(s) # and add them to get the total count of # a word def add_combine_op(x, y): return x + y word_count = change_to_key_value.foldby(lambda x: x[0], add_bin_op, 0, add_combine_op) . word_count.take(10) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;from&#39;, 186), (&#39;the&#39;, 1555), (&#39;underground&#39;, 26), (&#39;by&#39;, 153), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;this&#39;, 237)) . much_easier = all_words.frequencies() . much_easier.take(10) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;from&#39;, 186), (&#39;the&#39;, 1555), (&#39;underground&#39;, 26), (&#39;by&#39;, 153), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;this&#39;, 237)) . Removing stop words in top word frequency counts . from spacy.lang.en import STOP_WORDS . without_stopwords = all_words.filter(lambda x: x not in STOP_WORDS) . new_freq = without_stopwords.frequencies() . new_freq.take(20) . ((&#39;project&#39;, 87), (&#39;gutenbergs&#39;, 2), (&#39;notes&#39;, 11), (&#39;underground&#39;, 26), (&#39;feodor&#39;, 3), (&#39;dostoevsky&#39;, 3), (&#39;ebook&#39;, 9), (&#39;use&#39;, 18), (&#39;cost&#39;, 5), (&#39;restrictions&#39;, 3), (&#39;whatsoever&#39;, 2), (&#39;&#39;, 1896), (&#39;copy&#39;, 12), (&#39;away&#39;, 59), (&#39;reuse&#39;, 2), (&#39;terms&#39;, 24), (&#39;gutenberg&#39;, 28), (&#39;license&#39;, 15), (&#39;included&#39;, 6), (&#39;online&#39;, 4)) . new_freq.topk(10) . dask.bag&lt;topk-aggregate, npartitions=1&gt; . new_freq.topk(10, key=lambda x: x[1]).compute() . [(&#39;&#39;, 1896), (&#39;man&#39;, 122), (&#39;know&#39;, 90), (&#39;project&#39;, 87), (&#39;time&#39;, 83), (&#39;like&#39;, 82), (&#39;come&#39;, 74), (&#39;course&#39;, 73), (&#39;love&#39;, 72), (&#39;life&#39;, 69)] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/17/dask-NLP-gutenberg-books.html",
            "relUrl": "/2020/10/17/dask-NLP-gutenberg-books.html",
            "date": " • Oct 17, 2020"
        }
        
    
  
    
        ,"post52": {
            "title": "Friday links",
            "content": "Spacy V3 . | why-congress-should-invest-in-open-source-software . | National Research Cloud AI/ML . | Favicons . | Babies’ random choices become their preferences: “We assume we choose things that we like, but research suggests that’s sometimes backward: We like things because we choose them, and we dislike things that we don’t choose.” . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/16/links.html",
            "relUrl": "/links/2020/10/16/links.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "Using Dask with dask.bag and regex to parse The Brothers Karamazov from project gutenberg",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import dask.bag as db import re . book_bag = db.from_url(&#39;https://www.gutenberg.org/files/28054/28054-0.txt&#39;) . book_bag.take(5) . (b&#39; xef xbb xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor r n&#39;, b&#39;Dostoyevsky r n&#39;, b&#39; r n&#39;, b&#39; r n&#39;, b&#39; r n&#39;) . remove_spaces = book_bag.map(lambda x:x.strip()) . remove_spaces.take(10) . (b&#39; xef xbb xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor&#39;, b&#39;Dostoyevsky&#39;, b&#39;&#39;, b&#39;&#39;, b&#39;&#39;, b&#39;This ebook is for the use of anyone anywhere in the United States and most&#39;, b&#39;other parts of the world at no cost and with almost no restrictions&#39;, b&#39;whatsoever. You may copy it, give it away or re xe2 x80 x90use it under the terms of&#39;, b&#39;the Project Gutenberg License included with this eBook or online at&#39;, b&#39;http://www.gutenberg.org/license. If you are not located in the United&#39;) . def decode_to_ascii(x): return x.decode(&quot;ascii&quot;,&quot;ignore&quot;) . ascii_text = remove_spaces.map(decode_to_ascii) . ascii_text.take(10) . (&#39;The Project Gutenberg EBook of The Brothers Karamazov by Fyodor&#39;, &#39;Dostoyevsky&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;This ebook is for the use of anyone anywhere in the United States and most&#39;, &#39;other parts of the world at no cost and with almost no restrictions&#39;, &#39;whatsoever. You may copy it, give it away or reuse it under the terms of&#39;, &#39;the Project Gutenberg License included with this eBook or online at&#39;, &#39;http://www.gutenberg.org/license. If you are not located in the United&#39;) . def remove_punctuation(x): return re.sub(r&#39;[^ w s]&#39;,&#39;&#39;,x) . remove_punctuation = ascii_text.map(remove_punctuation) . remove_punctuation.take(10) . (&#39;The Project Gutenberg EBook of The Brothers Karamazov by Fyodor&#39;, &#39;Dostoyevsky&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;This ebook is for the use of anyone anywhere in the United States and most&#39;, &#39;other parts of the world at no cost and with almost no restrictions&#39;, &#39;whatsoever You may copy it give it away or reuse it under the terms of&#39;, &#39;the Project Gutenberg License included with this eBook or online at&#39;, &#39;httpwwwgutenbergorglicense If you are not located in the United&#39;) . lower_text = remove_punctuation.map(str.lower) . lower_text.take(10) . (&#39;the project gutenberg ebook of the brothers karamazov by fyodor&#39;, &#39;dostoyevsky&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;this ebook is for the use of anyone anywhere in the united states and most&#39;, &#39;other parts of the world at no cost and with almost no restrictions&#39;, &#39;whatsoever you may copy it give it away or reuse it under the terms of&#39;, &#39;the project gutenberg license included with this ebook or online at&#39;, &#39;httpwwwgutenbergorglicense if you are not located in the united&#39;) . split_word_list = lower_text.map(lambda x: x.split(&#39; &#39;)) . split_word_list.take(10) . ([&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;, &#39;by&#39;, &#39;fyodor&#39;], [&#39;dostoyevsky&#39;], [&#39;&#39;], [&#39;&#39;], [&#39;&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;, &#39;states&#39;, &#39;and&#39;, &#39;most&#39;], [&#39;other&#39;, &#39;parts&#39;, &#39;of&#39;, &#39;the&#39;, &#39;world&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;], [&#39;whatsoever&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;, &#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;], [&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;, &#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;], [&#39;httpwwwgutenbergorglicense&#39;, &#39;if&#39;, &#39;you&#39;, &#39;are&#39;, &#39;not&#39;, &#39;located&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;]) . def remove_empty_words(word_list): return list(filter(lambda a: a != &#39;&#39;, word_list)) non_empty_words = split_word_list.filter(remove_empty_words) . non_empty_words.take(10) . ([&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;, &#39;by&#39;, &#39;fyodor&#39;], [&#39;dostoyevsky&#39;], [&#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;, &#39;states&#39;, &#39;and&#39;, &#39;most&#39;], [&#39;other&#39;, &#39;parts&#39;, &#39;of&#39;, &#39;the&#39;, &#39;world&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;], [&#39;whatsoever&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;, &#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;], [&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;, &#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;], [&#39;httpwwwgutenbergorglicense&#39;, &#39;if&#39;, &#39;you&#39;, &#39;are&#39;, &#39;not&#39;, &#39;located&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;], [&#39;states&#39;, &#39;youll&#39;, &#39;have&#39;, &#39;to&#39;, &#39;check&#39;, &#39;the&#39;, &#39;laws&#39;, &#39;of&#39;, &#39;the&#39;, &#39;country&#39;, &#39;where&#39;, &#39;you&#39;, &#39;are&#39;, &#39;located&#39;], [&#39;before&#39;, &#39;using&#39;, &#39;this&#39;, &#39;ebook&#39;], [&#39;title&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;]) . all_words = non_empty_words.flatten() . type(all_words) . dask.bag.core.Bag . all_words.take(30) . (&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;brothers&#39;, &#39;karamazov&#39;, &#39;by&#39;, &#39;fyodor&#39;, &#39;dostoyevsky&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;in&#39;, &#39;the&#39;, &#39;united&#39;, &#39;states&#39;, &#39;and&#39;, &#39;most&#39;, &#39;other&#39;, &#39;parts&#39;, &#39;of&#39;, &#39;the&#39;) . change_to_key_value = all_words.map(lambda x: (x, 1)) . change_to_key_value.take(4) . ((&#39;the&#39;, 1), (&#39;project&#39;, 1), (&#39;gutenberg&#39;, 1), (&#39;ebook&#39;, 1)) . grouped_words = all_words.groupby(lambda x:x) . grouped_words.take(1) . ((&#39;the&#39;, [&#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, &#39;the&#39;, ...]),) . word_count = grouped_words.map(lambda x: (x[0], len(x[1]))) . word_count.take(10) . ((&#39;the&#39;, 15379), (&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;of&#39;, 7410), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;by&#39;, 1165), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3)) . change_to_key_value.take(10) . ((&#39;the&#39;, 1), (&#39;project&#39;, 1), (&#39;gutenberg&#39;, 1), (&#39;ebook&#39;, 1), (&#39;of&#39;, 1), (&#39;the&#39;, 1), (&#39;brothers&#39;, 1), (&#39;karamazov&#39;, 1), (&#39;by&#39;, 1), (&#39;fyodor&#39;, 1)) . # Take a running count of a word # In this case, the default value of # count needs to be provided def add_bin_op(count, x): return count + x[1] # Take the output from multiple bin_op(s) # and add them to get the total count of # a word def add_combine_op(x, y): return x + y word_count = change_to_key_value.foldby(lambda x: x[0], add_bin_op, 0, add_combine_op) . word_count.take(10) . ((&#39;the&#39;, 15379), (&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;of&#39;, 7410), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;by&#39;, 1165), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3)) . much_easier = all_words.frequencies() . much_easier.take(10) . ((&#39;the&#39;, 15379), (&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;of&#39;, 7410), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;by&#39;, 1165), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3)) . Removing stop words in top word frequency counts . from spacy.lang.en import STOP_WORDS . without_stopwords = all_words.filter(lambda x: x not in STOP_WORDS) . new_freq = without_stopwords.frequencies() . new_freq.take(20) . ((&#39;project&#39;, 89), (&#39;gutenberg&#39;, 88), (&#39;ebook&#39;, 14), (&#39;brothers&#39;, 82), (&#39;karamazov&#39;, 170), (&#39;fyodor&#39;, 303), (&#39;dostoyevsky&#39;, 3), (&#39;use&#39;, 77), (&#39;united&#39;, 24), (&#39;states&#39;, 21), (&#39;parts&#39;, 19), (&#39;world&#39;, 182), (&#39;cost&#39;, 12), (&#39;restrictions&#39;, 2), (&#39;whatsoever&#39;, 5), (&#39;copy&#39;, 16), (&#39;away&#39;, 445), (&#39;reuse&#39;, 2), (&#39;terms&#39;, 33), (&#39;license&#39;, 14)) . new_freq.topk(10) . dask.bag&lt;topk-aggregate, npartitions=1&gt; . new_freq.topk(10, key=lambda x: x[1]).compute() . [(&#39;alyosha&#39;, 1176), (&#39;said&#39;, 993), (&#39;know&#39;, 843), (&#39;man&#39;, 842), (&#39;mitya&#39;, 814), (&#39;dont&#39;, 784), (&#39;come&#39;, 772), (&#39;father&#39;, 721), (&#39;ivan&#39;, 677), (&#39;time&#39;, 669)] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/16/dask-NLP-gutenberg-books.html",
            "relUrl": "/2020/10/16/dask-NLP-gutenberg-books.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post54": {
            "title": "Linear Regression using Dask Data Frames",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&quot;sqlite:///fiscal.db&quot;) connection = engine.connect() metadata = db.MetaData() . #engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . client.restart() . Client . Scheduler: inproc://192.168.1.71/9672/30 | Dashboard: http://192.168.1.71:46451/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object float64 int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . client.id . &#39;Client-4c6edbde-0e23-11eb-a5c8-4b14c8b4f4db&#39; . Selecting Features and Target . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . #OHE from dask_ml.preprocessing import OneHotEncoder . ddf = ddf.categorize(cat_feat_list) . ohe = OneHotEncoder(sparse=False) . ohe_ddf = ohe.fit_transform(ddf[cat_feat_list]) . feat_list = feat_list + ohe_ddf.columns.tolist() feat_list = [f for f in feat_list if f not in cat_feat_list] . ddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target]) . ddf_processed.compute() . year fdi region_East China region_North China region_Southwest China region_Northwest China region_South Central China region_Northeast China province_Anhui province_Beijing ... province_Shandong province_Shanghai province_Shanxi province_Sichuan province_Tianjin province_Tibet province_Xinjiang province_Yunnan province_Zhejiang gdp . 0 1996 | 50661.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2093.30 | . 1 1997 | 43443.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2347.32 | . 2 1998 | 27673.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2542.96 | . 3 1999 | 26131.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2712.34 | . 4 2000 | 31847.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2902.09 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 2003 | 498055.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 9705.02 | . 356 2004 | 668128.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 11648.70 | . 357 2005 | 772000.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 13417.68 | . 358 2006 | 888935.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 15718.47 | . 359 2007 | 1036576.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 18753.73 | . 360 rows × 75 columns . feat_list . [&#39;year&#39;, &#39;fdi&#39;, &#39;region_East China&#39;, &#39;region_North China&#39;, &#39;region_Southwest China&#39;, &#39;region_Northwest China&#39;, &#39;region_South Central China&#39;, &#39;region_Northeast China&#39;, &#39;province_Anhui&#39;, &#39;province_Beijing&#39;, &#39;province_Chongqing&#39;, &#39;province_Fujian&#39;, &#39;province_Gansu&#39;, &#39;province_Guangdong&#39;, &#39;province_Guangxi&#39;, &#39;province_Guizhou&#39;, &#39;province_Hainan&#39;, &#39;province_Hebei&#39;, &#39;province_Heilongjiang&#39;, &#39;province_Henan&#39;, &#39;province_Hubei&#39;, &#39;province_Hunan&#39;, &#39;province_Jiangsu&#39;, &#39;province_Jiangxi&#39;, &#39;province_Jilin&#39;, &#39;province_Liaoning&#39;, &#39;province_Ningxia&#39;, &#39;province_Qinghai&#39;, &#39;province_Shaanxi&#39;, &#39;province_Shandong&#39;, &#39;province_Shanghai&#39;, &#39;province_Shanxi&#39;, &#39;province_Sichuan&#39;, &#39;province_Tianjin&#39;, &#39;province_Tibet&#39;, &#39;province_Xinjiang&#39;, &#39;province_Yunnan&#39;, &#39;province_Zhejiang&#39;, &#39;region_East China&#39;, &#39;region_North China&#39;, &#39;region_Southwest China&#39;, &#39;region_Northwest China&#39;, &#39;region_South Central China&#39;, &#39;region_Northeast China&#39;, &#39;province_Anhui&#39;, &#39;province_Beijing&#39;, &#39;province_Chongqing&#39;, &#39;province_Fujian&#39;, &#39;province_Gansu&#39;, &#39;province_Guangdong&#39;, &#39;province_Guangxi&#39;, &#39;province_Guizhou&#39;, &#39;province_Hainan&#39;, &#39;province_Hebei&#39;, &#39;province_Heilongjiang&#39;, &#39;province_Henan&#39;, &#39;province_Hubei&#39;, &#39;province_Hunan&#39;, &#39;province_Jiangsu&#39;, &#39;province_Jiangxi&#39;, &#39;province_Jilin&#39;, &#39;province_Liaoning&#39;, &#39;province_Ningxia&#39;, &#39;province_Qinghai&#39;, &#39;province_Shaanxi&#39;, &#39;province_Shandong&#39;, &#39;province_Shanghai&#39;, &#39;province_Shanxi&#39;, &#39;province_Sichuan&#39;, &#39;province_Tianjin&#39;, &#39;province_Tibet&#39;, &#39;province_Xinjiang&#39;, &#39;province_Yunnan&#39;, &#39;province_Zhejiang&#39;] . Dask Linear Regression . X=ddf_processed[feat_list].persist() y=ddf_processed[target].persist() . from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge . X . Dask DataFrame Structure: year fdi region_East China region_East China region_North China region_North China region_Southwest China region_Southwest China region_Northwest China region_Northwest China region_South Central China region_South Central China region_Northeast China region_Northeast China province_Anhui province_Anhui province_Beijing province_Beijing province_Chongqing province_Chongqing province_Fujian province_Fujian province_Gansu province_Gansu province_Guangdong province_Guangdong province_Guangxi province_Guangxi province_Guizhou province_Guizhou province_Hainan province_Hainan province_Hebei province_Hebei province_Heilongjiang province_Heilongjiang province_Henan province_Henan province_Hubei province_Hubei province_Hunan province_Hunan province_Jiangsu province_Jiangsu province_Jiangxi province_Jiangxi province_Jilin province_Jilin province_Liaoning province_Liaoning province_Ningxia province_Ningxia province_Qinghai province_Qinghai province_Shaanxi province_Shaanxi province_Shandong province_Shandong province_Shanghai province_Shanghai province_Shanxi province_Shanxi province_Sichuan province_Sichuan province_Tianjin province_Tianjin province_Tibet province_Tibet province_Xinjiang province_Xinjiang province_Yunnan province_Yunnan province_Zhejiang province_Zhejiang region_East China region_East China region_North China region_North China region_Southwest China region_Southwest China region_Northwest China region_Northwest China region_South Central China region_South Central China region_Northeast China region_Northeast China province_Anhui province_Anhui province_Beijing province_Beijing province_Chongqing province_Chongqing province_Fujian province_Fujian province_Gansu province_Gansu province_Guangdong province_Guangdong province_Guangxi province_Guangxi province_Guizhou province_Guizhou province_Hainan province_Hainan province_Hebei province_Hebei province_Heilongjiang province_Heilongjiang province_Henan province_Henan province_Hubei province_Hubei province_Hunan province_Hunan province_Jiangsu province_Jiangsu province_Jiangxi province_Jiangxi province_Jilin province_Jilin province_Liaoning province_Liaoning province_Ningxia province_Ningxia province_Qinghai province_Qinghai province_Shaanxi province_Shaanxi province_Shandong province_Shandong province_Shanghai province_Shanghai province_Shanxi province_Shanxi province_Sichuan province_Sichuan province_Tianjin province_Tianjin province_Tibet province_Tibet province_Xinjiang province_Xinjiang province_Yunnan province_Yunnan province_Zhejiang province_Zhejiang . npartitions=5 . 0 int64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | float64 | . 72 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: getitem, 5 tasks X.compute() . year fdi region_East China region_East China region_North China region_North China region_Southwest China region_Southwest China region_Northwest China region_Northwest China ... province_Tianjin province_Tianjin province_Tibet province_Tibet province_Xinjiang province_Xinjiang province_Yunnan province_Yunnan province_Zhejiang province_Zhejiang . 0 1996 | 50661.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 1997 | 43443.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1998 | 27673.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1999 | 26131.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 2000 | 31847.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 2003 | 498055.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 356 2004 | 668128.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 357 2005 | 772000.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 358 2006 | 888935.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 359 2007 | 1036576.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 360 rows × 146 columns . y . Dask DataFrame Structure: gdp . npartitions=5 . 0 float64 | . 72 ... | . ... ... | . 288 ... | . 359 ... | . Dask Name: getitem, 5 tasks LinReg = LinearRegression() . LinReg.fit(X, y) . LinearRegression() . RidgeReg = Ridge() RidgeReg.fit(x, y) . Ridge() . LinReg.predict(x)[:5] . array([[1830.87851079], [2076.99855135], [2220.28956053], [2534.65768132], [2936.29581027]]) . RidgeReg.predict(x)[:5] . array([[1804.41754025], [2053.19939587], [2200.05297844], [2516.48507702], [2919.42271884]]) . client.restart() . Client . Scheduler: inproc://192.168.1.71/9672/30 | Dashboard: http://192.168.1.71:46451/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/15/dask-xgboost-Linear-Regresion-Copy1.html",
            "relUrl": "/2020/10/15/dask-xgboost-Linear-Regresion-Copy1.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "Using dask_ml.preprocessing and OneHotEncoder for categorical encoding with Dask",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&quot;sqlite:///fiscal.db&quot;) connection = engine.connect() metadata = db.MetaData() . #engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/9390/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.restart() . Client . Scheduler: inproc://192.168.1.71/9390/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object float64 int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . client.id . &#39;Client-0ac0cc94-0e22-11eb-a4ae-d71460f30774&#39; . # Selecting Features and Target . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . #OHE from dask_ml.preprocessing import OneHotEncoder . ddf = ddf.categorize(cat_feat_list) . ohe = OneHotEncoder(sparse=False) . ohe_ddf = ohe.fit_transform(ddf[cat_feat_list]) . feat_list = feat_list + ohe_ddf.columns.tolist() feat_list = [f for f in feat_list if f not in cat_feat_list] #client.close() . ddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target]) . ddf_processed.compute() . year fdi region_East China region_North China region_Southwest China region_Northwest China region_South Central China region_Northeast China province_Anhui province_Beijing ... province_Shandong province_Shanghai province_Shanxi province_Sichuan province_Tianjin province_Tibet province_Xinjiang province_Yunnan province_Zhejiang gdp . 0 1996 | 50661.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2093.30 | . 1 1997 | 43443.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2347.32 | . 2 1998 | 27673.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2542.96 | . 3 1999 | 26131.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2712.34 | . 4 2000 | 31847.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2902.09 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 2003 | 498055.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 9705.02 | . 356 2004 | 668128.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 11648.70 | . 357 2005 | 772000.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 13417.68 | . 358 2006 | 888935.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 15718.47 | . 359 2007 | 1036576.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 18753.73 | . 360 rows × 39 columns . client.restart() . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/14/dask-xgboost-HT-OHE.html",
            "relUrl": "/2020/10/14/dask-xgboost-HT-OHE.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post56": {
            "title": "Moving Dask XGboost with Fiscal Data, saving and loading Dask XGboost models",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal.db&#39;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_table LIMIT 10&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, 631930, 147002, 2093.3, 50661), (1997, &#39;East China&#39;, &#39;Anhui&#39;, 657860, 151981, 2347.32, 43443), (1998, &#39;East China&#39;, &#39;Anhui&#39;, 889463, 174930, 2542.96, 27673), (1999, &#39;East China&#39;, &#39;Anhui&#39;, 1227364, 285324, 2712.34, 26131), (2000, &#39;East China&#39;, &#39;Anhui&#39;, 1499110, 195580, 2902.09, 31847), (2001, &#39;East China&#39;, &#39;Anhui&#39;, 2165189, 250898, 3246.71, 33672), (2002, &#39;East China&#39;, &#39;Anhui&#39;, 2404936, 434149, 3519.72, 38375), (2003, &#39;East China&#39;, &#39;Anhui&#39;, 2815820, 619201, 3923.11, 36720), (2004, &#39;East China&#39;, &#39;Anhui&#39;, 3422176, 898441, 4759.3, 54669), (2005, &#39;East China&#39;, &#39;Anhui&#39;, 3874846, 898441, 5350.17, 69000)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . df.gdp.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2677958128&gt; . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/13442/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.restart() . Client . Scheduler: inproc://192.168.1.71/13442/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object float64 int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . client.id . &#39;Client-e79fe0fe-0d59-11eb-b482-f9dc9eaa58ee&#39; . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) # ddf[&quot;province&quot;] = ddf[&quot;province&quot;].astype(float) # ddf[&quot;region&quot;] = ddf[&quot;region&quot;].astype(float) . x=ddf[feat_list].persist() y=ddf[target].persist() . x . Dask DataFrame Structure: year fdi . npartitions=5 . 0 int64 | float64 | . 72 ... | ... | . ... ... | ... | . 288 ... | ... | . 359 ... | ... | . Dask Name: getitem, 5 tasks y.compute() . gdp . 0 2093.30 | . 1 2347.32 | . 2 2542.96 | . 3 2712.34 | . 4 2902.09 | . ... ... | . 355 9705.02 | . 356 11648.70 | . 357 13417.68 | . 358 15718.47 | . 359 18753.73 | . 360 rows × 1 columns . print(x.shape,y.shape) . (Delayed(&#39;int-97d0cf00-db85-425b-a0d2-08297142db86&#39;), 2) (Delayed(&#39;int-01cdae78-a995-48c1-9b93-277a008ad57a&#39;), 1) . x.count().compute() . year 360 fdi 360 dtype: int64 . from dask_ml.xgboost import XGBRegressor . XGBR = XGBRegressor() . %%time XGBR_model = XGBR.fit(x,y) . CPU times: user 54.2 s, sys: 1.02 s, total: 55.2 s Wall time: 18.4 s . XGBR_model . XGBRegressor() . XGBR_model.save_model(&#39;fiscal_model&#39;) . XGBR_model.load_model(&#39;fiscal_model&#39;) . [08:43:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/13/dask-xgboost-fiscal-data.html",
            "relUrl": "/2020/10/13/dask-xgboost-fiscal-data.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post57": {
            "title": "Monday links",
            "content": "Nobel Prize in Economics 2020, Auction Theory . | The Promise of Prediction Markets . | ENTRY DETERRENCE by Milgrom and John Roberts (1981) . | Auction Theory by Milgrom . | How Computer Science Informs Modern Auction Design . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/12/links.html",
            "relUrl": "/links/2020/10/12/links.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post58": {
            "title": "Visualizing Operations with Dask Dataframes on Fiscal Data",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&quot;sqlite:///fiscal_data.db&quot;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;, 50661, 631930, 147002)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.7 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . df.gdp.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0272396c50&gt; . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/5995/1 | Dashboard: http://192.168.1.71:8787/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object object int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.describe().visualize(filename=&#39;describe.png&#39;) . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . max_gdp_per_region = ddf.groupby(&#39;region&#39;)[&#39;gdp&#39;].max() . max_gdp_per_region.visualize() . max_gdp_per_region.compute() . region East China 9705.02 North China 9846.81 Northwest China 956.32 South Central China 9439.6 Southwest China 937.5 Northeast China 9304.52 Name: gdp, dtype: object . ddf . Dask DataFrame Structure: year region province gdp fdi it specific . npartitions=5 . 0 int64 | object | object | object | int64 | int64 | float64 | . 72 ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | . Dask Name: from_pandas, 5 tasks ddf.npartitions . 5 . ddf.npartitions . 5 . len(ddf) . 360 . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&quot;4GB&quot;) client . /home/gao/anaconda3/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use. Perhaps you already have a cluster running? Hosting the HTTP server on port 39701 instead http_address[&#34;port&#34;], self.http_server.port . Client . Scheduler: inproc://192.168.1.71/5995/20 | Dashboard: http://192.168.1.71:39701/status | . | Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | . client.id . &#39;Client-9f9a71c2-0c90-11eb-976b-cff3b7a8059e&#39; . ddf.describe().compute() . year fdi it specific . count 360.000000 | 3.600000e+02 | 3.600000e+02 | 3.560000e+02 | . mean 2001.500000 | 1.961394e+05 | 2.165819e+06 | 5.834707e+05 | . std 3.456857 | 3.030440e+05 | 1.769294e+06 | 6.540553e+05 | . min 1996.000000 | 2.000000e+00 | 1.478970e+05 | 8.964000e+03 | . 25% 1998.750000 | 3.309900e+04 | 1.077466e+06 | 2.237530e+05 | . 50% 2001.500000 | 1.411025e+05 | 2.020634e+06 | 4.243700e+05 | . 75% 2004.250000 | 4.065125e+05 | 3.375492e+06 | 1.011846e+06 | . max 2007.000000 | 1.743140e+06 | 1.053331e+07 | 3.937966e+06 | . ddf.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(int) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) # ddf[&quot;province&quot;] = ddf[&quot;province&quot;].astype(float) # ddf[&quot;region&quot;] = ddf[&quot;region&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . ddf . Dask DataFrame Structure: year region province gdp fdi it specific . npartitions=5 . 0 int64 | object | object | float64 | float64 | float64 | float64 | . 72 ... | ... | ... | ... | ... | ... | ... | . ... ... | ... | ... | ... | ... | ... | ... | . 288 ... | ... | ... | ... | ... | ... | ... | . 359 ... | ... | ... | ... | ... | ... | ... | . Dask Name: assign, 65 tasks ddf.nlargest(20, &#39;gdp&#39;).compute() . year region province gdp fdi it specific . 71 2007 | South Central China | Guangdong | 31777.01 | 1712603.0 | 4947824.0 | 859482.0 | . 70 2006 | South Central China | Guangdong | 26587.76 | 1451065.0 | 4559252.0 | 1897575.0 | . 263 2007 | East China | Shandong | 25776.91 | 1101159.0 | 6357869.0 | 2121243.0 | . 69 2005 | South Central China | Guangdong | 22557.37 | 1236400.0 | 4327217.0 | 1491588.0 | . 262 2006 | East China | Shandong | 21900.19 | 1000069.0 | 5304833.0 | 1204547.0 | . 179 2007 | East China | Jiangsu | 21742.05 | 1743140.0 | 3557071.0 | 1188989.0 | . 68 2004 | South Central China | Guangdong | 18864.62 | 1001158.0 | 5193902.0 | 1491588.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576.0 | 2939778.0 | 844647.0 | . 178 2006 | East China | Jiangsu | 18598.69 | 1318339.0 | 2926542.0 | 1388043.0 | . 261 2005 | East China | Shandong | 18366.87 | 897000.0 | 4142859.0 | 1011203.0 | . 67 2003 | South Central China | Guangdong | 15844.64 | 782294.0 | 4073606.0 | 1550764.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935.0 | 2553268.0 | 1017303.0 | . 260 2004 | East China | Shandong | 15021.84 | 870064.0 | 3732990.0 | 1011203.0 | . 143 2007 | South Central China | Henan | 15012.46 | 306162.0 | 10533312.0 | 3860764.0 | . 177 2005 | East China | Jiangsu | 15003.60 | 1213800.0 | 3479548.0 | 1483371.0 | . 119 2007 | North China | Hebei | 13607.32 | 241621.0 | 7537692.0 | 2981235.0 | . 66 2002 | South Central China | Guangdong | 13502.42 | 1133400.0 | 3545004.0 | 1235386.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000.0 | 2370200.0 | 656175.0 | . 275 2007 | East China | Shanghai | 12494.01 | 792000.0 | 2386339.0 | 272744.0 | . 176 2004 | East China | Jiangsu | 12442.87 | 1056365.0 | 2410257.0 | 1483371.0 | . without_ec = ddf[ddf.region !=&#39;East China&#39;] . without_ec.nlargest(20, &#39;gdp&#39;).compute() . year region province gdp fdi it specific . 71 2007 | South Central China | Guangdong | 31777.01 | 1712603.0 | 4947824.0 | 859482.0 | . 70 2006 | South Central China | Guangdong | 26587.76 | 1451065.0 | 4559252.0 | 1897575.0 | . 69 2005 | South Central China | Guangdong | 22557.37 | 1236400.0 | 4327217.0 | 1491588.0 | . 68 2004 | South Central China | Guangdong | 18864.62 | 1001158.0 | 5193902.0 | 1491588.0 | . 67 2003 | South Central China | Guangdong | 15844.64 | 782294.0 | 4073606.0 | 1550764.0 | . 143 2007 | South Central China | Henan | 15012.46 | 306162.0 | 10533312.0 | 3860764.0 | . 119 2007 | North China | Hebei | 13607.32 | 241621.0 | 7537692.0 | 2981235.0 | . 66 2002 | South Central China | Guangdong | 13502.42 | 1133400.0 | 3545004.0 | 1235386.0 | . 142 2006 | South Central China | Henan | 12362.79 | 184526.0 | 7601825.0 | 2018158.0 | . 65 2001 | South Central China | Guangdong | 12039.25 | 1193203.0 | 2152243.0 | 1257232.0 | . 118 2006 | North China | Hebei | 11467.60 | 201434.0 | 5831974.0 | 1253141.0 | . 64 2000 | South Central China | Guangdong | 10741.25 | 1128091.0 | 1927102.0 | 714572.0 | . 141 2005 | South Central China | Henan | 10587.42 | 123000.0 | 5676863.0 | 1171796.0 | . 299 2007 | Southwest China | Sichuan | 10562.39 | 149322.0 | 10384846.0 | 3937966.0 | . 117 2005 | North China | Hebei | 10012.11 | 191000.0 | 4503640.0 | 859056.0 | . 23 2007 | North China | Beijing | 9846.81 | 506572.0 | 1962192.0 | 752279.0 | . 167 2007 | South Central China | Hunan | 9439.60 | 327051.0 | 8340692.0 | 3156087.0 | . 155 2007 | South Central China | Hubei | 9333.40 | 276622.0 | 7666512.0 | 2922784.0 | . 215 2007 | Northeast China | Liaoning | 9304.52 | 598554.0 | 5502192.0 | 3396397.0 | . 63 1999 | South Central China | Guangdong | 9250.68 | 1165750.0 | 1789235.0 | 988521.0 | . ddf[&#39;province&#39;].compute() . 0 Anhui 1 Anhui 2 Anhui 3 Anhui 4 Anhui ... 355 Zhejiang 356 Zhejiang 357 Zhejiang 358 Zhejiang 359 Zhejiang Name: province, Length: 360, dtype: object . ddf.where(ddf[&#39;province&#39;]==&#39;Zhejiang&#39;).compute() . year region province gdp fdi it specific . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003.0 | East China | Zhejiang | 9705.02 | 498055.0 | 2261631.0 | 391292.0 | . 356 2004.0 | East China | Zhejiang | 11648.70 | 668128.0 | 3162299.0 | 656175.0 | . 357 2005.0 | East China | Zhejiang | 13417.68 | 772000.0 | 2370200.0 | 656175.0 | . 358 2006.0 | East China | Zhejiang | 15718.47 | 888935.0 | 2553268.0 | 1017303.0 | . 359 2007.0 | East China | Zhejiang | 18753.73 | 1036576.0 | 2939778.0 | 844647.0 | . 360 rows × 7 columns . mask_after_2010 = ddf.where(ddf[&#39;year&#39;]&gt;2000) . mask_after_2010.compute() . year region province gdp fdi it specific . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003.0 | East China | Zhejiang | 9705.02 | 498055.0 | 2261631.0 | 391292.0 | . 356 2004.0 | East China | Zhejiang | 11648.70 | 668128.0 | 3162299.0 | 656175.0 | . 357 2005.0 | East China | Zhejiang | 13417.68 | 772000.0 | 2370200.0 | 656175.0 | . 358 2006.0 | East China | Zhejiang | 15718.47 | 888935.0 | 2553268.0 | 1017303.0 | . 359 2007.0 | East China | Zhejiang | 18753.73 | 1036576.0 | 2939778.0 | 844647.0 | . 360 rows × 7 columns . def add_some_text(cname, *args, **kwargs): return &quot;Region name is &quot; + cname dummy_values = ddf[&#39;region&#39;].apply(add_some_text, axis=1) . /home/gao/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py:3208: UserWarning: You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly. To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using. Before: .apply(func) After: .apply(func, meta=(&#39;region&#39;, &#39;object&#39;)) warnings.warn(meta_warning(meta)) . dummy_values . Dask Series Structure: npartitions=5 0 object 72 ... ... 288 ... 359 ... Name: region, dtype: object Dask Name: apply, 75 tasks . dummy_values.visualize() . dummy_values.compute() . 0 Region name is East China 1 Region name is East China 2 Region name is East China 3 Region name is East China 4 Region name is East China ... 355 Region name is East China 356 Region name is East China 357 Region name is East China 358 Region name is East China 359 Region name is East China Name: region, Length: 360, dtype: object . max_per_region_yr = ddf.groupby(&#39;region&#39;).apply(lambda x: x.loc[x[&#39;gdp&#39;].idxmax(), &#39;year&#39;]) . /home/gao/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected. Before: .apply(func) After: .apply(func, meta={&#39;x&#39;: &#39;f8&#39;, &#39;y&#39;: &#39;f8&#39;}) for dataframe result or: .apply(func, meta=(&#39;x&#39;, &#39;f8&#39;)) for series result &#34;&#34;&#34;Entry point for launching an IPython kernel. . max_per_region_yr.visualize() . max_per_region_yr.compute() . region North China 2007 Northeast China 2007 Northwest China 2007 South Central China 2007 East China 2007 Southwest China 2007 dtype: int64 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/12/dask-xgboost-fiscal-data.html",
            "relUrl": "/2020/10/12/dask-xgboost-fiscal-data.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post59": {
            "title": "Moving Fiscal Data from a sqlite db to a dask dataframe",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal_data.db&#39;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;, 50661, 631930, 147002)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.7 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . df.gdp.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f41255eacc0&gt; . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . ddf.npartitions . ddf.npartitions . len(ddf) . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . ddf.describe().compute() . ddf.columns . feat_list = [&quot;year&quot;, &quot;fdi&quot;] cat_feat_list = [&quot;region&quot;, &quot;province&quot;] target = [&quot;gdp&quot;] . ddf[&quot;year&quot;] = ddf[&quot;year&quot;].astype(float) ddf[&quot;fdi&quot;] = ddf[&quot;fdi&quot;].astype(float) #ddf[&quot;province&quot;] = ddf[&quot;province&quot;].astype(float) #ddf[&quot;region&quot;] = ddf[&quot;region&quot;].astype(float) ddf[&quot;gdp&quot;] = ddf[&quot;gdp&quot;].astype(float) ddf[&quot;it&quot;] = ddf[&quot;it&quot;].astype(float) . type(target) . x=ddf[feat_list].persist() y=ddf[target].persist() . x . y.compute() . print(x.shape,y.shape) . x.count().compute() . from dask_ml.xgboost import XGBRegressor . XGBR = XGBRegressor() . %%time XGBR_model = XGBR.fit(x,y) . XGBR_model . client.close() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/11/dask-xgboost-fiscal-data.html",
            "relUrl": "/2020/10/11/dask-xgboost-fiscal-data.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "Working with dask data frames. Reading Fiscal Data from a sqlite db to a dask dataframe. Computing, visualizing and groupby with dask dataframes. Using dask.distributed locally.",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import dask.array as da import pandas as pd import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal_data.db&#39;) connection = engine.connect() metadata = db.MetaData() . engine.execute(&quot;SELECT * FROM fiscal_data LIMIT 1&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;, 50661, 631930, 147002)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp , fdi , it , specific FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.7 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df.columns . Index([&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;it&#39;, &#39;specific&#39;], dtype=&#39;object&#39;) . from dask import dataframe as dd . ddf = dd.from_pandas(df, npartitions=5) . print(ddf) . Dask DataFrame Structure: year region province gdp fdi it specific npartitions=5 0 int64 object object object int64 int64 float64 72 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 288 ... ... ... ... ... ... ... 359 ... ... ... ... ... ... ... Dask Name: from_pandas, 5 tasks . ddf.npartitions . 5 . ddf.npartitions . 5 . len(ddf) . 360 . from dask.distributed import Client client = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=&#39;4GB&#39;) client . Client . Scheduler: inproc://192.168.1.71/12451/1 | Dashboard: http://localhost:8787/status &lt;/ul&gt; &lt;/td&gt; Cluster . Workers: 3 | Cores: 6 | Memory: 12.00 GB | . | &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; ddf.describe().compute() . year fdi it specific . count 360.000000 | 3.600000e+02 | 3.600000e+02 | 3.560000e+02 | . mean 2001.500000 | 1.961394e+05 | 2.165819e+06 | 5.834707e+05 | . std 3.456857 | 3.030440e+05 | 1.769294e+06 | 6.540553e+05 | . min 1996.000000 | 2.000000e+00 | 1.478970e+05 | 8.964000e+03 | . 25% 1998.750000 | 3.309900e+04 | 1.077466e+06 | 2.237530e+05 | . 50% 2001.500000 | 1.411025e+05 | 2.020634e+06 | 4.243700e+05 | . 75% 2004.250000 | 4.065125e+05 | 3.375492e+06 | 1.011846e+06 | . max 2007.000000 | 1.743140e+06 | 1.053331e+07 | 3.937966e+06 | . ddf.head() . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.3 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . groupby_yr = ddf.groupby(&#39;year&#39;).count() . groupby_yr.compute() . region province gdp fdi it specific . year . 1996 30 | 30 | 30 | 30 | 30 | 29 | . 1997 30 | 30 | 30 | 30 | 30 | 28 | . 1998 30 | 30 | 30 | 30 | 30 | 30 | . 1999 30 | 30 | 30 | 30 | 30 | 30 | . 2000 30 | 30 | 30 | 30 | 30 | 29 | . 2001 30 | 30 | 30 | 30 | 30 | 30 | . 2002 30 | 30 | 30 | 30 | 30 | 30 | . 2003 30 | 30 | 30 | 30 | 30 | 30 | . 2004 30 | 30 | 30 | 30 | 30 | 30 | . 2005 30 | 30 | 30 | 30 | 30 | 30 | . 2006 30 | 30 | 30 | 30 | 30 | 30 | . 2007 30 | 30 | 30 | 30 | 30 | 30 | . group_region = ddf.groupby(&#39;region&#39;)[&#39;gdp&#39;].sum() . group_region.compute() . region East China 2093.32347.322542.962712.342902.093246.713519.... North China 1789.22077.092377.182678.823161.663707.964315.... Northwest China 722.52793.57887.67956.321052.881125.371232.031... South Central China 6834.977774.538530.889250.6810741.2512039.2513... Southwest China 1315.121509.751602.381663.21791.01976.862232.8... Northeast China 2370.52667.52774.42866.33151.43390.13637.24057... Name: gdp, dtype: object . ddf.nlargest(5, &#39;fdi&#39;).compute() . year region province gdp fdi it specific . 179 2007 | East China | Jiangsu | 21742.05 | 1743140 | 3557071 | 1188989.0 | . 71 2007 | South Central China | Guangdong | 31777.01 | 1712603 | 4947824 | 859482.0 | . 70 2006 | South Central China | Guangdong | 26587.76 | 1451065 | 4559252 | 1897575.0 | . 178 2006 | East China | Jiangsu | 18598.69 | 1318339 | 2926542 | 1388043.0 | . 69 2005 | South Central China | Guangdong | 22557.37 | 1236400 | 4327217 | 1491588.0 | . ddf.sum().visualize() . ddf.sum().visualize(rankdir=&quot;LR&quot;) . (ddf).visualize(rankdir=&quot;LR&quot;) . ddf.visualize(rankdir=&quot;LR&quot;) . client.close() . &lt;/div&gt; | . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/10/Dask_Fiscal-db-to-dask-dataframe.html",
            "relUrl": "/2020/10/10/Dask_Fiscal-db-to-dask-dataframe.html",
            "date": " • Oct 10, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "Moving fiscal data from a pandas dataframe to a sqlite local database",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples. . import numpy as np import pandas as pd . df = pd.read_csv(&#39;df_panel_fix.csv&#39;) . df.columns . Index([&#39;Unnamed: 0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;], dtype=&#39;object&#39;) . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;]] df_subset . year reg province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;gdp&quot;, &quot;fdi&quot;, &#39;it&#39;,&quot;specific&quot;] . df_subset . year region province gdp fdi it specific . 0 1996 | East China | Anhui | 2093.30 | 50661 | 631930 | 147002.0 | . 1 1997 | East China | Anhui | 2347.32 | 43443 | 657860 | 151981.0 | . 2 1998 | East China | Anhui | 2542.96 | 27673 | 889463 | 174930.0 | . 3 1999 | East China | Anhui | 2712.34 | 26131 | 1227364 | 285324.0 | . 4 2000 | East China | Anhui | 2902.09 | 31847 | 1499110 | 195580.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 9705.02 | 498055 | 2261631 | 391292.0 | . 356 2004 | East China | Zhejiang | 11648.70 | 668128 | 3162299 | 656175.0 | . 357 2005 | East China | Zhejiang | 13417.68 | 772000 | 2370200 | 656175.0 | . 358 2006 | East China | Zhejiang | 15718.47 | 888935 | 2553268 | 1017303.0 | . 359 2007 | East China | Zhejiang | 18753.73 | 1036576 | 2939778 | 844647.0 | . 360 rows × 7 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal_data.db&#39;) connection = engine.connect() metadata = db.MetaData() . fiscal_data = db.Table(&#39;fiscal_data&#39;, metadata, db.Column(&#39;year&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;region&#39;,db.String, nullable=True), db.Column(&#39;province&#39;,db.String, nullable=True), db.Column(&#39;gdp&#39;,db.String, nullable=True), db.Column(&#39;fdi&#39;,db.Integer, nullable=True), db.Column(&#39;it&#39;,db.Integer, nullable=True), db.Column(&#39;specific&#39;, db.Integer, nullable=True) ) . metadata.create_all(engine) #Creates the table . fiscal_data . Table(&#39;fiscal_data&#39;, MetaData(bind=None), Column(&#39;year&#39;, Integer(), table=&lt;fiscal_data&gt;), Column(&#39;region&#39;, String(), table=&lt;fiscal_data&gt;), Column(&#39;province&#39;, String(), table=&lt;fiscal_data&gt;), Column(&#39;gdp&#39;, String(), table=&lt;fiscal_data&gt;), Column(&#39;fdi&#39;, Integer(), table=&lt;fiscal_data&gt;), Column(&#39;it&#39;, Integer(), table=&lt;fiscal_data&gt;), Column(&#39;specific&#39;, Integer(), table=&lt;fiscal_data&gt;), schema=None) . df_subset.to_sql(&#39;fiscal_data&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT year, region, province, gdp FROM fiscal_data LIMIT 10&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2093.3&#39;), (1997, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2347.32&#39;), (1998, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2542.96&#39;), (1999, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2712.34&#39;), (2000, &#39;East China&#39;, &#39;Anhui&#39;, &#39;2902.09&#39;), (2001, &#39;East China&#39;, &#39;Anhui&#39;, &#39;3246.71&#39;), (2002, &#39;East China&#39;, &#39;Anhui&#39;, &#39;3519.72&#39;), (2003, &#39;East China&#39;, &#39;Anhui&#39;, &#39;3923.11&#39;), (2004, &#39;East China&#39;, &#39;Anhui&#39;, &#39;4759.3&#39;), (2005, &#39;East China&#39;, &#39;Anhui&#39;, &#39;5350.17&#39;)] . sql = &quot;&quot;&quot; SELECT year , region , province , gdp FROM fiscal_data &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df.tail(30) . year region province gdp . 330 2002 | Northwest China | Xinjiang | 1612.65 | . 331 2003 | Northwest China | Xinjiang | 1886.35 | . 332 2004 | Northwest China | Xinjiang | 2209.09 | . 333 2005 | Northwest China | Xinjiang | 2604.19 | . 334 2006 | Northwest China | Xinjiang | 3045.26 | . 335 2007 | Northwest China | Xinjiang | 3523.16 | . 336 1996 | Southwest China | Yunnan | 1517.69 | . 337 1997 | Southwest China | Yunnan | 1676.17 | . 338 1998 | Southwest China | Yunnan | 1831.33 | . 339 1999 | Southwest China | Yunnan | 1899.82 | . 340 2000 | Southwest China | Yunnan | 2011.19 | . 341 2001 | Southwest China | Yunnan | 2138.31 | . 342 2002 | Southwest China | Yunnan | 2312.82 | . 343 2003 | Southwest China | Yunnan | 2556.02 | . 344 2004 | Southwest China | Yunnan | 3081.91 | . 345 2005 | Southwest China | Yunnan | 3462.73 | . 346 2006 | Southwest China | Yunnan | 3988.14 | . 347 2007 | Southwest China | Yunnan | 4772.52 | . 348 1996 | East China | Zhejiang | 4188.53 | . 349 1997 | East China | Zhejiang | 4686.11 | . 350 1998 | East China | Zhejiang | 5052.62 | . 351 1999 | East China | Zhejiang | 5443.92 | . 352 2000 | East China | Zhejiang | 6141.03 | . 353 2001 | East China | Zhejiang | 6898.34 | . 354 2002 | East China | Zhejiang | 8003.67 | . 355 2003 | East China | Zhejiang | 9705.02 | . 356 2004 | East China | Zhejiang | 11648.7 | . 357 2005 | East China | Zhejiang | 13417.68 | . 358 2006 | East China | Zhejiang | 15718.47 | . 359 2007 | East China | Zhejiang | 18753.73 | . #http://manpages.ubuntu.com/manpages/precise/man1/sqlite3.1.html . # sqlite3 fiscal_data.db # create table memos(text, priority INTEGER); # insert into memos values(&#39;example 1&#39;, 10); # insert into memos values(&#39;example 2&#39;, 100); # select * from memos; # sqlite3 -line fiscal_data.db &#39;select * from memos where priority &gt; 20;&#39; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/09/Dask_Fiscal-Data-Sqlite-db.html",
            "relUrl": "/2020/10/09/Dask_Fiscal-Data-Sqlite-db.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post62": {
            "title": "Using Dask for Arrays",
            "content": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask. . import numpy as np import dask.array as da . np_arr = np.random.randint(20, size=20) np_arr . array([12, 18, 17, 7, 5, 9, 11, 3, 5, 15, 13, 13, 5, 12, 11, 16, 4, 10, 9, 7]) . dask_arr = da.random.randint(20, size=20, chunks=5) . dask_arr . | Array Chunk . Bytes 160 B | 40 B | . Shape (20,) | (5,) | . Count 4 Tasks | 4 Chunks | . Type int64 | numpy.ndarray | . | 20 1 | . ## This is simply because Dask does lazy evaluaion. ### You need to call `compute()` to start the execution . dask_arr.compute() . array([ 3, 17, 5, 11, 19, 14, 14, 11, 9, 18, 9, 7, 10, 13, 10, 10, 11, 10, 9, 2]) . dask_arr.chunks . ((5, 5, 5, 5),) . dask_arr_from_np = da.from_array(np_arr, chunks=5) . dask_arr_from_np . | Array Chunk . Bytes 160 B | 40 B | . Shape (20,) | (5,) | . Count 5 Tasks | 4 Chunks | . Type int64 | numpy.ndarray | . | 20 1 | . dask_arr_from_np.compute() . array([12, 18, 17, 7, 5, 9, 11, 3, 5, 15, 13, 13, 5, 12, 11, 16, 4, 10, 9, 7]) . ### array operations into a graph to tasks #### See : http://docs.dask.org/en/latest/graphviz.html . dask_arr_from_np.sum().visualize() . dask_arr_from_np.sum().visualize(rankdir=&quot;LR&quot;) . (dask_arr_from_np+1).visualize(rankdir=&quot;LR&quot;) . dask_arr_mean = da.mean(dask_arr_from_np) dask_arr_mean.compute() . 10.1 . dask_arr_mean.visualize(rankdir=&quot;LR&quot;) . x = da.random.random(10, chunks=2) y = da.random.random(10, chunks=2) sum_x_y = da.add(x, y) #similar to numpy.add mean_x_y = da.mean(sum_x_y) . sum_x_y.compute() . array([0.96028343, 0.55946179, 1.11161829, 1.28233368, 0.53130934, 0.86805782, 0.20173099, 0.77596276, 0.92576765, 1.04750609]) . sum_x_y.visualize() . mean_x_y.visualize() . da_arr_large = da.random.randint(10000, size=(50000, 50000), chunks=(5000, 1000)) da_sum_large = da_arr_large.sum() . ### Get no. bytes using `nbytes` : http://docs.dask.org/en/latest/array-api.html#dask.array.Array.nbytes . da_arr_large.nbytes . 20000000000 . ### Convert bytes to GB, 1Gb = 1e+9 bytes . da_arr_large.nbytes/1e+9 . 20.0 . da_sum_large.compute() . 12498643590734 . # Dask 2 . size_tuple = (500,500) chunks_tuple = (10,500) . da_arr = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) da_arr2 = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) . def random_func(x): return np.mean((((x * 2).T)**2),axis=0) . gufoo = da.gufunc(random_func, signature=&quot;(i)-&gt;()&quot;, output_dtypes=float, vectorize=True) . random_op_arr = gufoo(da_arr) random_op_arr.compute() . array([112.056, 107.44 , 111.024, 109.656, 118.832, 109.84 , 117.2 , 111.952, 116.312, 117.368, 128.568, 111.144, 110.656, 112.648, 115.24 , 114.624, 113.912, 109.632, 112.864, 113.488, 119.248, 121.4 , 108.272, 118.784, 114.968, 115.216, 107.872, 113.6 , 112.456, 112.48 , 114.864, 119.28 , 112.656, 110.208, 109.728, 120.576, 119.632, 118.12 , 112.888, 116.384, 113.192, 106.84 , 111.72 , 115.928, 106.08 , 114.568, 121.512, 115.384, 113.864, 107.104, 114.32 , 116.176, 117.28 , 116.976, 117.784, 110.088, 121.696, 114.2 , 113.864, 116.072, 112.344, 113.808, 113.968, 110.472, 119.536, 113.84 , 109.328, 116.552, 119.056, 113.84 , 117.872, 114.928, 116.336, 115.192, 115.808, 106.984, 116.984, 114.536, 116.496, 111.968, 115.216, 108.24 , 119.52 , 116.136, 111.144, 111.712, 119.224, 114.312, 110.464, 110.216, 111.288, 119.6 , 108.264, 114.456, 119.016, 107.032, 114.832, 108.056, 105.712, 110.64 , 103.4 , 106.768, 118.216, 112.44 , 113.728, 114.6 , 117.832, 108.288, 117.92 , 113.12 , 121.984, 112.776, 123.144, 115.968, 112.44 , 115.712, 112.144, 108.448, 114.752, 108.376, 101.296, 102.992, 117.872, 114.056, 115.736, 115.528, 122.072, 130.168, 106.992, 109.912, 117.872, 112.152, 112.184, 113.544, 116.496, 112.832, 108.712, 116.96 , 120.984, 117.808, 112.272, 111.816, 118.872, 116.376, 118.992, 112.344, 124.672, 97.576, 112.496, 117.92 , 102.392, 109.992, 112.016, 117.92 , 108.352, 112.376, 121.008, 117.808, 113.504, 125.592, 114.936, 111.456, 116.488, 104.744, 114.136, 114. , 107.256, 117.84 , 111.872, 109.152, 118.752, 112.32 , 116.16 , 106.696, 109.472, 111.968, 118.264, 115.088, 112.864, 110.016, 111.888, 111.84 , 118.488, 107.952, 121.52 , 126.52 , 112.12 , 110.952, 115.328, 110.064, 106.36 , 118.96 , 109.68 , 117.776, 107.112, 111.152, 113.888, 113.408, 114.992, 117.632, 116.648, 117.112, 118.2 , 116.36 , 113.104, 113.6 , 112.208, 112.592, 117.192, 102.832, 112.08 , 113.744, 116.048, 117.368, 113.96 , 111.24 , 121.824, 112.56 , 110.192, 130.776, 111.656, 119.984, 113.592, 113.592, 106.664, 125.192, 113.6 , 117.12 , 106.24 , 112.856, 114.544, 117.16 , 108.344, 112.208, 109.112, 124.824, 109.824, 106.352, 115.568, 112.64 , 112.904, 112.736, 112.52 , 124.808, 120.32 , 114.472, 119.528, 113.456, 112.448, 118.672, 110.016, 116.16 , 122.048, 111.088, 114.56 , 107.448, 115.328, 111.656, 108.688, 116.904, 110.8 , 108.896, 112.136, 115.896, 111.848, 108.808, 114.504, 124.552, 116.248, 114.576, 110.56 , 112.152, 117.576, 125.44 , 110.72 , 108.072, 115.192, 116.048, 107.76 , 111.376, 121.608, 115.256, 113.84 , 105.672, 115.024, 115.864, 114.304, 123.344, 114.624, 115.696, 113.288, 116.688, 109.048, 125.264, 118.8 , 112.2 , 114.312, 109.728, 116.064, 113.808, 106.912, 109.288, 117. , 114.632, 114.456, 110.168, 111.976, 117.816, 110.04 , 103.048, 113.656, 112.504, 113.8 , 120.04 , 120.224, 110.68 , 110.096, 116.12 , 113.424, 107.408, 111.296, 111.512, 117.432, 105.96 , 115.992, 118.44 , 110.024, 119.216, 111.664, 119.184, 109.824, 116.736, 116.76 , 107.544, 120.44 , 115.08 , 110.136, 112.144, 113.888, 111.32 , 109.952, 117.096, 111.152, 115.728, 110.832, 113.312, 113.664, 112.016, 111.952, 114.896, 114.728, 107.848, 108.832, 122.384, 111.824, 107.384, 117.504, 117.344, 110.144, 109.568, 101.36 , 111.944, 105.512, 115.792, 112.08 , 104.568, 109.008, 108.992, 114.936, 113.008, 120.088, 117.328, 117.008, 107.584, 111.688, 115.664, 108.416, 119.48 , 107.336, 120.184, 111.952, 115.824, 113.928, 117.064, 114.296, 111.56 , 120.04 , 112.256, 115.368, 109.112, 112.184, 112.128, 111.288, 117.856, 109.184, 113.128, 119.888, 110.656, 111.992, 116.704, 107.696, 111.608, 121.504, 110.296, 111.008, 112.072, 117.072, 115.68 , 108.888, 117.704, 113.112, 101.144, 112.36 , 122.688, 112.016, 111.64 , 113.992, 117.08 , 109.976, 108.048, 110.504, 112.936, 111.776, 117.392, 116.568, 106.896, 105.224, 115.512, 117. , 116.192, 113.344, 111.776, 114.312, 113.008, 114.768, 121.712, 112.528, 108.976, 106.648, 107.8 , 122.696, 104.064, 117.072, 119.064, 111.472, 112.752, 109.52 , 123.712, 114.032, 120.888, 109.84 , 123.36 , 111.576, 118.56 , 116.328, 113.048, 111.68 , 106.072, 109.752, 112.32 , 114.344, 114.976, 114.072, 121.792, 113.024, 109.864, 115.84 , 115.752, 118.648, 107.52 , 116.104, 112.464, 123.232, 112.32 , 116.952, 106.32 , 110.992, 111.256, 113.616, 111.344, 115.216, 121.504, 117.504, 115.816, 116.6 , 111.08 , 108.776, 110.672, 109.464, 107.096, 112.928, 106.8 , 110.4 , 112.576, 114.648, 113.272, 112.504, 112.888, 112.84 , 111.496]) . random_op_arr.shape . (500,) . @da.as_gufunc(signature=&quot;(m,n),(n,j)-&gt;(m,j)&quot;, output_dtypes=int, allow_rechunk=True) def random_func(x, y): return np.matmul(x, y)**2 . da_arr3 = da.random.randint(10, size=(200, 100), chunks=(10, 100)) da_arr4 = da.random.randint(10, size=(100, 300), chunks=(5,5)) . # random_matmul = random_func(da_arr3, da_arr4) # random_matmul.compute() . random_matmul.shape . (200, 300) . # Dask 3 . my_arr = da.random.randint(10, size=20, chunks=3) . my_arr.compute() . array([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5]) . my_hundred_arr = my_arr + 100 my_hundred_arr.compute() . array([103, 108, 108, 107, 104, 101, 105, 107, 102, 107, 104, 104, 108, 100, 109, 103, 106, 107, 101, 105]) . (my_arr * (-1)).compute() . array([-3, -8, -8, -7, -4, -1, -5, -7, -2, -7, -4, -4, -8, 0, -9, -3, -6, -7, -1, -5]) . dask_sum = my_arr.sum() dask_sum . | Array Chunk . Bytes 8 B | 8 B | . Shape () | () | . Count 17 Tasks | 1 Chunks | . Type int64 | numpy.ndarray | . | | . my_arr.compute() . array([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5]) . dask_sum.compute() . 99 . my_ones_arr = da.ones((10,10), chunks=2, dtype=int) . my_ones_arr.compute() . array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) . my_ones_arr.mean(axis=0).compute() . array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) . my_custom_array = da.random.randint(10, size=(4,4), chunks=(1,4)) . my_custom_array.compute() . array([[0, 1, 7, 6], [0, 1, 2, 4], [6, 3, 5, 3], [3, 2, 2, 6]]) . my_custom_array.mean(axis=0).compute() . array([2.25, 1.75, 4. , 4.75]) . my_custom_array.mean(axis=1).compute() . array([3.5 , 1.75, 4.25, 3.25]) . ## Slicing . my_custom_array[1:3, 2:4] . | Array Chunk . Bytes 32 B | 16 B | . Shape (2, 2) | (1, 2) | . Count 6 Tasks | 2 Chunks | . Type int64 | numpy.ndarray | . | 2 2 | . my_custom_array[1:3, 2:4].compute() . array([[2, 4], [5, 3]]) . ## Broadcasting . my_custom_array.compute() . array([[0, 1, 7, 6], [0, 1, 2, 4], [6, 3, 5, 3], [3, 2, 2, 6]]) . my_small_arr = da.ones(4, chunks=2) my_small_arr.compute() . array([1., 1., 1., 1.]) . brd_example1 = da.add(my_custom_array, my_small_arr) . brd_example1.compute() . array([[1., 2., 8., 7.], [1., 2., 3., 5.], [7., 4., 6., 4.], [4., 3., 3., 7.]]) . ten_arr = da.full_like(my_small_arr, 10) . ten_arr.compute() . array([10., 10., 10., 10.]) . brd_example2 = da.add(my_custom_array, ten_arr) . brd_example2.compute() . array([[10., 11., 17., 16.], [10., 11., 12., 14.], [16., 13., 15., 13.], [13., 12., 12., 16.]]) . ## Reshaping . my_custom_array.shape . (4, 4) . custom_arr_1d = my_custom_array.reshape(16) . custom_arr_1d . | Array Chunk . Bytes 128 B | 32 B | . Shape (16,) | (4,) | . Count 8 Tasks | 4 Chunks | . Type int64 | numpy.ndarray | . | 16 1 | . custom_arr_1d.compute() . array([0, 1, 7, 6, 0, 1, 2, 4, 6, 3, 5, 3, 3, 2, 2, 6]) . # Stacking . stacked_arr = da.stack([brd_example1, brd_example2]) . stacked_arr.compute() . array([[[ 1., 2., 8., 7.], [ 1., 2., 3., 5.], [ 7., 4., 6., 4.], [ 4., 3., 3., 7.]], [[10., 11., 17., 16.], [10., 11., 12., 14.], [16., 13., 15., 13.], [13., 12., 12., 16.]]]) . another_stacked = da.stack([brd_example1, brd_example2], axis=1) . another_stacked.compute() . array([[[ 1., 2., 8., 7.], [10., 11., 17., 16.]], [[ 1., 2., 3., 5.], [10., 11., 12., 14.]], [[ 7., 4., 6., 4.], [16., 13., 15., 13.]], [[ 4., 3., 3., 7.], [13., 12., 12., 16.]]]) . # Concatenate . concate_arr = da.concatenate([brd_example1, brd_example2]) . concate_arr.compute() . array([[ 1., 2., 8., 7.], [ 1., 2., 3., 5.], [ 7., 4., 6., 4.], [ 4., 3., 3., 7.], [10., 11., 17., 16.], [10., 11., 12., 14.], [16., 13., 15., 13.], [13., 12., 12., 16.]]) . another_concate_arr = da.concatenate([brd_example1, brd_example2],axis=1) . another_concate_arr.compute() . array([[ 1., 2., 8., 7., 10., 11., 17., 16.], [ 1., 2., 3., 5., 10., 11., 12., 14.], [ 7., 4., 6., 4., 16., 13., 15., 13.], [ 4., 3., 3., 7., 13., 12., 12., 16.]]) . # Dask 4 . import numpy as np import dask.array as da . size_tuple = (18000,18000) np_arr = np.random.randint(10, size=size_tuple) np_arr2 = np.random.randint(10, size=size_tuple) . %time (((np_arr * 2).T)**2 + np_arr2 + 100).sum(axis=1).mean() . MemoryError Traceback (most recent call last) &lt;timed eval&gt; in &lt;module&gt; MemoryError: . chunks_tuple = (500, 500) da_arr = da.from_array(np_arr, chunks=chunks_tuple) da_arr2 = da.from_array(np_arr2, chunks=chunks_tuple) . %time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute() . CPU times: user 10.1 s, sys: 362 ms, total: 10.5 s Wall time: 2.47 s . 3933124.5174444444 . size_tuple = (50000, 50000) np_arr = np.random.randint(10, size=size_tuple) np_arr2 = np.random.randint(10, size=size_tuple) . MemoryError Traceback (most recent call last) &lt;ipython-input-5-9ce9976b2eaf&gt; in &lt;module&gt; 1 size_tuple = (50000, 50000) -&gt; 2 np_arr = np.random.randint(10, size=size_tuple) 3 np_arr2 = np.random.randint(10, size=size_tuple) mtrand.pyx in mtrand.RandomState.randint() mtrand.pyx in mtrand.RandomState.randint() randint_helpers.pxi in mtrand._rand_int64() MemoryError: . chunks_tuple = (5000, 5000) da_arr = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) da_arr2 = da.random.randint(10, size=size_tuple, chunks=chunks_tuple) . %time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute() . CPU times: user 3min 10s, sys: 10.5 s, total: 3min 20s Wall time: 28.2 s . 10925051.41748 . da_arr.nbytes/1e+9 . 20.0 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/08/Dask-Day-1.html",
            "relUrl": "/2020/10/08/Dask-Day-1.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post63": {
            "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Fiscal Data",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . df=pd.read_csv(&#39;df_panel_fix.csv&#39;) . df . Unnamed: 0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.30 | 50661 | 0.000000 | 0.000000 | 0.000000 | 1128873 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.32 | 43443 | 0.000000 | 0.000000 | 0.000000 | 1356287 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.96 | 27673 | 0.000000 | 0.000000 | 0.000000 | 1518236 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.34 | 26131 | NaN | NaN | NaN | 1646891 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.000000 | 0.000000 | 0.000000 | 1601508 | East China | 1499110 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 355 355 | Zhejiang | 391292.0 | 260313.0 | 2003 | 9705.02 | 498055 | 1.214286 | 0.035714 | 0.035714 | 6217715 | East China | 2261631 | . 356 356 | Zhejiang | 656175.0 | 276652.0 | 2004 | 11648.70 | 668128 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 3162299 | . 357 357 | Zhejiang | 656175.0 | NaN | 2005 | 13417.68 | 772000 | 1.214286 | 0.035714 | 0.035714 | NaN | East China | 2370200 | . 358 358 | Zhejiang | 1017303.0 | 394795.0 | 2006 | 15718.47 | 888935 | 1.214286 | 0.035714 | 0.035714 | 11537149 | East China | 2553268 | . 359 359 | Zhejiang | 844647.0 | 0.0 | 2007 | 18753.73 | 1036576 | 0.047619 | 0.000000 | 0.000000 | 16494981 | East China | 2939778 | . 360 rows × 13 columns . df_subset = df[[&quot;year&quot;, &quot;reg&quot;, &quot;province&quot;, &quot;it&quot;, &quot;specific&quot;, &#39;gdp&#39;,&quot;fdi&quot;]] df_subset . year reg province it specific gdp fdi . 0 1996 | East China | Anhui | 631930 | 147002.0 | 2093.30 | 50661 | . 1 1997 | East China | Anhui | 657860 | 151981.0 | 2347.32 | 43443 | . 2 1998 | East China | Anhui | 889463 | 174930.0 | 2542.96 | 27673 | . 3 1999 | East China | Anhui | 1227364 | 285324.0 | 2712.34 | 26131 | . 4 2000 | East China | Anhui | 1499110 | 195580.0 | 2902.09 | 31847 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 2261631 | 391292.0 | 9705.02 | 498055 | . 356 2004 | East China | Zhejiang | 3162299 | 656175.0 | 11648.70 | 668128 | . 357 2005 | East China | Zhejiang | 2370200 | 656175.0 | 13417.68 | 772000 | . 358 2006 | East China | Zhejiang | 2553268 | 1017303.0 | 15718.47 | 888935 | . 359 2007 | East China | Zhejiang | 2939778 | 844647.0 | 18753.73 | 1036576 | . 360 rows × 7 columns . df_subset.columns = [&quot;year&quot;, &quot;region&quot;, &quot;province&quot;, &quot;it&quot;, &quot;specific&quot;, &#39;gdp&#39;,&quot;fdi&quot;] . df_subset . year region province it specific gdp fdi . 0 1996 | East China | Anhui | 631930 | 147002.0 | 2093.30 | 50661 | . 1 1997 | East China | Anhui | 657860 | 151981.0 | 2347.32 | 43443 | . 2 1998 | East China | Anhui | 889463 | 174930.0 | 2542.96 | 27673 | . 3 1999 | East China | Anhui | 1227364 | 285324.0 | 2712.34 | 26131 | . 4 2000 | East China | Anhui | 1499110 | 195580.0 | 2902.09 | 31847 | . ... ... | ... | ... | ... | ... | ... | ... | . 355 2003 | East China | Zhejiang | 2261631 | 391292.0 | 9705.02 | 498055 | . 356 2004 | East China | Zhejiang | 3162299 | 656175.0 | 11648.70 | 668128 | . 357 2005 | East China | Zhejiang | 2370200 | 656175.0 | 13417.68 | 772000 | . 358 2006 | East China | Zhejiang | 2553268 | 1017303.0 | 15718.47 | 888935 | . 359 2007 | East China | Zhejiang | 2939778 | 844647.0 | 18753.73 | 1036576 | . 360 rows × 7 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///fiscal.db&#39;) connection = engine.connect() metadata = db.MetaData() . fiscal_table = db.Table(&#39;fiscal_table&#39;, metadata, db.Column(&#39;year&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;region&#39;,db.Integer, nullable=True), db.Column(&#39;province&#39;,db.Integer, nullable=True), db.Column(&#39;it&#39;,db.Integer, nullable=True), db.Column(&#39;specific&#39;,db.Integer, nullable=True), db.Column(&#39;gdp&#39;,db.Integer, nullable=True), db.Column(&#39;fdi&#39;, db.Numeric, nullable=True) ) . metadata.create_all(engine) #Creates the table . df_subset.to_sql(&#39;fiscal_table&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT * FROM fiscal_table LIMIT 10&quot;).fetchall() . [(1996, &#39;East China&#39;, &#39;Anhui&#39;, 631930, 147002, 2093.3, 50661), (1997, &#39;East China&#39;, &#39;Anhui&#39;, 657860, 151981, 2347.32, 43443), (1998, &#39;East China&#39;, &#39;Anhui&#39;, 889463, 174930, 2542.96, 27673), (1999, &#39;East China&#39;, &#39;Anhui&#39;, 1227364, 285324, 2712.34, 26131), (2000, &#39;East China&#39;, &#39;Anhui&#39;, 1499110, 195580, 2902.09, 31847), (2001, &#39;East China&#39;, &#39;Anhui&#39;, 2165189, 250898, 3246.71, 33672), (2002, &#39;East China&#39;, &#39;Anhui&#39;, 2404936, 434149, 3519.72, 38375), (2003, &#39;East China&#39;, &#39;Anhui&#39;, 2815820, 619201, 3923.11, 36720), (2004, &#39;East China&#39;, &#39;Anhui&#39;, 3422176, 898441, 4759.3, 54669), (2005, &#39;East China&#39;, &#39;Anhui&#39;, 3874846, 898441, 5350.17, 69000)] . sql = &quot;&quot;&quot; SELECT year , region , province , it --, CURRENT_DATE() FROM fiscal_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df.tail(30) . year region province it . 330 2002 | Northwest China | Xinjiang | 2150325 | . 331 2003 | Northwest China | Xinjiang | 2355164 | . 332 2004 | Northwest China | Xinjiang | 2838346 | . 333 2005 | Northwest China | Xinjiang | 3421743 | . 334 2006 | Northwest China | Xinjiang | 4686125 | . 335 2007 | Northwest China | Xinjiang | 5502470 | . 336 1996 | Southwest China | Yunnan | 1374111 | . 337 1997 | Southwest China | Yunnan | 1452425 | . 338 1998 | Southwest China | Yunnan | 1617463 | . 339 1999 | Southwest China | Yunnan | 1888666 | . 340 2000 | Southwest China | Yunnan | 2254281 | . 341 2001 | Southwest China | Yunnan | 2856307 | . 342 2002 | Southwest China | Yunnan | 3035767 | . 343 2003 | Southwest China | Yunnan | 3388449 | . 344 2004 | Southwest China | Yunnan | 3957158 | . 345 2005 | Southwest China | Yunnan | 4280994 | . 346 2006 | Southwest China | Yunnan | 5046865 | . 347 2007 | Southwest China | Yunnan | 6832541 | . 348 1996 | East China | Zhejiang | 740327 | . 349 1997 | East China | Zhejiang | 814253 | . 350 1998 | East China | Zhejiang | 923455 | . 351 1999 | East China | Zhejiang | 1001703 | . 352 2000 | East China | Zhejiang | 1135215 | . 353 2001 | East China | Zhejiang | 1203372 | . 354 2002 | East China | Zhejiang | 1962633 | . 355 2003 | East China | Zhejiang | 2261631 | . 356 2004 | East China | Zhejiang | 3162299 | . 357 2005 | East China | Zhejiang | 2370200 | . 358 2006 | East China | Zhejiang | 2553268 | . 359 2007 | East China | Zhejiang | 2939778 | . #df[&#39;it&#39;].plot(figsize = (12, 8)) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/07/Fiscal_data-sqlitedb-Copy1.html",
            "relUrl": "/2020/10/07/Fiscal_data-sqlitedb-Copy1.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post64": {
            "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for NLP",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . df=pd.read_csv(&#39;nf_complete.csv&#39;) . df.columns . Index([&#39;Unnamed: 0&#39;, &#39;year&#39;, &#39;title&#39;, &#39;abstract&#39;, &#39;theme&#39;, &#39;China&#39;, &#39;Russia&#39;, &#39;War&#39;, &#39;President&#39;, &#39;US&#39;, &#39;Vietnam&#39;, &#39;Cold War&#39;, &#39;World War&#39;, &#39;Vietnam War&#39;, &#39;Korean War&#39;, &#39;Survey&#39;, &#39;Case Study&#39;, &#39;Trade&#39;, &#39;Humanitarian&#39;, &#39;fixed_effects&#39;, &#39;instrumental_variable&#39;, &#39;regression&#39;, &#39;experimental&#39;], dtype=&#39;object&#39;) . df[[&quot;year&quot;,&quot;title&quot;]] . year title . 0 2000 | &quot;Institutions at the Domestic/International Ne... | . 1 2000 | Born to Lose and Doomed to Survive: State Deat... | . 2 2000 | The significance of “allegiance” in internatio... | . 3 2000 | The significance of “allegiance” in internatio... | . 4 2000 | Truth-Telling and Mythmaking in Post-Soviet Ru... | . ... ... | ... | . 121 2018 | Planning for the Short Haul: Trade Among Belli... | . 122 2018 | Clinging to the Anti-Imperial Mantle: The Repu... | . 123 2018 | The New Navy&#39;s Pacific Wars: Peripheral Confl... | . 124 2018 | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | . 125 2018 | Unexpected Humanitarians: Albania, the U.S. Mi... | . 126 rows × 2 columns . df_subset = df[[&quot;year&quot;, &quot;title&quot;, &quot;abstract&quot;, &quot;theme&quot;, &quot;War&quot;, &#39;Cold War&#39;,&quot;Trade&quot;]] df_subset . year title abstract theme War Cold War Trade . 0 2000 | &quot;Institutions at the Domestic/International Ne... | Civil-military relations are frequently studie... | IR scholarship | 1 | 0 | 0 | . 1 2000 | Born to Lose and Doomed to Survive: State Deat... | Under what conditions do states die, or exit t... | IR scholarship | 1 | 1 | 0 | . 2 2000 | The significance of “allegiance” in internatio... | My dissertation employs original and secondary... | IR scholarship | 1 | 0 | 0 | . 3 2000 | The significance of “allegiance” in internatio... | nThis study revises prevailing interpretation... | Conflit Between States | 0 | 1 | 0 | . 4 2000 | Truth-Telling and Mythmaking in Post-Soviet Ru... | Can distorted and pernicious ideas about histo... | Conflict Between States | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 121 2018 | Planning for the Short Haul: Trade Among Belli... | In times of war, why do belligerents continue ... | Conflict between states | 1 | 0 | 1 | . 122 2018 | Clinging to the Anti-Imperial Mantle: The Repu... | My dissertation project, Clinging to the Anti-... | Cold War | 0 | 1 | 0 | . 123 2018 | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Using a transnational methodology and sources ... | Military History | 1 | 0 | 0 | . 124 2018 | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | There is a dilemma at the heart of coercion. S... | IR Scholarship | 0 | 0 | 1 | . 125 2018 | Unexpected Humanitarians: Albania, the U.S. Mi... | Using archives and oral history, this disserta... | Military History | 0 | 0 | 0 | . 126 rows × 7 columns . df_subset.columns = [&quot;year&quot;, &quot;title&quot;, &quot;abstract&quot;, &quot;theme&quot;, &quot;War&quot;, &#39;Cold War&#39;,&quot;Trade&quot;] . df_subset . year title abstract theme War Cold War Trade . 0 2000 | &quot;Institutions at the Domestic/International Ne... | Civil-military relations are frequently studie... | IR scholarship | 1 | 0 | 0 | . 1 2000 | Born to Lose and Doomed to Survive: State Deat... | Under what conditions do states die, or exit t... | IR scholarship | 1 | 1 | 0 | . 2 2000 | The significance of “allegiance” in internatio... | My dissertation employs original and secondary... | IR scholarship | 1 | 0 | 0 | . 3 2000 | The significance of “allegiance” in internatio... | nThis study revises prevailing interpretation... | Conflit Between States | 0 | 1 | 0 | . 4 2000 | Truth-Telling and Mythmaking in Post-Soviet Ru... | Can distorted and pernicious ideas about histo... | Conflict Between States | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 121 2018 | Planning for the Short Haul: Trade Among Belli... | In times of war, why do belligerents continue ... | Conflict between states | 1 | 0 | 1 | . 122 2018 | Clinging to the Anti-Imperial Mantle: The Repu... | My dissertation project, Clinging to the Anti-... | Cold War | 0 | 1 | 0 | . 123 2018 | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Using a transnational methodology and sources ... | Military History | 1 | 0 | 0 | . 124 2018 | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | There is a dilemma at the heart of coercion. S... | IR Scholarship | 0 | 0 | 1 | . 125 2018 | Unexpected Humanitarians: Albania, the U.S. Mi... | Using archives and oral history, this disserta... | Military History | 0 | 0 | 0 | . 126 rows × 7 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///nf_nlp.db&#39;) connection = engine.connect() metadata = db.MetaData() . nf_nlp_table = db.Table(&#39;nf_nlp_table&#39;, metadata, db.Column(&#39;year&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;title&#39;,db.String, nullable=True), db.Column(&#39;abstract&#39;,db.String, nullable=True), db.Column(&#39;theme&#39;,db.String, nullable=True), db.Column(&#39;War&#39;,db.Integer, nullable=True), db.Column(&#39;Cold War&#39;,db.Integer, nullable=True), db.Column(&#39;Trade&#39;, db.Integer, nullable=True) ) . metadata.create_all(engine) #Creates the table . nf_nlp_table . Table(&#39;nf_nlp_table&#39;, MetaData(bind=None), Column(&#39;year&#39;, Integer(), table=&lt;nf_nlp_table&gt;), Column(&#39;title&#39;, String(), table=&lt;nf_nlp_table&gt;), Column(&#39;abstract&#39;, String(), table=&lt;nf_nlp_table&gt;), Column(&#39;theme&#39;, String(), table=&lt;nf_nlp_table&gt;), Column(&#39;War&#39;, Integer(), table=&lt;nf_nlp_table&gt;), Column(&#39;Cold War&#39;, Integer(), table=&lt;nf_nlp_table&gt;), Column(&#39;Trade&#39;, Integer(), table=&lt;nf_nlp_table&gt;), schema=None) . df_subset.to_sql(&#39;nf_nlp_table&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT year, theme, title FROM nf_nlp_table LIMIT 10&quot;).fetchall() . [(2000, &#39;IR scholarship&#39;, &#39;&#34;Institutions at the Domestic/International Nexus: the political-military origins of military effectiveness, strategic integration and war&#39;), (2000, &#39;IR scholarship&#39;, &#39;Born to Lose and Doomed to Survive: State Death and Survival in the International System&#39;), (2000, &#39;IR scholarship&#39;, &#39;The significance of “allegiance” in international relations&#39;), (2000, &#39;Conflit Between States&#39;, &#39;The significance of “allegiance” in international relations&#39;), (2000, &#39;Conflict Between States&#39;, &#39;Truth-Telling and Mythmaking in Post-Soviet Russia: Historical Ideas, Mass Education, and Interstate Conflict&#39;), (2000, &#39;Domestic Military History&#39;, &#39;Building a Cape Fear Metropolis: Fort Bragg, Fayetteville, and the Sandhills of North Carolina&#39;), (2000, &#39;Culture&#39;, &#39;The Glories and the Sadness: Shaping the national Memory of the First World War in Great Britain, Canada and Australia&#39;), (2000, &#39;Culture / Peace Process&#39;, &#39;What leads longstanding adversaries to engage in conflict resolution&#39;), (2001, &#39;Military History&#39;, &#39;A School for the Nation: Military Institutions and the Boundaries of Nationality&#39;), (2001, &#39;Military History&#39;, &#34;The &#39;American Century&#39; Army: The Origins of the U.S. Cold War Army, 1949-1959&#34;)] . sql = &quot;&quot;&quot; SELECT year , theme , title FROM nf_nlp_table &quot;&quot;&quot; cnxn = connection . df = pd.read_sql(sql, cnxn) . df.tail(30) . year theme title . 96 2014 | IR Scholarship | “Multiparty Mediation: Identifying Characteris... | . 97 2014 | IR Scholarship | The Justice Dilemma: International Criminal Ac... | . 98 2014 | IR Scholarship | Beyond Revolution and Repression: U.S. Foreign... | . 99 2014 | IR Scholarship | Protection States Trust?: Major Power Patronag... | . 100 2014 | Nuclear Weapons | The Constraining Power of the Nuclear Nonproli... | . 101 2015 | Military History | Selling Her the Military: Recruiting Women int... | . 102 2015 | IR Scholarship | American Evangelicals, Israel, and Modern Chri... | . 103 2015 | Non-state | Who Can Keep the Peace? Insurgent Organization... | . 104 2015 | IR Scholarship | Credibility in Crisis: The Role of Leadership ... | . 105 2015 | IR Scholarship | Evaluating the Changing of the Guards: Survey ... | . 106 2015 | Soviet Union | Extracting the Eagle’s Talons: The Soviet Unio... | . 107 2015 | IR Scholarship | The Control War: Communist Revolutionary Warfa... | . 108 2015 | Nuclear Weapons | Nuclear Weapons and Foreign Policy | . 109 2016 | Civ-Mil | Securing Control and Controlling Security: Civ... | . 110 2016 | Military History | Digging for Victory: The Stalinist State’s Mob... | . 111 2016 | Non-state | Persuading Power: Insurgent Diplomacy and the ... | . 112 2016 | Conflict between states | A Prelude to Violence? The Effect of Nationali... | . 113 2016 | Conflict between states | Engaging the ‘Evil Empire’: East – West Relati... | . 114 2017 | IR Scholarship | More Talk, Less Action: Why Costless Diplomacy... | . 115 2017 | Cold War | Experiments in Peace: Asian Neutralism, Human ... | . 116 2017 | IR Scholarship | Fully Committed? Religiously Committed State P... | . 117 2017 | Military History | Straddling the Threshold of Two Worlds: Soldie... | . 118 2017 | Military History | U.S. Army’s Investigation and Adjudication of ... | . 119 2017 | IR Scholarship | Grand Strategic Crucibles: The Lasting Effects... | . 120 2018 | Nuclear Weapons | Trust in International Politics: The Role of L... | . 121 2018 | Conflict between states | Planning for the Short Haul: Trade Among Belli... | . 122 2018 | Cold War | Clinging to the Anti-Imperial Mantle: The Repu... | . 123 2018 | Military History | The New Navy&#39;s Pacific Wars: Peripheral Confl... | . 124 2018 | IR Scholarship | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | . 125 2018 | Military History | Unexpected Humanitarians: Albania, the U.S. Mi... | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/06/NLP-sqlitedb.html",
            "relUrl": "/2020/10/06/NLP-sqlitedb.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post65": {
            "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . df = pd.read_csv(&#39;https://stocks-snp-500.herokuapp.com/stocks/stocks_table.csv?_size=max&#39;) . df . rowid Date MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . 0 1 | 2020-01-02 00:00:00.000000 | 158.779999 | 68.800003 | NaN | 112.980003 | . 1 2 | 2020-01-03 00:00:00.000000 | 158.320007 | 67.620003 | NaN | 112.190002 | . 2 3 | 2020-01-06 00:00:00.000000 | 157.080002 | 66.629997 | NaN | 112.589996 | . 3 4 | 2020-01-07 00:00:00.000000 | 159.320007 | 70.290001 | NaN | 112.290001 | . 4 5 | 2020-01-08 00:00:00.000000 | 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | ... | ... | . 258 259 | 2021-01-11 00:00:00.000000 | 218.470001 | 344.980011 | 295.000000 | 131.750000 | . 259 260 | 2021-01-12 00:00:00.000000 | 216.500000 | 333.200012 | 298.000000 | 131.800003 | . 260 261 | 2021-01-13 00:00:00.000000 | 214.020004 | 360.000000 | 295.000000 | 132.100006 | . 261 262 | 2021-01-14 00:00:00.000000 | 215.910004 | 371.000000 | 305.000000 | 131.619995 | . 262 263 | 2021-01-15 00:00:00.000000 | 213.520004 | 397.709991 | 306.820007 | 130.679993 | . 263 rows × 6 columns . start = pd.to_datetime(&#39;2020-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Open&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Volume&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f7f95b65f40&gt; . FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . stocks = pd.concat([MSFT_stock[&#39;Open&#39;], ZOOM_stock[&#39;Open&#39;], SNOW_stock[&#39;Open&#39;], FXAIX_stock[&#39;Open&#39;]], axis = 1) . stocks.reset_index(level=0, inplace=True) . stocks . Date Open Open Open Open . 0 2020-01-02 | 158.779999 | 68.800003 | NaN | 112.980003 | . 1 2020-01-03 | 158.320007 | 67.620003 | NaN | 112.190002 | . 2 2020-01-06 | 157.080002 | 66.629997 | NaN | 112.589996 | . 3 2020-01-07 | 159.320007 | 70.290001 | NaN | 112.290001 | . 4 2020-01-08 | 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | ... | . 258 2021-01-11 | 218.470001 | 344.980011 | 295.000000 | 131.750000 | . 259 2021-01-12 | 216.500000 | 333.200012 | 298.000000 | 131.800003 | . 260 2021-01-13 | 214.020004 | 360.000000 | 295.000000 | 132.100006 | . 261 2021-01-14 | 215.910004 | 371.000000 | 305.000000 | 131.619995 | . 262 2021-01-15 | 213.520004 | 397.709991 | 306.820007 | 130.679993 | . 263 rows × 5 columns . stocks.columns = [&#39;Date&#39;,&#39;MSFT_stock&#39;,&#39;ZOOM_stock&#39;,&#39;SNOW_stock&#39;,&#39;FXAIX_stock&#39;] . stocks . Date MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . 0 2020-01-02 | 158.779999 | 68.800003 | NaN | 112.980003 | . 1 2020-01-03 | 158.320007 | 67.620003 | NaN | 112.190002 | . 2 2020-01-06 | 157.080002 | 66.629997 | NaN | 112.589996 | . 3 2020-01-07 | 159.320007 | 70.290001 | NaN | 112.290001 | . 4 2020-01-08 | 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | ... | . 258 2021-01-11 | 218.470001 | 344.980011 | 295.000000 | 131.750000 | . 259 2021-01-12 | 216.500000 | 333.200012 | 298.000000 | 131.800003 | . 260 2021-01-13 | 214.020004 | 360.000000 | 295.000000 | 132.100006 | . 261 2021-01-14 | 215.910004 | 371.000000 | 305.000000 | 131.619995 | . 262 2021-01-15 | 213.520004 | 397.709991 | 306.820007 | 130.679993 | . 263 rows × 5 columns . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///stocks.db&#39;) connection = engine.connect() metadata = db.MetaData() . stocks_table = db.Table(&#39;stocks_table&#39;, metadata, db.Column(&#39;Date&#39;,db.Integer, nullable=True, index=False), db.Column(&#39;MSFT_stock&#39;,db.Integer, nullable=True), db.Column(&#39;ZOOM_stock&#39;,db.Integer, nullable=True), db.Column(&#39;SNOW_stock&#39;,db.Integer, nullable=True), db.Column(&#39;FXAIX_stock&#39;, db.Numeric, nullable=True) ) . metadata.create_all(engine) #Creates the table . stocks_table . Table(&#39;stocks_table&#39;, MetaData(bind=None), Column(&#39;Date&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;MSFT_stock&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;ZOOM_stock&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;SNOW_stock&#39;, Integer(), table=&lt;stocks_table&gt;), Column(&#39;FXAIX_stock&#39;, Numeric(), table=&lt;stocks_table&gt;), schema=None) . stocks.to_sql(&#39;stocks_table&#39;, con=engine, if_exists=&#39;append&#39;, index=False) . engine.execute(&quot;SELECT * FROM stocks_table LIMIT 10&quot;).fetchall() . [(&#39;2020-01-02 00:00:00.000000&#39;, 158.77999877929688, 68.80000305175781, None, 112.9800033569336), (&#39;2020-01-03 00:00:00.000000&#39;, 158.32000732421875, 67.62000274658203, None, 112.19000244140625), (&#39;2020-01-06 00:00:00.000000&#39;, 157.0800018310547, 66.62999725341797, None, 112.58999633789062), (&#39;2020-01-07 00:00:00.000000&#39;, 159.32000732421875, 70.29000091552734, None, 112.29000091552734), (&#39;2020-01-08 00:00:00.000000&#39;, 158.92999267578125, 71.80999755859375, None, 112.83999633789062), (&#39;2020-01-09 00:00:00.000000&#39;, 161.83999633789062, 73.98999786376953, None, 113.62000274658203), (&#39;2020-01-10 00:00:00.000000&#39;, 162.82000732421875, 73.08000183105469, None, 113.30000305175781), (&#39;2020-01-13 00:00:00.000000&#39;, 161.75999450683594, 73.88999938964844, None, 114.08999633789062), (&#39;2020-01-14 00:00:00.000000&#39;, 163.38999938964844, 74.31999969482422, None, 113.93000030517578), (&#39;2020-01-15 00:00:00.000000&#39;, 162.6199951171875, 73.27999877929688, None, 114.13999938964844)] . sql = &quot;&quot;&quot; SELECT DATE(date) AS DATE , FXAIX_stock , MSFT_stock , SNOW_stock , row_number() OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_NBR , COUNT(*) OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_CNT , CASE WHEN FXAIX_stock &gt;= 120 THEN &#39;SNP_High&#39; ELSE &#39;SNP_low&#39; END AS SNP_HIGH_LOW FROM stocks_table --WHERE FXAIX_stock &gt;= 120 &quot;&quot;&quot; cnxn = connection . stocks = pd.read_sql(sql, cnxn) . stocks.tail(30) . DATE FXAIX_stock MSFT_stock SNOW_stock REC_NBR REC_CNT SNP_HIGH_LOW . 233 2020-12-03 | 127.540001 | 214.610001 | 290.540009 | 1 | 1 | SNP_High | . 234 2020-12-04 | 128.669998 | 214.220001 | 335.399994 | 1 | 1 | SNP_High | . 235 2020-12-07 | 128.419998 | 214.369995 | 393.500000 | 1 | 1 | SNP_High | . 236 2020-12-08 | 128.789993 | 213.970001 | 388.799988 | 1 | 1 | SNP_High | . 237 2020-12-09 | 127.769997 | 215.160004 | 393.399994 | 1 | 1 | SNP_High | . 238 2020-12-10 | 127.610001 | 211.770004 | 362.000000 | 1 | 1 | SNP_High | . 239 2020-12-11 | 126.870003 | 210.050003 | 360.399994 | 1 | 1 | SNP_High | . 240 2020-12-14 | 126.339996 | 213.100006 | 352.489990 | 1 | 1 | SNP_High | . 241 2020-12-15 | 127.970001 | 215.169998 | 308.980011 | 1 | 1 | SNP_High | . 242 2020-12-16 | 128.199997 | 214.750000 | 328.429993 | 1 | 1 | SNP_High | . 243 2020-12-17 | 128.940002 | 219.869995 | 333.820007 | 1 | 1 | SNP_High | . 244 2020-12-18 | 128.500000 | 218.589996 | 332.769989 | 1 | 1 | SNP_High | . 245 2020-12-21 | 128.000000 | 217.550003 | 329.000000 | 1 | 1 | SNP_High | . 246 2020-12-22 | 127.750000 | 222.690002 | 349.890015 | 1 | 1 | SNP_High | . 247 2020-12-23 | 127.839996 | 223.110001 | 341.160004 | 1 | 1 | SNP_High | . 248 2020-12-24 | 128.309998 | 221.419998 | 334.100006 | 1 | 1 | SNP_High | . 249 2020-12-28 | 129.429993 | 224.449997 | 324.869995 | 1 | 1 | SNP_High | . 250 2020-12-29 | 129.139999 | 226.309998 | 305.250000 | 1 | 1 | SNP_High | . 251 2020-12-30 | 129.330002 | 225.229996 | 304.000000 | 1 | 1 | SNP_High | . 252 2020-12-31 | 130.169998 | 221.699997 | 299.700012 | 1 | 1 | SNP_High | . 253 2021-01-04 | 128.259995 | 222.529999 | 285.410004 | 1 | 1 | SNP_High | . 254 2021-01-05 | 129.179993 | 217.259995 | 280.619995 | 1 | 1 | SNP_High | . 255 2021-01-06 | 129.919998 | 212.169998 | 279.989990 | 1 | 1 | SNP_High | . 256 2021-01-07 | 131.880005 | 214.039993 | 272.589996 | 1 | 1 | SNP_High | . 257 2021-01-08 | 132.619995 | 218.679993 | 315.000000 | 1 | 1 | SNP_High | . 258 2021-01-11 | 131.750000 | 218.470001 | 295.000000 | 1 | 1 | SNP_High | . 259 2021-01-12 | 131.800003 | 216.500000 | 298.000000 | 1 | 1 | SNP_High | . 260 2021-01-13 | 132.100006 | 214.020004 | 295.000000 | 1 | 1 | SNP_High | . 261 2021-01-14 | 131.619995 | 215.910004 | 305.000000 | 1 | 1 | SNP_High | . 262 2021-01-15 | 130.679993 | 213.520004 | 306.820007 | 1 | 1 | SNP_High | . stocks[&#39;FXAIX_stock&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 in 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 in 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/05/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "relUrl": "/2020/10/05/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post66": {
            "title": "Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020 with pandas_datareader and writing to at sqlite database",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here . import pandas as pd import numpy as np import pandas_datareader.data as web import datetime import matplotlib.pyplot as plt %matplotlib inline . # # start = datetime.datetime(2016, 1, 1) # # end = datetime.datetime(2017, 5, 17) # start = datetime.datetime(2010, 1, 1) # end = datetime.datetime(2020, 1, 1) . start = pd.to_datetime(&#39;2020-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Open&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) FXAIX_stock[&#39;Volume&#39;].plot(label=&#39;SNP_500&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fae549b8ba8&gt; . FXAIX_stock = web.DataReader(&#39;FXAIX&#39;, &#39;yahoo&#39;, start, end) FXAIX_stock.head() MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() . High Low Open Close Volume Adj Close . Date . 2020-09-16 319.0 | 231.110001 | 245.000000 | 253.929993 | 36099700 | 253.929993 | . 2020-09-17 241.5 | 215.240005 | 230.759995 | 227.539993 | 11907500 | 227.539993 | . 2020-09-18 249.0 | 218.589996 | 235.000000 | 240.000000 | 7475400 | 240.000000 | . 2020-09-21 241.5 | 218.600006 | 230.000000 | 228.850006 | 5524900 | 228.850006 | . 2020-09-22 239.0 | 225.149994 | 238.500000 | 235.160004 | 3889100 | 235.160004 | . stocks = pd.concat([MSFT_stock[&#39;Open&#39;], ZOOM_stock[&#39;Open&#39;], SNOW_stock[&#39;Open&#39;], FXAIX_stock[&#39;Open&#39;]], axis = 1) . stocks . Open Open Open Open . Date . 2020-01-02 158.779999 | 68.800003 | NaN | 112.980003 | . 2020-01-03 158.320007 | 67.620003 | NaN | 112.190002 | . 2020-01-06 157.080002 | 66.629997 | NaN | 112.589996 | . 2020-01-07 159.320007 | 70.290001 | NaN | 112.290001 | . 2020-01-08 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | . 2020-09-28 210.880005 | 502.410004 | 235.929993 | 116.650002 | . 2020-09-29 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 2020-09-30 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 2020-10-01 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 2020-10-02 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 rows × 4 columns . stocks.columns = [&#39;MSFT_stock&#39;,&#39;ZOOM_stock&#39;,&#39;SNOW_stock&#39;,&#39;FXAIX_stock&#39;] . stocks . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 158.779999 | 68.800003 | NaN | 112.980003 | . 2020-01-03 158.320007 | 67.620003 | NaN | 112.190002 | . 2020-01-06 157.080002 | 66.629997 | NaN | 112.589996 | . 2020-01-07 159.320007 | 70.290001 | NaN | 112.290001 | . 2020-01-08 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | . 2020-09-28 210.880005 | 502.410004 | 235.929993 | 116.650002 | . 2020-09-29 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 2020-09-30 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 2020-10-01 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 2020-10-02 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 rows × 4 columns . mean_daily_ret = stocks.pct_change(1).mean() mean_daily_ret . MSFT_stock 0.001751 ZOOM_stock 0.011973 SNOW_stock -0.002546 FXAIX_stock 0.000440 dtype: float64 . stocks.pct_change(1).corr() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . MSFT_stock 1.000000 | 0.209041 | 0.661827 | 0.382807 | . ZOOM_stock 0.209041 | 1.000000 | 0.095052 | 0.127526 | . SNOW_stock 0.661827 | 0.095052 | 1.000000 | 0.292117 | . FXAIX_stock 0.382807 | 0.127526 | 0.292117 | 1.000000 | . stock_normed = stocks/stocks.iloc[0] stock_normed.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fae54a74a90&gt; . stock_daily_ret = stocks.pct_change(1) stock_daily_ret.head() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 NaN | NaN | NaN | NaN | . 2020-01-03 -0.002897 | -0.017151 | NaN | -0.006992 | . 2020-01-06 -0.007832 | -0.014641 | NaN | 0.003565 | . 2020-01-07 0.014260 | 0.054930 | NaN | -0.002664 | . 2020-01-08 -0.002448 | 0.021625 | NaN | 0.004898 | . log_ret = np.log(stocks / stocks.shift(1)) log_ret.head() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 NaN | NaN | NaN | NaN | . 2020-01-03 -0.002901 | -0.017300 | NaN | -0.007017 | . 2020-01-06 -0.007863 | -0.014749 | NaN | 0.003559 | . 2020-01-07 0.014160 | 0.053475 | NaN | -0.002668 | . 2020-01-08 -0.002451 | 0.021394 | NaN | 0.004886 | . log_ret.hist(bins = 100, figsize = (12, 6)); plt.tight_layout() . log_ret.describe().transpose() . count mean std min 25% 50% 75% max . MSFT_stock 190.0 | 0.001421 | 0.025752 | -0.087821 | -0.012115 | 0.004000 | 0.016980 | 0.081248 | . ZOOM_stock 190.0 | 0.010279 | 0.056461 | -0.142569 | -0.017014 | 0.011119 | 0.035968 | 0.368600 | . SNOW_stock 12.0 | -0.004386 | 0.063753 | -0.131433 | -0.033113 | 0.019477 | 0.034320 | 0.077728 | . FXAIX_stock 190.0 | 0.000144 | 0.024461 | -0.127150 | -0.007774 | 0.002806 | 0.010082 | 0.089894 | . log_ret.mean() * 252 . MSFT_stock 0.358130 ZOOM_stock 2.590236 SNOW_stock -1.105148 FXAIX_stock 0.036359 dtype: float64 . log_ret.cov() . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . MSFT_stock 0.000663 | 0.000323 | 0.001291 | 0.000245 | . ZOOM_stock 0.000323 | 0.003188 | 0.000290 | 0.000184 | . SNOW_stock 0.001291 | 0.000290 | 0.004064 | 0.000231 | . FXAIX_stock 0.000245 | 0.000184 | 0.000231 | 0.000598 | . # Set seed (optional) np.random.seed(101) # Stock Columns print(&#39;Stocks&#39;) print(stocks.columns) print(&#39; n&#39;) # Create Random Weights print(&#39;Creating Random Weights&#39;) weights = np.array(np.random.random(4)) print(weights) print(&#39; n&#39;) # Rebalance Weights print(&#39;Rebalance to sum to 1.0&#39;) weights = weights / np.sum(weights) print(weights) print(&#39; n&#39;) # Expected Return print(&#39;Expected Portfolio Return&#39;) exp_ret = np.sum(log_ret.mean() * weights) *252 print(exp_ret) print(&#39; n&#39;) # Expected Variance print(&#39;Expected Volatility&#39;) exp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) print(exp_vol) print(&#39; n&#39;) # Sharpe Ratio SR = exp_ret/exp_vol print(&#39;Sharpe Ratio&#39;) print(SR) . Stocks Index([&#39;MSFT_stock&#39;, &#39;ZOOM_stock&#39;, &#39;SNOW_stock&#39;, &#39;FXAIX_stock&#39;], dtype=&#39;object&#39;) Creating Random Weights [0.51639863 0.57066759 0.02847423 0.17152166] Rebalance to sum to 1.0 [0.40122278 0.44338777 0.02212343 0.13326603] Expected Portfolio Return 1.272564336318203 Expected Volatility 0.4864366288684257 Sharpe Ratio 2.6160948020680697 . num_ports = 15000 all_weights = np.zeros((num_ports, len(stocks.columns))) ret_arr = np.zeros(num_ports) vol_arr = np.zeros(num_ports) sharpe_arr = np.zeros(num_ports) for ind in range(num_ports): # Create Random Weights weights = np.array(np.random.random(4)) # Rebalance Weights weights = weights / np.sum(weights) # Save Weights all_weights[ind,:] = weights # Expected Return ret_arr[ind] = np.sum((log_ret.mean() * weights) *252) # Expected Variance vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) # Sharpe Ratio sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind] . sharpe_arr.max() . 2.8667995807841824 . sharpe_arr.argmax() . 5483 . all_weights[10619,:] . array([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01]) . max_sr_ret = ret_arr[1419] max_sr_vol = vol_arr[1419] . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add red dot for max SR plt.scatter(max_sr_vol, max_sr_ret, c = &#39;red&#39;, s = 50, edgecolors = &#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7fae54366048&gt; . def get_ret_vol_sr(weights): &quot;&quot;&quot; Takes in weights, returns array or return,volatility, sharpe ratio &quot;&quot;&quot; weights = np.array(weights) ret = np.sum(log_ret.mean() * weights) * 252 vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) sr = ret/vol return np.array([ret, vol, sr]) from scipy.optimize import minimize import numpy as np def neg_sharpe(weights): return get_ret_vol_sr(weights)[2] * -1 # Contraints def check_sum(weights): &#39;&#39;&#39; Returns 0 if sum of weights is 1.0 &#39;&#39;&#39; return np.sum(weights) - 1 # By convention of minimize function it should be a function that returns zero for conditions cons = ({&#39;type&#39; : &#39;eq&#39;, &#39;fun&#39;: check_sum}) # 0-1 bounds for each weight bounds = ((0, 1), (0, 1), (0, 1), (0, 1)) # Initial Guess (equal distribution) init_guess = [0.25, 0.25, 0.25, 0.25] # Sequential Least Squares opt_results = minimize(neg_sharpe, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) opt_results . fun: -2.8998675936504807 jac: array([-3.57061625e-04, 6.75618649e-05, 1.98669076e+00, 1.90789163e-01]) message: &#39;Optimization terminated successfully.&#39; nfev: 42 nit: 7 njev: 7 status: 0 success: True x: array([1.59222977e-01, 8.40777023e-01, 7.68699340e-16, 0.00000000e+00]) . opt_results.x get_ret_vol_sr(opt_results.x) . array([2.23483308, 0.77066728, 2.89986759]) . frontier_y = np.linspace(0, 0.3, 100) . def minimize_volatility(weights): return get_ret_vol_sr(weights)[1] frontier_volatility = [] for possible_return in frontier_y: # function for return cons = ({&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: check_sum}, {&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: lambda w: get_ret_vol_sr(w)[0] - possible_return}) result = minimize(minimize_volatility, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) frontier_volatility.append(result[&#39;fun&#39;]) . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add frontier line plt.plot(frontier_volatility, frontier_y, &#39;g--&#39;, linewidth = 3) . [&lt;matplotlib.lines.Line2D at 0x7fae542ed9e8&gt;] . stocks[&#39;FXAIX_stock&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 in 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 in 2020 Value&#39;) . import sqlalchemy as db from sqlalchemy import create_engine import sqlite3 import pandas as pd . stocks . MSFT_stock ZOOM_stock SNOW_stock FXAIX_stock . Date . 2020-01-02 158.779999 | 68.800003 | NaN | 112.980003 | . 2020-01-03 158.320007 | 67.620003 | NaN | 112.190002 | . 2020-01-06 157.080002 | 66.629997 | NaN | 112.589996 | . 2020-01-07 159.320007 | 70.290001 | NaN | 112.290001 | . 2020-01-08 158.929993 | 71.809998 | NaN | 112.839996 | . ... ... | ... | ... | ... | . 2020-09-28 210.880005 | 502.410004 | 235.929993 | 116.650002 | . 2020-09-29 209.350006 | 488.130005 | 255.000000 | 116.099998 | . 2020-09-30 207.729996 | 464.209991 | 261.500000 | 117.070000 | . 2020-10-01 213.490005 | 477.000000 | 255.250000 | 117.699997 | . 2020-10-02 208.000000 | 485.005005 | 232.440002 | 116.120003 | . 191 rows × 4 columns . engine = db.create_engine(&#39;sqlite:///stocks.sqlite&#39;) . connection = engine.connect() metadata = db.MetaData() . stocks.to_sql(&#39;stocks&#39;, con=engine, if_exists=&#39;append&#39;, index=True) . engine.execute(&quot;SELECT * FROM stocks LIMIT 10&quot;).fetchall() . [(158.77999877929688, 68.80000305175781, None, 112.9800033569336), (158.32000732421875, 67.62000274658203, None, 112.19000244140625), (157.0800018310547, 66.62999725341797, None, 112.58999633789062), (159.32000732421875, 70.29000091552734, None, 112.29000091552734), (158.92999267578125, 71.80999755859375, None, 112.83999633789062), (161.83999633789062, 73.98999786376953, None, 113.62000274658203), (162.82000732421875, 73.08000183105469, None, 113.30000305175781), (161.75999450683594, 73.88999938964844, None, 114.08999633789062), (163.38999938964844, 74.31999969482422, None, 113.93000030517578), (162.6199951171875, 73.27999877929688, None, 114.13999938964844)] . engine.execute(&quot;SELECT FXAIX_stock FROM stocks LIMIT 10&quot;).fetchall() . [(112.9800033569336,), (112.19000244140625,), (112.58999633789062,), (112.29000091552734,), (112.83999633789062,), (113.62000274658203,), (113.30000305175781,), (114.08999633789062,), (113.93000030517578,), (114.13999938964844,)] . # df = pd.DataFrame({&#39;name&#39; : [&#39;User 1&#39;, &#39;User 2&#39;, &#39;User 3&#39;]}) # df # df.to_sql(&#39;users&#39;, con=engine) # engine.execute(&quot;SELECT * FROM users&quot;).fetchall() .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/04/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "relUrl": "/2020/10/04/StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post67": {
            "title": "Friday links",
            "content": "Escaping strings in Bash using !:q / | . bash # This string ‘has single’ “and double” quotes and a $ bash !:q .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/02/links2.html",
            "relUrl": "/links/2020/10/02/links2.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post68": {
            "title": "Friday books",
            "content": "21 Lessons for the 21st Century/ . | Outliers: The Story of Success/ . | David and Goliath: Underdogs, Misfits, and the Art of Battling Giants / . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/books/2020/10/02/links.html",
            "relUrl": "/books/2020/10/02/links.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post69": {
            "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite",
            "content": "This post includes code adapted from SQLAlchemy Example. . import sqlalchemy as db import sqlite3 import pandas as pd . from sqlalchemy import Column, Integer, String, ForeignKey, create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import relationship, backref, sessionmaker, joinedload # For this example we will use an in-memory sqlite DB. # Let&#39;s also configure it to echo everything it does to the screen. engine = create_engine(&#39;sqlite:///:memory:&#39;, echo=True) . # The base class which our objects will be defined on. Base = declarative_base() # Our User object, mapped to the &#39;users&#39; table class User(Base): __tablename__ = &#39;users&#39; # Every SQLAlchemy table should have a primary key named &#39;id&#39; id = Column(Integer, primary_key=True) name = Column(String) fullname = Column(String) password = Column(String) # Lets us print out a user object conveniently. def __repr__(self): return &quot;&lt;User(name=&#39;%s&#39;, fullname=&#39;%s&#39;, password&#39;%s&#39;)&gt;&quot; % ( self.name, self.fullname, self.password) . # The Address object stores the addresses # of a user in the &#39;adressess&#39; table. class Address(Base): __tablename__ = &#39;addresses&#39; id = Column(Integer, primary_key=True) email_address = Column(String, nullable=False) # Since we have a 1:n relationship, we need to store a foreign key # to the users table. user_id = Column(Integer, ForeignKey(&#39;users.id&#39;)) # Defines the 1:n relationship between users and addresses. # Also creates a backreference which is accessible from a User object. user = relationship(&quot;User&quot;, backref=backref(&#39;addresses&#39;)) # Lets us print out an address object conveniently. def __repr__(self): return &quot;&lt;Address(email_address=&#39;%s&#39;)&gt;&quot; % self.email_address . # Create all tables by issuing CREATE TABLE commands to the DB. Base.metadata.create_all(engine) # Creates a new session to the database by using the engine we described. Session = sessionmaker(bind=engine) session = Session() # Let&#39;s create a user and add two e-mail addresses to that user. example_user = User(name=&#39;example&#39;, fullname=&#39;example last_name_example&#39;, password=&#39;examplepassword&#39;) example_user.addresses = [Address(email_address=&#39;example@gmail.com&#39;), Address(email_address=&#39;example@yahoo.com&#39;)] # Let&#39;s add the user and its addresses we&#39;ve created to the DB and commit. session.add(example_user) session.commit() # Now let&#39;s query the user that has the e-mail address ed@google.com # SQLAlchemy will construct a JOIN query automatically. user_by_email = session.query(User) .filter(Address.email_address==&#39;example@gmail.com&#39;) .first() . 2020-10-02 08:55:48,507 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(&#34;users&#34;) 2020-10-02 08:55:48,508 INFO sqlalchemy.engine.base.Engine () 2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(&#34;addresses&#34;) 2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine () 2020-10-02 08:55:48,513 INFO sqlalchemy.engine.base.Engine BEGIN (implicit) 2020-10-02 08:55:48,514 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name, fullname, password) VALUES (?, ?, ?) 2020-10-02 08:55:48,515 INFO sqlalchemy.engine.base.Engine (&#39;example&#39;, &#39;example last_name_example&#39;, &#39;examplepassword&#39;) 2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?) 2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine (&#39;example@gmail.com&#39;, 1) 2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?) 2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine (&#39;example@yahoo.com&#39;, 1) 2020-10-02 08:55:48,520 INFO sqlalchemy.engine.base.Engine COMMIT 2020-10-02 08:55:48,522 INFO sqlalchemy.engine.base.Engine BEGIN (implicit) 2020-10-02 08:55:48,523 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password FROM users, addresses WHERE addresses.email_address = ? LIMIT ? OFFSET ? 2020-10-02 08:55:48,524 INFO sqlalchemy.engine.base.Engine (&#39;example@gmail.com&#39;, 1, 0) . print(user_by_email) . &lt;User(name=&#39;example&#39;, fullname=&#39;example last_name_example&#39;, password&#39;examplepassword&#39;)&gt; . # This will cause an additional query by lazy loading from the DB. print(user_by_email.addresses) . 2020-10-02 08:55:53,081 INFO sqlalchemy.engine.base.Engine SELECT addresses.id AS addresses_id, addresses.email_address AS addresses_email_address, addresses.user_id AS addresses_user_id FROM addresses WHERE ? = addresses.user_id 2020-10-02 08:55:53,083 INFO sqlalchemy.engine.base.Engine (1,) [&lt;Address(email_address=&#39;example@gmail.com&#39;)&gt;, &lt;Address(email_address=&#39;example@yahoo.com&#39;)&gt;] . # To avoid querying again when getting all addresses of a user, # we use the joinedload option. SQLAlchemy will load all results and hide # the duplicate entries from us, so we can then get for # the user&#39;s addressess without an additional query to the DB. user_by_email = session.query(User) .filter(Address.email_address==&#39;example@gmail.com&#39;) .options(joinedload(User.addresses)) .first() . 2020-10-02 08:56:04,305 INFO sqlalchemy.engine.base.Engine SELECT anon_1.users_id AS anon_1_users_id, anon_1.users_name AS anon_1_users_name, anon_1.users_fullname AS anon_1_users_fullname, anon_1.users_password AS anon_1_users_password, addresses_1.id AS addresses_1_id, addresses_1.email_address AS addresses_1_email_address, addresses_1.user_id AS addresses_1_user_id FROM (SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password FROM users, addresses WHERE addresses.email_address = ? LIMIT ? OFFSET ?) AS anon_1 LEFT OUTER JOIN addresses AS addresses_1 ON anon_1.users_id = addresses_1.user_id 2020-10-02 08:56:04,306 INFO sqlalchemy.engine.base.Engine (&#39;example@gmail.com&#39;, 1, 0) . print(user_by_email) . &lt;User(name=&#39;example&#39;, fullname=&#39;example last_name_example&#39;, password&#39;examplepassword&#39;)&gt; . print(user_by_email.addresses) . [&lt;Address(email_address=&#39;example@gmail.com&#39;)&gt;, &lt;Address(email_address=&#39;example@yahoo.com&#39;)&gt;] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/02/databases_sqllite_sqlalchemy_27-Copy1.html",
            "relUrl": "/2020/10/02/databases_sqllite_sqlalchemy_27-Copy1.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post70": {
            "title": "Thursday Links",
            "content": "Test-Driven Data Science Development - From TWD. Using pytest. . | Top 10 Essential Data Science Topics to Real-World Application from the Industry Perspectives - from the BBC. Review of Data Science and Causal inference. . | Parser for Kindle My Clippings.txt file . | Forecasting Newsletter. . | High Replicability of Newly-Discovered Social-behavioral Findings is Achievable - ‘Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal’ . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/01/links.html",
            "relUrl": "/links/2020/10/01/links.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post71": {
            "title": "Stock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_YIELD_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-31 1.98 | . 2010-02-28 2.03 | . 2010-03-31 1.90 | . 2010-04-30 1.83 | . 2010-05-31 1.95 | . ... ... | . 2020-07-01 1.91 | . 2020-07-31 1.86 | . 2020-08-31 1.76 | . 2020-09-01 1.69 | . 2020-09-30 1.69 | . 136 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_PE_RATIO_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 20.70 | . 2010-02-01 18.91 | . 2010-03-01 18.91 | . 2010-04-01 19.01 | . 2010-05-01 17.30 | . ... ... | . 2020-07-01 27.57 | . 2020-07-31 28.12 | . 2020-08-01 29.16 | . 2020-08-31 30.09 | . 2020-09-01 30.32 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_PE_RATIO_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 20.70 | . 2010-02-01 18.91 | . 2010-03-01 18.91 | . 2010-04-01 19.01 | . 2010-05-01 17.30 | . ... ... | . 2020-07-01 27.57 | . 2020-07-31 28.12 | . 2020-08-01 29.16 | . 2020-08-31 30.09 | . 2020-09-01 30.32 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_EARNINGS_YIELD_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 4.83 | . 2010-02-01 5.29 | . 2010-03-01 5.29 | . 2010-04-01 5.26 | . 2010-05-01 5.78 | . ... ... | . 2020-07-01 3.63 | . 2020-07-31 3.56 | . 2020-08-01 3.43 | . 2020-08-31 3.32 | . 2020-09-01 3.30 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_INFLADJ_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 1343.51 | . 2010-02-01 1302.03 | . 2010-03-01 1371.58 | . 2010-04-01 1423.00 | . 2010-05-01 1336.08 | . ... ... | . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-31 26.59 | . 2010-02-28 26.38 | . 2010-03-31 26.08 | . 2010-04-30 26.09 | . 2010-05-31 26.12 | . ... ... | . 2020-02-29 59.23 | . 2020-03-31 59.81 | . 2020-04-30 60.25 | . 2020-05-31 60.28 | . 2020-06-30 59.99 | . 126 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_YEAR&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-12-31 26.87 | . 2011-12-31 30.34 | . 2012-12-31 35.26 | . 2013-12-31 38.90 | . 2014-12-31 43.52 | . 2015-12-31 47.53 | . 2016-12-31 49.05 | . 2017-12-31 51.43 | . 2018-12-31 55.43 | . 2019-03-31 55.35 | . 2019-06-30 56.17 | . 2019-09-30 57.32 | . 2019-12-31 58.72 | . 2020-03-31 59.18 | . 2020-06-30 59.99 | . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_GROWTH_YEAR&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-12-31 1.45 | . 2011-12-31 16.26 | . 2012-12-31 18.25 | . 2013-12-31 11.99 | . 2014-12-31 12.72 | . 2015-12-31 10.00 | . 2016-12-31 5.33 | . 2017-12-31 7.07 | . 2018-12-31 9.84 | . 2019-03-31 9.87 | . 2019-06-30 9.98 | . 2019-09-30 9.32 | . 2019-12-31 8.36 | . 2020-03-31 8.45 | . 2020-06-30 6.43 | . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_GROWTH_QUARTER&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-03-31 -19.63 | . 2010-06-30 -13.90 | . 2010-09-30 -6.48 | . 2010-12-31 1.45 | . 2011-03-31 6.97 | . 2011-06-30 10.46 | . 2011-09-30 12.65 | . 2011-12-31 16.26 | . 2012-03-31 16.74 | . 2012-06-30 16.35 | . 2012-09-30 17.51 | . 2012-12-31 18.25 | . 2013-03-31 17.40 | . 2013-06-30 17.47 | . 2013-09-30 16.27 | . 2013-12-31 11.99 | . 2014-03-31 12.82 | . 2014-06-30 12.37 | . 2014-09-30 11.89 | . 2014-12-31 12.72 | . 2015-03-31 12.64 | . 2015-06-30 11.67 | . 2015-09-30 10.43 | . 2015-12-31 10.00 | . 2016-03-31 7.52 | . 2016-06-30 6.51 | . 2016-09-30 5.92 | . 2016-12-31 5.33 | . 2017-03-31 5.71 | . 2017-06-30 6.21 | . 2017-09-30 6.99 | . 2017-12-31 7.07 | . 2018-03-31 7.81 | . 2018-06-30 7.99 | . 2018-09-30 8.65 | . 2018-12-31 9.84 | . 2019-03-31 9.87 | . 2019-06-30 9.98 | . 2019-09-30 9.32 | . 2019-12-31 8.36 | . 2020-03-31 8.45 | . 2020-06-30 6.43 | . SP500.head() . Value . Date . 2010-01-01 1123.58 | . 2010-02-01 1089.16 | . 2010-03-01 1152.05 | . 2010-04-01 1197.32 | . 2010-05-01 1125.06 | . SP500.tail() . Value . Date . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . import matplotlib.pyplot as plt %matplotlib inline . SP500[&#39;Value&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/01/StockMarketPortfolioAnaylsis_snp+.html",
            "relUrl": "/2020/10/01/StockMarketPortfolioAnaylsis_snp+.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post72": {
            "title": "Wednesday Links",
            "content": "Artificial Intelligence, Algorithmic Pricing, and Collusion - From American Economic Review. Abstract - ‘Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty.’ . | 北極之最：格陵蘭史帕特冰川大片冰舌脫落 後果多嚴重 - from the BBC. . | Vì sao Ròm trở thành hiện tượng phòng vé nhưng lại gây chia rẽ khán giả? - From BBC. . | Ròm - From IMDB. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/30/links.html",
            "relUrl": "/links/2020/09/30/links.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post73": {
            "title": "Stock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) . SP500.head() . Value . Date . 2010-01-01 1123.58 | . 2010-02-01 1089.16 | . 2010-03-01 1152.05 | . 2010-04-01 1197.32 | . 2010-05-01 1125.06 | . SP500.tail() . Value . Date . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . import matplotlib.pyplot as plt %matplotlib inline . SP500[&#39;Value&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/30/StockMarketPortfolioAnaylsis_snp.html",
            "relUrl": "/2020/09/30/StockMarketPortfolioAnaylsis_snp.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post74": {
            "title": "Tuesday Links",
            "content": "A Satellite Account for Health in the United States - From NBER. The paper ‘measures the change in medical spending and health outcomes for a comprehensive set of 80 conditions’ . | 全球暖化下的西伯利亞：BBC採訪團隊在地見聞 - from the BBC. . | Dùng sầu riêng và mít để sạc điện thoại - From BBC. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/29/links.html",
            "relUrl": "/links/2020/09/29/links.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post75": {
            "title": "Monday Links",
            "content": "澳大利亞也可以看到絢麗的極光 - from the BBC. . | Does Machine Translation Affect International Trade? Evidence from a Large Digital Platform - From NBER. . | 3.How to digitize your lab notebooks - From Nature. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/28/links.html",
            "relUrl": "/links/2020/09/28/links.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post76": {
            "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite3",
            "content": "This post includes code adapted from these sqlalchemy and sqlite gists and the sqlite3 documentation. . import sqlalchemy as db import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///music.sqlite&#39;) . connection = engine.connect() metadata = db.MetaData() music = db.Table(&#39;music&#39;, metadata, db.Column(&#39;Id&#39;, db.Integer()), db.Column(&#39;song&#39;, db.String(255), nullable=False), db.Column(&#39;album&#39;, db.String(255), nullable=False), db.Column(&#39;artist&#39;, db.String(255), nullable=False) ) metadata.create_all(engine) . #Inserting one record query = db.insert(music).values(Id=1, song=&#39;song3&#39;, album=&#39;album3&#39;, artist=&#39;artist3&#39;) ResultProxy = connection.execute(query) . #Inserting many records query = db.insert(music) values_list = [{&#39;Id&#39;:&#39;2&#39;, &#39;song&#39;:&#39;song1&#39;, &#39;album&#39;:&#39;album1&#39;, &#39;artist&#39;:&#39;artist1&#39;}, {&#39;Id&#39;:&#39;3&#39;, &#39;song&#39;:&#39;song2&#39;, &#39;album&#39;:&#39;album2&#39;, &#39;artist&#39;:&#39;artist2&#39;}] ResultProxy = connection.execute(query,values_list) results = connection.execute(db.select([music])).fetchall() df = pd.DataFrame(results) df.columns = results[0].keys() df.head(10) . Id song album artist . 0 1 | song3 | album3 | artist3 | . 1 2 | song1 | album1 | artist1 | . 2 3 | song2 | album2 | artist2 | . 3 2 | song1 | album1 | artist1 | . 4 3 | song2 | album2 | artist2 | . results = connection.execute(db.select([music])).fetchall() df = pd.DataFrame(results) df.columns = results[0].keys() df.head(4) . query = db.select([music]).where(db.and_(music.columns.song == &#39;song3&#39;, music.columns.artist == &#39;artist3&#39;)) result = connection.execute(query).fetchall() result[:3] . [(1, &#39;song3&#39;, &#39;album3&#39;, &#39;artist3&#39;)] . conn = sqlite3.connect(&#39;music.sqlite&#39;) . c = conn.cursor() # Create table c.execute(&#39;&#39;&#39;CREATE TABLE stockmarket (date text, trans text, symbol text, qty real, price real)&#39;&#39;&#39;) # Insert a row of data c.execute(&quot;INSERT INTO stockmarket VALUES (&#39;2006-01-05&#39;,&#39;BUY&#39;,&#39;RHAT&#39;,100,35.14)&quot;) # Save (commit) the changes conn.commit() # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn.close() . conn = sqlite3.connect(&#39;music.sqlite&#39;) c = conn.cursor() . symbol = &#39;RHAT&#39; c.execute(&quot;SELECT * FROM stockmarket WHERE symbol = &#39;%s&#39;&quot; % symbol) . &lt;sqlite3.Cursor at 0x7f0098ae0180&gt; . t = (&#39;RHAT&#39;,) c.execute(&#39;SELECT * FROM stockmarket WHERE symbol=?&#39;, t) print(c.fetchone()) . (&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14) . # Larger example that inserts many records at a time purchases = [(&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000, 45.00), (&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000, 72.00), (&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500, 53.00), ] c.executemany(&#39;INSERT INTO stockmarket VALUES (?,?,?,?,?)&#39;, purchases) . &lt;sqlite3.Cursor at 0x7f0098ae0180&gt; . for row in c.execute(&#39;SELECT * FROM stockmarket ORDER BY price&#39;): print(row) . (&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14) (&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000.0, 45.0) (&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500.0, 53.0) (&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000.0, 72.0) . # Use dbeaver to examine .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/28/databases_sqllite_sqlalchemy_27.html",
            "relUrl": "/2020/09/28/databases_sqllite_sqlalchemy_27.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post77": {
            "title": "What I've been reading.",
            "content": "Gödel, Escher, Bach - | 21 Lessons for the 21st Century | Grit: The Power of Passion and Perseverance Hardcover | The notebooks for The possessed- Published 1968 by University of Chicago Press. Found at local used book store. |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/books/2020/09/27/reading.html",
            "relUrl": "/books/2020/09/27/reading.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post78": {
            "title": "Sat Links",
            "content": "What Is Your List of 10 Challenges in Data Science? - from the HDSR. . | What’s Wrong with Social Science and How to Fix It - ‘surely notice that all you have is a n=23, p=0.049 three-way interaction effect (one of dozens you tested, and with no multiple testing adjustments of course’ . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/26/links.html",
            "relUrl": "/links/2020/09/26/links.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post79": {
            "title": "Friday Links",
            "content": "What quantum computers reveal about innovation - from the Economist. . | The Past, Present, and (Near) Future of Gene Therapy and Gene Editing - from NEJM . | 中国首富换人做 农夫山泉钟睒睒登顶富豪榜 - 半小时的首富 - 来自BBC。 . | Messi bị kéo vào vụ chỉ trích tuyển Argentina - Vnexpress. . | Earth, Wind &amp; Fire - September . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/25/links.html",
            "relUrl": "/links/2020/09/25/links.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post80": {
            "title": "Analyzing Size of Armed Forces From 1947 - 1963 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.longley.load_pandas().data #print(sm.datasets.longley.NOTE) . df.head() . TOTEMP GNPDEFL GNP UNEMP ARMED POP YEAR . 0 60323.0 | 83.0 | 234289.0 | 2356.0 | 1590.0 | 107608.0 | 1947.0 | . 1 61122.0 | 88.5 | 259426.0 | 2325.0 | 1456.0 | 108632.0 | 1948.0 | . 2 60171.0 | 88.2 | 258054.0 | 3682.0 | 1616.0 | 109773.0 | 1949.0 | . 3 61187.0 | 89.5 | 284599.0 | 3351.0 | 1650.0 | 110929.0 | 1950.0 | . 4 63221.0 | 96.2 | 328975.0 | 2099.0 | 3099.0 | 112075.0 | 1951.0 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1947&#39;, &#39;1962&#39;)) df.index = index df.head() . TOTEMP GNPDEFL GNP UNEMP ARMED POP YEAR . 1947-12-31 60323.0 | 83.0 | 234289.0 | 2356.0 | 1590.0 | 107608.0 | 1947.0 | . 1948-12-31 61122.0 | 88.5 | 259426.0 | 2325.0 | 1456.0 | 108632.0 | 1948.0 | . 1949-12-31 60171.0 | 88.2 | 258054.0 | 3682.0 | 1616.0 | 109773.0 | 1949.0 | . 1950-12-31 61187.0 | 89.5 | 284599.0 | 3351.0 | 1650.0 | 110929.0 | 1950.0 | . 1951-12-31 63221.0 | 96.2 | 328975.0 | 2099.0 | 3099.0 | 112075.0 | 1951.0 | . df[&#39;ARMED&#39;].plot() plt.ylabel(&quot;ARMED&quot;) . Text(0, 0.5, &#39;ARMED&#39;) . # unpacking cycle, trend = sm.tsa.filters.hpfilter(df.ARMED) cycle . 1947-12-31 -497.642333 1948-12-31 -713.661033 1949-12-31 -635.368706 1950-12-31 -682.008289 1951-12-31 688.574390 1952-12-31 1108.959755 1953-12-31 992.297873 1954-12-31 731.045710 1955-12-31 370.040046 1956-12-31 124.660757 1957-12-31 15.056446 1958-12-31 -193.702199 1959-12-31 -324.553899 1960-12-31 -407.316313 1961-12-31 -393.604252 1962-12-31 -182.777954 Name: ARMED, dtype: float64 . type(cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = trend df[[&#39;trend&#39;,&#39;ARMED&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;ARMED&#39;]][&quot;1950-01-01&quot;:&quot;1955-01-01&quot;].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2d5af13940&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/25/AnalyzingSizeofArmedForces.html",
            "relUrl": "/2020/09/25/AnalyzingSizeofArmedForces.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post81": {
            "title": "Analyzing US Real Interest Rate From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;realint&#39;].plot() plt.ylabel(&quot;realint&quot;) . Text(0, 0.5, &#39;realint&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking cycle, trend = sm.tsa.filters.hpfilter(df.realint) cycle . 1959-03-31 -1.195751 1959-06-30 -0.505792 1959-09-30 -0.205086 1959-12-31 2.717430 1960-03-31 -0.197051 ... 2008-09-30 4.330269 2008-12-31 8.961987 2009-03-31 -0.596183 2009-06-30 -3.008487 2009-09-30 -3.188797 Name: realint, Length: 203, dtype: float64 . type(cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = trend df[[&#39;trend&#39;,&#39;realint&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;realint&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2a8100438&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/24/AnalyzingUSRealInterestRate.html",
            "relUrl": "/2020/09/24/AnalyzingUSRealInterestRate.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post82": {
            "title": "Analyzing US Inflation From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;infl&#39;].plot() plt.ylabel(&quot;infl&quot;) . Text(0, 0.5, &#39;infl&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking infl_cycle, infl_trend = sm.tsa.filters.hpfilter(df.infl) infl_cycle . 1959-03-31 -1.206811 1959-06-30 1.141499 1959-09-30 1.550564 1959-12-31 -0.909577 1960-03-31 1.140149 ... 2008-09-30 -5.064733 2008-12-31 -10.550048 2009-03-31 -0.681429 2009-06-30 1.883255 2009-09-30 2.206560 Name: infl, Length: 203, dtype: float64 . type(infl_cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = infl_trend df[[&#39;trend&#39;,&#39;infl&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;infl&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fec38616be0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/23/AnalyzingUSInflation.html",
            "relUrl": "/2020/09/23/AnalyzingUSInflation.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post83": {
            "title": "Analyzing US Unemployment From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;unemp&#39;].plot() plt.ylabel(&quot;unemp&quot;) . Text(0, 0.5, &#39;unemp&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking unemp_cycle, unemp_trend = sm.tsa.filters.hpfilter(df.unemp) unemp_cycle . 1959-03-31 0.011338 1959-06-30 -0.702548 1959-09-30 -0.516441 1959-12-31 -0.229910 1960-03-31 -0.642198 ... 2008-09-30 -0.481666 2008-12-31 0.198598 2009-03-31 1.171440 2009-06-30 2.040247 2009-09-30 2.207674 Name: unemp, Length: 203, dtype: float64 . type(unemp_cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = unemp_trend df[[&#39;trend&#39;,&#39;unemp&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;unemp&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdb5c2ecdd8&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/22/AnalyzingUSUnemployment.html",
            "relUrl": "/2020/09/22/AnalyzingUSUnemployment.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post84": {
            "title": "Stock Market and Optimal Portfolio Anaylsis scipy and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import numpy as np import quandl %matplotlib inline . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . # Grabbing a bunch of tech stocks for our portfolio COST = quandl.get(&#39;WIKI/COST.11&#39;, start_date = start, end_date = end) NLSN = quandl.get(&#39;WIKI/NLSN.11&#39;, start_date = start, end_date = end) NKE = quandl.get(&#39;WIKI/NKE.11&#39;, start_date = start, end_date = end) DIS = quandl.get(&#39;WIKI/DIS.11&#39;, start_date = start, end_date = end) . stocks = pd.concat([COST, NLSN, NKE, DIS], axis = 1) stocks.columns = [&#39;COST&#39;,&#39;NLSN&#39;,&#39;NKE&#39;,&#39;DIS&#39;] . stocks . COST NLSN NKE DIS . Date . 2010-01-04 49.085078 | NaN | 14.751122 | 28.960651 | . 2010-01-05 48.936361 | NaN | 14.809811 | 28.888407 | . 2010-01-06 49.572542 | NaN | 14.719521 | 28.734890 | . 2010-01-07 49.332941 | NaN | 14.863985 | 28.743920 | . 2010-01-08 48.977671 | NaN | 14.834641 | 28.789072 | . ... ... | ... | ... | ... | . 2018-03-21 186.070000 | 32.44 | 66.350000 | 101.820000 | . 2018-03-22 182.640000 | 31.82 | 64.420000 | 100.600000 | . 2018-03-23 180.840000 | 31.51 | 64.630000 | 98.540000 | . 2018-03-26 187.220000 | 32.03 | 65.900000 | 100.650000 | . 2018-03-27 183.150000 | 32.09 | 66.170000 | 99.360000 | . 2071 rows × 4 columns . mean_daily_ret = stocks.pct_change(1).mean() mean_daily_ret . COST 0.000699 NLSN 0.000312 NKE 0.000833 DIS 0.000683 dtype: float64 . stocks.pct_change(1).corr() . COST NLSN NKE DIS . COST 1.000000 | 0.265003 | 0.370978 | 0.415377 | . NLSN 0.265003 | 1.000000 | 0.312192 | 0.392808 | . NKE 0.370978 | 0.312192 | 1.000000 | 0.446150 | . DIS 0.415377 | 0.392808 | 0.446150 | 1.000000 | . stocks.head() . COST NLSN NKE DIS . Date . 2010-01-04 49.085078 | NaN | 14.751122 | 28.960651 | . 2010-01-05 48.936361 | NaN | 14.809811 | 28.888407 | . 2010-01-06 49.572542 | NaN | 14.719521 | 28.734890 | . 2010-01-07 49.332941 | NaN | 14.863985 | 28.743920 | . 2010-01-08 48.977671 | NaN | 14.834641 | 28.789072 | . stock_normed = stocks/stocks.iloc[0] stock_normed.plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . stock_daily_ret = stocks.pct_change(1) stock_daily_ret.head() . COST NLSN NKE DIS . Date . 2010-01-04 NaN | NaN | NaN | NaN | . 2010-01-05 -0.003030 | NaN | 0.003979 | -0.002495 | . 2010-01-06 0.013000 | NaN | -0.006097 | -0.005314 | . 2010-01-07 -0.004833 | NaN | 0.009814 | 0.000314 | . 2010-01-08 -0.007201 | NaN | -0.001974 | 0.001571 | . log_ret = np.log(stocks / stocks.shift(1)) log_ret.head() . COST NLSN NKE DIS . Date . 2010-01-04 NaN | NaN | NaN | NaN | . 2010-01-05 -0.003034 | NaN | 0.003971 | -0.002498 | . 2010-01-06 0.012916 | NaN | -0.006115 | -0.005328 | . 2010-01-07 -0.004845 | NaN | 0.009767 | 0.000314 | . 2010-01-08 -0.007228 | NaN | -0.001976 | 0.001570 | . log_ret.hist(bins = 100, figsize = (12, 6)); plt.tight_layout() . log_ret.describe().transpose() . count mean std min 25% 50% 75% max . COST 2068.0 | 0.000633 | 0.011172 | -0.083110 | -0.005293 | 0.000413 | 0.006618 | 0.060996 | . NLSN 1801.0 | 0.000198 | 0.015121 | -0.185056 | -0.007131 | 0.000000 | 0.008051 | 0.095201 | . NKE 2070.0 | 0.000725 | 0.014682 | -0.098743 | -0.006602 | 0.000656 | 0.008155 | 0.115342 | . DIS 2070.0 | 0.000596 | 0.013220 | -0.096190 | -0.005710 | 0.000776 | 0.007453 | 0.073531 | . log_ret.mean() * 252 . COST 0.159439 NLSN 0.049979 NKE 0.182719 DIS 0.150081 dtype: float64 . # Compute pairwise covariance of columns log_ret.cov() . COST NLSN NKE DIS . COST 0.000125 | 0.000045 | 0.000061 | 0.000061 | . NLSN 0.000045 | 0.000229 | 0.000070 | 0.000077 | . NKE 0.000061 | 0.000070 | 0.000216 | 0.000087 | . DIS 0.000061 | 0.000077 | 0.000087 | 0.000175 | . # Set seed (optional) np.random.seed(101) # Stock Columns print(&#39;Stocks&#39;) print(stocks.columns) print(&#39; n&#39;) # Create Random Weights print(&#39;Creating Random Weights&#39;) weights = np.array(np.random.random(4)) print(weights) print(&#39; n&#39;) # Rebalance Weights print(&#39;Rebalance to sum to 1.0&#39;) weights = weights / np.sum(weights) print(weights) print(&#39; n&#39;) # Expected Return print(&#39;Expected Portfolio Return&#39;) exp_ret = np.sum(log_ret.mean() * weights) *252 print(exp_ret) print(&#39; n&#39;) # Expected Variance print(&#39;Expected Volatility&#39;) exp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) print(exp_vol) print(&#39; n&#39;) # Sharpe Ratio SR = exp_ret/exp_vol print(&#39;Sharpe Ratio&#39;) print(SR) . Stocks Index([&#39;COST&#39;, &#39;NLSN&#39;, &#39;NKE&#39;, &#39;DIS&#39;], dtype=&#39;object&#39;) Creating Random Weights [0.51639863 0.57066759 0.02847423 0.17152166] Rebalance to sum to 1.0 [0.40122278 0.44338777 0.02212343 0.13326603] Expected Portfolio Return 0.11017373023155777 Expected Volatility 0.16110487214223854 Sharpe Ratio 0.6838634286260817 . num_ports = 15000 all_weights = np.zeros((num_ports, len(stocks.columns))) ret_arr = np.zeros(num_ports) vol_arr = np.zeros(num_ports) sharpe_arr = np.zeros(num_ports) for ind in range(num_ports): # Create Random Weights weights = np.array(np.random.random(4)) # Rebalance Weights weights = weights / np.sum(weights) # Save Weights all_weights[ind,:] = weights # Expected Return ret_arr[ind] = np.sum((log_ret.mean() * weights) *252) # Expected Variance vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) # Sharpe Ratio sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind] . sharpe_arr.max() . 1.042687299617254 . sharpe_arr.argmax() . 10619 . all_weights[10619,:] . array([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01]) . max_sr_ret = ret_arr[1419] max_sr_vol = vol_arr[1419] . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add red dot for max SR plt.scatter(max_sr_vol, max_sr_ret, c = &#39;red&#39;, s = 50, edgecolors = &#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7f703b26fd00&gt; . def get_ret_vol_sr(weights): &quot;&quot;&quot; Takes in weights, returns array or return,volatility, sharpe ratio &quot;&quot;&quot; weights = np.array(weights) ret = np.sum(log_ret.mean() * weights) * 252 vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) sr = ret/vol return np.array([ret, vol, sr]) from scipy.optimize import minimize import numpy as np def neg_sharpe(weights): return get_ret_vol_sr(weights)[2] * -1 # Contraints def check_sum(weights): &#39;&#39;&#39; Returns 0 if sum of weights is 1.0 &#39;&#39;&#39; return np.sum(weights) - 1 # By convention of minimize function it should be a function that returns zero for conditions cons = ({&#39;type&#39; : &#39;eq&#39;, &#39;fun&#39;: check_sum}) # 0-1 bounds for each weight bounds = ((0, 1), (0, 1), (0, 1), (0, 1)) # Initial Guess (equal distribution) init_guess = [0.25, 0.25, 0.25, 0.25] # Sequential Least Squares opt_results = minimize(neg_sharpe, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) opt_results . fun: -1.0442236428192482 jac: array([-1.85623765e-04, 3.00063133e-01, 3.43203545e-04, 1.72853470e-05]) message: &#39;Optimization terminated successfully&#39; nfev: 20 nit: 4 njev: 4 status: 0 success: True x: array([0.53438392, 0. , 0.27969302, 0.18592306]) . opt_results.x get_ret_vol_sr(opt_results.x) . array([0.16421049, 0.15725605, 1.04422364]) . frontier_y = np.linspace(0, 0.3, 100) . def minimize_volatility(weights): return get_ret_vol_sr(weights)[1] frontier_volatility = [] for possible_return in frontier_y: # function for return cons = ({&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: check_sum}, {&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: lambda w: get_ret_vol_sr(w)[0] - possible_return}) result = minimize(minimize_volatility, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) frontier_volatility.append(result[&#39;fun&#39;]) . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add frontier line plt.plot(frontier_volatility, frontier_y, &#39;g--&#39;, linewidth = 3) . [&lt;matplotlib.lines.Line2D at 0x7f703b1949a0&gt;] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/21/StockMarketPortfolioAnaylsis2.html",
            "relUrl": "/2020/09/21/StockMarketPortfolioAnaylsis2.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post85": {
            "title": "Stock Market and Portfolio Anaylsis with pandas_datareader and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd from pandas_datareader import data, wb import datetime . start = pd.to_datetime(&#39;2020-02-04&#39;) end = pd.to_datetime(&#39;today&#39;) MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f557129fa90&gt; . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . # Grabbing a bunch of tech stocks for our portfolio COST = quandl.get(&#39;WIKI/COST.11&#39;, start_date = start, end_date = end) NLSN = quandl.get(&#39;WIKI/NLSN.11&#39;, start_date = start, end_date = end) NKE = quandl.get(&#39;WIKI/NKE.11&#39;, start_date = start, end_date = end) DIS = quandl.get(&#39;WIKI/DIS.11&#39;, start_date = start, end_date = end) # Example COST.iloc[0][&#39;Adj. Close&#39;] for stock_df in (COST, NLSN, NKE, DIS): stock_df[&#39;Normed Return&#39;] = stock_df[&#39;Adj. Close&#39;] / stock_df.iloc[0][&#39;Adj. Close&#39;] COST.head() COST.tail() ## Allocations for stock_df,allo in zip([COST,NLSN,NKE,DIS],[.2, .1, .4, .3]): stock_df[&#39;Allocation&#39;] = stock_df[&#39;Normed Return&#39;] * allo COST.head() ## Investment for stock_df in [COST,NLSN,NKE,DIS]: stock_df[&#39;Position Values&#39;] = stock_df[&#39;Allocation&#39;] * 1000000 ## Total Portfolio Value portfolio_val = pd.concat([COST[&#39;Position Values&#39;], NLSN[&#39;Position Values&#39;], NKE[&#39;Position Values&#39;], DIS[&#39;Position Values&#39;]], axis = 1) portfolio_val.head() portfolio_val.columns = [&#39;COST Pos&#39;, &#39;NLSN Pos&#39;, &#39;NKE Pos&#39;, &#39;DIS Pos&#39;] portfolio_val.head() portfolio_val[&#39;Total Pos&#39;] = portfolio_val.sum(axis = 1) portfolio_val.head() import matplotlib.pyplot as plt %matplotlib inline portfolio_val[&#39;Total Pos&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total Portfolio Value&#39;) portfolio_val.drop(&#39;Total Pos&#39;, axis = 1).plot(kind = &#39;line&#39;, figsize = (12, 8)) portfolio_val.tail() . COST Pos NLSN Pos NKE Pos DIS Pos Total Pos . Date . 2018-03-21 758153.014553 | 144489.827125 | 1.799185e+06 | 1.054741e+06 | 3.756569e+06 | . 2018-03-22 744177.280475 | 141728.307617 | 1.746850e+06 | 1.042104e+06 | 3.674859e+06 | . 2018-03-23 736843.076002 | 140347.547864 | 1.752545e+06 | 1.020764e+06 | 3.650500e+06 | . 2018-03-26 762838.756299 | 142663.660999 | 1.786983e+06 | 1.042622e+06 | 3.735107e+06 | . 2018-03-27 746255.305075 | 142930.904822 | 1.794304e+06 | 1.029259e+06 | 3.712749e+06 | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/19/StockMarketPortfolioAnaylsis.html",
            "relUrl": "/2020/09/19/StockMarketPortfolioAnaylsis.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post86": {
            "title": "NLP Heatmaps with Seaborn",
            "content": "from jupyterthemes import jtplot import warnings from imblearn.over_sampling import SMOTE import seaborn as sns from sklearn.model_selection import train_test_split import pandas as pd import numpy as np import pandas_profiling from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn import preprocessing import matplotlib.pyplot as plt %matplotlib inline # ignore warnings warnings.filterwarnings(&#39;ignore&#39;) jtplot.style(theme=&#39;oceans16&#39;, context=&#39;notebook&#39;, ticks=True, grid=False, figsize=(10, 9)) . df=pd.read_csv(&#39;../processed_data/nf_complete.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 126 entries, 0 to 125 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 Unnamed: 0 126 non-null int64 1 year 126 non-null int64 2 title 126 non-null object 3 abstract 126 non-null object 4 theme 126 non-null object 5 China 126 non-null int64 6 Russia 126 non-null int64 7 War 126 non-null int64 8 President 126 non-null int64 9 US 126 non-null int64 10 Vietnam 126 non-null int64 11 Cold War 126 non-null int64 12 World War 126 non-null int64 13 Vietnam War 126 non-null int64 14 Korean War 126 non-null int64 15 Survey 126 non-null int64 16 Case Study 126 non-null int64 17 Trade 126 non-null int64 18 Humanitarian 126 non-null int64 19 fixed_effects 126 non-null int64 20 instrumental_variable 126 non-null int64 21 regression 126 non-null int64 22 experimental 126 non-null int64 dtypes: int64(20), object(3) memory usage: 22.8+ KB . import plotly_express as ple ple.histogram(df.sort_values(&#39;year&#39;).groupby([&#39;year&#39;,&#39;theme&#39;])[&#39;Cold War&#39;].sum().reset_index(), x=&quot;year&quot;, y=&quot;Cold War&quot;, histfunc=&quot;sum&quot;, color=&quot;theme&quot;) . # ple.lidifferences(dfm_regional.sort_values(&#39;year&#39;).groupby([&#39;year&#39;,&#39;theme&#39;])[&#39;Cold War&#39;].sum().reset_index(), # x=&#39;year&#39;, # y=&#39;Cold War&#39;, # line_group=&#39;theme&#39;, # color=&#39;theme&#39; # ) . # Create the crosstab DataFrame pd_crosstab = pd.crosstab(df[&quot;theme&quot;], df[&quot;year&quot;]) # Plot a heatmap of the table with no color bar and using the BuGn palette sns.heatmap(pd_crosstab, cbar=False, cmap=&quot;GnBu&quot;, linewidths=0.3) # Rotate tick marks for visibility plt.yticks(rotation=0) plt.xticks(rotation=90) plt.tight_layout() #plt.savefig(&#39;./img/theme_heat_1.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500) #Show the plot plt.show() plt.clf() . &lt;Figure size 720x648 with 0 Axes&gt; . sns.clustermap(pd_crosstab, cmap=&#39;Greens&#39;, robust=True) # plot using a color palette #sns.heatmap(df, cmap=&quot;YlGnBu&quot;) #sns.heatmap(df, cmap=&quot;Blues&quot;) #sns.heatmap(df, cmap=&quot;BuPu&quot;) #sns.heatmap(df, cmap=&quot;Greens&quot;) . &lt;seaborn.matrix.ClusterGrid at 0x7f978c07a470&gt; . # Import seaborn library import seaborn as sns # Get correlation matrix of the meat DataFrame corr_meat = df.corr(method=&#39;pearson&#39;) # Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels fig = sns.clustermap(pd_crosstab, row_cluster=True, col_cluster=True, figsize=(10, 10)) plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90) plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0) plt.show() . data_normalized = pd_crosstab # Standardize the mean and variance within a stat, so different stats can be comparable # (This is the same as changing all the columns to Z-scores) data_normalized = (data_normalized - data_normalized.mean())/data_normalized.var() # Normalize these values to range from -1 to 1 data_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min()) data_normalized = data_normalized.T # Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram. sns.clustermap(data_normalized, cmap=&#39;Blues&#39;); . data_normalized = pd_crosstab # Standardize the mean and variance within a stat, so different stats can be comparable # (This is the same as changing all the columns to Z-scores) data_normalized = (data_normalized - data_normalized.mean())/data_normalized.var() # Normalize these values to range from -1 to 1 data_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min()) #data_normalized = data_normalized.T # Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram. sns.clustermap(data_normalized, cmap=&#39;BuPu&#39;); . import matplotlib.pyplot as plt sns.clustermap(data_normalized); fig = plt.gcf() fig.savefig(&#39;clusteredheatmap_bbox_tight.png&#39;, bbox_inches=&#39;tight&#39;) . tidy_df = pd.melt(df.reset_index(), id_vars=&#39;index&#39;) df.T.head() . 0 1 2 3 4 5 6 7 8 9 ... 116 117 118 119 120 121 122 123 124 125 . Unnamed: 0 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 14 | 13 | ... | 128 | 130 | 123 | 125 | 131 | 132 | 133 | 134 | 135 | 136 | . year 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2001 | 2001 | ... | 2017 | 2017 | 2017 | 2017 | 2018 | 2018 | 2018 | 2018 | 2018 | 2018 | . title &quot;Institutions at the Domestic/International Ne... | Born to Lose and Doomed to Survive: State Deat... | The significance of “allegiance” in internatio... | The significance of “allegiance” in internatio... | Truth-Telling and Mythmaking in Post-Soviet Ru... | Building a Cape Fear Metropolis: Fort Bragg, F... | The Glories and the Sadness: Shaping the natio... | What leads longstanding adversaries to engage ... | A School for the Nation: Military Institution... | The &#39;American Century&#39; Army: The Origins of t... | ... | Fully Committed? Religiously Committed State P... | Straddling the Threshold of Two Worlds: Soldie... | U.S. Army’s Investigation and Adjudication of ... | Grand Strategic Crucibles: The Lasting Effects... | Trust in International Politics: The Role of L... | Planning for the Short Haul: Trade Among Belli... | Clinging to the Anti-Imperial Mantle: The Repu... | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | Unexpected Humanitarians: Albania, the U.S. Mi... | . abstract Civil-military relations are frequently studie... | Under what conditions do states die, or exit t... | My dissertation employs original and secondary... | nThis study revises prevailing interpretation... | Can distorted and pernicious ideas about histo... | My dissertation examines the cultural and econ... | In my dissertation I compare the ways in whic... | This dissertation develops a socio-psychoanal... | Beginning in Europe in the latter half of the ... | This dissertation covers the period 1949-1959 ... | ... | This dissertation argues that the higher the l... | This dissertation explores how American soldie... | This dissertation examines the U.S. Army’s res... | When and how do military interventions shape g... | In my dissertation, I focus on how leader rela... | In times of war, why do belligerents continue ... | My dissertation project, Clinging to the Anti-... | Using a transnational methodology and sources ... | There is a dilemma at the heart of coercion. S... | Using archives and oral history, this disserta... | . theme IR scholarship | IR scholarship | IR scholarship | Conflit Between States | Conflict Between States | Domestic Military History | Culture | Culture / Peace Process | Military History | Military History | ... | IR Scholarship | Military History | Military History | IR Scholarship | Nuclear Weapons | Conflict between states | Cold War | Military History | IR Scholarship | Military History | . 5 rows × 126 columns .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/03/NLP_Seaborn_Heatmaps.html",
            "relUrl": "/2020/09/03/NLP_Seaborn_Heatmaps.html",
            "date": " • Sep 3, 2020"
        }
        
    
  
    
        ,"post87": {
            "title": "NLP ngrams With Python",
            "content": "&#39;In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.&#39;, from wikipedia . import pandas as pd df=pd.read_csv(&#39;../../processed_data/nf_complete.csv&#39;) . Pre-processing text . def preprocessor(text): text = re.sub(&#39;&lt;[^&gt;]*&gt;&#39;, &#39;&#39;, text) emoticons = re.findall(&#39;(?::|;|=)(?:-)?(?: )| (|D|P)&#39;, text) text = re.sub(&#39;[ W]+&#39;, &#39; &#39;, text.lower()) + &#39; &#39;.join(emoticons).replace(&#39;-&#39;, &#39;&#39;) return text . Find Total Word Count . text = &quot; &quot;.join(review for review in df.abstract) print (&quot;There are {} words in the combination of all abstracts.&quot;.format(len(text))) . There are 272025 words in the combination of all abstracts. . from urllib.request import urlopen from random import randint def wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sum def retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return word def buildWordDict(text): # Remove newlines and quotes text = text.replace(&#39; n&#39;, &#39; &#39;); text = text.replace(&#39;&quot;&#39;, &#39;&#39;); # Make sure punctuation marks are treated as their own &quot;words,&quot; # so that they will be included in the Markov chain punctuation = [&#39;,&#39;,&#39;.&#39;,&#39;;&#39;,&#39;:&#39;] for symbol in punctuation: text = text.replace(symbol, &#39; {} &#39;.format(symbol)); words = text.split(&#39; &#39;) # Filter out empty words words = [word for word in words if word != &#39;&#39;] wordDict = {} for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = {} if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDict wordDict = buildWordDict(text) #Generate a Markov chain of length 100 length = 100 chain = [&#39;Vietnam&#39;] for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord) print(&#39; &#39;.join(chain)) . Vietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate . ” and . def getFirstSentenceContaining(ngram, text): #print(ngram) sentences = text.upper().split(&quot;. &quot;) for sentence in sentences: if ngram in sentence: return sentence+&#39; n&#39; return &quot;&quot; print(getFirstSentenceContaining(&#39;I&#39;, text)) . CIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS . #text . from urllib.request import urlopen from bs4 import BeautifulSoup import re import string from collections import Counter def cleanSentence(sentence): sentence = sentence.split(&#39; &#39;) sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence] sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == &#39;a&#39; or word.lower() == &#39;i&#39;)] return sentence def cleanInput(content): content = content.upper() content = re.sub(&#39; n&#39;, &#39; &#39;, content) content = bytes(content, &#39;UTF-8&#39;) content = content.decode(&#39;ascii&#39;, &#39;ignore&#39;) sentences = content.split(&#39;. &#39;) return [cleanSentence(sentence) for sentence in sentences] def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return output def getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [&#39; &#39;.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams) content = str(text) ngrams = getNgrams(content, 3) #print(ngrams) . from urllib.request import urlopen from bs4 import BeautifulSoup import re import string from collections import Counter def isCommon(ngram): commonWords = [&#39;THE&#39;, &#39;BE&#39;, &#39;AND&#39;, &#39;OF&#39;, &#39;A&#39;, &#39;IN&#39;, &#39;TO&#39;, &#39;HAVE&#39;, &#39;IT&#39;, &#39;I&#39;, &#39;THAT&#39;, &#39;FOR&#39;, &#39;YOU&#39;, &#39;HE&#39;, &#39;WITH&#39;, &#39;ON&#39;, &#39;DO&#39;, &#39;SAY&#39;, &#39;THIS&#39;, &#39;THEY&#39;, &#39;IS&#39;, &#39;AN&#39;, &#39;AT&#39;, &#39;BUT&#39;, &#39;WE&#39;, &#39;HIS&#39;, &#39;FROM&#39;, &#39;THAT&#39;, &#39;NOT&#39;, &#39;BY&#39;, &#39;SHE&#39;, &#39;OR&#39;, &#39;AS&#39;, &#39;WHAT&#39;, &#39;GO&#39;, &#39;THEIR&#39;, &#39;CAN&#39;, &#39;WHO&#39;, &#39;GET&#39;, &#39;IF&#39;, &#39;WOULD&#39;, &#39;HER&#39;, &#39;ALL&#39;, &#39;MY&#39;, &#39;MAKE&#39;, &#39;ABOUT&#39;, &#39;KNOW&#39;, &#39;WILL&#39;, &#39;AS&#39;, &#39;UP&#39;, &#39;ONE&#39;, &#39;TIME&#39;, &#39;HAS&#39;, &#39;BEEN&#39;, &#39;THERE&#39;, &#39;YEAR&#39;, &#39;SO&#39;, &#39;THINK&#39;, &#39;WHEN&#39;, &#39;WHICH&#39;, &#39;THEM&#39;, &#39;SOME&#39;, &#39;ME&#39;, &#39;PEOPLE&#39;, &#39;TAKE&#39;, &#39;OUT&#39;, &#39;INTO&#39;, &#39;JUST&#39;, &#39;SEE&#39;, &#39;HIM&#39;, &#39;YOUR&#39;, &#39;COME&#39;, &#39;COULD&#39;, &#39;NOW&#39;, &#39;THAN&#39;, &#39;LIKE&#39;, &#39;OTHER&#39;, &#39;HOW&#39;, &#39;THEN&#39;, &#39;ITS&#39;, &#39;OUR&#39;, &#39;TWO&#39;, &#39;MORE&#39;, &#39;THESE&#39;, &#39;WANT&#39;, &#39;WAY&#39;, &#39;LOOK&#39;, &#39;FIRST&#39;, &#39;ALSO&#39;, &#39;NEW&#39;, &#39;BECAUSE&#39;, &#39;DAY&#39;, &#39;MORE&#39;, &#39;USE&#39;, &#39;NO&#39;, &#39;MAN&#39;, &#39;FIND&#39;, &#39;HERE&#39;, &#39;THING&#39;, &#39;GIVE&#39;, &#39;MANY&#39;, &#39;WELL&#39;] for word in ngram: if word in commonWords: return True return False def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): if not isCommon(content[i:i+n]): output.append(content[i:i+n]) return output ngrams = getNgrams(content, 3) #print(ngrams) . def getFirstSentenceContaining(ngram, content): #print(ngram) sentences = content.upper().split(&quot;. &quot;) for sentence in sentences: if ngram in sentence: return sentence+&#39; n&#39; return &quot;&quot; print(getFirstSentenceContaining(&#39;SINO-JAPANESE WAR 1894-1895&#39;, content)) print(getFirstSentenceContaining(&#39;2ND VIETNAM WAR&#39;, content)) print(getFirstSentenceContaining(&#39;COLD WAR ARMY&#39;, content)) print(getFirstSentenceContaining(&#39;WORLD WAR II&#39;, content)) print(getFirstSentenceContaining(&#39;ARMS CONTROL AGREEMENTS&#39;, content)) . IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD. THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE ‘OTHER SIDE,’ ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION COLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTION’S INTELLECTUAL AND HISTORICAL FRAMEWORK OF “PREPAREDNESS” TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR I ARGUE THAT A NORM PROTECTING STATES’ TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS. THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY . from urllib.request import urlopen from random import randint def wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sum def retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return word def buildWordDict(text): # Remove newlines and quotes text = text.replace(&#39; n&#39;, &#39; &#39;); text = text.replace(&#39;&quot;&#39;, &#39;&#39;); # Make sure punctuation marks are treated as their own &quot;words,&quot; # so that they will be included in the Markov chain punctuation = [&#39;,&#39;,&#39;.&#39;,&#39;;&#39;,&#39;:&#39;] for symbol in punctuation: text = text.replace(symbol, &#39; {} &#39;.format(symbol)); words = text.split(&#39; &#39;) # Filter out empty words words = [word for word in words if word != &#39;&#39;] wordDict = {} for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = {} if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDict wordDict = buildWordDict(text) #Generate a Markov chain of length 100 length = 100 chain = [&#39;I&#39;] for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord) #print(&#39; &#39;.join(chain)) . import re def getNgrams(content, n): content = re.sub(&#39; n|[[ d+ ]]&#39;, &#39; &#39;, content) content = bytes(content, &#39;UTF-8&#39;) content = content.decode(&#39;ascii&#39;, &#39;ignore&#39;) content = content.split(&#39; &#39;) content = [word for word in content if word != &#39;&#39;] output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return output . from collections import Counter def getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [&#39; &#39;.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams) . #print(getNgrams(content, 2)) . def isCommon(ngram): commonWords = [&#39;THE&#39;, &#39;BE&#39;, &#39;AND&#39;, &#39;OF&#39;, &#39;A&#39;, &#39;IN&#39;, &#39;TO&#39;, &#39;HAVE&#39;, &#39;IT&#39;, &#39;I&#39;, &#39;THAT&#39;, &#39;FOR&#39;, &#39;YOU&#39;, &#39;HE&#39;, &#39;WITH&#39;, &#39;ON&#39;, &#39;DO&#39;, &#39;SAY&#39;, &#39;THIS&#39;, &#39;THEY&#39;, &#39;IS&#39;, &#39;AN&#39;, &#39;AT&#39;, &#39;BUT&#39;, &#39;WE&#39;, &#39;HIS&#39;, &#39;FROM&#39;, &#39;THAT&#39;, &#39;NOT&#39;, &#39;BY&#39;, &#39;SHE&#39;, &#39;OR&#39;, &#39;AS&#39;, &#39;WHAT&#39;, &#39;GO&#39;, &#39;THEIR&#39;, &#39;CAN&#39;, &#39;WHO&#39;, &#39;GET&#39;, &#39;IF&#39;, &#39;WOULD&#39;, &#39;HER&#39;, &#39;ALL&#39;, &#39;MY&#39;, &#39;MAKE&#39;, &#39;ABOUT&#39;, &#39;KNOW&#39;, &#39;WILL&#39;, &#39;AS&#39;, &#39;UP&#39;, &#39;ONE&#39;, &#39;TIME&#39;, &#39;HAS&#39;, &#39;BEEN&#39;, &#39;THERE&#39;, &#39;YEAR&#39;, &#39;SO&#39;, &#39;THINK&#39;, &#39;WHEN&#39;, &#39;WHICH&#39;, &#39;THEM&#39;, &#39;SOME&#39;, &#39;ME&#39;, &#39;PEOPLE&#39;, &#39;TAKE&#39;, &#39;OUT&#39;, &#39;INTO&#39;, &#39;JUST&#39;, &#39;SEE&#39;, &#39;HIM&#39;, &#39;YOUR&#39;, &#39;COME&#39;, &#39;COULD&#39;, &#39;NOW&#39;, &#39;THAN&#39;, &#39;LIKE&#39;, &#39;OTHER&#39;, &#39;HOW&#39;, &#39;THEN&#39;, &#39;ITS&#39;, &#39;OUR&#39;, &#39;TWO&#39;, &#39;MORE&#39;, &#39;THESE&#39;, &#39;WANT&#39;, &#39;WAY&#39;, &#39;LOOK&#39;, &#39;FIRST&#39;, &#39;ALSO&#39;, &#39;NEW&#39;, &#39;BECAUSE&#39;, &#39;DAY&#39;, &#39;MORE&#39;, &#39;USE&#39;, &#39;NO&#39;, &#39;MAN&#39;, &#39;FIND&#39;, &#39;HERE&#39;, &#39;THING&#39;, &#39;GIVE&#39;, &#39;MANY&#39;, &#39;WELL&#39;] for word in ngram: if word in commonWords: return True return False def getNgramsFromSentence(text, n): output = [] for i in range(len(text)-n+1): if not isCommon(text[i:i+n]): output.append(text[i:i+n]) return output ngrams = getNgrams(text, 3) #print(ngrams) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/02/NLP_Ngram.html",
            "relUrl": "/2020/09/02/NLP_Ngram.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post88": {
            "title": "NLP with Pyspark",
            "content": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. . from pyspark.ml.feature import Tokenizer, RegexTokenizer from pyspark.sql.functions import col, udf from pyspark.sql.types import IntegerType . sentenceDataFrame = spark.createDataFrame([ (0, &quot;Hi I heard about Spark&quot;), (1, &quot;I wish Java could use case classes&quot;), (2, &quot;Logistic,regression,models,are,neat&quot;) ], [&quot;id&quot;, &quot;sentence&quot;]) . sentenceDataFrame.show() . ++--+ id| sentence| ++--+ 0|Hi I heard about ...| 1|I wish Java could...| 2|Logistic,regressi...| ++--+ Using Tokenizer and RegexTokenizer . tokenizer = Tokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;) regexTokenizer = RegexTokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;, pattern=&quot; W&quot;) # alternatively, pattern=&quot; w+&quot;, gaps(False) countTokens = udf(lambda words: len(words), IntegerType()) tokenized = tokenizer.transform(sentenceDataFrame) tokenized.select(&quot;sentence&quot;, &quot;words&quot;) .withColumn(&quot;tokens&quot;, countTokens(col(&quot;words&quot;))).show(truncate=False) regexTokenized = regexTokenizer.transform(sentenceDataFrame) regexTokenized.select(&quot;sentence&quot;, &quot;words&quot;) .withColumn(&quot;tokens&quot;, countTokens(col(&quot;words&quot;))).show(truncate=False) . +--+++ sentence |words |tokens| +--+++ Hi I heard about Spark |[hi, i, heard, about, spark] |5 | I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 | Logistic,regression,models,are,neat|[logistic,regression,models,are,neat] |1 | +--+++ +--+++ sentence |words |tokens| +--+++ Hi I heard about Spark |[hi, i, heard, about, spark] |5 | I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 | Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5 | +--+++ Removing Stop Words . from pyspark.ml.feature import StopWordsRemover sentenceData = spark.createDataFrame([ (0, [&quot;I&quot;, &quot;saw&quot;, &quot;the&quot;, &quot;red&quot;, &quot;balloon&quot;]), (1, [&quot;Mary&quot;, &quot;had&quot;, &quot;a&quot;, &quot;little&quot;, &quot;lamb&quot;]) ], [&quot;id&quot;, &quot;raw&quot;]) remover = StopWordsRemover(inputCol=&quot;raw&quot;, outputCol=&quot;filtered&quot;) remover.transform(sentenceData).show(truncate=False) . ++-+--+ id |raw |filtered | ++-+--+ 0 |[I, saw, the, red, balloon] |[saw, red, balloon] | 1 |[Mary, had, a, little, lamb]|[Mary, little, lamb]| ++-+--+ n-grams . from pyspark.ml.feature import NGram wordDataFrame = spark.createDataFrame([ (0, [&quot;Hi&quot;, &quot;I&quot;, &quot;heard&quot;, &quot;about&quot;, &quot;Spark&quot;]), (1, [&quot;I&quot;, &quot;wish&quot;, &quot;Java&quot;, &quot;could&quot;, &quot;use&quot;, &quot;case&quot;, &quot;classes&quot;]), (2, [&quot;Logistic&quot;, &quot;regression&quot;, &quot;models&quot;, &quot;are&quot;, &quot;neat&quot;]) ], [&quot;id&quot;, &quot;words&quot;]) ngram = NGram(n=2, inputCol=&quot;words&quot;, outputCol=&quot;ngrams&quot;) ngramDataFrame = ngram.transform(wordDataFrame) ngramDataFrame.select(&quot;ngrams&quot;).show(truncate=False) . ++ ngrams | ++ [Hi I, I heard, heard about, about Spark] | [I wish, wish Java, Java could, could use, use case, case classes]| [Logistic regression, regression models, models are, are neat] | ++ from pyspark.ml.feature import HashingTF, IDF, Tokenizer sentenceData = spark.createDataFrame([ (0.0, &quot;Hi I heard about Spark&quot;), (0.0, &quot;I wish Java could use case classes&quot;), (1.0, &quot;Logistic regression models are neat&quot;) ], [&quot;label&quot;, &quot;sentence&quot;]) sentenceData.show() . +--+--+ label| sentence| +--+--+ 0.0|Hi I heard about ...| 0.0|I wish Java could...| 1.0|Logistic regressi...| +--+--+ tokenizer = Tokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;) wordsData = tokenizer.transform(sentenceData) wordsData.show() . +--+--+--+ label| sentence| words| +--+--+--+ 0.0|Hi I heard about ...|[hi, i, heard, ab...| 0.0|I wish Java could...|[i, wish, java, c...| 1.0|Logistic regressi...|[logistic, regres...| +--+--+--+ hashingTF = HashingTF(inputCol=&quot;words&quot;, outputCol=&quot;rawFeatures&quot;, numFeatures=20) featurizedData = hashingTF.transform(wordsData) # alternatively, CountVectorizer can also be used to get term frequency vectors idf = IDF(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;) idfModel = idf.fit(featurizedData) rescaledData = idfModel.transform(featurizedData) rescaledData.select(&quot;label&quot;, &quot;features&quot;).show() . +--+--+ label| features| +--+--+ 0.0|(20,[6,8,13,16],[...| 0.0|(20,[0,2,7,13,15,...| 1.0|(20,[3,4,6,11,19]...| +--+--+ CountVectorizer . from pyspark.ml.feature import CountVectorizer # Input data: Each row is a bag of words with a ID. df = spark.createDataFrame([ (0, &quot;a b c&quot;.split(&quot; &quot;)), (1, &quot;a b b c a&quot;.split(&quot; &quot;)) ], [&quot;id&quot;, &quot;words&quot;]) # fit a CountVectorizerModel from the corpus. cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;features&quot;, vocabSize=3, minDF=2.0) model = cv.fit(df) result = model.transform(df) result.show(truncate=False) . +++-+ id |words |features | +++-+ 0 |[a, b, c] |(3,[0,1,2],[1.0,1.0,1.0])| 1 |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])| +++-+ df = spark.read.load(&quot;/FileStore/tables/SMSSpamCollection&quot;, format=&quot;csv&quot;, sep=&quot; t&quot;, inferSchema=&quot;true&quot;, header=&quot;false&quot;) . df.printSchema() . root -- _c0: string (nullable = true) -- _c1: string (nullable = true) data = df.withColumnRenamed(&#39;_c0&#39;,&#39;class&#39;).withColumnRenamed(&#39;_c1&#39;,&#39;text&#39;) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) Clean and Prepare the Data . from pyspark.sql.functions import length . data = data.withColumn(&#39;length&#39;,length(data[&#39;text&#39;])) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) -- length: integer (nullable = true) # Pretty Clear Difference data.groupby(&#39;class&#39;).mean().show() . +--+--+ class| avg(length)| +--+--+ ham| 71.4545266210897| spam|138.6706827309237| +--+--+ from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer tokenizer = Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;token_text&quot;) stopremove = StopWordsRemover(inputCol=&#39;token_text&#39;,outputCol=&#39;stop_tokens&#39;) count_vec = CountVectorizer(inputCol=&#39;stop_tokens&#39;,outputCol=&#39;c_vec&#39;) idf = IDF(inputCol=&quot;c_vec&quot;, outputCol=&quot;tf_idf&quot;) ham_spam_to_num = StringIndexer(inputCol=&#39;class&#39;,outputCol=&#39;label&#39;) . from pyspark.ml.feature import VectorAssembler from pyspark.ml.linalg import Vector . clean_up = VectorAssembler(inputCols=[&#39;tf_idf&#39;,&#39;length&#39;],outputCol=&#39;features&#39;) . Naive Bayes . from pyspark.ml.classification import NaiveBayes . # Use defaults nb = NaiveBayes() . ### Pipeline . from pyspark.ml import Pipeline . data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up]) . cleaner = data_prep_pipe.fit(data) . clean_data = cleaner.transform(data) . Training and Evaluation . clean_data = clean_data.select([&#39;label&#39;,&#39;features&#39;]) . clean_data.show() . +--+--+ label| features| +--+--+ 0.0|(13424,[7,11,31,6...| 0.0|(13424,[0,24,297,...| 1.0|(13424,[2,13,19,3...| 0.0|(13424,[0,70,80,1...| 0.0|(13424,[36,134,31...| 1.0|(13424,[10,60,139...| 0.0|(13424,[10,53,103...| 0.0|(13424,[125,184,4...| 1.0|(13424,[1,47,118,...| 1.0|(13424,[0,1,13,27...| 0.0|(13424,[18,43,120...| 1.0|(13424,[8,17,37,8...| 1.0|(13424,[13,30,47,...| 0.0|(13424,[39,96,217...| 0.0|(13424,[552,1697,...| 1.0|(13424,[30,109,11...| 0.0|(13424,[82,214,47...| 0.0|(13424,[0,2,49,13...| 0.0|(13424,[0,74,105,...| 1.0|(13424,[4,30,33,5...| +--+--+ only showing top 20 rows (training,testing) = clean_data.randomSplit([0.7,0.3]) . spam_predictor = nb.fit(training) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) -- length: integer (nullable = true) test_results = spam_predictor.transform(testing) . test_results.show() . +--+--+--+--+-+ label| features| rawPrediction| probability|prediction| +--+--+--+--+-+ 0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...| 0.0| 0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...| 0.0| 0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...| 0.0| 0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...| 0.0| 0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...| 0.0| 0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...| 0.0| 0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...| 0.0| 0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...| 0.0| 0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...| 0.0| 0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...| 0.0| 0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...| 1.0| 0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...| 0.0| 0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...| 0.0| 0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...| 0.0| 0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...| 0.0| 0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...| 0.0| 0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...| 0.0| 0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...| 0.0| 0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...| 0.0| 0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...| 0.0| +--+--+--+--+-+ only showing top 20 rows ## Evaluating Model Accuracy . from pyspark.ml.evaluation import MulticlassClassificationEvaluator . acc_eval = MulticlassClassificationEvaluator() acc = acc_eval.evaluate(test_results) print(&quot;Accuracy of model at predicting spam was: {}&quot;.format(acc)) . Accuracy of model at predicting spam was: 0.9204435112848836",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/30/NLP-with-Pyspark.html",
            "relUrl": "/2020/08/30/NLP-with-Pyspark.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post89": {
            "title": "Clustering with Pyspark",
            "content": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler . feat_cols = [&#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;,&#39;fr&#39;] . feat_cols = [&#39;gdp&#39;] . vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol=&#39;features&#39;) . final_df = vec_assembler.transform(df) . Using the StandardScaler . from pyspark.ml.feature import StandardScaler . scaler = StandardScaler(inputCol=&quot;features&quot;, outputCol=&quot;scaledFeatures&quot;, withStd=True, withMean=False) . Fitting the StandardScaler . # Compute summary statistics by fitting the StandardScaler scalerModel = scaler.fit(final_df) . # Normalize each feature to have unit standard deviation. cluster_final_data = scalerModel.transform(final_df) . kmeans3 = KMeans(featuresCol=&#39;scaledFeatures&#39;,k=3) kmeans2 = KMeans(featuresCol=&#39;scaledFeatures&#39;,k=2) . model_k3 = kmeans3.fit(cluster_final_data) model_k2 = kmeans2.fit(cluster_final_data) . model_k3.transform(cluster_final_data).groupBy(&#39;prediction&#39;).count().show() . +-+--+ prediction|count| +-+--+ 1| 15| 2| 86| 0| 259| +-+--+ model_k2.transform(cluster_final_data).groupBy(&#39;prediction&#39;).count().show() . +-+--+ prediction|count| +-+--+ 1| 308| 0| 52| +-+--+",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/29/Clustering-with-Pyspark.html",
            "relUrl": "/2020/08/29/Clustering-with-Pyspark.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post90": {
            "title": "Regression and Classification with Pyspark ML",
            "content": "Linear Regression and Random Forest/GBT Classification with Pyspark . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType from pyspark.sql.functions import * data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows df.groupBy(&#39;province&#39;).count().show() . ++--+ province|count| ++--+ Guangdong| 12| Hunan| 12| Shanxi| 12| Tibet| 12| Hubei| 12| Tianjin| 12| Beijing| 12| Heilongjiang| 12| Liaoning| 12| Henan| 12| Anhui| 12| Xinjiang| 12| Fujian| 12| Jiangxi| 12| Jilin| 12| Chongqing| 12| Shaanxi| 12| Sichuan| 12| Yunnan| 12| Gansu| 12| ++--+ only showing top 20 rows Imputation of mean values to prepare the data . mean_val = df.select(mean(df[&#39;general&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;general&quot;]) . mean_val = df.select(mean(df[&#39;specific&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;specific&quot;]) . mean_val = df.select(mean(df[&#39;rr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;rr&quot;]) . mean_val = df.select(mean(df[&#39;fr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;fr&quot;]) . mean_val = df.select(mean(df[&#39;rnr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;rnr&quot;]) . mean_val = df.select(mean(df[&#39;i&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;i&quot;]) . Creating binary target feature from extant column for classification . from pyspark.sql.functions import * df = df.withColumn(&#39;specific_classification&#39;,when(df.specific &gt;= 583470.7303370787,1).otherwise(0)) . Using StringIndexer for categorical encoding of string type columns . from pyspark.ml.feature import StringIndexer . indexer = StringIndexer(inputCol=&quot;province&quot;, outputCol=&quot;provinceIndex&quot;) df = indexer.fit(df).transform(df) . indexer = StringIndexer(inputCol=&quot;reg&quot;, outputCol=&quot;regionIndex&quot;) df = indexer.fit(df).transform(df) . df.show() . ++--+++-+-+--++-+-+-+--+-+--+-+--+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it|specific_classification|provinceIndex|regionIndex| ++--+++-+-+--++-+-+-+--+-+--+-+--+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 0| 0.0| 0.0| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 0| 0.0| 0.0| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 0| 0.0| 0.0| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131.0|0.0355944252244898|0.05968862|0.08376352|1646891| East China|1227364| 0| 0.0| 0.0| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 0| 0.0| 0.0| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 0| 0.0| 0.0| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 0| 0.0| 0.0| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 1| 0.0| 0.0| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0|2522449| East China|3422176| 1| 0.0| 0.0| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000.0| 0.0| 0.0| 0.3243243|2522449| East China|3874846| 1| 0.0| 0.0| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354.0| 0.0| 0.0| 0.3243243|3434548| East China|5167300| 1| 0.0| 0.0| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892.0| 0.0| 0.0| 0.3243243|4468640| East China|7040099| 1| 0.0| 0.0| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290.0|0.0355944252244898|0.05968862|0.08376352| 634562|North China| 508135| 0| 1.0| 4.0| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 0| 1.0| 4.0| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 0| 1.0| 4.0| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525.0| 0.0| 0.0| 0.53|2522449|North China| 944047| 0| 1.0| 4.0| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 0| 1.0| 4.0| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 0| 1.0| 4.0| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 0| 1.0| 4.0| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126.0| 0.0| 0.7948718| 0.0|2823366|North China|1426600| 1| 1.0| 4.0| ++--+++-+-+--++-+-+-+--+-+--+-+--+ only showing top 20 rows Using VectorAssembler to prepare features for machine learning . from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler . df.columns . Out[375]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;, &#39;specific_classification&#39;, &#39;provinceIndex&#39;, &#39;regionIndex&#39;] assembler = VectorAssembler( inputCols=[ &#39;provinceIndex&#39;, # &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, #&#39;rnr&#39;, #&#39;rr&#39;, #&#39;i&#39;, #&#39;fr&#39;, &#39;regionIndex&#39;, &#39;it&#39; ], outputCol=&quot;features&quot;) . output = assembler.transform(df) . final_data = output.select(&quot;features&quot;, &quot;specific&quot;) . Spliting data into train and test . train_data,test_data = final_data.randomSplit([0.7,0.3]) . Regression with Pyspark ML . from pyspark.ml.regression import LinearRegression lr = LinearRegression(labelCol=&#39;specific&#39;) . Fitting the linear regression model to the training data . lrModel = lr.fit(train_data) . Coefficients and Intercept of the linear regression model . print(&quot;Coefficients: {} Intercept: {}&quot;.format(lrModel.coefficients,lrModel.intercept)) . Coefficients: [-4936.461707001148,0.8007702471080539,-3994.683052325085,-7.5033201950338,0.42095493334994133,50994.51222529955,0.2531915644818595] Intercept: 7695214.561654471 Evaluating trained linear regression model on the test data . test_results = lrModel.evaluate(test_data) . Metrics of trained linear regression model on the test data (RMSE, MSE, R2) . print(&quot;RMSE: {}&quot;.format(test_results.rootMeanSquaredError)) print(&quot;MSE: {}&quot;.format(test_results.meanSquaredError)) print(&quot;R2: {}&quot;.format(test_results.r2)) . RMSE: 292695.0825058327 MSE: 85670411323.0962 R2: 0.7853651103073853 Looking at correlations with corr . from pyspark.sql.functions import corr . df.select(corr(&#39;specific&#39;,&#39;gdp&#39;)).show() . +-+ corr(specific, gdp)| +-+ 0.5141876884991972| +-+ Classification with Pyspark ML . from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier from pyspark.ml import Pipeline . DecisionTreeClassifier, RandomForestClassifier and GBTClassifier . dtc = DecisionTreeClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) rfc = RandomForestClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) gbt = GBTClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) . Selecting features and binary target . final_data = output.select(&quot;features&quot;, &quot;specific_classification&quot;) train_data,test_data = final_data.randomSplit([0.7,0.3]) . Fitting the Classifiers to the Training Data . rfc_model = rfc.fit(train_data) gbt_model = gbt.fit(train_data) dtc_model = dtc.fit(train_data) . Classifier predictions on test data . dtc_predictions = dtc_model.transform(test_data) rfc_predictions = rfc_model.transform(test_data) gbt_predictions = gbt_model.transform(test_data) . Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator . from pyspark.ml.evaluation import MulticlassClassificationEvaluator . Classifier Accuracy . acc_evaluator = MulticlassClassificationEvaluator(labelCol=&quot;specific_classification&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;) . Classifier Accuracy Metrics . dtc_acc = acc_evaluator.evaluate(dtc_predictions) rfc_acc = acc_evaluator.evaluate(rfc_predictions) gbt_acc = acc_evaluator.evaluate(gbt_predictions) . print(&#39;-&#39;*80) print(&#39;Decision tree accuracy: {0:2.2f}%&#39;.format(dtc_acc*100)) print(&#39;-&#39;*80) print(&#39;Random forest ensemble accuracy: {0:2.2f}%&#39;.format(rfc_acc*100)) print(&#39;-&#39;*80) print(&#39;GBT accuracy: {0:2.2f}%&#39;.format(gbt_acc*100)) print(&#39;-&#39;*80) . -- Decision tree accuracy: 81.98% -- Random forest ensemble accuracy: 88.29% -- GBT accuracy: 81.08% -- Classification Correlation with Corr . df.select(corr(&#39;specific_classification&#39;,&#39;fdi&#39;)).show() . +-+ corr(specific_classification, fdi)| +-+ 0.307429849493392| +-+ df.select(corr(&#39;specific_classification&#39;,&#39;gdp&#39;)).show() . +-+ corr(specific_classification, gdp)| +-+ 0.492176921599151| +-+ This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/25/Linear-Regression-and-Random-Forest_GBT-Classification-with-Pyspark.html",
            "relUrl": "/2020/08/25/Linear-Regression-and-Random-Forest_GBT-Classification-with-Pyspark.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post91": {
            "title": "Window functions and Pivot Tables with Pyspark",
            "content": "Resilient Distributed Datasets . Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory. . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType from pyspark.sql.functions import * data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows Using toPandas to look at the data . df.limit(10).toPandas() . _c0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.300049 | 50661.0 | 0.0 | 0.0 | 0.000000 | 1128873.0 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.320068 | 43443.0 | 0.0 | 0.0 | 0.000000 | 1356287.0 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.959961 | 27673.0 | 0.0 | 0.0 | 0.000000 | 1518236.0 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.340088 | 26131.0 | NaN | NaN | NaN | 1646891.0 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.090088 | 31847.0 | 0.0 | 0.0 | 0.000000 | 1601508.0 | East China | 1499110 | . 5 5 | Anhui | 250898.0 | NaN | 2001 | 3246.709961 | 33672.0 | 0.0 | 0.0 | 0.000000 | 1672445.0 | East China | 2165189 | . 6 6 | Anhui | 434149.0 | 66529.0 | 2002 | 3519.719971 | 38375.0 | 0.0 | 0.0 | 0.000000 | 1677840.0 | East China | 2404936 | . 7 7 | Anhui | 619201.0 | 52108.0 | 2003 | 3923.110107 | 36720.0 | 0.0 | 0.0 | 0.000000 | 1896479.0 | East China | 2815820 | . 8 8 | Anhui | 898441.0 | 349699.0 | 2004 | 4759.299805 | 54669.0 | 0.0 | 0.0 | 0.000000 | NaN | East China | 3422176 | . 9 9 | Anhui | 898441.0 | NaN | 2005 | 5350.169922 | 69000.0 | 0.0 | 0.0 | 0.324324 | NaN | East China | 3874846 | . Renaming Columns . df = df.withColumnRenamed(&quot;reg&quot;,&quot;region&quot;) . df.limit(10).toPandas() . _c0 province specific general year gdp fdi rnr rr i fr region it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.300049 | 50661.0 | 0.0 | 0.0 | 0.000000 | 1128873.0 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.320068 | 43443.0 | 0.0 | 0.0 | 0.000000 | 1356287.0 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.959961 | 27673.0 | 0.0 | 0.0 | 0.000000 | 1518236.0 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.340088 | 26131.0 | NaN | NaN | NaN | 1646891.0 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.090088 | 31847.0 | 0.0 | 0.0 | 0.000000 | 1601508.0 | East China | 1499110 | . 5 5 | Anhui | 250898.0 | NaN | 2001 | 3246.709961 | 33672.0 | 0.0 | 0.0 | 0.000000 | 1672445.0 | East China | 2165189 | . 6 6 | Anhui | 434149.0 | 66529.0 | 2002 | 3519.719971 | 38375.0 | 0.0 | 0.0 | 0.000000 | 1677840.0 | East China | 2404936 | . 7 7 | Anhui | 619201.0 | 52108.0 | 2003 | 3923.110107 | 36720.0 | 0.0 | 0.0 | 0.000000 | 1896479.0 | East China | 2815820 | . 8 8 | Anhui | 898441.0 | 349699.0 | 2004 | 4759.299805 | 54669.0 | 0.0 | 0.0 | 0.000000 | NaN | East China | 3422176 | . 9 9 | Anhui | 898441.0 | NaN | 2005 | 5350.169922 | 69000.0 | 0.0 | 0.0 | 0.324324 | NaN | East China | 3874846 | . # df = df.toDF(*[&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;it&#39;, &#39;fr&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;_c0&#39;, &#39;specific_classification&#39;, &#39;provinceIndex&#39;, &#39;regionIndex&#39;]) . Selecting Columns of Interest . df = df.select(&#39;year&#39;,&#39;region&#39;,&#39;province&#39;,&#39;gdp&#39;, &#39;fdi&#39;) . df.sort(&quot;gdp&quot;).show() . +-++--++-+ year| region|province| gdp| fdi| +-++--++-+ 1996|Southwest China| Tibet| 64.98| 679.0| 1997|Southwest China| Tibet| 77.24| 63.0| 1998|Southwest China| Tibet| 91.5| 481.0| 1999|Southwest China| Tibet|105.98| 196.0| 2000|Southwest China| Tibet| 117.8| 2.0| 2001|Southwest China| Tibet|139.16| 106.0| 2002|Southwest China| Tibet|162.04| 293.0| 1996|Northwest China| Qinghai|184.17| 576.0| 2003|Southwest China| Tibet|185.09| 467.0| 1997|Northwest China| Qinghai|202.79| 247.0| 1996|Northwest China| Ningxia| 202.9| 2826.0| 2004|Southwest China| Tibet|220.34| 2699.0| 1998|Northwest China| Qinghai|220.92| 1010.0| 1997|Northwest China| Ningxia|224.59| 671.0| 1999|Northwest China| Qinghai|239.38| 459.0| 1998|Northwest China| Ningxia|245.44| 1856.0| 2005|Southwest China| Tibet| 248.8| 1151.0| 2000|Northwest China| Qinghai|263.68|11020.0| 1999|Northwest China| Ningxia|264.58| 5134.0| 2006|Southwest China| Tibet|290.76| 1522.0| +-++--++-+ only showing top 20 rows Sorting RDDs by Columns . from pyspark.sql import functions as F df.sort(F.desc(&quot;gdp&quot;)).show() . +-+-++--++ year| region| province| gdp| fdi| +-+-++--++ 2007|South Central China|Guangdong|31777.01|1712603.0| 2006|South Central China|Guangdong|26587.76|1451065.0| 2007| East China| Shandong|25776.91|1101159.0| 2005|South Central China|Guangdong|22557.37|1236400.0| 2006| East China| Shandong|21900.19|1000069.0| 2007| East China| Jiangsu|21742.05|1743140.0| 2004|South Central China|Guangdong|18864.62|1001158.0| 2007| East China| Zhejiang|18753.73|1036576.0| 2006| East China| Jiangsu|18598.69|1318339.0| 2005| East China| Shandong|18366.87| 897000.0| 2003|South Central China|Guangdong|15844.64| 782294.0| 2006| East China| Zhejiang|15718.47| 888935.0| 2004| East China| Shandong|15021.84| 870064.0| 2007|South Central China| Henan|15012.46| 306162.0| 2005| East China| Jiangsu| 15003.6|1213800.0| 2007| North China| Hebei|13607.32| 241621.0| 2002|South Central China|Guangdong|13502.42|1133400.0| 2005| East China| Zhejiang|13417.68| 772000.0| 2007| East China| Shanghai|12494.01| 792000.0| 2004| East China| Jiangsu|12442.87|1056365.0| +-+-++--++ only showing top 20 rows Casting Data Types . from pyspark.sql.types import IntegerType, StringType, DoubleType df = df.withColumn(&#39;gdp&#39;, F.col(&#39;gdp&#39;).cast(DoubleType())) . df = df.withColumn(&#39;province&#39;, F.col(&#39;province&#39;).cast(StringType())) . df.filter((df.gdp&gt;10000) &amp; (df.region==&#39;East China&#39;)).show() . +-+-+--+-++ year| region|province| gdp| fdi| +-+-+--+-++ 2003|East China| Jiangsu| 10606.849609375|1018960.0| 2004|East China| Jiangsu|12442.8701171875|1056365.0| 2005|East China| Jiangsu| 15003.599609375|1213800.0| 2006|East China| Jiangsu| 18598.689453125|1318339.0| 2007|East China| Jiangsu| 21742.05078125|1743140.0| 2002|East China|Shandong| 10275.5| 473404.0| 2003|East China|Shandong| 12078.150390625| 601617.0| 2004|East China|Shandong| 15021.83984375| 870064.0| 2005|East China|Shandong| 18366.869140625| 897000.0| 2006|East China|Shandong| 21900.189453125|1000069.0| 2007|East China|Shandong| 25776.91015625|1101159.0| 2006|East China|Shanghai| 10572.240234375| 710700.0| 2007|East China|Shanghai| 12494.009765625| 792000.0| 2004|East China|Zhejiang|11648.7001953125| 668128.0| 2005|East China|Zhejiang| 13417.6796875| 772000.0| 2006|East China|Zhejiang|15718.4697265625| 888935.0| 2007|East China|Zhejiang| 18753.73046875|1036576.0| +-+-+--+-++ Aggregating using groupBy, .agg and sum/max . from pyspark.sql import functions as F df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg(F.sum(&quot;gdp&quot;) ,F.max(&quot;gdp&quot;)).show() . +-++++ region| province| sum(gdp)| max(gdp)| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg(F.sum(&quot;gdp&quot;).alias(&quot;SumGDP&quot;),F.max(&quot;gdp&quot;).alias(&quot;MaxGDP&quot;)).show() . +-++++ region| province| GDP| MaxGDP| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg( F.sum(&quot;gdp&quot;).alias(&quot;SumGDP&quot;), F.max(&quot;gdp&quot;).alias(&quot;MaxGDP&quot;) ).show() . +-++++ region| province| SumGDP| MaxGDP| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.limit(10).toPandas() . year region province gdp fdi . 0 1996 | East China | Anhui | 2093.300049 | 50661.0 | . 1 1997 | East China | Anhui | 2347.320068 | 43443.0 | . 2 1998 | East China | Anhui | 2542.959961 | 27673.0 | . 3 1999 | East China | Anhui | 2712.340088 | 26131.0 | . 4 2000 | East China | Anhui | 2902.090088 | 31847.0 | . 5 2001 | East China | Anhui | 3246.709961 | 33672.0 | . 6 2002 | East China | Anhui | 3519.719971 | 38375.0 | . 7 2003 | East China | Anhui | 3923.110107 | 36720.0 | . 8 2004 | East China | Anhui | 4759.299805 | 54669.0 | . 9 2005 | East China | Anhui | 5350.169922 | 69000.0 | . Exponentials using exp . df = df.withColumn(&quot;Exp_GDP&quot;, F.exp(&quot;gdp&quot;)) df.show() . +-+--+--+--+--+--+ year| region|province| gdp| fdi| Exp_GDP| +-+--+--+--+--+--+ 1996| East China| Anhui|2093.300048828125| 50661.0|Infinity| 1997| East China| Anhui|2347.320068359375| 43443.0|Infinity| 1998| East China| Anhui| 2542.9599609375| 27673.0|Infinity| 1999| East China| Anhui|2712.340087890625| 26131.0|Infinity| 2000| East China| Anhui|2902.090087890625| 31847.0|Infinity| 2001| East China| Anhui| 3246.7099609375| 33672.0|Infinity| 2002| East China| Anhui|3519.719970703125| 38375.0|Infinity| 2003| East China| Anhui|3923.110107421875| 36720.0|Infinity| 2004| East China| Anhui| 4759.2998046875| 54669.0|Infinity| 2005| East China| Anhui| 5350.169921875| 69000.0|Infinity| 2006| East China| Anhui| 6112.5|139354.0|Infinity| 2007| East China| Anhui| 7360.919921875|299892.0|Infinity| 1996|North China| Beijing|1789.199951171875|155290.0|Infinity| 1997|North China| Beijing|2077.090087890625|159286.0|Infinity| 1998|North China| Beijing|2377.179931640625|216800.0|Infinity| 1999|North China| Beijing|2678.820068359375|197525.0|Infinity| 2000|North China| Beijing|3161.659912109375|168368.0|Infinity| 2001|North China| Beijing| 3707.9599609375|176818.0|Infinity| 2002|North China| Beijing| 4315.0|172464.0|Infinity| 2003|North China| Beijing| 5007.2099609375|219126.0|Infinity| +-+--+--+--+--+--+ only showing top 20 rows Window functions . . Note: Window functions . # Window functions from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(F.desc(&#39;gdp&#39;)) df.withColumn(&quot;rank&quot;,F.rank().over(windowSpec)).show() . +-+-++--++--+-+ year| region| province| gdp| fdi| Exp_GDP|rank| +-+-++--++--+-+ 2007|South Central China|Guangdong| 31777.009765625|1712603.0|Infinity| 1| 2006|South Central China|Guangdong| 26587.759765625|1451065.0|Infinity| 2| 2005|South Central China|Guangdong| 22557.369140625|1236400.0|Infinity| 3| 2004|South Central China|Guangdong| 18864.619140625|1001158.0|Infinity| 4| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity| 5| 2002|South Central China|Guangdong| 13502.419921875|1133400.0|Infinity| 6| 2001|South Central China|Guangdong| 12039.25|1193203.0|Infinity| 7| 2000|South Central China|Guangdong| 10741.25|1128091.0|Infinity| 8| 1999|South Central China|Guangdong| 9250.6796875|1165750.0|Infinity| 9| 1998|South Central China|Guangdong| 8530.8798828125|1201994.0|Infinity| 10| 1997|South Central China|Guangdong| 7774.52978515625|1171083.0|Infinity| 11| 1996|South Central China|Guangdong| 6834.97021484375|1162362.0|Infinity| 12| 2007|South Central China| Hunan| 9439.599609375| 327051.0|Infinity| 1| 2006|South Central China| Hunan| 7688.669921875| 259335.0|Infinity| 2| 2005|South Central China| Hunan| 6596.10009765625| 207200.0|Infinity| 3| 2004|South Central China| Hunan| 5641.93994140625| 141800.0|Infinity| 4| 2003|South Central China| Hunan| 4659.990234375| 101835.0|Infinity| 5| 2002|South Central China| Hunan| 4151.5400390625| 90022.0|Infinity| 6| 2001|South Central China| Hunan| 3831.89990234375| 81011.0|Infinity| 7| 2000|South Central China| Hunan|3551.489990234375| 67833.0|Infinity| 8| +-+-++--++--+-+ only showing top 20 rows from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;) . Lagging Variables . dfWithLag = df.withColumn(&quot;lag_7&quot;,F.lag(&quot;gdp&quot;, 7).over(windowSpec)) . df.filter(df.year&gt;&#39;2000&#39;).show() . +-++++--+--+ year| region| province| gdp| fdi| Exp_GDP| +-++++--+--+ 2001| East China| Anhui| 3246.7099609375| 33672.0|Infinity| 2002| East China| Anhui| 3519.719970703125| 38375.0|Infinity| 2003| East China| Anhui| 3923.110107421875| 36720.0|Infinity| 2004| East China| Anhui| 4759.2998046875| 54669.0|Infinity| 2005| East China| Anhui| 5350.169921875| 69000.0|Infinity| 2006| East China| Anhui| 6112.5|139354.0|Infinity| 2007| East China| Anhui| 7360.919921875|299892.0|Infinity| 2001| North China| Beijing| 3707.9599609375|176818.0|Infinity| 2002| North China| Beijing| 4315.0|172464.0|Infinity| 2003| North China| Beijing| 5007.2099609375|219126.0|Infinity| 2004| North China| Beijing| 6033.2099609375|308354.0|Infinity| 2005| North China| Beijing| 6969.52001953125|352638.0|Infinity| 2006| North China| Beijing| 8117.77978515625|455191.0|Infinity| 2007| North China| Beijing| 9846.8095703125|506572.0|Infinity| 2001|Southwest China|Chongqing|1976.8599853515625| 25649.0|Infinity| 2002|Southwest China|Chongqing| 2232.860107421875| 19576.0|Infinity| 2003|Southwest China|Chongqing| 2555.719970703125| 26083.0|Infinity| 2004|Southwest China|Chongqing| 3034.580078125| 40508.0|Infinity| 2005|Southwest China|Chongqing| 3467.719970703125| 51600.0|Infinity| 2006|Southwest China|Chongqing| 3907.22998046875| 69595.0|Infinity| +-++++--+--+ only showing top 20 rows Looking at windows within the data . from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;).rowsBetween(-6,0) . dfWithRoll = df.withColumn(&quot;roll_7_confirmed&quot;,F.mean(&quot;gdp&quot;).over(windowSpec)) . dfWithRoll.filter(dfWithLag.year&gt;&#39;2001&#39;).show() . +-+-++++--++ year| region| province| gdp| fdi| Exp_GDP| roll_7_confirmed| +-+-++++--++ 2002|South Central China|Guangdong| 13502.419921875|1133400.0| Infinity| 9810.56849888393| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0| Infinity|11097.664132254464| 2004|South Central China|Guangdong| 18864.619140625|1001158.0| Infinity|12681.962611607143| 2005|South Central China|Guangdong| 22557.369140625|1236400.0| Infinity|14685.746791294643| 2006|South Central China|Guangdong| 26587.759765625|1451065.0| Infinity|17162.472516741072| 2007|South Central China|Guangdong| 31777.009765625|1712603.0| Infinity| 20167.5810546875| 2002|South Central China| Hunan| 4151.5400390625| 90022.0| Infinity|3309.1999860491073| 2003|South Central China| Hunan| 4659.990234375| 101835.0| Infinity| 3612.037179129464| 2004|South Central China| Hunan| 5641.93994140625| 141800.0| Infinity|4010.9900251116073| 2005|South Central China| Hunan| 6596.10009765625| 207200.0| Infinity| 4521.07146344866| 2006|South Central China| Hunan| 7688.669921875| 259335.0| Infinity| 5160.232875279018| 2007|South Central China| Hunan| 9439.599609375| 327051.0| Infinity| 6001.391392299107| 2002| North China| Shanxi| 2324.800048828125| 21164.0| Infinity|1749.4771379743304| 2003| North China| Shanxi| 2855.22998046875| 21361.0| Infinity| 1972.779994419643| 2004| North China| Shanxi| 3571.3701171875| 62184.0| Infinity| 2272.118582589286| 2005| North China| Shanxi| 4230.52978515625| 27516.0| Infinity| 2646.325701032366| 2006| North China| Shanxi| 4878.60986328125| 47199.0| Infinity|3105.1128278459823| 2007| North China| Shanxi| 6024.4501953125| 134283.0| Infinity| 3702.074288504464| 2002| Southwest China| Tibet| 162.0399932861328| 293.0|2.360885537826244E70|108.38571493966239| 2003| Southwest China| Tibet|185.08999633789062| 467.0|2.418600091901801E80|125.54428536551339| +-+-++++--++ only showing top 20 rows from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;).rowsBetween(Window.unboundedPreceding,Window.currentRow) . dfWithRoll = df.withColumn(&quot;cumulative_gdp&quot;,F.sum(&quot;gdp&quot;).over(windowSpec)) . dfWithRoll.filter(dfWithLag.year&gt;&#39;1999&#39;).show() . +-+-++--++--++ year| region| province| gdp| fdi| Exp_GDP| cumulative_gdp| +-+-++--++--++ 2000|South Central China|Guangdong| 10741.25|1128091.0|Infinity| 43132.3095703125| 2001|South Central China|Guangdong| 12039.25|1193203.0|Infinity| 55171.5595703125| 2002|South Central China|Guangdong| 13502.419921875|1133400.0|Infinity| 68673.9794921875| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity| 84518.619140625| 2004|South Central China|Guangdong| 18864.619140625|1001158.0|Infinity| 103383.23828125| 2005|South Central China|Guangdong| 22557.369140625|1236400.0|Infinity| 125940.607421875| 2006|South Central China|Guangdong| 26587.759765625|1451065.0|Infinity| 152528.3671875| 2007|South Central China|Guangdong| 31777.009765625|1712603.0|Infinity| 184305.376953125| 2000|South Central China| Hunan|3551.489990234375| 67833.0|Infinity| 15180.9599609375| 2001|South Central China| Hunan| 3831.89990234375| 81011.0|Infinity| 19012.85986328125| 2002|South Central China| Hunan| 4151.5400390625| 90022.0|Infinity| 23164.39990234375| 2003|South Central China| Hunan| 4659.990234375| 101835.0|Infinity| 27824.39013671875| 2004|South Central China| Hunan| 5641.93994140625| 141800.0|Infinity| 33466.330078125| 2005|South Central China| Hunan| 6596.10009765625| 207200.0|Infinity| 40062.43017578125| 2006|South Central China| Hunan| 7688.669921875| 259335.0|Infinity| 47751.10009765625| 2007|South Central China| Hunan| 9439.599609375| 327051.0|Infinity| 57190.69970703125| 2000| North China| Shanxi|1845.719970703125| 22472.0|Infinity|7892.0098876953125| 2001| North China| Shanxi|2029.530029296875| 23393.0|Infinity| 9921.539916992188| 2002| North China| Shanxi|2324.800048828125| 21164.0|Infinity|12246.339965820312| 2003| North China| Shanxi| 2855.22998046875| 21361.0|Infinity|15101.569946289062| +-+-++--++--++ only showing top 20 rows Pivot Dataframes . . Note: Pivot Dataframes . pivoted_df = df.groupBy(&#39;year&#39;).pivot(&#39;province&#39;) .agg(F.sum(&#39;gdp&#39;).alias(&#39;gdp&#39;) , F.sum(&#39;fdi&#39;).alias(&#39;fdi&#39;)) pivoted_df.limit(10).toPandas() . year Anhui_gdp Anhui_fdi Beijing_gdp Beijing_fdi Chongqing_gdp Chongqing_fdi Fujian_gdp Fujian_fdi Gansu_gdp Gansu_fdi Guangdong_gdp Guangdong_fdi Guangxi_gdp Guangxi_fdi Guizhou_gdp Guizhou_fdi Hainan_gdp Hainan_fdi Hebei_gdp Hebei_fdi Heilongjiang_gdp Heilongjiang_fdi Henan_gdp Henan_fdi Hubei_gdp Hubei_fdi Hunan_gdp Hunan_fdi Jiangsu_gdp Jiangsu_fdi Jiangxi_gdp Jiangxi_fdi Jilin_gdp Jilin_fdi Liaoning_gdp Liaoning_fdi Ningxia_gdp Ningxia_fdi Qinghai_gdp Qinghai_fdi Shaanxi_gdp Shaanxi_fdi Shandong_gdp Shandong_fdi Shanghai_gdp Shanghai_fdi Shanxi_gdp Shanxi_fdi Sichuan_gdp Sichuan_fdi Tianjin_gdp Tianjin_fdi Tibet_gdp Tibet_fdi Xinjiang_gdp Xinjiang_fdi Yunnan_gdp Yunnan_fdi Zhejiang_gdp Zhejiang_fdi . 0 2003 | 3923.110107 | 36720.0 | 5007.209961 | 219126.0 | 2555.719971 | 26083.0 | 4983.669922 | 259903.0 | 1399.829956 | 2342.0 | 15844.639648 | 782294.0 | 2821.110107 | 41856.0 | 1426.339966 | 4521.0 | 713.960022 | 42125.0 | 6921.290039 | 96405.0 | 4057.399902 | 32180.0 | 6867.700195 | 53903.0 | 4757.450195 | 156886.0 | 4659.990234 | 101835.0 | 10606.849609 | 1018960.0 | 2450.479980 | 108197.0 | 2348.540039 | 24468.0 | 5458.220215 | 341168.0 | 445.359985 | 1743.0 | 390.200012 | 2522.0 | 2587.719971 | 33190.0 | 12078.150391 | 601617.0 | 6694.229980 | 546849.0 | 2855.229980 | 21361.0 | 5333.089844 | 41231.0 | 2578.030029 | 153473.0 | 185.089996 | 467.0 | 1886.349976 | 1534.0 | 2556.020020 | 8384.0 | 9705.019531 | 498055.0 | . 1 2007 | 7360.919922 | 299892.0 | 9846.809570 | 506572.0 | 4676.129883 | 108534.0 | 9248.530273 | 406058.0 | 2703.979980 | 11802.0 | 31777.009766 | 1712603.0 | 5823.410156 | 68396.0 | 2884.110107 | 12651.0 | 1254.170044 | 112001.0 | 13607.320312 | 241621.0 | 7104.000000 | 208508.0 | 15012.459961 | 306162.0 | 9333.400391 | 276622.0 | 9439.599609 | 327051.0 | 21742.050781 | 1743140.0 | 4820.529785 | 280657.0 | 4275.120117 | 76064.0 | 9304.519531 | 598554.0 | 919.109985 | 5047.0 | 797.349976 | 31000.0 | 5757.290039 | 119516.0 | 25776.910156 | 1101159.0 | 12494.009766 | 792000.0 | 6024.450195 | 134283.0 | 10562.389648 | 149322.0 | 5252.759766 | 527776.0 | 341.429993 | 2418.0 | 3523.159912 | 12484.0 | 4772.520020 | 39453.0 | 18753.730469 | 1036576.0 | . 2 2006 | 6112.500000 | 139354.0 | 8117.779785 | 455191.0 | 3907.229980 | 69595.0 | 7583.850098 | 322047.0 | 2277.350098 | 2954.0 | 26587.759766 | 1451065.0 | 4746.160156 | 44740.0 | 2338.979980 | 9384.0 | 1065.670044 | 74878.0 | 11467.599609 | 201434.0 | 6211.799805 | 170801.0 | 12362.790039 | 184526.0 | 7617.470215 | 244853.0 | 7688.669922 | 259335.0 | 18598.689453 | 1318339.0 | 4056.760010 | 242000.0 | 3620.270020 | 66100.0 | 8047.259766 | 359000.0 | 725.900024 | 3718.0 | 648.500000 | 27500.0 | 4743.609863 | 92489.0 | 21900.189453 | 1000069.0 | 10572.240234 | 710700.0 | 4878.609863 | 47199.0 | 8690.240234 | 120819.0 | 4462.740234 | 413077.0 | 290.760010 | 1522.0 | 3045.260010 | 10366.0 | 3988.139893 | 30234.0 | 15718.469727 | 888935.0 | . 3 1997 | 2347.320068 | 43443.0 | 2077.090088 | 159286.0 | 1509.750000 | 38675.0 | 2870.899902 | 419666.0 | 793.570007 | 4144.0 | 7774.529785 | 1171083.0 | 1817.250000 | 87986.0 | 805.789978 | 4977.0 | 411.160004 | 70554.0 | 3953.780029 | 110064.0 | 2667.500000 | 73485.0 | 4041.090088 | 69204.0 | 2856.469971 | 79019.0 | 2849.270020 | 91702.0 | 6004.209961 | 507208.0 | 1409.739990 | 30068.0 | 1346.790039 | 45155.0 | 3157.689941 | 167142.0 | 224.589996 | 671.0 | 202.789993 | 247.0 | 1363.599976 | 62816.0 | 6537.069824 | 249294.0 | 3438.790039 | 422536.0 | 1476.000000 | 26592.0 | 3241.469971 | 24846.0 | 1264.630005 | 251135.0 | 77.239998 | 63.0 | 1039.849976 | 2472.0 | 1676.170044 | 16566.0 | 4686.109863 | 150345.0 | . 4 2004 | 4759.299805 | 54669.0 | 6033.209961 | 308354.0 | 3034.580078 | 40508.0 | 5763.350098 | 474801.0 | 1688.489990 | 3539.0 | 18864.619141 | 1001158.0 | 3433.500000 | 29579.0 | 1677.800049 | 6533.0 | 819.659973 | 64343.0 | 8477.629883 | 162341.0 | 4750.600098 | 123639.0 | 8553.790039 | 87367.0 | 5633.240234 | 207126.0 | 5641.939941 | 141800.0 | 12442.870117 | 1056365.0 | 2807.409912 | 161202.0 | 2662.080078 | 19059.0 | 6002.540039 | 282410.0 | 537.109985 | 6689.0 | 466.100006 | 22500.0 | 3175.580078 | 52664.0 | 15021.839844 | 870064.0 | 8072.830078 | 654100.0 | 3571.370117 | 62184.0 | 6379.629883 | 70129.0 | 3110.969971 | 247243.0 | 220.339996 | 2699.0 | 2209.090088 | 4586.0 | 3081.909912 | 14200.0 | 11648.700195 | 668128.0 | . 5 1996 | 2093.300049 | 50661.0 | 1789.199951 | 155290.0 | 1315.119995 | 21878.0 | 2484.250000 | 407876.0 | 722.520020 | 9002.0 | 6834.970215 | 1162362.0 | 1697.900024 | 66618.0 | 723.179993 | 3138.0 | 389.679993 | 78960.0 | 3452.969971 | 123652.0 | 2370.500000 | 54841.0 | 3634.689941 | 52566.0 | 2499.770020 | 68878.0 | 2540.129883 | 70344.0 | 5155.250000 | 478058.0 | 1169.729980 | 28818.0 | 1137.229980 | 39876.0 | 2793.370117 | 140405.0 | 202.899994 | 2826.0 | 184.169998 | 576.0 | 1215.839966 | 33008.0 | 5883.799805 | 259041.0 | 2957.550049 | 471578.0 | 1292.109985 | 13802.0 | 2871.649902 | 22519.0 | 1121.930054 | 200587.0 | 64.980003 | 679.0 | 900.929993 | 6639.0 | 1517.689941 | 18000.0 | 4188.529785 | 152021.0 | . 6 1998 | 2542.959961 | 27673.0 | 2377.179932 | 216800.0 | 1602.380005 | 43107.0 | 3159.909912 | 421211.0 | 887.669983 | 3864.0 | 8530.879883 | 1201994.0 | 1911.300049 | 88613.0 | 858.390015 | 4535.0 | 442.130005 | 71715.0 | 4256.009766 | 142868.0 | 2774.399902 | 52639.0 | 4308.240234 | 61654.0 | 3114.020020 | 97294.0 | 3025.530029 | 81816.0 | 6680.339844 | 543511.0 | 1605.770020 | 47768.0 | 1464.339966 | 40227.0 | 3582.459961 | 220470.0 | 245.440002 | 1856.0 | 220.919998 | 1010.0 | 1458.400024 | 30010.0 | 7021.350098 | 220274.0 | 3801.090088 | 360150.0 | 1611.079956 | 24451.0 | 3474.090088 | 37248.0 | 1374.599976 | 211361.0 | 91.500000 | 481.0 | 1106.949951 | 2167.0 | 1831.329956 | 14568.0 | 5052.620117 | 131802.0 | . 7 2001 | 3246.709961 | 33672.0 | 3707.959961 | 176818.0 | 1976.859985 | 25649.0 | 4072.850098 | 391804.0 | 1125.369995 | 7439.0 | 12039.250000 | 1193203.0 | 2279.340088 | 38416.0 | 1133.270020 | 2829.0 | 579.169983 | 46691.0 | 5516.759766 | 66989.0 | 3390.100098 | 34114.0 | 5533.009766 | 45729.0 | 3880.530029 | 118860.0 | 3831.899902 | 81011.0 | 8553.690430 | 642550.0 | 2003.069946 | 22724.0 | 1951.510010 | 33701.0 | 4669.060059 | 204446.0 | 337.440002 | 1680.0 | 300.130005 | 3649.0 | 2010.619995 | 35174.0 | 9195.040039 | 352093.0 | 5210.120117 | 429159.0 | 2029.530029 | 23393.0 | 4293.490234 | 58188.0 | 1919.089966 | 213348.0 | 139.160004 | 106.0 | 1491.599976 | 2035.0 | 2138.310059 | 6457.0 | 6898.339844 | 221162.0 | . 8 2005 | 5350.169922 | 69000.0 | 6969.520020 | 352638.0 | 3467.719971 | 51600.0 | 6554.689941 | 260800.0 | 1933.979980 | 2000.0 | 22557.369141 | 1236400.0 | 3984.100098 | 37866.0 | 2005.420044 | 10768.0 | 918.750000 | 68400.0 | 10012.110352 | 191000.0 | 5513.700195 | 145000.0 | 10587.419922 | 123000.0 | 6590.189941 | 218500.0 | 6596.100098 | 207200.0 | 15003.599609 | 1213800.0 | 3456.699951 | 205238.0 | 3122.010010 | 45266.0 | 6672.000000 | 540679.0 | 612.609985 | 14100.0 | 543.320007 | 26600.0 | 3933.719971 | 62800.0 | 18366.869141 | 897000.0 | 9247.660156 | 685000.0 | 4230.529785 | 27516.0 | 7385.100098 | 88686.0 | 3905.639893 | 332885.0 | 248.800003 | 1151.0 | 2604.189941 | 4700.0 | 3462.729980 | 17352.0 | 13417.679688 | 772000.0 | . 9 2000 | 2902.090088 | 31847.0 | 3161.659912 | 168368.0 | 1791.000000 | 24436.0 | 3764.540039 | 343191.0 | 1052.880005 | 6235.0 | 10741.250000 | 1128091.0 | 2080.040039 | 52466.0 | 1029.920044 | 2501.0 | 526.820007 | 43080.0 | 5043.959961 | 67923.0 | 3151.399902 | 30086.0 | 5052.990234 | 56403.0 | 3545.389893 | 94368.0 | 3551.489990 | 67833.0 | 7697.819824 | 607756.0 | 1853.650024 | 32080.0 | 1672.959961 | 30120.0 | 4171.689941 | 106173.0 | 295.019989 | 1741.0 | 263.679993 | 11020.0 | 1804.000000 | 28842.0 | 8337.469727 | 297119.0 | 4771.169922 | 316014.0 | 1845.719971 | 22472.0 | 3928.199951 | 43694.0 | 1701.880005 | 116601.0 | 117.800003 | 2.0 | 1363.560059 | 1911.0 | 2011.189941 | 12812.0 | 6141.029785 | 161266.0 | . pivoted_df.columns . Out[55]: [&#39;year&#39;, &#39;Anhui_gdp&#39;, &#39;Anhui_fdi&#39;, &#39;Beijing_gdp&#39;, &#39;Beijing_fdi&#39;, &#39;Chongqing_gdp&#39;, &#39;Chongqing_fdi&#39;, &#39;Fujian_gdp&#39;, &#39;Fujian_fdi&#39;, &#39;Gansu_gdp&#39;, &#39;Gansu_fdi&#39;, &#39;Guangdong_gdp&#39;, &#39;Guangdong_fdi&#39;, &#39;Guangxi_gdp&#39;, &#39;Guangxi_fdi&#39;, &#39;Guizhou_gdp&#39;, &#39;Guizhou_fdi&#39;, &#39;Hainan_gdp&#39;, &#39;Hainan_fdi&#39;, &#39;Hebei_gdp&#39;, &#39;Hebei_fdi&#39;, &#39;Heilongjiang_gdp&#39;, &#39;Heilongjiang_fdi&#39;, &#39;Henan_gdp&#39;, &#39;Henan_fdi&#39;, &#39;Hubei_gdp&#39;, &#39;Hubei_fdi&#39;, &#39;Hunan_gdp&#39;, &#39;Hunan_fdi&#39;, &#39;Jiangsu_gdp&#39;, &#39;Jiangsu_fdi&#39;, &#39;Jiangxi_gdp&#39;, &#39;Jiangxi_fdi&#39;, &#39;Jilin_gdp&#39;, &#39;Jilin_fdi&#39;, &#39;Liaoning_gdp&#39;, &#39;Liaoning_fdi&#39;, &#39;Ningxia_gdp&#39;, &#39;Ningxia_fdi&#39;, &#39;Qinghai_gdp&#39;, &#39;Qinghai_fdi&#39;, &#39;Shaanxi_gdp&#39;, &#39;Shaanxi_fdi&#39;, &#39;Shandong_gdp&#39;, &#39;Shandong_fdi&#39;, &#39;Shanghai_gdp&#39;, &#39;Shanghai_fdi&#39;, &#39;Shanxi_gdp&#39;, &#39;Shanxi_fdi&#39;, &#39;Sichuan_gdp&#39;, &#39;Sichuan_fdi&#39;, &#39;Tianjin_gdp&#39;, &#39;Tianjin_fdi&#39;, &#39;Tibet_gdp&#39;, &#39;Tibet_fdi&#39;, &#39;Xinjiang_gdp&#39;, &#39;Xinjiang_fdi&#39;, &#39;Yunnan_gdp&#39;, &#39;Yunnan_fdi&#39;, &#39;Zhejiang_gdp&#39;, &#39;Zhejiang_fdi&#39;] newColnames = [x.replace(&quot;-&quot;,&quot;_&quot;) for x in pivoted_df.columns] . pivoted_df = pivoted_df.toDF(*newColnames) . expression = &quot;&quot; cnt=0 for column in pivoted_df.columns: if column!=&#39;year&#39;: cnt +=1 expression += f&quot;&#39;{column}&#39; , {column},&quot; expression = f&quot;stack({cnt}, {expression[:-1]}) as (Type,Value)&quot; . Unpivoting RDDs . unpivoted_df = pivoted_df.select(&#39;year&#39;,F.expr(expression)) unpivoted_df.show() . +-+-++ year| Type| Value| +-+-++ 2003| Anhui_gdp| 3923.110107421875| 2003| Anhui_fdi| 36720.0| 2003| Beijing_gdp| 5007.2099609375| 2003| Beijing_fdi| 219126.0| 2003|Chongqing_gdp| 2555.719970703125| 2003|Chongqing_fdi| 26083.0| 2003| Fujian_gdp| 4983.669921875| 2003| Fujian_fdi| 259903.0| 2003| Gansu_gdp|1399.8299560546875| 2003| Gansu_fdi| 2342.0| 2003|Guangdong_gdp| 15844.6396484375| 2003|Guangdong_fdi| 782294.0| 2003| Guangxi_gdp| 2821.110107421875| 2003| Guangxi_fdi| 41856.0| 2003| Guizhou_gdp|1426.3399658203125| 2003| Guizhou_fdi| 4521.0| 2003| Hainan_gdp| 713.9600219726562| 2003| Hainan_fdi| 42125.0| 2003| Hebei_gdp| 6921.2900390625| 2003| Hebei_fdi| 96405.0| +-+-++ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/22/Window-functions-and-Pivot-Tables-with-Pyspark.html",
            "relUrl": "/2020/08/22/Window-functions-and-Pivot-Tables-with-Pyspark.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post92": {
            "title": "RDDs and Schemas and Data Types with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) df.columns . Out[64]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;] df.describe() . Out[65]: DataFrame[summary: string, _c0: string, province: string, specific: string, general: string, year: string, gdp: string, fdi: string, rnr: string, rr: string, i: string, fr: string, reg: string, it: string] Setting Data Schema and Data Types . from pyspark.sql.types import StructField,StringType,IntegerType,StructType . data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, IntegerType(), True) ,StructField(&quot;general&quot;, IntegerType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, IntegerType(), True) ,StructField(&quot;fdi&quot;, IntegerType(), True) ,StructField(&quot;rnr&quot;, IntegerType(), True) ,StructField(&quot;rr&quot;, IntegerType(), True) ,StructField(&quot;i&quot;, IntegerType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] . final_struc = StructType(fields=data_schema) . Applying the Data Schema/Data Types while reading in a CSV . df = spark.read.format(&quot;CSV&quot;).schema(final_struc).load(file_location) . df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) df.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows df[&#39;fr&#39;] . Out[72]: Column&lt;b&#39;fr&#39;&gt; type(df[&#39;fr&#39;]) . Out[73]: pyspark.sql.column.Column df.select(&#39;fr&#39;) . Out[74]: DataFrame[fr: int] type(df.select(&#39;fr&#39;)) . Out[75]: pyspark.sql.dataframe.DataFrame df.select(&#39;fr&#39;).show() . +-+ fr| +-+ null| 1128873| 1356287| 1518236| 1646891| 1601508| 1672445| 1677840| 1896479| null| null| 3434548| 4468640| 634562| 634562| 938788| null| 1667114| 2093925| 2511249| +-+ only showing top 20 rows df.head(2) . Out[77]: [Row(_c0=None, province=&#39;province&#39;, specific=None, general=None, year=None, gdp=None, fdi=None, rnr=None, rr=None, i=None, fr=None, reg=&#39;reg&#39;, it=None), Row(_c0=0, province=&#39;Anhui&#39;, specific=None, general=None, year=1996, gdp=None, fdi=50661, rnr=None, rr=None, i=None, fr=1128873, reg=&#39;East China&#39;, it=631930)] df.select([&#39;reg&#39;,&#39;fr&#39;]) . Out[78]: DataFrame[reg: string, fr: int] Using select with RDDs . df.select([&#39;reg&#39;,&#39;fr&#39;]).show() . +--+-+ reg| fr| +--+-+ reg| null| East China|1128873| East China|1356287| East China|1518236| East China|1646891| East China|1601508| East China|1672445| East China|1677840| East China|1896479| East China| null| East China| null| East China|3434548| East China|4468640| North China| 634562| North China| 634562| North China| 938788| North China| null| North China|1667114| North China|2093925| North China|2511249| +--+-+ only showing top 20 rows df.withColumn(&#39;fiscal_revenue&#39;,df[&#39;fr&#39;]).show() . +-+--+--+-+-+-++-+-+-+-+--+-+--+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-+--+ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1128873| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 1356287| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 1518236| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 1646891| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 1601508| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 1672445| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 1677840| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 1896479| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 3434548| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 4468640| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 634562| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 634562| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 938788| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 1667114| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 2093925| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 2511249| +-+--+--+-+-+-++-+-+-+-+--+-+--+ only showing top 20 rows df.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows Renaming Columns using withColumnRenamed . df.withColumnRenamed(&#39;fr&#39;,&#39;new_fiscal_revenue&#39;).show() . +-+--+--+-+-+-++-+-+-++--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i|new_fiscal_revenue| reg| it| +-+--+--+-+-+-++-+-+-++--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null| 1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null| 1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null| 1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null| 1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null| 1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null| 1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null| 1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null| 1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null| 3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null| 4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null| 1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null| 2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null| 2511249|North China|1078754| +-+--+--+-+-+-++-+-+-++--+-+ only showing top 20 rows New Columns by Transforming extant Columns using withColumn . df.withColumn(&#39;double_fiscal_revenue&#39;,df[&#39;fr&#39;]*2).show() . +-+--+--+-+-+-++-+-+-+-+--+-++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|double_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-++ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 2257746| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2712574| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3036472| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 3293782| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 3203016| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 3344890| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 3355680| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 3792958| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 6869096| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 8937280| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 1269124| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 1269124| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 1877576| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 3334228| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 4187850| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 5022498| +-+--+--+-+-+-++-+-+-+-+--+-++ only showing top 20 rows df.withColumn(&#39;add_fiscal_revenue&#39;,df[&#39;fr&#39;]+1).show() . +-+--+--+-+-+-++-+-+-+-+--+-++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|add_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-++ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1128874| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 1356288| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 1518237| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 1646892| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 1601509| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 1672446| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 1677841| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 1896480| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 3434549| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 4468641| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 634563| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 634563| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 938789| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 1667115| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 2093926| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 2511250| +-+--+--+-+-+-++-+-+-+-+--+-++ only showing top 20 rows df.withColumn(&#39;half_fiscal_revenue&#39;,df[&#39;fr&#39;]/2).show() . +-+--+--+-+-+-++-+-+-+-+--+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|half_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 564436.5| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 678143.5| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 759118.0| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 823445.5| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 800754.0| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 836222.5| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 838920.0| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 948239.5| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 1717274.0| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 2234320.0| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 317281.0| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 317281.0| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 469394.0| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 833557.0| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 1046962.5| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 1255624.5| +-+--+--+-+-+-++-+-+-+-+--+-+-+ only showing top 20 rows df.withColumn(&#39;half_fr&#39;,df[&#39;fr&#39;]/2) . Out[86]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int, half_fr: double] Spark SQL for SQL functionality using createOrReplaceTempView . df.createOrReplaceTempView(&quot;economic_data&quot;) . sql_results = spark.sql(&quot;SELECT * FROM economic_data&quot;) . sql_results . Out[89]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int] sql_results.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows spark.sql(&quot;SELECT * FROM economic_data WHERE fr=634562&quot;).show() . ++--+--+-+-+-++-+-+-++--++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+-++-+-+-++--++ 12| Beijing| null| null|1996|null|155290|null|null|null|634562|North China|508135| 13| Beijing| null| null|1997|null|159286|null|null|null|634562|North China|569283| ++--+--+-+-+-++-+-+-++--++ This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/21/RDDs-and-Schemas-and-Data-Types-with-Pyspark.html",
            "relUrl": "/2020/08/21/RDDs-and-Schemas-and-Data-Types-with-Pyspark.html",
            "date": " • Aug 21, 2020"
        }
        
    
  
    
        ,"post93": {
            "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.columns . Out[85]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;] df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) # for row in df.head(5): # print(row) # print(&#39; n&#39;) . df.describe().show() . +-++--+--+++--++-+--+-++++ summary| _c0|province| specific| general| year| gdp| fdi| rnr| rr| i| fr| reg| it| +-++--+--+++--++-+--+-++++ count| 360| 360| 356| 169| 360| 360| 360| 294| 296| 287| 295| 360| 360| mean| 179.5| null|583470.7303370787|309127.53846153844| 2001.5|4428.653416666667|196139.38333333333| 0.0355944252244898|0.059688621057432424|0.08376351662369343|2522449.0034013605| null|2165819.2583333333| stddev|104.06728592598157| null|654055.3290782663| 355423.5760674793|3.4568570586927794|4484.668659976412|303043.97011891654|0.16061503029299648| 0.15673351824073453| 0.1838933104683607|3491329.8613106664| null|1769294.2935487411| min| 0| Anhui| 8964.0| 0.0| 1996| 64.98| 2| 0.0| 0.0| 0.0| #REF!| East China| 147897| max| 359|Zhejiang| 3937966.0| 1737800.0| 2007| 31777.01| 1743140| 1.214285714| 0.84| 1.05| 9898522|Southwest China| 10533312| +-++--+--+++--++-+--+-++++ df.describe().printSchema() . root -- summary: string (nullable = true) -- _c0: string (nullable = true) -- province: string (nullable = true) -- specific: string (nullable = true) -- general: string (nullable = true) -- year: string (nullable = true) -- gdp: string (nullable = true) -- fdi: string (nullable = true) -- rnr: string (nullable = true) -- rr: string (nullable = true) -- i: string (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: string (nullable = true) Casting Data Types and Formatting Significant Digits . from pyspark.sql.functions import format_number . result = df.describe() result.select(result[&#39;province&#39;] ,format_number(result[&#39;specific&#39;].cast(&#39;float&#39;),2).alias(&#39;specific&#39;) ,format_number(result[&#39;general&#39;].cast(&#39;float&#39;),2).alias(&#39;general&#39;) ,format_number(result[&#39;year&#39;].cast(&#39;int&#39;),2).alias(&#39;year&#39;),format_number(result[&#39;gdp&#39;].cast(&#39;float&#39;),2).alias(&#39;gdp&#39;) ,format_number(result[&#39;rnr&#39;].cast(&#39;int&#39;),2).alias(&#39;rnr&#39;),format_number(result[&#39;rr&#39;].cast(&#39;float&#39;),2).alias(&#39;rr&#39;) ,format_number(result[&#39;fdi&#39;].cast(&#39;int&#39;),2).alias(&#39;fdi&#39;),format_number(result[&#39;it&#39;].cast(&#39;float&#39;),2).alias(&#39;it&#39;) ,result[&#39;reg&#39;].cast(&#39;string&#39;).alias(&#39;reg&#39;) ).show() . +--+++--+++++-++ province| specific| general| year| gdp| rnr| rr| fdi| it| reg| +--+++--+++++-++ 360| 356.00| 169.00| 360.00| 360.00|294.00|296.00| 360.00| 360.00| 360| null| 583,470.75| 309,127.53|2,001.00| 4,428.65| 0.00| 0.06| 196,139.00| 2,165,819.25| null| null| 654,055.31| 355,423.56| 3.00| 4,484.67| 0.00| 0.16| 303,043.00| 1,769,294.25| null| Anhui| 8,964.00| 0.00|1,996.00| 64.98| 0.00| 0.00| 2.00| 147,897.00| East China| Zhejiang|3,937,966.00|1,737,800.00|2,007.00|31,777.01| 1.00| 0.84|1,743,140.00|10,533,312.00|Southwest China| +--+++--+++++-++ New Columns generated from extant columns using withColumn . df2 = df.withColumn(&quot;specific_gdp_ratio&quot;,df[&quot;specific&quot;]/(df[&quot;gdp&quot;]*100))#.show() . df2.select(&#39;specific_gdp_ratio&#39;).show() . ++ specific_gdp_ratio| ++ 0.7022500358285959| 0.6474660463848132| 0.6878991411583352| 1.0519477646607727| 0.673928100093381| 0.7727761333780966| 1.233475958314866| 1.5783421826051272| 1.8877587040110941| 1.6792756118029895| 2.3850666666666664| 3.0077639751552794| 0.9275486250838364| 0.7989880072601573| 1.0314658544998698| 1.448708759827088| 0.8912058855158366| 1.1918224576316896| 1.2944820393974508| 1.283311464867661| ++ only showing top 20 rows df.orderBy(df[&quot;specific&quot;].asc()).head(1)[0][0] . Out[94]: 24 Finding the Mean, Max, and Min . from pyspark.sql.functions import mean df.select(mean(&quot;specific&quot;)).show() . +--+ avg(specific)| +--+ 583470.7303370787| +--+ from pyspark.sql.functions import max,min . df.select(max(&quot;specific&quot;),min(&quot;specific&quot;)).show() . +-+-+ max(specific)|min(specific)| +-+-+ 3937966.0| 8964.0| +-+-+ df.filter(&quot;specific &lt; 60000&quot;).count() . Out[98]: 23 df.filter(df[&#39;specific&#39;] &lt; 60000).count() . Out[99]: 23 from pyspark.sql.functions import count result = df.filter(df[&#39;specific&#39;] &lt; 60000) result.select(count(&#39;specific&#39;)).show() . ++ count(specific)| ++ 23| ++ (df.filter(df[&quot;gdp&quot;]&gt;8000).count()*1.0/df.count())*100 . Out[101]: 14.444444444444443 from pyspark.sql.functions import corr df.select(corr(&quot;gdp&quot;,&quot;fdi&quot;)).show() . ++ corr(gdp, fdi)| ++ 0.8366328478935896| ++ Finding the max value by Year . from pyspark.sql.functions import year #yeardf = df.withColumn(&quot;Year&quot;,year(df[&quot;year&quot;])) . max_df = df.groupBy(&#39;year&#39;).max() . max_df.select(&#39;year&#39;,&#39;max(gdp)&#39;).show() . +-+--+ year|max(gdp)| +-+--+ 2003|15844.64| 2007|31777.01| 2006|26587.76| 1997| 7774.53| 2004|18864.62| 1996| 6834.97| 1998| 8530.88| 2001|12039.25| 2005|22557.37| 2000|10741.25| 1999| 9250.68| 2002|13502.42| +-+--+ from pyspark.sql.functions import month . #df.select(&quot;year&quot;,&quot;avg(gdp)&quot;).orderBy(&#39;year&#39;).show() . This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/20/Pyspark-Dataframes-Data-Types.html",
            "relUrl": "/2020/08/20/Pyspark-Dataframes-Data-Types.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post94": {
            "title": "Dataframe Filitering and Operations with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . Filtering on values in a column . df.filter(&quot;specific&lt;10000&quot;).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| 269|Shanghai| 9834.0| null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917| ++--+--+-+-+-++++-+-+-+-+ df.filter(&quot;specific&lt;10000&quot;).select(&#39;province&#39;).show() . +--+ province| +--+ Shanghai| Shanghai| +--+ df.filter(&quot;specific&lt;10000&quot;).select([&#39;province&#39;,&#39;year&#39;]).show() . +--+-+ province|year| +--+-+ Shanghai|2000| Shanghai|2001| +--+-+ df.filter(df[&quot;specific&quot;] &lt; 10000).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| 269|Shanghai| 9834.0| null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917| ++--+--+-+-+-++++-+-+-+-+ Filtering on values in 2+ columns . df.filter((df[&quot;specific&quot;] &lt; 55000) &amp; (df[&#39;gdp&#39;] &gt; 200) ).show() . ++--+--+-+-+--++-+-+-+-+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+--++-+-+-+-+-+-+ 98| Hainan| 54462.0| null|1998| 442.13| 71715|null|null|null| 236461|South Central China| 177748| 216| Ningxia| 32088.0| null|1996| 202.9| 2826|null|null|null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671|null|null|null| 102083| Northwest China| 195295| 268|Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269|Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 270|Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271|Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 272|Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 273|Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 274|Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6|211361|null|null|null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464| North China| 547120| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| ++--+--+-+-+--++-+-+-+-+-+-+ df.filter((df[&quot;specific&quot;] &lt; 55000) | (df[&#39;gdp&#39;] &gt; 20000) ).show() . ++++--+-+--+-+--+-+--+--+-+-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++++--+-+--+-+--+-+--+--+-+-+ 69|Guangdong|1491588.0| null|2005|22557.37|1236400|0.027027027000000002| 0.0| 0.0| null|South Central China|4327217| 70|Guangdong|1897575.0|498913.0|2006|26587.76|1451065|0.027027027000000002| 0.0| 0.0|16804703|South Central China|4559252| 71|Guangdong| 859482.0| 0.0|2007|31777.01|1712603|0.027027027000000002| 0.0| 0.0|27858007|South Central China|4947824| 98| Hainan| 54462.0| null|1998| 442.13| 71715| null|null| null| 236461|South Central China| 177748| 179| Jiangsu|1188989.0| 0.0|2007|21742.05|1743140| 0.0| 0.0|0.275862069|22377276| East China|3557071| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null| null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null| null| 102083| Northwest China| 195295| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null| null| 73260| Northwest China| 218361| 262| Shandong|1204547.0|112137.0|2006|21900.19|1000069| 0.0| 0.0| 0.0|11673659| East China|5304833| 263| Shandong|2121243.0|581800.0|2007|25776.91|1101159| 0.0| 0.0| 0.0|16753980| East China|6357869| 268| Shanghai| 8964.0| null|2000| 4771.17| 316014| 0.0| 0.0| 0.44| 2224124| East China|1212473| 269| Shanghai| 9834.0| null|2001| 5210.12| 429159| 0.0| 0.0| 0.44| 2947285| East China|1053917| 270| Shanghai| 19985.0| null|2002| 5741.03| 427229| 0.0| 0.0| 0.44| 3380397| East China|1572208| 271| Shanghai| 23547.0| null|2003| 6694.23| 546849| 0.0|0.53| 0.0| 4461153| East China|2031496| 272| Shanghai| 29943.0| null|2004| 8072.83| 654100| 0.0|0.53| 0.0| null| East China|2703643| 273| Shanghai| 29943.0| null|2005| 9247.66| 685000| 0.0|0.53| 0.0| null| East China|2140461| 274| Shanghai| 42928.0| null|2006|10572.24| 710700| 0.0|0.53| 0.0| 8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6| 211361| null|null| null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95| 176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88| 116601| 0.0| 0.0| 0.0| 757464| North China| 547120| ++++--+-+--+-+--+-+--+--+-+-+ only showing top 20 rows df.filter((df[&quot;specific&quot;] &lt; 55000) &amp; ~(df[&#39;gdp&#39;] &gt; 20000) ).show() . ++--+--+-+-+--++--+-+-+-+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+--++--+-+-+-+-+-+ 98| Hainan| 54462.0| null|1998| 442.13| 71715| null|null|null| 236461|South Central China| 177748| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null|null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null|null| 102083| Northwest China| 195295| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null|null| 73260| Northwest China| 218361| 268|Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269|Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 270|Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271|Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 272|Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 273|Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 274|Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6|211361| null|null|null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464| North China| 547120| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| 312| Tibet| 18829.0| null|1996| 64.98| 679|0.181818182| 0.0| 0.0| 27801| Southwest China| 306114| 313| Tibet| 25185.0| null|1997| 77.24| 63|0.181818182| 0.0| 0.0| 33787| Southwest China| 346368| 314| Tibet| 48197.0| null|1998| 91.5| 481| 0.0|0.24| 0.0| 3810| Southwest China| 415547| ++--+--+-+-+--++--+-+-+-+-+-+ df.filter(df[&quot;specific&quot;] == 8964.0).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| ++--+--+-+-+-++++-+-+-+-+ df.filter(df[&quot;province&quot;] == &quot;Zhejiang&quot;).show() . ++--++--+-+--+-+--+--+--+--+-+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+--+-+--+--+--+--+-+-+ 348|Zhejiang| 273253.0| null|1996| 4188.53| 152021| 0.0| 0.0| 0.0| 1291252|East China| 740327| 349|Zhejiang| 330558.0| null|1997| 4686.11| 150345| 0.0| 0.0| 0.0| 1432453|East China| 814253| 350|Zhejiang| 426756.0| null|1998| 5052.62| 131802| 0.0| 0.0| 0.0| 1761084|East China| 923455| 351|Zhejiang| 586457.0| null|1999| 5443.92| 123262| 0.0| 0.0| 0.0| 2146200|East China|1001703| 352|Zhejiang| 408151.0| null|2000| 6141.03| 161266| 0.0| 0.0| 0.0| 2955508|East China|1135215| 353|Zhejiang| 358714.0| null|2001| 6898.34| 221162| 0.0| 0.0| 0.0| 4436868|East China|1203372| 354|Zhejiang| 365437.0|321686.0|2002| 8003.67| 307610| 0.0| 0.0| 0.0| 4958329|East China|1962633| 355|Zhejiang| 391292.0|260313.0|2003| 9705.02| 498055|1.214285714|0.035714286|0.035714286| 6217715|East China|2261631| 356|Zhejiang| 656175.0|276652.0|2004| 11648.7| 668128|1.214285714|0.035714286|0.035714286| null|East China|3162299| 357|Zhejiang| 656175.0| null|2005|13417.68| 772000|1.214285714|0.035714286|0.035714286| null|East China|2370200| 358|Zhejiang|1017303.0|394795.0|2006|15718.47| 888935|1.214285714|0.035714286|0.035714286|11537149|East China|2553268| 359|Zhejiang| 844647.0| 0.0|2007|18753.73|1036576|0.047619048| 0.0| 0.0|16494981|East China|2939778| ++--++--+-+--+-+--+--+--+--+-+-+ df.filter(df[&quot;specific&quot;] == 8964.0).collect() . Out[15]: [Row(_c0=268, province=&#39;Shanghai&#39;, specific=8964.0, general=None, year=2000, gdp=4771.17, fdi=316014, rnr=0.0, rr=0.0, i=0.44, fr=&#39;2224124&#39;, reg=&#39;East China&#39;, it=1212473)] result = df.filter(df[&quot;specific&quot;] == 8964.0).collect() . type(result[0]) . Out[17]: pyspark.sql.types.Row row = result[0] . row.asDict() . Out[19]: {&#39;_c0&#39;: 268, &#39;province&#39;: &#39;Shanghai&#39;, &#39;specific&#39;: 8964.0, &#39;general&#39;: None, &#39;year&#39;: 2000, &#39;gdp&#39;: 4771.17, &#39;fdi&#39;: 316014, &#39;rnr&#39;: 0.0, &#39;rr&#39;: 0.0, &#39;i&#39;: 0.44, &#39;fr&#39;: &#39;2224124&#39;, &#39;reg&#39;: &#39;East China&#39;, &#39;it&#39;: 1212473} for item in result[0]: print(item) . 268 Shanghai 8964.0 None 2000 4771.17 316014 0.0 0.0 0.44 2224124 East China 1212473 This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/19/Pyspark-Filtering.html",
            "relUrl": "/2020/08/19/Pyspark-Filtering.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post95": {
            "title": "Handling Missing Data with Pyspark",
            "content": "from pyspark.sql import SparkSession from pyspark.sql.functions import countDistinct, avg,stddev # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Dropping Columns without non-null values . # Has to have at least 2 NON-null values df.na.drop(thresh=2).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Dropping any row that contains missing data . df.na.drop().show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| 46| Fujian| 397517.0| 149549.0|2006|7583.85|322047| 0.4| 0.0| 0.0| 4830320| East China|2135224| 47| Fujian| 753552.0| 317700.0|2007|9248.53|406058| 0.4| 0.0| 0.0| 6994577| East China|2649011| 52| Gansu| 223984.0| 58533.0|2000|1052.88| 6235| 0.0|0.153846154| 0.0| 505196|Northwest China|1258100| 54| Gansu| 337894.0| 129791.0|2002|1232.03| 6121| 0.0| 0.13| 0.0| 597159|Northwest China|1898911| 58| Gansu| 833430.0| 516342.0|2006|2277.35| 2954| 0.0| 0.0|0.128205128| 924080|Northwest China|3847158| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(subset=[&quot;general&quot;]).show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 20| Beijing|1009936.0| 309025.0|2004|6033.21|308354| 0.0|0.794871795| 0.0| null| North China|1644601| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 30|Chongqing| 311770.0| 41907.0|2002|2232.86| 19576| null| null| null| 762806|Southwest China|1906968| 31|Chongqing| 335715.0| 18700.0|2003|2555.72| 26083| null| null| null| 929935|Southwest China|1778125| 32|Chongqing| 568835.0| 97500.0|2004|3034.58| 40508| null| null| null| null|Southwest China|2197948| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(how=&#39;any&#39;).show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| 46| Fujian| 397517.0| 149549.0|2006|7583.85|322047| 0.4| 0.0| 0.0| 4830320| East China|2135224| 47| Fujian| 753552.0| 317700.0|2007|9248.53|406058| 0.4| 0.0| 0.0| 6994577| East China|2649011| 52| Gansu| 223984.0| 58533.0|2000|1052.88| 6235| 0.0|0.153846154| 0.0| 505196|Northwest China|1258100| 54| Gansu| 337894.0| 129791.0|2002|1232.03| 6121| 0.0| 0.13| 0.0| 597159|Northwest China|1898911| 58| Gansu| 833430.0| 516342.0|2006|2277.35| 2954| 0.0| 0.0|0.128205128| 924080|Northwest China|3847158| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(how=&#39;all&#39;).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of Null Values . df.na.fill(&#39;example&#39;).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0|example| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324|example| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53|example|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of 0 . df.na.fill(0).show() . ++--++--+-+-+++--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--++--+-+-+++--+--+-+--+-+ 0| Anhui| 147002.0| 0.0|1996| 2093.3| 50661|0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| 0.0|1997|2347.32| 43443|0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| 0.0|1998|2542.96| 27673|0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| 0.0|1999|2712.34| 26131|0.0| 0.0| 0.0|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847|0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| 0.0|2001|3246.71| 33672|0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375|0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720|0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669|0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| 0.0|2005|5350.17| 69000|0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354|0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892|0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| 0.0|1996| 1789.2|155290|0.0| 0.0| 0.0| 634562|North China| 508135| 13| Beijing| 165957.0| 0.0|1997|2077.09|159286|0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| 0.0|1998|2377.18|216800|0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| 0.0|1999|2678.82|197525|0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368|0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| 0.0|2001|3707.96|176818|0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464|0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126|0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-+++--+--+-+--+-+ only showing top 20 rows df.na.fill(&#39;example&#39;,subset=[&#39;fr&#39;]).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0|example| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324|example| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53|example|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows df.na.fill(0,subset=[&#39;general&#39;]).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| 0.0|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| 0.0|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| 0.0|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| 0.0|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| 0.0|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| 0.0|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| 0.0|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| 0.0|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| 0.0|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| 0.0|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| 0.0|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of the Mean . from pyspark.sql.functions import mean mean_val = df.select(mean(df[&#39;general&#39;])).collect() . mean_val[0][0] . Out[19]: 309127.53846153844 mean_gen = mean_val[0][0] . df.na.fill(mean_gen,[&quot;general&quot;]).show() . ++--+++-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+++-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--+++-+-++-+--+--+-+--+-+ only showing top 20 rows df.na.fill(df.select(mean(df[&#39;general&#39;])).collect()[0][0],[&#39;general&#39;]).show() . ++--+++-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+++-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--+++-+-++-+--+--+-+--+-+ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/18/Pyspark-NAs.html",
            "relUrl": "/2020/08/18/Pyspark-NAs.html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post96": {
            "title": "Group By and Aggregation with Pyspark",
            "content": "&quot;Group By and Aggregation with Pyspark&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [pyspark, jupyter] | description: Group By and Aggregation with Pyspark | title: Group By and Aggregation with Pyspark | . Read CSV and inferSchema . from pyspark.sql import SparkSession from pyspark.sql.functions import countDistinct, avg,stddev # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) Using groupBy for Averages and Counts . df.groupBy(&quot;province&quot;) . Out[8]: &lt;pyspark.sql.group.GroupedData at 0x7f939a0aada0&gt; df.groupBy(&quot;province&quot;).mean().show() . ++--++++++--+--+--++ province|avg(_c0)| avg(specific)| avg(general)|avg(year)| avg(gdp)| avg(fdi)| avg(rnr)| avg(rr)| avg(i)| avg(it)| ++--++++++--+--+--++ Guangdong| 65.5|1123328.0833333333| 312308.0| 2001.5|15358.781666666668| 1194950.25|0.011261261250000001| 0.0| 0.0| 3099014.25| Hunan| 161.5| 824676.9166666666| 480788.3333333333| 2001.5| 4765.891666666666| 132110.25| 0.0| 0.07291666666666667| 0.0| 3215128.5| Shanxi| 281.5| 577540.4166666666| 351680.0| 2001.5| 2817.210833333333|38628.833333333336| 0.0| 0.0| 0.0|1983718.3333333333| Tibet| 317.5|189219.91666666666|165365.33333333334| 2001.5|170.42666666666665| 839.75| 0.03030303033333333| 0.15583333333333335| 0.20278090583333333|1174175.5833333333| Hubei| 149.5| 595463.25| 391326.5| 2001.5| 4772.503333333333| 149713.25| 0.045045045| 0.11386386375000002| 0.06230392158333333| 2904659.75| Tianjin| 305.5| 76884.16666666667| 126636.0| 2001.5|2528.6650000000004|250173.33333333334| 0.0| 0.0| 0.0| 831028.4166666666| Beijing| 17.5| 581440.8333333334| 412825.0| 2001.5| 4673.453333333333|257369.33333333334| 0.0| 0.3613053613636364| 0.29545454545454547|1175965.4166666667| Heilongjiang| 125.5|1037878.1666666666| 315925.3333333333| 2001.5| 4041.241666666667| 82719.33333333333| 0.0| 0.0| 0.03931203927272728|3230451.1666666665| Liaoning| 209.5| 1111002.75|185280.83333333334| 2001.5| 5231.135000000001| 285925.3333333333| 0.11469534044444446| 0.0| null|2628358.4166666665| Henan| 137.5| 955407.4166666666| 673392.5| 2001.5| 7208.966666666667| 94426.0| 0.0| 0.04| 0.08602150533333335|3671970.6666666665| Anhui| 5.5| 643984.1666666666|159698.83333333334| 2001.5|3905.8700000000003| 70953.08333333333| 0.0| 0.0| 0.08845208836363637|2649674.4166666665| Xinjiang| 329.5| 345334.3333333333| 412906.0| 2001.5|1828.8966666666665| 4433.083333333333| 0.0| 0.0| 0.0| 2251012.0| Fujian| 41.5|246144.16666666666|140619.33333333334| 2001.5|4864.0233333333335| 374466.4166666667| 0.1366666666666667|0.049999999999999996| 0.09999999999999999| 1274116.75| Jiangxi| 185.5| 592906.3333333334| 458268.6666666667| 2001.5| 2460.7825| 103735.25| 0.0| 0.1491841490909091|0.042727272727272725| 1760613.25| Jilin| 197.5| 711132.25| 348186.0| 2001.5|2274.8541666666665|41226.583333333336| 0.0| 0.0| 0.0|2136634.9166666665| Chongqing| 29.5| 561854.1111111111| 151201.4| 2001.5| 2477.7125|41127.833333333336| 0.09677419400000001| 0.0| 0.0|1636146.4166666667| Shaanxi| 245.5| 387167.1666666667| 386760.5| 2001.5| 2658.034166666667|50892.583333333336|0.002840909090909091| 0.0| 0.07386363636363637|2474031.4166666665| Sichuan| 293.5| 1194640.5| 707032.8333333334| 2001.5| 5377.79|62197.166666666664| 0.00818181818181818| 0.00818181818181818| 0.2|4016479.5833333335| Yunnan| 341.5| 802151.1666666666| 200426.0| 2001.5| 2604.054166666667|17048.333333333332| 0.0| 0.0| 0.0|3165418.9166666665| Gansu| 53.5| 498930.9166666667| 382092.6666666667| 2001.5|1397.8325000000002| 5295.5| 0.11111111120000002| 0.088974359| 0.13038461533333334| 2045347.0| ++--++++++--+--+--++ only showing top 20 rows df.groupBy(&quot;reg&quot;).mean().show() . +-+++++++--+--+--++ reg| avg(_c0)| avg(specific)| avg(general)|avg(year)| avg(gdp)| avg(fdi)| avg(rnr)| avg(rr)| avg(i)| avg(it)| +-+++++++--+--+--++ Southwest China| 214.3| 648086.8070175438| 327627.0| 2001.5|2410.3988333333336|25405.083333333332| 0.01764440930612245|0.053185448081632655| 0.13679739081632653| 2424971.4| East China|183.78571428571428|517524.90476190473|230217.37142857144| 2001.5| 7126.732976190476|414659.03571428574| 0.08284508739240506| 0.05701117448101268| 0.09036240282278483|1949130.4761904762| Northeast China| 177.5| 953337.7222222222|283130.72222222225| 2001.5| 3849.076944444444| 136623.75| 0.03686635942857143| 0.0| 0.02275960168421053|2665148.1666666665| North China| 179.5|506433.57446808513|334689.14285714284| 2001.5| 4239.038541666667|169600.58333333334| 0.0| 0.15428824051724138| 0.11206896551724138|1733718.7291666667| Northwest China| 216.7|324849.06666666665|293066.73333333334| 2001.5|1340.0261666666668|15111.133333333333|0.022847222240000003|0.033887245249999996|0.048179240615384616| 1703537.75| South Central China| 115.5| 690125.8333333334| 382414.8888888889| 2001.5| 5952.826944444445|281785.59722222225|0.014928879322033899| 0.07324349771186443| 0.06797753142372882| 2626299.875| +-+++++++--+--+--++ # Count df.groupBy(&quot;reg&quot;).count().show() . +-+--+ reg|count| +-+--+ Southwest China| 60| East China| 84| Northeast China| 36| North China| 48| Northwest China| 60| South Central China| 72| +-+--+ # Max df.groupBy(&quot;reg&quot;).max().show() . +-+--+-+++--+--++--+-+--+ reg|max(_c0)|max(specific)|max(general)|max(year)|max(gdp)|max(fdi)| max(rnr)| max(rr)| max(i)| max(it)| +-+--+-+++--+--++--+-+--+ Southwest China| 347| 3937966.0| 1725100.0| 2007|10562.39| 149322| 0.181818182| 0.84| 0.75|10384846| East China| 359| 2213991.0| 1272600.0| 2007|25776.91| 1743140| 1.214285714| 0.53| 0.6| 7040099| Northeast China| 215| 3847672.0| 1046700.0| 2007| 9304.52| 598554| 0.516129032| 0.0|0.21621621600000002| 7968319| North China| 311| 2981235.0| 1023453.0| 2007|13607.32| 527776| 0.0|0.794871795| 0.6| 7537692| Northwest China| 335| 2669238.0| 1197400.0| 2007| 5757.29| 119516|0.5555555560000001| 0.5| 1.05| 6308151| South Central China| 167| 3860764.0| 1737800.0| 2007|31777.01| 1712603| 0.27027027| 0.4375| 0.6176470589999999|10533312| +-+--+-+++--+--++--+-+--+ # Min df.groupBy(&quot;reg&quot;).min().show() . +-+--+-+++--+--+--+-++-+ reg|min(_c0)|min(specific)|min(general)|min(year)|min(gdp)|min(fdi)|min(rnr)|min(rr)|min(i)|min(it)| +-+--+-+++--+--+--+-++-+ Southwest China| 24| 18829.0| 18700.0| 1996| 64.98| 2| 0.0| 0.0| 0.0| 176802| East China| 0| 8964.0| 0.0| 1996| 1169.73| 22724| 0.0| 0.0| 0.0| 489132| Northeast China| 120| 80595.0| 19360.0| 1996| 1137.23| 19059| 0.0| 0.0| 0.0| 625471| North China| 12| 35084.0| 32119.0| 1996| 1121.93| 13802| 0.0| 0.0| 0.0| 303992| Northwest China| 48| 32088.0| 2990.0| 1996| 184.17| 247| 0.0| 0.0| 0.0| 178668| South Central China| 60| 54462.0| 0.0| 1996| 389.68| 29579| 0.0| 0.0| 0.0| 147897| +-+--+-+++--+--+--+-++-+ # Sum df.groupBy(&quot;reg&quot;).sum().show() . +-+--+-++++--+++-++ reg|sum(_c0)|sum(specific)|sum(general)|sum(year)| sum(gdp)|sum(fdi)| sum(rnr)| sum(rr)| sum(i)| sum(it)| +-+--+-++++--+++-++ Southwest China| 12858| 3.6940948E7| 9501183.0| 120090|144623.93000000002| 1524305| 0.864576056| 2.606086956| 6.70307215|145498284| East China| 15438| 4.3472092E7| 8057608.0| 168126| 598645.57|34831359| 6.544761904| 4.503882784000002| 7.138629823000002|163726960| Northeast China| 6390| 3.4320158E7| 5096353.0| 72054| 138566.77| 4918455| 1.032258064| 0.0|0.43243243200000003| 95945334| North China| 8616| 2.3802378E7| 7028472.0| 96072| 203473.85| 8140828| 0.0| 4.474358975| 3.25| 83218499| Northwest China| 13002| 1.9490944E7| 8792002.0| 120090| 80401.57| 906668|1.1423611120000001|1.7621367529999998| 2.505320512|102212265| South Central China| 8316| 4.968906E7| 1.3766936E7| 144108|428603.54000000004|20288563| 0.88080388| 4.321366365000001| 4.010674354000001|189093591| +-+--+-++++--+++-++ # Max it across everything df.agg({&#39;specific&#39;:&#39;max&#39;}).show() . +-+ max(specific)| +-+ 3937966.0| +-+ grouped = df.groupBy(&quot;reg&quot;) grouped.agg({&quot;it&quot;:&#39;max&#39;}).show() . +-+--+ reg| max(it)| +-+--+ Southwest China|10384846| East China| 7040099| Northeast China| 7968319| North China| 7537692| Northwest China| 6308151| South Central China|10533312| +-+--+ df.select(countDistinct(&quot;reg&quot;)).show() . +-+ count(DISTINCT reg)| +-+ 6| +-+ df.select(countDistinct(&quot;reg&quot;).alias(&quot;Distinct Region&quot;)).show() . ++ Distinct Region| ++ 6| ++ df.select(avg(&#39;specific&#39;)).show() . +--+ avg(specific)| +--+ 583470.7303370787| +--+ df.select(stddev(&quot;specific&quot;)).show() . ++ stddev_samp(specific)| ++ 654055.3290782663| ++ Choosing Significant Digits with format_number . from pyspark.sql.functions import format_number . specific_std = df.select(stddev(&quot;specific&quot;).alias(&#39;std&#39;)) specific_std.show() . +--+ std| +--+ 654055.3290782663| +--+ specific_std.select(format_number(&#39;std&#39;,0)).show() . ++ format_number(std, 0)| ++ 654,055| ++ Using orderBy . df.orderBy(&quot;specific&quot;).show() . +++--+-+-+--++--+-+-+-++-+ _c0| province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++--+-+-+--++--+-+-+-++-+ 28|Chongqing| null| null|2000| 1791.0| 24436| null|null|null| null|Southwest China|1022148| 109| Hebei| null| null|1997| 3953.78|110064| null|null|null| null| North China| 826734| 24|Chongqing| null| null|1996| 1315.12| 21878| null|null|null| null|Southwest China| 176802| 25|Chongqing| null| null|1997| 1509.75| 38675| null|null|null| null|Southwest China| 383402| 268| Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269| Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 312| Tibet| 18829.0| null|1996| 64.98| 679|0.181818182| 0.0| 0.0| 27801|Southwest China| 306114| 270| Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271| Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 313| Tibet| 25185.0| null|1997| 77.24| 63|0.181818182| 0.0| 0.0| 33787|Southwest China| 346368| 273| Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 272| Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null|null| 90805|Northwest China| 178668| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null|null| 73260|Northwest China| 218361| 302| Tianjin| 39364.0| null|1998| 1374.6|211361| null|null|null| 540178| North China| 361723| 274| Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null|null| 102083|Northwest China| 195295| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 314| Tibet| 48197.0| null|1998| 91.5| 481| 0.0|0.24| 0.0| 3810|Southwest China| 415547| +++--+-+-+--++--+-+-+-++-+ only showing top 20 rows df.orderBy(df[&quot;specific&quot;].desc()).show() . +++++-+--+-+--+--+-+--+-+--+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+--+-+--+--+-+--+-+--+ 299| Sichuan|3937966.0|1725100.0|2007|10562.39| 149322| null| null| null| 8508606| Southwest China|10384846| 143| Henan|3860764.0|1737800.0|2007|15012.46| 306162| 0.0| 0.0| 0.0| 8620804|South Central China|10533312| 131|Heilongjiang|3847672.0|1046700.0|2007| 7104.0| 208508| 0.0| 0.0|0.21621621600000002| 4404689| Northeast China| 7968319| 215| Liaoning|3396397.0| 599600.0|2007| 9304.52| 598554|0.516129032| 0.0| null|10826948| Northeast China| 5502192| 167| Hunan|3156087.0|1329200.0|2007| 9439.6| 327051| 0.0| 0.4375| 0.0| 6065508|South Central China| 8340692| 119| Hebei|2981235.0| 694400.0|2007|13607.32| 241621| 0.0| 0.5| 0.0| 7891198| North China| 7537692| 155| Hubei|2922784.0|1263500.0|2007| 9333.4| 276622| 0.0|0.111111111| 0.0| 5903552|South Central China| 7666512| 251| Shaanxi|2669238.0|1081000.0|2007| 5757.29| 119516| 0.03125| 0.0| 0.8125| 4752398| Northwest China| 6308151| 203| Jilin|2663667.0|1016400.0|2007| 4275.12| 76064| 0.0| 0.0| 0.0| 3206892| Northeast China| 4607955| 347| Yunnan|2482173.0| 564400.0|2007| 4772.52| 39453| 0.0| 0.0| 0.0| 4867146| Southwest China| 6832541| 298| Sichuan|2225220.0|1187958.0|2006| 8690.24| 120819| 0.0| 0.0| 0.55| 4247403| Southwest China| 7646885| 11| Anhui|2213991.0| 178705.0|2007| 7360.92| 299892| 0.0| 0.0| 0.324324324| 4468640| East China| 7040099| 287| Shanxi|2189020.0| 661200.0|2007| 6024.45| 134283| null| null| null| 5978870| North China| 5070166| 263| Shandong|2121243.0| 581800.0|2007|25776.91|1101159| 0.0| 0.0| 0.0|16753980| East China| 6357869| 191| Jiangxi|2045869.0|1272600.0|2007| 4820.53| 280657| 0.0| 0.41025641| 0.0| 3898510| East China| 4229821| 83| Guangxi|2022957.0|1214100.0|2007| 5823.41| 68396|0.205128205| 0.0|0.23076923100000002| 4188265|South Central China| 6185600| 142| Henan|2018158.0|1131615.0|2006|12362.79| 184526| 0.0| 0.0| 0.0| 6212824|South Central China| 7601825| 59| Gansu|2010553.0|1039400.0|2007| 2703.98| 11802| null| 0.0| 1.05| 1909107| Northwest China| 5111059| 95| Guizhou|1956261.0|1239200.0|2007| 2884.11| 12651| 0.0| 0.0| 0.7105263159999999| 2851375| Southwest China| 5639838| 214| Liaoning|1947031.0| 179893.0|2006| 8047.26| 359000|0.516129032| 0.0| null| 6530236| Northeast China| 4605917| +++++-+--+-+--+--+-+--+-+--+ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/17/Pyspark-Group-By.html",
            "relUrl": "/2020/08/17/Pyspark-Group-By.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post97": {
            "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
            "content": "# Import required packages import numpy as np import pandas as pd %matplotlib inline import seaborn as sns import statsmodels.formula.api as smf from matplotlib import pyplot as plt from matplotlib.lines import Line2D %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . np.random.seed(42) import matplotlib as mpl mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) . Read required datasets . df = pd.read_csv(&#39;ttb_county_clean.csv&#39;) df1 = pd.read_csv(&#39;df_panel_fix.csv&#39;) . Figure 1 . df.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=df[&quot;specific&quot;]/100, label=&quot;Specific Purpose Transfers&quot;, figsize=(12,8), c=&quot;nightlights&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) #save_fig(&quot;cn-spt-county-heat&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f0a4e16b4e0&gt; . Panel regression framework with year and province fixed effects . lin_reg = smf.ols(&#39;np.log(specific) ~ np.log(gdp) + np.log(fdi) + i + rnr + rr + C(province) + C(year)&#39;, data=df1).fit() . #lin_reg.summary() . Figure 2 . coef_df = pd.read_csv(&#39;coef.csv&#39;) fig, ax = plt.subplots(figsize=(16, 10)) coef_df.plot(x=&#39;varname&#39;, y=&#39;coef&#39;, kind=&#39;bar&#39;, ax=ax, color=&#39;none&#39;, yerr=&#39;err&#39;, legend=False) ax.set_ylabel(&#39;Specific Purpose Transfers (ln)&#39;) ax.set_xlabel(&#39;Independant Variables&#39;) ax.scatter(x=pd.np.arange(coef_df.shape[0]), marker=&#39;s&#39;, s=120, y=coef_df[&#39;coef&#39;], color=&#39;black&#39;) ax.axhline(y=0, linestyle=&#39;--&#39;, color=&#39;blue&#39;, linewidth=4) ax.xaxis.set_ticks_position(&#39;none&#39;) _ = ax.set_xticklabels([&#39;GDP&#39;, &#39;FDI&#39;, &#39;Incumbent&#39;, &#39;Non Relevant Rival&#39;, &#39;Relevant Rival&#39;], rotation=0, fontsize=20) fs = 16 ax.annotate(&#39;Controls&#39;, xy=(0.2, -0.2), xytext=(0.2, -0.3), xycoords=&#39;axes fraction&#39;, textcoords=&#39;axes fraction&#39;, fontsize=fs, ha=&#39;center&#39;, va=&#39;bottom&#39;, bbox=dict(boxstyle=&#39;square&#39;, fc=&#39;white&#39;, ec=&#39;blue&#39;), arrowprops=dict(arrowstyle=&#39;-[, widthB=5.5, lengthB=1.2&#39;, lw=2.0, color=&#39;blue&#39;)) _ = ax.annotate(&#39;Connections&#39;, xy=(0.7, -0.2), xytext=(0.7, -0.3), xycoords=&#39;axes fraction&#39;, textcoords=&#39;axes fraction&#39;, fontsize=fs, ha=&#39;center&#39;, va=&#39;bottom&#39;, bbox=dict(boxstyle=&#39;square&#39;, fc=&#39;white&#39;, ec=&#39;red&#39;), arrowprops=dict(arrowstyle=&#39;-[, widthB=10.5, lengthB=1.2&#39;, lw=2.0, color=&#39;red&#39;)) #save_fig(&quot;i-coef-plot&quot;) . Figure 3 . import numpy as np from plotly import __version__ from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot print (__version__) # requires version &gt;= 1.9.0 #Always run this the command before at the start of notebook init_notebook_mode(connected=True) import plotly.graph_objs as go trace1 = go.Bar( x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007], y=[188870900000.0, 185182900000.0, 237697500000.0, 347187900000.0, 296716700000.0, 397833100000.0, 440204800000.0, 514254300000.0, 686016600000.0, 677746300000.0, 940057900000.0, 1881304000000], name=&#39;All Other Province Leaders&#39;, marker=dict( color=&#39;rgb(55, 83, 109)&#39; ) ) trace2 = go.Bar( x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007], y=[260376000000.0, 264934700000.0, 367350200000.0, 463861200000.0, 199068500000.0, 216582600000.0, 298631800000.0, 409759300000.0, 830363200000.0, 878158000000.0, 1143745000000.0, 2125891000000.0], name=&#39;Incumbent Connected Province Leaders&#39;, marker=dict( color=&#39;rgb(26, 118, 255)&#39; ) ) data = [trace1, trace2] layout = go.Layout( title=&#39;Specific Purpose Transfers&#39;, xaxis=dict( tickfont=dict( size=14, color=&#39;rgb(107, 107, 107)&#39; ) ), yaxis=dict( title=&#39;RMB&#39;, titlefont=dict( size=16, color=&#39;rgb(107, 107, 107)&#39; ), tickfont=dict( size=14, color=&#39;rgb(107, 107, 107)&#39; ) ), legend=dict( x=0, y=1.0, bgcolor=&#39;rgba(255, 255, 255, 0)&#39;, bordercolor=&#39;rgba(255, 255, 255, 0)&#39; ), barmode=&#39;group&#39;, bargap=0.15, bargroupgap=0.1 ) fig = go.Figure(data=data, layout=layout) #iplot(fig, filename=&#39;style-bar&#39;) iplot(fig, image=&#39;png&#39;,filename=&#39;spt-i-bar&#39;) . 4.1.1 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/16/pandas-stats-fiscal.html",
            "relUrl": "/2020/08/16/pandas-stats-fiscal.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post98": {
            "title": "Pyspark Regression with Fiscal Data",
            "content": "&quot;A minimal example of using Pyspark for Linear Regression&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [pyspark, jupyter] | description: A minimal example of using Pyspark for Linear Regression | title: Pyspark Regression with Fiscal Data | . Bring in needed imports . from pyspark.sql.functions import col from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType from pyspark.sql.functions import * . Load data from CSV . #collapse-hide # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.createOrReplaceTempView(&quot;fiscal_stats&quot;) sums = spark.sql(&quot;&quot;&quot; select year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr from fiscal_stats group by 1 order by year asc &quot;&quot;&quot;) sums.show() . +-+++ year|total_yearly_it|total_yearly_fr| +-+++ 1996| 19825341| 2.9579215E7| 1997| 21391321| 2.9110765E7| 1998| 25511453| 3.8154711E7| 1999| 31922107| 4.2128627E7| 2000| 38721293| 4.8288092E7| 2001| 50754944| 5.8910649E7| 2002| 62375881| 6.2071474E7| 2003| 69316709| 7.2479293E7| 2004| 88626786| null| 2005| 98263665| null| 2006| 119517822| 1.3349148E8| 2007| 153467611| 2.27385701E8| +-+++ Describing the Data . df.describe().toPandas().transpose() . 0 1 2 3 4 . summary count | mean | stddev | min | max | . _c0 360 | 179.5 | 104.06728592598157 | 0 | 359 | . province 360 | None | None | Anhui | Zhejiang | . specific 356 | 583470.7303370787 | 654055.3290782663 | 8964.0 | 3937966.0 | . general 169 | 309127.53846153844 | 355423.5760674793 | 0.0 | 1737800.0 | . year 360 | 2001.5 | 3.4568570586927794 | 1996 | 2007 | . gdp 360 | 4428.653416666667 | 4484.668659976412 | 64.98 | 31777.01 | . fdi 360 | 196139.38333333333 | 303043.97011891654 | 2 | 1743140 | . rnr 294 | 0.0355944252244898 | 0.16061503029299648 | 0.0 | 1.214285714 | . rr 296 | 0.059688621057432424 | 0.15673351824073453 | 0.0 | 0.84 | . i 287 | 0.08376351662369343 | 0.1838933104683607 | 0.0 | 1.05 | . fr 295 | 2522449.0034013605 | 3491329.8613106664 | #REF! | 9898522 | . reg 360 | None | None | East China | Southwest China | . it 360 | 2165819.2583333333 | 1769294.2935487411 | 147897 | 10533312 | . Cast Data Type . df2 = df.withColumn(&quot;gdp&quot;,col(&quot;gdp&quot;).cast(IntegerType())) .withColumn(&quot;specific&quot;,col(&quot;specific&quot;).cast(IntegerType())) .withColumn(&quot;general&quot;,col(&quot;general&quot;).cast(IntegerType())) .withColumn(&quot;year&quot;,col(&quot;year&quot;).cast(IntegerType())) .withColumn(&quot;fdi&quot;,col(&quot;fdi&quot;).cast(IntegerType())) .withColumn(&quot;rnr&quot;,col(&quot;rnr&quot;).cast(IntegerType())) .withColumn(&quot;rr&quot;,col(&quot;rr&quot;).cast(IntegerType())) .withColumn(&quot;i&quot;,col(&quot;i&quot;).cast(IntegerType())) .withColumn(&quot;fr&quot;,col(&quot;fr&quot;).cast(IntegerType())) . printSchema . df2.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import LinearRegression assembler = VectorAssembler(inputCols=[&#39;gdp&#39;, &#39;fdi&#39;], outputCol=&quot;features&quot;) train_df = assembler.transform(df2) . train_df.select(&quot;specific&quot;, &quot;year&quot;).show() . +--+-+ specific|year| +--+-+ 147002|1996| 151981|1997| 174930|1998| 285324|1999| 195580|2000| 250898|2001| 434149|2002| 619201|2003| 898441|2004| 898441|2005| 1457872|2006| 2213991|2007| 165957|1996| 165957|1997| 245198|1998| 388083|1999| 281769|2000| 441923|2001| 558569|2002| 642581|2003| +--+-+ only showing top 20 rows Linear Regression in Pyspark . lr = LinearRegression(featuresCol = &#39;features&#39;, labelCol=&#39;it&#39;) lr_model = lr.fit(train_df) trainingSummary = lr_model.summary print(&quot;Coefficients: &quot; + str(lr_model.coefficients)) print(&quot;RMSE: %f&quot; % trainingSummary.rootMeanSquaredError) print(&quot;R2: %f&quot; % trainingSummary.r2) . Coefficients: [495.05888709337756,-4.968141828763066] RMSE: 1234228.673087 R2: 0.512023 lr_predictions = lr_model.transform(train_df) lr_predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show(5) from pyspark.ml.evaluation import RegressionEvaluator lr_evaluator = RegressionEvaluator(predictionCol=&quot;prediction&quot;, labelCol=&quot;it&quot;,metricName=&quot;r2&quot;) . ++-+-+ prediction| it| features| ++-+-+ 1732528.7382477913| 631930|[2093.0,50661.0]| 1894133.7432895212| 657860|[2347.0,43443.0]| 2069017.8229123235| 889463|[2542.0,27673.0]| 2160838.7084181504|1227364|[2712.0,26131.0]| 2226501.9982726825|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows print(&quot;R Squared (R2) on test data = %g&quot; % lr_evaluator.evaluate(lr_predictions)) . R Squared (R2) on test data = 0.512023 print(&quot;numIterations: %d&quot; % trainingSummary.totalIterations) print(&quot;objectiveHistory: %s&quot; % str(trainingSummary.objectiveHistory)) trainingSummary.residuals.show() . numIterations: 1 objectiveHistory: [0.0] +-+ residuals| +-+ -1100598.7382477913| -1236273.7432895212| -1179554.8229123235| -933474.7084181504| -727391.9982726825| -222546.39659531135| -94585.30175113119| 108072.63313654158| 389732.58121094666| 621021.2194867637| 1885768.997742407| 3938310.059555837| -554084.125169754| -615660.3899049093| -352195.3468934437| -348450.00565795833| -918476.5594253046| -710059.9133252408| -1148661.0062004486| -911572.322055324| +-+ only showing top 20 rows predictions = lr_model.transform(test_df) predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show() . ++-++ prediction| it| features| ++-++ 976371.9212205639| 306114| [64.0,679.0]| 990722.2032541803| 415547| [91.0,481.0]| 1016348.0830204486| 983251| [139.0,106.0]| 1036290.7062801318| 218361| [184.0,576.0]| 1034023.4471330958| 178668| [202.0,2826.0]| 1060130.0768520113| 274994| [245.0,1856.0]| 1023513.0851009073| 546541|[263.0,11020.0]| 1053250.6267921| 361358| [264.0,5134.0]| 1123768.8091592425| 866691| [377.0,2200.0]| 1128604.8330225947| 948521| [390.0,2522.0]| 810587.2575938476| 177748|[442.0,71715.0]| 1159703.254297337| 736165| [445.0,1743.0]| 1066975.770986663|1260633|[466.0,22500.0]| 1288507.6625716756|1423771| [725.0,3718.0]| 1320055.238474972| 573905| [793.0,4144.0]| 1188611.0570700848|2347862|[797.0,31000.0]| 1321857.482976733| 582711| [805.0,4977.0]| 1033849.5995896922| 746784|[819.0,64343.0]| 1445051.792853667|1216605|[1029.0,2501.0]| 1437887.1056682135|1258100|[1052.0,6235.0]| ++-++ only showing top 20 rows from pyspark.ml.regression import DecisionTreeRegressor dt = DecisionTreeRegressor(featuresCol =&#39;features&#39;, labelCol = &#39;it&#39;) dt_model = dt.fit(train_df) dt_predictions = dt_model.transform(train_df) dt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = dt_evaluator.evaluate(dt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . Root Mean Squared Error (RMSE) on test data = 1.01114e+06 from pyspark.ml.regression import GBTRegressor gbt = GBTRegressor(featuresCol = &#39;features&#39;, labelCol = &#39;it&#39;, maxIter=10) gbt_model = gbt.fit(train_df) gbt_predictions = gbt_model.transform(train_df) gbt_predictions.select(&#39;prediction&#39;, &#39;it&#39;, &#39;features&#39;).show(5) gbt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = gbt_evaluator.evaluate(gbt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . ++-+-+ prediction| it| features| ++-+-+ 1388898.308543053| 631930|[2093.0,50661.0]| 1388898.308543053| 657860|[2347.0,43443.0]| 1649083.6277172007| 889463|[2542.0,27673.0]| 1649083.6277172007|1227364|[2712.0,26131.0]| 1649083.6277172007|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows Root Mean Squared Error (RMSE) on test data = 778728 This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "relUrl": "/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post99": {
            "title": "Pyspark for Data Science",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. . # Prints &#39;2&#39; print(1+1) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/06/28/2st-markdown-post.html",
            "relUrl": "/data%20science%20content/2020/06/28/2st-markdown-post.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post100": {
            "title": "Data Science, Big Data and Healthcare Research",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. . # Prints &#39;2&#39; print(1+1) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/data%20science%20content/2020/06/08/1st-markdown-post.html",
            "relUrl": "/data%20science%20content/2020/06/08/1st-markdown-post.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post101": {
            "title": "A timer for ML functions",
            "content": "&quot;A timer for ML functions&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [timer, jupyter] | description: A timer for ML functions | title: A timer for ML functions | . #collapse-hide from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper . . @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . from collections import Counter import math, random # # data splitting # def split_data(data, prob): &quot;&quot;&quot;split data into fractions [prob, 1 - prob]&quot;&quot;&quot; results = [], [] for row in data: results[0 if random.random() &lt; prob else 1].append(row) return results def train_test_split(x, y, test_pct): data = list(zip(x, y)) # pair corresponding values train, test = split_data(data, 1 - test_pct) # split the dataset of pairs x_train, y_train = list(zip(*train)) # magical un-zip trick x_test, y_test = list(zip(*test)) return x_train, x_test, y_train, y_test # # correctness # def accuracy(tp, fp, fn, tn): correct = tp + tn total = tp + fp + fn + tn return correct / total def precision(tp, fp, fn, tn): return tp / (tp + fp) def recall(tp, fp, fn, tn): return tp / (tp + fn) def f1_score(tp, fp, fn, tn): p = precision(tp, fp, fn, tn) r = recall(tp, fp, fn, tn) return 2 * p * r / (p + r) if __name__ == &quot;__main__&quot;: print(&quot;accuracy(70, 4930, 13930, 981070)&quot;, accuracy(70, 4930, 13930, 981070)) print(&quot;precision(70, 4930, 13930, 981070)&quot;, precision(70, 4930, 13930, 981070)) print(&quot;recall(70, 4930, 13930, 981070)&quot;, recall(70, 4930, 13930, 981070)) print(&quot;f1_score(70, 4930, 13930, 981070)&quot;, f1_score(70, 4930, 13930, 981070)) . favorite_number = 7 def add(a, b): return a + b def sub(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): return a / b def count_vowels(word): count = 0 for letter in word.lower(): count += letter in &#39;aeiou&#39; return count . # import example_module as sm # print(sm.favorite_number) # # add two numbers together # print(sm.add(3, 8)) # # count the number of vowels in a string # print(sm.count_vowels(&#39;Testing&#39;)) . import pandas as pd from alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler config_handler.set_global(theme=&#39;ascii&#39;, spinner=&#39;notes&#39;, bar=&#39;solid&#39;) with alive_bar(3) as bar: df = pd.read_csv(&#39;https://gist.githubusercontent.com/davidrkearney/bb461ba351da484336a19bd00a2612e2/raw/18dd90b57fec46a247248d161ffd8085de2a00db/china_province_economicdata_1996_2007.csv&#39;) bar(&#39;file read, printing file&#39;) print(df.head) bar(&#39;data printed ok, printing methods of data&#39;) print(dir(df)) bar(&#39;process complete&#39;) . from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . This post includes code adapted from Data Science from Scratch .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/06/07/kwargs-decorators.html",
            "relUrl": "/2020/06/07/kwargs-decorators.html",
            "date": " • Jun 7, 2020"
        }
        
    
  
    
        ,"post102": {
            "title": "精卫填海",
            "content": "精卫填海 . 精卫论填海的成语。这则成语出自『山海经*北山经』。炎帝的女儿，叫女娃。性格善温柔，善良开朗活泼。生得娇小文弱。被东海龙王的太子龙太子掀起的狂风恶狼的活活淹死。虽说人死不能复生，但她心有不甘，灵魂化成了一只叫『精卫』的小鸟。随后就跟无情大海势不两立。一直来来往往，把小树枝，石头投进海里，永不停歇。此悲壮之举跟Camus的『Myth of Sisyphus』颇有异曲同工之妙。共同之处就是两者的『知其不可为而为之』的精神。与其说这是什么可贵的锲而不舍，孜孜不倦的执着精神倒不如说这是创造人生意义的唯一方法,其他高明哲学所不如的。 . 『知其不可为而为之』： 出处:《论语·宪问》。孔子被那个在鲁国都城的外门看门的人说是是那个明知做不到却还要去做的人。 . 陶渊明云『精卫衔微木， 将以填沧海』: . 精卫衔微木，将以填沧海。刑天舞干戚，猛志固常在。同物既无虑，化去不复悔。徒没在昔心，良辰讵可待！ .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/%E5%93%B2%E5%AD%A6%E6%80%9D%E8%80%83/2020/01/01/zw-letter-1.html",
            "relUrl": "/%E5%93%B2%E5%AD%A6%E6%80%9D%E8%80%83/2020/01/01/zw-letter-1.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post103": {
            "title": "40 năm ngày kỷ niệm đám cưới của ba mẹ",
            "content": "40 năm ngày kỷ niệm đám cưới của ba mẹ . Lần đầu gặp Trang Anh, con không biết ba mẹ và gia đình Trang Anh sẽ như thế nào. Nhưng từ những lời nói và việc làm rất chu đáo và vui vẻ của Trang Anh, con có thể thấy rằng những người đã nuôi dạy Trang Anh là hai người đặc biệt. . Sau này con được biết ba mẹ Trang Anh là hai giáo viên giỏi, con cũng không ngạc nhiên. . Ba mẹ quan tâm đến con nhiều như Trang Anh, đôi khi thậm chí còn chiều con hơn. Trở thành con của ba mẹ, con có thêm được nhiều niềm hạnh phúc. Ba và mẹ là 2 người đặc biệt quan trọng trong cuộc đời bọn con. Chúc mừng ba, mẹ của bọn con, ông bà ngoại của bé Su Kem nhân 40 năm ngày kỷ niệm đám cưới của ba mẹ. . Ba mẹ nuôi dạy Trang Anh trở thành một người xuất sắc và dạy Trang Anh rằng để làm được điều gì đó đáng giá, người ta phải rất kiên trì. Làm tiến sĩ và học ngoại ngữ cũng vậy. Bọn con sẽ cố gắng truyền sự bền bỉ này cho thế hệ sau. . Con 2 Mét .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/family%20letters/2020/01/01/viet-letter-1.html",
            "relUrl": "/family%20letters/2020/01/01/viet-letter-1.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Senior Data Scientist at CVS Health. | Ph.D. from Duke University | . .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  

  

  

  

  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}