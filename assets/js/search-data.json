{
  
    
        "post0": {
            "title": "Friday Links",
            "content": "Escaping strings in Bash using !:q / | . bash # This string ‘has single’ “and double” quotes and a $ bash !:q .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/02/links2.html",
            "relUrl": "/links/2020/10/02/links2.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Friday Books",
            "content": "21 Lessons for the 21st Century/ . | Outliers: The Story of Success/ . | David and Goliath: Underdogs, Misfits, and the Art of Battling Giants / . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/books/2020/10/02/links.html",
            "relUrl": "/books/2020/10/02/links.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Thursday Links",
            "content": "Test-Driven Data Science Development - From TWD. Using pytest. . | Top 10 Essential Data Science Topics to Real-World Application from the Industry Perspectives - from the BBC. Review of Data Science and Causal inference. . | Parser for Kindle My Clippings.txt file . | Forecasting Newsletter. . | High Replicability of Newly-Discovered Social-behavioral Findings is Achievable - ‘Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal’ . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/10/01/links.html",
            "relUrl": "/links/2020/10/01/links.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Stock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_YIELD_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-31 1.98 | . 2010-02-28 2.03 | . 2010-03-31 1.90 | . 2010-04-30 1.83 | . 2010-05-31 1.95 | . ... ... | . 2020-07-01 1.91 | . 2020-07-31 1.86 | . 2020-08-31 1.76 | . 2020-09-01 1.69 | . 2020-09-30 1.69 | . 136 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_PE_RATIO_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 20.70 | . 2010-02-01 18.91 | . 2010-03-01 18.91 | . 2010-04-01 19.01 | . 2010-05-01 17.30 | . ... ... | . 2020-07-01 27.57 | . 2020-07-31 28.12 | . 2020-08-01 29.16 | . 2020-08-31 30.09 | . 2020-09-01 30.32 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_PE_RATIO_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 20.70 | . 2010-02-01 18.91 | . 2010-03-01 18.91 | . 2010-04-01 19.01 | . 2010-05-01 17.30 | . ... ... | . 2020-07-01 27.57 | . 2020-07-31 28.12 | . 2020-08-01 29.16 | . 2020-08-31 30.09 | . 2020-09-01 30.32 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_EARNINGS_YIELD_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 4.83 | . 2010-02-01 5.29 | . 2010-03-01 5.29 | . 2010-04-01 5.26 | . 2010-05-01 5.78 | . ... ... | . 2020-07-01 3.63 | . 2020-07-31 3.56 | . 2020-08-01 3.43 | . 2020-08-31 3.32 | . 2020-09-01 3.30 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_INFLADJ_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-01 1343.51 | . 2010-02-01 1302.03 | . 2010-03-01 1371.58 | . 2010-04-01 1423.00 | . 2010-05-01 1336.08 | . ... ... | . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . 138 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_MONTH&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-01-31 26.59 | . 2010-02-28 26.38 | . 2010-03-31 26.08 | . 2010-04-30 26.09 | . 2010-05-31 26.12 | . ... ... | . 2020-02-29 59.23 | . 2020-03-31 59.81 | . 2020-04-30 60.25 | . 2020-05-31 60.28 | . 2020-06-30 59.99 | . 126 rows × 1 columns . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_YEAR&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-12-31 26.87 | . 2011-12-31 30.34 | . 2012-12-31 35.26 | . 2013-12-31 38.90 | . 2014-12-31 43.52 | . 2015-12-31 47.53 | . 2016-12-31 49.05 | . 2017-12-31 51.43 | . 2018-12-31 55.43 | . 2019-03-31 55.35 | . 2019-06-30 56.17 | . 2019-09-30 57.32 | . 2019-12-31 58.72 | . 2020-03-31 59.18 | . 2020-06-30 59.99 | . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_GROWTH_YEAR&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-12-31 1.45 | . 2011-12-31 16.26 | . 2012-12-31 18.25 | . 2013-12-31 11.99 | . 2014-12-31 12.72 | . 2015-12-31 10.00 | . 2016-12-31 5.33 | . 2017-12-31 7.07 | . 2018-12-31 9.84 | . 2019-03-31 9.87 | . 2019-06-30 9.98 | . 2019-09-30 9.32 | . 2019-12-31 8.36 | . 2020-03-31 8.45 | . 2020-06-30 6.43 | . SP500 = quandl.get(&quot;MULTPL/SP500_DIV_GROWTH_QUARTER&quot;,start_date = start,end_date = end) SP500 . Value . Date . 2010-03-31 -19.63 | . 2010-06-30 -13.90 | . 2010-09-30 -6.48 | . 2010-12-31 1.45 | . 2011-03-31 6.97 | . 2011-06-30 10.46 | . 2011-09-30 12.65 | . 2011-12-31 16.26 | . 2012-03-31 16.74 | . 2012-06-30 16.35 | . 2012-09-30 17.51 | . 2012-12-31 18.25 | . 2013-03-31 17.40 | . 2013-06-30 17.47 | . 2013-09-30 16.27 | . 2013-12-31 11.99 | . 2014-03-31 12.82 | . 2014-06-30 12.37 | . 2014-09-30 11.89 | . 2014-12-31 12.72 | . 2015-03-31 12.64 | . 2015-06-30 11.67 | . 2015-09-30 10.43 | . 2015-12-31 10.00 | . 2016-03-31 7.52 | . 2016-06-30 6.51 | . 2016-09-30 5.92 | . 2016-12-31 5.33 | . 2017-03-31 5.71 | . 2017-06-30 6.21 | . 2017-09-30 6.99 | . 2017-12-31 7.07 | . 2018-03-31 7.81 | . 2018-06-30 7.99 | . 2018-09-30 8.65 | . 2018-12-31 9.84 | . 2019-03-31 9.87 | . 2019-06-30 9.98 | . 2019-09-30 9.32 | . 2019-12-31 8.36 | . 2020-03-31 8.45 | . 2020-06-30 6.43 | . SP500.head() . Value . Date . 2010-01-01 1123.58 | . 2010-02-01 1089.16 | . 2010-03-01 1152.05 | . 2010-04-01 1197.32 | . 2010-05-01 1125.06 | . SP500.tail() . Value . Date . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . import matplotlib.pyplot as plt %matplotlib inline . SP500[&#39;Value&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/10/01/StockMarketPortfolioAnaylsis_snp+.html",
            "relUrl": "/2020/10/01/StockMarketPortfolioAnaylsis_snp+.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Wednesday Links",
            "content": "Artificial Intelligence, Algorithmic Pricing, and Collusion - From American Economic Review. Abstract - ‘Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty.’ . | 北極之最：格陵蘭史帕特冰川大片冰舌脫落 後果多嚴重 - from the BBC. . | Vì sao Ròm trở thành hiện tượng phòng vé nhưng lại gây chia rẽ khán giả? - From BBC. . | Ròm - From IMDB. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/30/links.html",
            "relUrl": "/links/2020/09/30/links.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Stock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . SP500 = quandl.get(&quot;MULTPL/SP500_REAL_PRICE_MONTH&quot;,start_date = start,end_date = end) . SP500.head() . Value . Date . 2010-01-01 1123.58 | . 2010-02-01 1089.16 | . 2010-03-01 1152.05 | . 2010-04-01 1197.32 | . 2010-05-01 1125.06 | . SP500.tail() . Value . Date . 2020-07-01 3207.62 | . 2020-07-31 3271.12 | . 2020-08-01 3391.71 | . 2020-08-31 3500.31 | . 2020-09-01 3526.65 | . import matplotlib.pyplot as plt %matplotlib inline . SP500[&#39;Value&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) . Text(0.5, 1.0, &#39;Total S&amp;P 500 from 2010 to 2020 Value&#39;) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/30/StockMarketPortfolioAnaylsis_snp.html",
            "relUrl": "/2020/09/30/StockMarketPortfolioAnaylsis_snp.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Tuesday Links",
            "content": "A Satellite Account for Health in the United States - From NBER. The paper ‘measures the change in medical spending and health outcomes for a comprehensive set of 80 conditions’ . | 全球暖化下的西伯利亞：BBC採訪團隊在地見聞 - from the BBC. . | Dùng sầu riêng và mít để sạc điện thoại - From BBC. . | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/29/links.html",
            "relUrl": "/links/2020/09/29/links.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Monday Links",
            "content": "澳大利亞也可以看到絢麗的極光 - from the BBC. . | Does Machine Translation Affect International Trade? Evidence from a Large Digital Platform - From NBER. . | 3.How to digitize your lab notebooks - From Nature. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/28/links.html",
            "relUrl": "/links/2020/09/28/links.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite3",
            "content": "This post includes code adapted from these sqlalchemy and sqlite gists and the sqlite3 documentation. . import sqlalchemy as db import sqlite3 import pandas as pd . engine = db.create_engine(&#39;sqlite:///music.sqlite&#39;) . connection = engine.connect() metadata = db.MetaData() music = db.Table(&#39;music&#39;, metadata, db.Column(&#39;Id&#39;, db.Integer()), db.Column(&#39;song&#39;, db.String(255), nullable=False), db.Column(&#39;album&#39;, db.String(255), nullable=False), db.Column(&#39;artist&#39;, db.String(255), nullable=False) ) metadata.create_all(engine) . #Inserting one record query = db.insert(music).values(Id=1, song=&#39;song3&#39;, album=&#39;album3&#39;, artist=&#39;artist3&#39;) ResultProxy = connection.execute(query) . #Inserting many records query = db.insert(music) values_list = [{&#39;Id&#39;:&#39;2&#39;, &#39;song&#39;:&#39;song1&#39;, &#39;album&#39;:&#39;album1&#39;, &#39;artist&#39;:&#39;artist1&#39;}, {&#39;Id&#39;:&#39;3&#39;, &#39;song&#39;:&#39;song2&#39;, &#39;album&#39;:&#39;album2&#39;, &#39;artist&#39;:&#39;artist2&#39;}] ResultProxy = connection.execute(query,values_list) results = connection.execute(db.select([music])).fetchall() df = pd.DataFrame(results) df.columns = results[0].keys() df.head(10) . Id song album artist . 0 1 | song3 | album3 | artist3 | . 1 2 | song1 | album1 | artist1 | . 2 3 | song2 | album2 | artist2 | . 3 2 | song1 | album1 | artist1 | . 4 3 | song2 | album2 | artist2 | . results = connection.execute(db.select([music])).fetchall() df = pd.DataFrame(results) df.columns = results[0].keys() df.head(4) . query = db.select([music]).where(db.and_(music.columns.song == &#39;song3&#39;, music.columns.artist == &#39;artist3&#39;)) result = connection.execute(query).fetchall() result[:3] . [(1, &#39;song3&#39;, &#39;album3&#39;, &#39;artist3&#39;)] . conn = sqlite3.connect(&#39;music.sqlite&#39;) . c = conn.cursor() # Create table c.execute(&#39;&#39;&#39;CREATE TABLE stockmarket (date text, trans text, symbol text, qty real, price real)&#39;&#39;&#39;) # Insert a row of data c.execute(&quot;INSERT INTO stockmarket VALUES (&#39;2006-01-05&#39;,&#39;BUY&#39;,&#39;RHAT&#39;,100,35.14)&quot;) # Save (commit) the changes conn.commit() # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn.close() . conn = sqlite3.connect(&#39;music.sqlite&#39;) c = conn.cursor() . symbol = &#39;RHAT&#39; c.execute(&quot;SELECT * FROM stockmarket WHERE symbol = &#39;%s&#39;&quot; % symbol) . &lt;sqlite3.Cursor at 0x7f0098ae0180&gt; . t = (&#39;RHAT&#39;,) c.execute(&#39;SELECT * FROM stockmarket WHERE symbol=?&#39;, t) print(c.fetchone()) . (&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14) . # Larger example that inserts many records at a time purchases = [(&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000, 45.00), (&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000, 72.00), (&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500, 53.00), ] c.executemany(&#39;INSERT INTO stockmarket VALUES (?,?,?,?,?)&#39;, purchases) . &lt;sqlite3.Cursor at 0x7f0098ae0180&gt; . for row in c.execute(&#39;SELECT * FROM stockmarket ORDER BY price&#39;): print(row) . (&#39;2006-01-05&#39;, &#39;BUY&#39;, &#39;RHAT&#39;, 100.0, 35.14) (&#39;2006-03-28&#39;, &#39;BUY&#39;, &#39;IBM&#39;, 1000.0, 45.0) (&#39;2006-04-06&#39;, &#39;SELL&#39;, &#39;IBM&#39;, 500.0, 53.0) (&#39;2006-04-05&#39;, &#39;BUY&#39;, &#39;MSFT&#39;, 1000.0, 72.0) . # Use dbeaver to examine .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/28/databases_sqllite_sqlalchemy_27.html",
            "relUrl": "/2020/09/28/databases_sqllite_sqlalchemy_27.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "What I've been reading.",
            "content": "Gödel, Escher, Bach - | 21 Lessons for the 21st Century | Grit: The Power of Passion and Perseverance Hardcover | The notebooks for The possessed- Published 1968 by University of Chicago Press. Found at local used book store. |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/books/2020/09/27/reading.html",
            "relUrl": "/books/2020/09/27/reading.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Sat Links",
            "content": "What Is Your List of 10 Challenges in Data Science? - from the HDSR. . | What’s Wrong with Social Science and How to Fix It - ‘surely notice that all you have is a n=23, p=0.049 three-way interaction effect (one of dozens you tested, and with no multiple testing adjustments of course’ . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/26/links.html",
            "relUrl": "/links/2020/09/26/links.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Friday Links",
            "content": "What quantum computers reveal about innovation - from the Economist. . | The Past, Present, and (Near) Future of Gene Therapy and Gene Editing - from NEJM . | 中国首富换人做 农夫山泉钟睒睒登顶富豪榜 - 半小时的首富 - 来自BBC。 . | Messi bị kéo vào vụ chỉ trích tuyển Argentina - Vnexpress. . | Earth, Wind &amp; Fire - September . |",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/links/2020/09/25/links.html",
            "relUrl": "/links/2020/09/25/links.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Analyzing Size of Armed Forces From 1947 - 1963 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.longley.load_pandas().data #print(sm.datasets.longley.NOTE) . df.head() . TOTEMP GNPDEFL GNP UNEMP ARMED POP YEAR . 0 60323.0 | 83.0 | 234289.0 | 2356.0 | 1590.0 | 107608.0 | 1947.0 | . 1 61122.0 | 88.5 | 259426.0 | 2325.0 | 1456.0 | 108632.0 | 1948.0 | . 2 60171.0 | 88.2 | 258054.0 | 3682.0 | 1616.0 | 109773.0 | 1949.0 | . 3 61187.0 | 89.5 | 284599.0 | 3351.0 | 1650.0 | 110929.0 | 1950.0 | . 4 63221.0 | 96.2 | 328975.0 | 2099.0 | 3099.0 | 112075.0 | 1951.0 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1947&#39;, &#39;1962&#39;)) df.index = index df.head() . TOTEMP GNPDEFL GNP UNEMP ARMED POP YEAR . 1947-12-31 60323.0 | 83.0 | 234289.0 | 2356.0 | 1590.0 | 107608.0 | 1947.0 | . 1948-12-31 61122.0 | 88.5 | 259426.0 | 2325.0 | 1456.0 | 108632.0 | 1948.0 | . 1949-12-31 60171.0 | 88.2 | 258054.0 | 3682.0 | 1616.0 | 109773.0 | 1949.0 | . 1950-12-31 61187.0 | 89.5 | 284599.0 | 3351.0 | 1650.0 | 110929.0 | 1950.0 | . 1951-12-31 63221.0 | 96.2 | 328975.0 | 2099.0 | 3099.0 | 112075.0 | 1951.0 | . df[&#39;ARMED&#39;].plot() plt.ylabel(&quot;ARMED&quot;) . Text(0, 0.5, &#39;ARMED&#39;) . # unpacking cycle, trend = sm.tsa.filters.hpfilter(df.ARMED) cycle . 1947-12-31 -497.642333 1948-12-31 -713.661033 1949-12-31 -635.368706 1950-12-31 -682.008289 1951-12-31 688.574390 1952-12-31 1108.959755 1953-12-31 992.297873 1954-12-31 731.045710 1955-12-31 370.040046 1956-12-31 124.660757 1957-12-31 15.056446 1958-12-31 -193.702199 1959-12-31 -324.553899 1960-12-31 -407.316313 1961-12-31 -393.604252 1962-12-31 -182.777954 Name: ARMED, dtype: float64 . type(cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = trend df[[&#39;trend&#39;,&#39;ARMED&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;ARMED&#39;]][&quot;1950-01-01&quot;:&quot;1955-01-01&quot;].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2d5af13940&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/25/AnalyzingSizeofArmedForces.html",
            "relUrl": "/2020/09/25/AnalyzingSizeofArmedForces.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Analyzing US Real Interest Rate From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . /home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;realint&#39;].plot() plt.ylabel(&quot;realint&quot;) . Text(0, 0.5, &#39;realint&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking cycle, trend = sm.tsa.filters.hpfilter(df.realint) cycle . 1959-03-31 -1.195751 1959-06-30 -0.505792 1959-09-30 -0.205086 1959-12-31 2.717430 1960-03-31 -0.197051 ... 2008-09-30 4.330269 2008-12-31 8.961987 2009-03-31 -0.596183 2009-06-30 -3.008487 2009-09-30 -3.188797 Name: realint, Length: 203, dtype: float64 . type(cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = trend df[[&#39;trend&#39;,&#39;realint&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;realint&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2a8100438&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/24/AnalyzingUSRealInterestRate.html",
            "relUrl": "/2020/09/24/AnalyzingUSRealInterestRate.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Analyzing US Inflation From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;infl&#39;].plot() plt.ylabel(&quot;infl&quot;) . Text(0, 0.5, &#39;infl&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking infl_cycle, infl_trend = sm.tsa.filters.hpfilter(df.infl) infl_cycle . 1959-03-31 -1.206811 1959-06-30 1.141499 1959-09-30 1.550564 1959-12-31 -0.909577 1960-03-31 1.140149 ... 2008-09-30 -5.064733 2008-12-31 -10.550048 2009-03-31 -0.681429 2009-06-30 1.883255 2009-09-30 2.206560 Name: infl, Length: 203, dtype: float64 . type(infl_cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = infl_trend df[[&#39;trend&#39;,&#39;infl&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;infl&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fec38616be0&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/23/AnalyzingUSInflation.html",
            "relUrl": "/2020/09/23/AnalyzingUSInflation.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Analyzing US Unemployment From 1959 - 2009 with statsmodels",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib inline df = sm.datasets.macrodata.load_pandas().data print(sm.datasets.macrodata.NOTE) . :: Number of Observations - 203 Number of Variables - 14 Variable name definitions:: year - 1959q1 - 2009q3 quarter - 1-4 realgdp - Real gross domestic product (Bil. of chained 2005 US$, seasonally adjusted annual rate) realcons - Real personal consumption expenditures (Bil. of chained 2005 US$, seasonally adjusted annual rate) realinv - Real gross private domestic investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realgovt - Real federal consumption expenditures &amp; gross investment (Bil. of chained 2005 US$, seasonally adjusted annual rate) realdpi - Real private disposable income (Bil. of chained 2005 US$, seasonally adjusted annual rate) cpi - End of the quarter consumer price index for all urban consumers: all items (1982-84 = 100, seasonally adjusted). m1 - End of the quarter M1 nominal money stock (Seasonally adjusted) tbilrate - Quarterly monthly average of the monthly 3-month treasury bill: secondary market rate unemp - Seasonally adjusted unemployment rate (%) pop - End of the quarter total population: all ages incl. armed forces over seas infl - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400) realint - Real interest rate (tbilrate - infl) . df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 0 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 2 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 3 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 4 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . index = pd.Index(sm.tsa.datetools.dates_from_range(&#39;1959Q1&#39;, &#39;2009Q3&#39;)) df.index = index df.head() . year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . 1959-03-31 1959.0 | 1.0 | 2710.349 | 1707.4 | 286.898 | 470.045 | 1886.9 | 28.98 | 139.7 | 2.82 | 5.8 | 177.146 | 0.00 | 0.00 | . 1959-06-30 1959.0 | 2.0 | 2778.801 | 1733.7 | 310.859 | 481.301 | 1919.7 | 29.15 | 141.7 | 3.08 | 5.1 | 177.830 | 2.34 | 0.74 | . 1959-09-30 1959.0 | 3.0 | 2775.488 | 1751.8 | 289.226 | 491.260 | 1916.4 | 29.35 | 140.5 | 3.82 | 5.3 | 178.657 | 2.74 | 1.09 | . 1959-12-31 1959.0 | 4.0 | 2785.204 | 1753.7 | 299.356 | 484.052 | 1931.3 | 29.37 | 140.0 | 4.33 | 5.6 | 179.386 | 0.27 | 4.06 | . 1960-03-31 1960.0 | 1.0 | 2847.699 | 1770.5 | 331.722 | 462.199 | 1955.5 | 29.54 | 139.6 | 3.50 | 5.2 | 180.007 | 2.31 | 1.19 | . df[&#39;unemp&#39;].plot() plt.ylabel(&quot;unemp&quot;) . Text(0, 0.5, &#39;unemp&#39;) . $ min_{ { tau_{t} } } sum_{t}^{T} zeta_{t}^{2}+ lambda sum_{t=1}^{T} left[ left( tau_{t}- tau_{t-1} right)- left( tau_{t-1}- tau_{t-2} right) right]^{2}$ . # unpacking unemp_cycle, unemp_trend = sm.tsa.filters.hpfilter(df.unemp) unemp_cycle . 1959-03-31 0.011338 1959-06-30 -0.702548 1959-09-30 -0.516441 1959-12-31 -0.229910 1960-03-31 -0.642198 ... 2008-09-30 -0.481666 2008-12-31 0.198598 2009-03-31 1.171440 2009-06-30 2.040247 2009-09-30 2.207674 Name: unemp, Length: 203, dtype: float64 . type(unemp_cycle) . pandas.core.series.Series . df[&quot;trend&quot;] = unemp_trend df[[&#39;trend&#39;,&#39;unemp&#39;]].plot(figsize = (12, 8)) df[[&#39;trend&#39;,&#39;unemp&#39;]][&quot;2000-03-31&quot;:].plot(figsize = (12, 8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdb5c2ecdd8&gt; .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/22/AnalyzingUSUnemployment.html",
            "relUrl": "/2020/09/22/AnalyzingUSUnemployment.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Stock Market and Optimal Portfolio Anaylsis scipy and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import numpy as np import quandl %matplotlib inline . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . # Grabbing a bunch of tech stocks for our portfolio COST = quandl.get(&#39;WIKI/COST.11&#39;, start_date = start, end_date = end) NLSN = quandl.get(&#39;WIKI/NLSN.11&#39;, start_date = start, end_date = end) NKE = quandl.get(&#39;WIKI/NKE.11&#39;, start_date = start, end_date = end) DIS = quandl.get(&#39;WIKI/DIS.11&#39;, start_date = start, end_date = end) . stocks = pd.concat([COST, NLSN, NKE, DIS], axis = 1) stocks.columns = [&#39;COST&#39;,&#39;NLSN&#39;,&#39;NKE&#39;,&#39;DIS&#39;] . stocks . COST NLSN NKE DIS . Date . 2010-01-04 49.085078 | NaN | 14.751122 | 28.960651 | . 2010-01-05 48.936361 | NaN | 14.809811 | 28.888407 | . 2010-01-06 49.572542 | NaN | 14.719521 | 28.734890 | . 2010-01-07 49.332941 | NaN | 14.863985 | 28.743920 | . 2010-01-08 48.977671 | NaN | 14.834641 | 28.789072 | . ... ... | ... | ... | ... | . 2018-03-21 186.070000 | 32.44 | 66.350000 | 101.820000 | . 2018-03-22 182.640000 | 31.82 | 64.420000 | 100.600000 | . 2018-03-23 180.840000 | 31.51 | 64.630000 | 98.540000 | . 2018-03-26 187.220000 | 32.03 | 65.900000 | 100.650000 | . 2018-03-27 183.150000 | 32.09 | 66.170000 | 99.360000 | . 2071 rows × 4 columns . mean_daily_ret = stocks.pct_change(1).mean() mean_daily_ret . COST 0.000699 NLSN 0.000312 NKE 0.000833 DIS 0.000683 dtype: float64 . stocks.pct_change(1).corr() . COST NLSN NKE DIS . COST 1.000000 | 0.265003 | 0.370978 | 0.415377 | . NLSN 0.265003 | 1.000000 | 0.312192 | 0.392808 | . NKE 0.370978 | 0.312192 | 1.000000 | 0.446150 | . DIS 0.415377 | 0.392808 | 0.446150 | 1.000000 | . stocks.head() . COST NLSN NKE DIS . Date . 2010-01-04 49.085078 | NaN | 14.751122 | 28.960651 | . 2010-01-05 48.936361 | NaN | 14.809811 | 28.888407 | . 2010-01-06 49.572542 | NaN | 14.719521 | 28.734890 | . 2010-01-07 49.332941 | NaN | 14.863985 | 28.743920 | . 2010-01-08 48.977671 | NaN | 14.834641 | 28.789072 | . stock_normed = stocks/stocks.iloc[0] stock_normed.plot() . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . stock_daily_ret = stocks.pct_change(1) stock_daily_ret.head() . COST NLSN NKE DIS . Date . 2010-01-04 NaN | NaN | NaN | NaN | . 2010-01-05 -0.003030 | NaN | 0.003979 | -0.002495 | . 2010-01-06 0.013000 | NaN | -0.006097 | -0.005314 | . 2010-01-07 -0.004833 | NaN | 0.009814 | 0.000314 | . 2010-01-08 -0.007201 | NaN | -0.001974 | 0.001571 | . log_ret = np.log(stocks / stocks.shift(1)) log_ret.head() . COST NLSN NKE DIS . Date . 2010-01-04 NaN | NaN | NaN | NaN | . 2010-01-05 -0.003034 | NaN | 0.003971 | -0.002498 | . 2010-01-06 0.012916 | NaN | -0.006115 | -0.005328 | . 2010-01-07 -0.004845 | NaN | 0.009767 | 0.000314 | . 2010-01-08 -0.007228 | NaN | -0.001976 | 0.001570 | . log_ret.hist(bins = 100, figsize = (12, 6)); plt.tight_layout() . log_ret.describe().transpose() . count mean std min 25% 50% 75% max . COST 2068.0 | 0.000633 | 0.011172 | -0.083110 | -0.005293 | 0.000413 | 0.006618 | 0.060996 | . NLSN 1801.0 | 0.000198 | 0.015121 | -0.185056 | -0.007131 | 0.000000 | 0.008051 | 0.095201 | . NKE 2070.0 | 0.000725 | 0.014682 | -0.098743 | -0.006602 | 0.000656 | 0.008155 | 0.115342 | . DIS 2070.0 | 0.000596 | 0.013220 | -0.096190 | -0.005710 | 0.000776 | 0.007453 | 0.073531 | . log_ret.mean() * 252 . COST 0.159439 NLSN 0.049979 NKE 0.182719 DIS 0.150081 dtype: float64 . # Compute pairwise covariance of columns log_ret.cov() . COST NLSN NKE DIS . COST 0.000125 | 0.000045 | 0.000061 | 0.000061 | . NLSN 0.000045 | 0.000229 | 0.000070 | 0.000077 | . NKE 0.000061 | 0.000070 | 0.000216 | 0.000087 | . DIS 0.000061 | 0.000077 | 0.000087 | 0.000175 | . # Set seed (optional) np.random.seed(101) # Stock Columns print(&#39;Stocks&#39;) print(stocks.columns) print(&#39; n&#39;) # Create Random Weights print(&#39;Creating Random Weights&#39;) weights = np.array(np.random.random(4)) print(weights) print(&#39; n&#39;) # Rebalance Weights print(&#39;Rebalance to sum to 1.0&#39;) weights = weights / np.sum(weights) print(weights) print(&#39; n&#39;) # Expected Return print(&#39;Expected Portfolio Return&#39;) exp_ret = np.sum(log_ret.mean() * weights) *252 print(exp_ret) print(&#39; n&#39;) # Expected Variance print(&#39;Expected Volatility&#39;) exp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) print(exp_vol) print(&#39; n&#39;) # Sharpe Ratio SR = exp_ret/exp_vol print(&#39;Sharpe Ratio&#39;) print(SR) . Stocks Index([&#39;COST&#39;, &#39;NLSN&#39;, &#39;NKE&#39;, &#39;DIS&#39;], dtype=&#39;object&#39;) Creating Random Weights [0.51639863 0.57066759 0.02847423 0.17152166] Rebalance to sum to 1.0 [0.40122278 0.44338777 0.02212343 0.13326603] Expected Portfolio Return 0.11017373023155777 Expected Volatility 0.16110487214223854 Sharpe Ratio 0.6838634286260817 . num_ports = 15000 all_weights = np.zeros((num_ports, len(stocks.columns))) ret_arr = np.zeros(num_ports) vol_arr = np.zeros(num_ports) sharpe_arr = np.zeros(num_ports) for ind in range(num_ports): # Create Random Weights weights = np.array(np.random.random(4)) # Rebalance Weights weights = weights / np.sum(weights) # Save Weights all_weights[ind,:] = weights # Expected Return ret_arr[ind] = np.sum((log_ret.mean() * weights) *252) # Expected Variance vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) # Sharpe Ratio sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind] . sharpe_arr.max() . 1.042687299617254 . sharpe_arr.argmax() . 10619 . all_weights[10619,:] . array([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01]) . max_sr_ret = ret_arr[1419] max_sr_vol = vol_arr[1419] . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add red dot for max SR plt.scatter(max_sr_vol, max_sr_ret, c = &#39;red&#39;, s = 50, edgecolors = &#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7f703b26fd00&gt; . def get_ret_vol_sr(weights): &quot;&quot;&quot; Takes in weights, returns array or return,volatility, sharpe ratio &quot;&quot;&quot; weights = np.array(weights) ret = np.sum(log_ret.mean() * weights) * 252 vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights))) sr = ret/vol return np.array([ret, vol, sr]) from scipy.optimize import minimize import numpy as np def neg_sharpe(weights): return get_ret_vol_sr(weights)[2] * -1 # Contraints def check_sum(weights): &#39;&#39;&#39; Returns 0 if sum of weights is 1.0 &#39;&#39;&#39; return np.sum(weights) - 1 # By convention of minimize function it should be a function that returns zero for conditions cons = ({&#39;type&#39; : &#39;eq&#39;, &#39;fun&#39;: check_sum}) # 0-1 bounds for each weight bounds = ((0, 1), (0, 1), (0, 1), (0, 1)) # Initial Guess (equal distribution) init_guess = [0.25, 0.25, 0.25, 0.25] # Sequential Least Squares opt_results = minimize(neg_sharpe, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) opt_results . fun: -1.0442236428192482 jac: array([-1.85623765e-04, 3.00063133e-01, 3.43203545e-04, 1.72853470e-05]) message: &#39;Optimization terminated successfully&#39; nfev: 20 nit: 4 njev: 4 status: 0 success: True x: array([0.53438392, 0. , 0.27969302, 0.18592306]) . opt_results.x get_ret_vol_sr(opt_results.x) . array([0.16421049, 0.15725605, 1.04422364]) . frontier_y = np.linspace(0, 0.3, 100) . def minimize_volatility(weights): return get_ret_vol_sr(weights)[1] frontier_volatility = [] for possible_return in frontier_y: # function for return cons = ({&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: check_sum}, {&#39;type&#39;:&#39;eq&#39;,&#39;fun&#39;: lambda w: get_ret_vol_sr(w)[0] - possible_return}) result = minimize(minimize_volatility, init_guess, method = &#39;SLSQP&#39;, bounds = bounds, constraints = cons) frontier_volatility.append(result[&#39;fun&#39;]) . plt.figure(figsize = (12, 8)) plt.scatter(vol_arr, ret_arr, c = sharpe_arr, cmap = &#39;plasma&#39;) plt.colorbar(label = &#39;Sharpe Ratio&#39;) plt.xlabel(&#39;Volatility&#39;) plt.ylabel(&#39;Return&#39;) # Add frontier line plt.plot(frontier_volatility, frontier_y, &#39;g--&#39;, linewidth = 3) . [&lt;matplotlib.lines.Line2D at 0x7f703b1949a0&gt;] .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/21/StockMarketPortfolioAnaylsis2.html",
            "relUrl": "/2020/09/21/StockMarketPortfolioAnaylsis2.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Stock Market and Portfolio Anaylsis with pandas_datareader and quandl",
            "content": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks. . import pandas as pd from pandas_datareader import data, wb import datetime . start = pd.to_datetime(&#39;2020-02-04&#39;) end = pd.to_datetime(&#39;today&#39;) MSFT_stock = web.DataReader(&#39;MSFT&#39;, &#39;yahoo&#39;, start, end) MSFT_stock.head() ZOOM_stock = web.DataReader(&#39;ZM&#39;, &#39;yahoo&#39;, start, end) ZOOM_stock.head() SNOW_stock = web.DataReader(&#39;SNOW&#39;, &#39;yahoo&#39;, start, end) SNOW_stock.head() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Open&#39;) MSFT_stock[&#39;Open&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Open&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Open&#39;].plot(label=&#39;Snowflake&#39;) plt.legend() fig = plt.figure(figsize=(12, 6)) plt.title(&#39;Volume&#39;) MSFT_stock[&#39;Volume&#39;].plot(label=&#39;Microsoft&#39;) ZOOM_stock[&#39;Volume&#39;].plot(label=&#39;Zoom&#39;) SNOW_stock[&#39;Volume&#39;].plot(label=&#39;Snowflake&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f557129fa90&gt; . import pandas as pd import quandl . start = pd.to_datetime(&#39;2010-01-01&#39;) end = pd.to_datetime(&#39;today&#39;) . # Grabbing a bunch of tech stocks for our portfolio COST = quandl.get(&#39;WIKI/COST.11&#39;, start_date = start, end_date = end) NLSN = quandl.get(&#39;WIKI/NLSN.11&#39;, start_date = start, end_date = end) NKE = quandl.get(&#39;WIKI/NKE.11&#39;, start_date = start, end_date = end) DIS = quandl.get(&#39;WIKI/DIS.11&#39;, start_date = start, end_date = end) # Example COST.iloc[0][&#39;Adj. Close&#39;] for stock_df in (COST, NLSN, NKE, DIS): stock_df[&#39;Normed Return&#39;] = stock_df[&#39;Adj. Close&#39;] / stock_df.iloc[0][&#39;Adj. Close&#39;] COST.head() COST.tail() ## Allocations for stock_df,allo in zip([COST,NLSN,NKE,DIS],[.2, .1, .4, .3]): stock_df[&#39;Allocation&#39;] = stock_df[&#39;Normed Return&#39;] * allo COST.head() ## Investment for stock_df in [COST,NLSN,NKE,DIS]: stock_df[&#39;Position Values&#39;] = stock_df[&#39;Allocation&#39;] * 1000000 ## Total Portfolio Value portfolio_val = pd.concat([COST[&#39;Position Values&#39;], NLSN[&#39;Position Values&#39;], NKE[&#39;Position Values&#39;], DIS[&#39;Position Values&#39;]], axis = 1) portfolio_val.head() portfolio_val.columns = [&#39;COST Pos&#39;, &#39;NLSN Pos&#39;, &#39;NKE Pos&#39;, &#39;DIS Pos&#39;] portfolio_val.head() portfolio_val[&#39;Total Pos&#39;] = portfolio_val.sum(axis = 1) portfolio_val.head() import matplotlib.pyplot as plt %matplotlib inline portfolio_val[&#39;Total Pos&#39;].plot(figsize = (12, 8)) plt.title(&#39;Total Portfolio Value&#39;) portfolio_val.drop(&#39;Total Pos&#39;, axis = 1).plot(kind = &#39;line&#39;, figsize = (12, 8)) portfolio_val.tail() . COST Pos NLSN Pos NKE Pos DIS Pos Total Pos . Date . 2018-03-21 758153.014553 | 144489.827125 | 1.799185e+06 | 1.054741e+06 | 3.756569e+06 | . 2018-03-22 744177.280475 | 141728.307617 | 1.746850e+06 | 1.042104e+06 | 3.674859e+06 | . 2018-03-23 736843.076002 | 140347.547864 | 1.752545e+06 | 1.020764e+06 | 3.650500e+06 | . 2018-03-26 762838.756299 | 142663.660999 | 1.786983e+06 | 1.042622e+06 | 3.735107e+06 | . 2018-03-27 746255.305075 | 142930.904822 | 1.794304e+06 | 1.029259e+06 | 3.712749e+06 | .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/19/StockMarketPortfolioAnaylsis.html",
            "relUrl": "/2020/09/19/StockMarketPortfolioAnaylsis.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "NLP Heatmaps with Seaborn",
            "content": "from jupyterthemes import jtplot import warnings from imblearn.over_sampling import SMOTE import seaborn as sns from sklearn.model_selection import train_test_split import pandas as pd import numpy as np import pandas_profiling from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn import preprocessing import matplotlib.pyplot as plt %matplotlib inline # ignore warnings warnings.filterwarnings(&#39;ignore&#39;) jtplot.style(theme=&#39;oceans16&#39;, context=&#39;notebook&#39;, ticks=True, grid=False, figsize=(10, 9)) . df=pd.read_csv(&#39;../processed_data/nf_complete.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 126 entries, 0 to 125 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 Unnamed: 0 126 non-null int64 1 year 126 non-null int64 2 title 126 non-null object 3 abstract 126 non-null object 4 theme 126 non-null object 5 China 126 non-null int64 6 Russia 126 non-null int64 7 War 126 non-null int64 8 President 126 non-null int64 9 US 126 non-null int64 10 Vietnam 126 non-null int64 11 Cold War 126 non-null int64 12 World War 126 non-null int64 13 Vietnam War 126 non-null int64 14 Korean War 126 non-null int64 15 Survey 126 non-null int64 16 Case Study 126 non-null int64 17 Trade 126 non-null int64 18 Humanitarian 126 non-null int64 19 fixed_effects 126 non-null int64 20 instrumental_variable 126 non-null int64 21 regression 126 non-null int64 22 experimental 126 non-null int64 dtypes: int64(20), object(3) memory usage: 22.8+ KB . import plotly_express as ple ple.histogram(df.sort_values(&#39;year&#39;).groupby([&#39;year&#39;,&#39;theme&#39;])[&#39;Cold War&#39;].sum().reset_index(), x=&quot;year&quot;, y=&quot;Cold War&quot;, histfunc=&quot;sum&quot;, color=&quot;theme&quot;) . # ple.lidifferences(dfm_regional.sort_values(&#39;year&#39;).groupby([&#39;year&#39;,&#39;theme&#39;])[&#39;Cold War&#39;].sum().reset_index(), # x=&#39;year&#39;, # y=&#39;Cold War&#39;, # line_group=&#39;theme&#39;, # color=&#39;theme&#39; # ) . # Create the crosstab DataFrame pd_crosstab = pd.crosstab(df[&quot;theme&quot;], df[&quot;year&quot;]) # Plot a heatmap of the table with no color bar and using the BuGn palette sns.heatmap(pd_crosstab, cbar=False, cmap=&quot;GnBu&quot;, linewidths=0.3) # Rotate tick marks for visibility plt.yticks(rotation=0) plt.xticks(rotation=90) plt.tight_layout() #plt.savefig(&#39;./img/theme_heat_1.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500) #Show the plot plt.show() plt.clf() . &lt;Figure size 720x648 with 0 Axes&gt; . sns.clustermap(pd_crosstab, cmap=&#39;Greens&#39;, robust=True) # plot using a color palette #sns.heatmap(df, cmap=&quot;YlGnBu&quot;) #sns.heatmap(df, cmap=&quot;Blues&quot;) #sns.heatmap(df, cmap=&quot;BuPu&quot;) #sns.heatmap(df, cmap=&quot;Greens&quot;) . &lt;seaborn.matrix.ClusterGrid at 0x7f978c07a470&gt; . # Import seaborn library import seaborn as sns # Get correlation matrix of the meat DataFrame corr_meat = df.corr(method=&#39;pearson&#39;) # Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels fig = sns.clustermap(pd_crosstab, row_cluster=True, col_cluster=True, figsize=(10, 10)) plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90) plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0) plt.show() . data_normalized = pd_crosstab # Standardize the mean and variance within a stat, so different stats can be comparable # (This is the same as changing all the columns to Z-scores) data_normalized = (data_normalized - data_normalized.mean())/data_normalized.var() # Normalize these values to range from -1 to 1 data_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min()) data_normalized = data_normalized.T # Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram. sns.clustermap(data_normalized, cmap=&#39;Blues&#39;); . data_normalized = pd_crosstab # Standardize the mean and variance within a stat, so different stats can be comparable # (This is the same as changing all the columns to Z-scores) data_normalized = (data_normalized - data_normalized.mean())/data_normalized.var() # Normalize these values to range from -1 to 1 data_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min()) #data_normalized = data_normalized.T # Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram. sns.clustermap(data_normalized, cmap=&#39;BuPu&#39;); . import matplotlib.pyplot as plt sns.clustermap(data_normalized); fig = plt.gcf() fig.savefig(&#39;clusteredheatmap_bbox_tight.png&#39;, bbox_inches=&#39;tight&#39;) . tidy_df = pd.melt(df.reset_index(), id_vars=&#39;index&#39;) df.T.head() . 0 1 2 3 4 5 6 7 8 9 ... 116 117 118 119 120 121 122 123 124 125 . Unnamed: 0 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 14 | 13 | ... | 128 | 130 | 123 | 125 | 131 | 132 | 133 | 134 | 135 | 136 | . year 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2000 | 2001 | 2001 | ... | 2017 | 2017 | 2017 | 2017 | 2018 | 2018 | 2018 | 2018 | 2018 | 2018 | . title &quot;Institutions at the Domestic/International Ne... | Born to Lose and Doomed to Survive: State Deat... | The significance of “allegiance” in internatio... | The significance of “allegiance” in internatio... | Truth-Telling and Mythmaking in Post-Soviet Ru... | Building a Cape Fear Metropolis: Fort Bragg, F... | The Glories and the Sadness: Shaping the natio... | What leads longstanding adversaries to engage ... | A School for the Nation: Military Institution... | The &#39;American Century&#39; Army: The Origins of t... | ... | Fully Committed? Religiously Committed State P... | Straddling the Threshold of Two Worlds: Soldie... | U.S. Army’s Investigation and Adjudication of ... | Grand Strategic Crucibles: The Lasting Effects... | Trust in International Politics: The Role of L... | Planning for the Short Haul: Trade Among Belli... | Clinging to the Anti-Imperial Mantle: The Repu... | The New Navy&#39;s Pacific Wars: Peripheral Confl... | Stop or I&#39;ll Shoot, Comply and I Won&#39;t: The Di... | Unexpected Humanitarians: Albania, the U.S. Mi... | . abstract Civil-military relations are frequently studie... | Under what conditions do states die, or exit t... | My dissertation employs original and secondary... | nThis study revises prevailing interpretation... | Can distorted and pernicious ideas about histo... | My dissertation examines the cultural and econ... | In my dissertation I compare the ways in whic... | This dissertation develops a socio-psychoanal... | Beginning in Europe in the latter half of the ... | This dissertation covers the period 1949-1959 ... | ... | This dissertation argues that the higher the l... | This dissertation explores how American soldie... | This dissertation examines the U.S. Army’s res... | When and how do military interventions shape g... | In my dissertation, I focus on how leader rela... | In times of war, why do belligerents continue ... | My dissertation project, Clinging to the Anti-... | Using a transnational methodology and sources ... | There is a dilemma at the heart of coercion. S... | Using archives and oral history, this disserta... | . theme IR scholarship | IR scholarship | IR scholarship | Conflit Between States | Conflict Between States | Domestic Military History | Culture | Culture / Peace Process | Military History | Military History | ... | IR Scholarship | Military History | Military History | IR Scholarship | Nuclear Weapons | Conflict between states | Cold War | Military History | IR Scholarship | Military History | . 5 rows × 126 columns .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/03/NLP_Seaborn_Heatmaps.html",
            "relUrl": "/2020/09/03/NLP_Seaborn_Heatmaps.html",
            "date": " • Sep 3, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "NLP ngrams With Python",
            "content": "&#39;In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.&#39;, from wikipedia . import pandas as pd df=pd.read_csv(&#39;../../processed_data/nf_complete.csv&#39;) . Pre-processing text . def preprocessor(text): text = re.sub(&#39;&lt;[^&gt;]*&gt;&#39;, &#39;&#39;, text) emoticons = re.findall(&#39;(?::|;|=)(?:-)?(?: )| (|D|P)&#39;, text) text = re.sub(&#39;[ W]+&#39;, &#39; &#39;, text.lower()) + &#39; &#39;.join(emoticons).replace(&#39;-&#39;, &#39;&#39;) return text . Find Total Word Count . text = &quot; &quot;.join(review for review in df.abstract) print (&quot;There are {} words in the combination of all abstracts.&quot;.format(len(text))) . There are 272025 words in the combination of all abstracts. . from urllib.request import urlopen from random import randint def wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sum def retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return word def buildWordDict(text): # Remove newlines and quotes text = text.replace(&#39; n&#39;, &#39; &#39;); text = text.replace(&#39;&quot;&#39;, &#39;&#39;); # Make sure punctuation marks are treated as their own &quot;words,&quot; # so that they will be included in the Markov chain punctuation = [&#39;,&#39;,&#39;.&#39;,&#39;;&#39;,&#39;:&#39;] for symbol in punctuation: text = text.replace(symbol, &#39; {} &#39;.format(symbol)); words = text.split(&#39; &#39;) # Filter out empty words words = [word for word in words if word != &#39;&#39;] wordDict = {} for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = {} if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDict wordDict = buildWordDict(text) #Generate a Markov chain of length 100 length = 100 chain = [&#39;Vietnam&#39;] for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord) print(&#39; &#39;.join(chain)) . Vietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate . ” and . def getFirstSentenceContaining(ngram, text): #print(ngram) sentences = text.upper().split(&quot;. &quot;) for sentence in sentences: if ngram in sentence: return sentence+&#39; n&#39; return &quot;&quot; print(getFirstSentenceContaining(&#39;I&#39;, text)) . CIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS . #text . from urllib.request import urlopen from bs4 import BeautifulSoup import re import string from collections import Counter def cleanSentence(sentence): sentence = sentence.split(&#39; &#39;) sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence] sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == &#39;a&#39; or word.lower() == &#39;i&#39;)] return sentence def cleanInput(content): content = content.upper() content = re.sub(&#39; n&#39;, &#39; &#39;, content) content = bytes(content, &#39;UTF-8&#39;) content = content.decode(&#39;ascii&#39;, &#39;ignore&#39;) sentences = content.split(&#39;. &#39;) return [cleanSentence(sentence) for sentence in sentences] def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return output def getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [&#39; &#39;.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams) content = str(text) ngrams = getNgrams(content, 3) #print(ngrams) . from urllib.request import urlopen from bs4 import BeautifulSoup import re import string from collections import Counter def isCommon(ngram): commonWords = [&#39;THE&#39;, &#39;BE&#39;, &#39;AND&#39;, &#39;OF&#39;, &#39;A&#39;, &#39;IN&#39;, &#39;TO&#39;, &#39;HAVE&#39;, &#39;IT&#39;, &#39;I&#39;, &#39;THAT&#39;, &#39;FOR&#39;, &#39;YOU&#39;, &#39;HE&#39;, &#39;WITH&#39;, &#39;ON&#39;, &#39;DO&#39;, &#39;SAY&#39;, &#39;THIS&#39;, &#39;THEY&#39;, &#39;IS&#39;, &#39;AN&#39;, &#39;AT&#39;, &#39;BUT&#39;, &#39;WE&#39;, &#39;HIS&#39;, &#39;FROM&#39;, &#39;THAT&#39;, &#39;NOT&#39;, &#39;BY&#39;, &#39;SHE&#39;, &#39;OR&#39;, &#39;AS&#39;, &#39;WHAT&#39;, &#39;GO&#39;, &#39;THEIR&#39;, &#39;CAN&#39;, &#39;WHO&#39;, &#39;GET&#39;, &#39;IF&#39;, &#39;WOULD&#39;, &#39;HER&#39;, &#39;ALL&#39;, &#39;MY&#39;, &#39;MAKE&#39;, &#39;ABOUT&#39;, &#39;KNOW&#39;, &#39;WILL&#39;, &#39;AS&#39;, &#39;UP&#39;, &#39;ONE&#39;, &#39;TIME&#39;, &#39;HAS&#39;, &#39;BEEN&#39;, &#39;THERE&#39;, &#39;YEAR&#39;, &#39;SO&#39;, &#39;THINK&#39;, &#39;WHEN&#39;, &#39;WHICH&#39;, &#39;THEM&#39;, &#39;SOME&#39;, &#39;ME&#39;, &#39;PEOPLE&#39;, &#39;TAKE&#39;, &#39;OUT&#39;, &#39;INTO&#39;, &#39;JUST&#39;, &#39;SEE&#39;, &#39;HIM&#39;, &#39;YOUR&#39;, &#39;COME&#39;, &#39;COULD&#39;, &#39;NOW&#39;, &#39;THAN&#39;, &#39;LIKE&#39;, &#39;OTHER&#39;, &#39;HOW&#39;, &#39;THEN&#39;, &#39;ITS&#39;, &#39;OUR&#39;, &#39;TWO&#39;, &#39;MORE&#39;, &#39;THESE&#39;, &#39;WANT&#39;, &#39;WAY&#39;, &#39;LOOK&#39;, &#39;FIRST&#39;, &#39;ALSO&#39;, &#39;NEW&#39;, &#39;BECAUSE&#39;, &#39;DAY&#39;, &#39;MORE&#39;, &#39;USE&#39;, &#39;NO&#39;, &#39;MAN&#39;, &#39;FIND&#39;, &#39;HERE&#39;, &#39;THING&#39;, &#39;GIVE&#39;, &#39;MANY&#39;, &#39;WELL&#39;] for word in ngram: if word in commonWords: return True return False def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): if not isCommon(content[i:i+n]): output.append(content[i:i+n]) return output ngrams = getNgrams(content, 3) #print(ngrams) . def getFirstSentenceContaining(ngram, content): #print(ngram) sentences = content.upper().split(&quot;. &quot;) for sentence in sentences: if ngram in sentence: return sentence+&#39; n&#39; return &quot;&quot; print(getFirstSentenceContaining(&#39;SINO-JAPANESE WAR 1894-1895&#39;, content)) print(getFirstSentenceContaining(&#39;2ND VIETNAM WAR&#39;, content)) print(getFirstSentenceContaining(&#39;COLD WAR ARMY&#39;, content)) print(getFirstSentenceContaining(&#39;WORLD WAR II&#39;, content)) print(getFirstSentenceContaining(&#39;ARMS CONTROL AGREEMENTS&#39;, content)) . IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD. THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE ‘OTHER SIDE,’ ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION COLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTION’S INTELLECTUAL AND HISTORICAL FRAMEWORK OF “PREPAREDNESS” TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR I ARGUE THAT A NORM PROTECTING STATES’ TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS. THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY . from urllib.request import urlopen from random import randint def wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sum def retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return word def buildWordDict(text): # Remove newlines and quotes text = text.replace(&#39; n&#39;, &#39; &#39;); text = text.replace(&#39;&quot;&#39;, &#39;&#39;); # Make sure punctuation marks are treated as their own &quot;words,&quot; # so that they will be included in the Markov chain punctuation = [&#39;,&#39;,&#39;.&#39;,&#39;;&#39;,&#39;:&#39;] for symbol in punctuation: text = text.replace(symbol, &#39; {} &#39;.format(symbol)); words = text.split(&#39; &#39;) # Filter out empty words words = [word for word in words if word != &#39;&#39;] wordDict = {} for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = {} if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDict wordDict = buildWordDict(text) #Generate a Markov chain of length 100 length = 100 chain = [&#39;I&#39;] for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord) #print(&#39; &#39;.join(chain)) . import re def getNgrams(content, n): content = re.sub(&#39; n|[[ d+ ]]&#39;, &#39; &#39;, content) content = bytes(content, &#39;UTF-8&#39;) content = content.decode(&#39;ascii&#39;, &#39;ignore&#39;) content = content.split(&#39; &#39;) content = [word for word in content if word != &#39;&#39;] output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return output . from collections import Counter def getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [&#39; &#39;.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams) . #print(getNgrams(content, 2)) . def isCommon(ngram): commonWords = [&#39;THE&#39;, &#39;BE&#39;, &#39;AND&#39;, &#39;OF&#39;, &#39;A&#39;, &#39;IN&#39;, &#39;TO&#39;, &#39;HAVE&#39;, &#39;IT&#39;, &#39;I&#39;, &#39;THAT&#39;, &#39;FOR&#39;, &#39;YOU&#39;, &#39;HE&#39;, &#39;WITH&#39;, &#39;ON&#39;, &#39;DO&#39;, &#39;SAY&#39;, &#39;THIS&#39;, &#39;THEY&#39;, &#39;IS&#39;, &#39;AN&#39;, &#39;AT&#39;, &#39;BUT&#39;, &#39;WE&#39;, &#39;HIS&#39;, &#39;FROM&#39;, &#39;THAT&#39;, &#39;NOT&#39;, &#39;BY&#39;, &#39;SHE&#39;, &#39;OR&#39;, &#39;AS&#39;, &#39;WHAT&#39;, &#39;GO&#39;, &#39;THEIR&#39;, &#39;CAN&#39;, &#39;WHO&#39;, &#39;GET&#39;, &#39;IF&#39;, &#39;WOULD&#39;, &#39;HER&#39;, &#39;ALL&#39;, &#39;MY&#39;, &#39;MAKE&#39;, &#39;ABOUT&#39;, &#39;KNOW&#39;, &#39;WILL&#39;, &#39;AS&#39;, &#39;UP&#39;, &#39;ONE&#39;, &#39;TIME&#39;, &#39;HAS&#39;, &#39;BEEN&#39;, &#39;THERE&#39;, &#39;YEAR&#39;, &#39;SO&#39;, &#39;THINK&#39;, &#39;WHEN&#39;, &#39;WHICH&#39;, &#39;THEM&#39;, &#39;SOME&#39;, &#39;ME&#39;, &#39;PEOPLE&#39;, &#39;TAKE&#39;, &#39;OUT&#39;, &#39;INTO&#39;, &#39;JUST&#39;, &#39;SEE&#39;, &#39;HIM&#39;, &#39;YOUR&#39;, &#39;COME&#39;, &#39;COULD&#39;, &#39;NOW&#39;, &#39;THAN&#39;, &#39;LIKE&#39;, &#39;OTHER&#39;, &#39;HOW&#39;, &#39;THEN&#39;, &#39;ITS&#39;, &#39;OUR&#39;, &#39;TWO&#39;, &#39;MORE&#39;, &#39;THESE&#39;, &#39;WANT&#39;, &#39;WAY&#39;, &#39;LOOK&#39;, &#39;FIRST&#39;, &#39;ALSO&#39;, &#39;NEW&#39;, &#39;BECAUSE&#39;, &#39;DAY&#39;, &#39;MORE&#39;, &#39;USE&#39;, &#39;NO&#39;, &#39;MAN&#39;, &#39;FIND&#39;, &#39;HERE&#39;, &#39;THING&#39;, &#39;GIVE&#39;, &#39;MANY&#39;, &#39;WELL&#39;] for word in ngram: if word in commonWords: return True return False def getNgramsFromSentence(text, n): output = [] for i in range(len(text)-n+1): if not isCommon(text[i:i+n]): output.append(text[i:i+n]) return output ngrams = getNgrams(text, 3) #print(ngrams) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/09/02/NLP_Ngram.html",
            "relUrl": "/2020/09/02/NLP_Ngram.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "NLP with Pyspark",
            "content": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. . from pyspark.ml.feature import Tokenizer, RegexTokenizer from pyspark.sql.functions import col, udf from pyspark.sql.types import IntegerType . sentenceDataFrame = spark.createDataFrame([ (0, &quot;Hi I heard about Spark&quot;), (1, &quot;I wish Java could use case classes&quot;), (2, &quot;Logistic,regression,models,are,neat&quot;) ], [&quot;id&quot;, &quot;sentence&quot;]) . sentenceDataFrame.show() . ++--+ id| sentence| ++--+ 0|Hi I heard about ...| 1|I wish Java could...| 2|Logistic,regressi...| ++--+ Using Tokenizer and RegexTokenizer . tokenizer = Tokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;) regexTokenizer = RegexTokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;, pattern=&quot; W&quot;) # alternatively, pattern=&quot; w+&quot;, gaps(False) countTokens = udf(lambda words: len(words), IntegerType()) tokenized = tokenizer.transform(sentenceDataFrame) tokenized.select(&quot;sentence&quot;, &quot;words&quot;) .withColumn(&quot;tokens&quot;, countTokens(col(&quot;words&quot;))).show(truncate=False) regexTokenized = regexTokenizer.transform(sentenceDataFrame) regexTokenized.select(&quot;sentence&quot;, &quot;words&quot;) .withColumn(&quot;tokens&quot;, countTokens(col(&quot;words&quot;))).show(truncate=False) . +--+++ sentence |words |tokens| +--+++ Hi I heard about Spark |[hi, i, heard, about, spark] |5 | I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 | Logistic,regression,models,are,neat|[logistic,regression,models,are,neat] |1 | +--+++ +--+++ sentence |words |tokens| +--+++ Hi I heard about Spark |[hi, i, heard, about, spark] |5 | I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 | Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5 | +--+++ Removing Stop Words . from pyspark.ml.feature import StopWordsRemover sentenceData = spark.createDataFrame([ (0, [&quot;I&quot;, &quot;saw&quot;, &quot;the&quot;, &quot;red&quot;, &quot;balloon&quot;]), (1, [&quot;Mary&quot;, &quot;had&quot;, &quot;a&quot;, &quot;little&quot;, &quot;lamb&quot;]) ], [&quot;id&quot;, &quot;raw&quot;]) remover = StopWordsRemover(inputCol=&quot;raw&quot;, outputCol=&quot;filtered&quot;) remover.transform(sentenceData).show(truncate=False) . ++-+--+ id |raw |filtered | ++-+--+ 0 |[I, saw, the, red, balloon] |[saw, red, balloon] | 1 |[Mary, had, a, little, lamb]|[Mary, little, lamb]| ++-+--+ n-grams . from pyspark.ml.feature import NGram wordDataFrame = spark.createDataFrame([ (0, [&quot;Hi&quot;, &quot;I&quot;, &quot;heard&quot;, &quot;about&quot;, &quot;Spark&quot;]), (1, [&quot;I&quot;, &quot;wish&quot;, &quot;Java&quot;, &quot;could&quot;, &quot;use&quot;, &quot;case&quot;, &quot;classes&quot;]), (2, [&quot;Logistic&quot;, &quot;regression&quot;, &quot;models&quot;, &quot;are&quot;, &quot;neat&quot;]) ], [&quot;id&quot;, &quot;words&quot;]) ngram = NGram(n=2, inputCol=&quot;words&quot;, outputCol=&quot;ngrams&quot;) ngramDataFrame = ngram.transform(wordDataFrame) ngramDataFrame.select(&quot;ngrams&quot;).show(truncate=False) . ++ ngrams | ++ [Hi I, I heard, heard about, about Spark] | [I wish, wish Java, Java could, could use, use case, case classes]| [Logistic regression, regression models, models are, are neat] | ++ from pyspark.ml.feature import HashingTF, IDF, Tokenizer sentenceData = spark.createDataFrame([ (0.0, &quot;Hi I heard about Spark&quot;), (0.0, &quot;I wish Java could use case classes&quot;), (1.0, &quot;Logistic regression models are neat&quot;) ], [&quot;label&quot;, &quot;sentence&quot;]) sentenceData.show() . +--+--+ label| sentence| +--+--+ 0.0|Hi I heard about ...| 0.0|I wish Java could...| 1.0|Logistic regressi...| +--+--+ tokenizer = Tokenizer(inputCol=&quot;sentence&quot;, outputCol=&quot;words&quot;) wordsData = tokenizer.transform(sentenceData) wordsData.show() . +--+--+--+ label| sentence| words| +--+--+--+ 0.0|Hi I heard about ...|[hi, i, heard, ab...| 0.0|I wish Java could...|[i, wish, java, c...| 1.0|Logistic regressi...|[logistic, regres...| +--+--+--+ hashingTF = HashingTF(inputCol=&quot;words&quot;, outputCol=&quot;rawFeatures&quot;, numFeatures=20) featurizedData = hashingTF.transform(wordsData) # alternatively, CountVectorizer can also be used to get term frequency vectors idf = IDF(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;) idfModel = idf.fit(featurizedData) rescaledData = idfModel.transform(featurizedData) rescaledData.select(&quot;label&quot;, &quot;features&quot;).show() . +--+--+ label| features| +--+--+ 0.0|(20,[6,8,13,16],[...| 0.0|(20,[0,2,7,13,15,...| 1.0|(20,[3,4,6,11,19]...| +--+--+ CountVectorizer . from pyspark.ml.feature import CountVectorizer # Input data: Each row is a bag of words with a ID. df = spark.createDataFrame([ (0, &quot;a b c&quot;.split(&quot; &quot;)), (1, &quot;a b b c a&quot;.split(&quot; &quot;)) ], [&quot;id&quot;, &quot;words&quot;]) # fit a CountVectorizerModel from the corpus. cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;features&quot;, vocabSize=3, minDF=2.0) model = cv.fit(df) result = model.transform(df) result.show(truncate=False) . +++-+ id |words |features | +++-+ 0 |[a, b, c] |(3,[0,1,2],[1.0,1.0,1.0])| 1 |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])| +++-+ df = spark.read.load(&quot;/FileStore/tables/SMSSpamCollection&quot;, format=&quot;csv&quot;, sep=&quot; t&quot;, inferSchema=&quot;true&quot;, header=&quot;false&quot;) . df.printSchema() . root -- _c0: string (nullable = true) -- _c1: string (nullable = true) data = df.withColumnRenamed(&#39;_c0&#39;,&#39;class&#39;).withColumnRenamed(&#39;_c1&#39;,&#39;text&#39;) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) Clean and Prepare the Data . from pyspark.sql.functions import length . data = data.withColumn(&#39;length&#39;,length(data[&#39;text&#39;])) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) -- length: integer (nullable = true) # Pretty Clear Difference data.groupby(&#39;class&#39;).mean().show() . +--+--+ class| avg(length)| +--+--+ ham| 71.4545266210897| spam|138.6706827309237| +--+--+ from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer tokenizer = Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;token_text&quot;) stopremove = StopWordsRemover(inputCol=&#39;token_text&#39;,outputCol=&#39;stop_tokens&#39;) count_vec = CountVectorizer(inputCol=&#39;stop_tokens&#39;,outputCol=&#39;c_vec&#39;) idf = IDF(inputCol=&quot;c_vec&quot;, outputCol=&quot;tf_idf&quot;) ham_spam_to_num = StringIndexer(inputCol=&#39;class&#39;,outputCol=&#39;label&#39;) . from pyspark.ml.feature import VectorAssembler from pyspark.ml.linalg import Vector . clean_up = VectorAssembler(inputCols=[&#39;tf_idf&#39;,&#39;length&#39;],outputCol=&#39;features&#39;) . Naive Bayes . from pyspark.ml.classification import NaiveBayes . # Use defaults nb = NaiveBayes() . ### Pipeline . from pyspark.ml import Pipeline . data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up]) . cleaner = data_prep_pipe.fit(data) . clean_data = cleaner.transform(data) . Training and Evaluation . clean_data = clean_data.select([&#39;label&#39;,&#39;features&#39;]) . clean_data.show() . +--+--+ label| features| +--+--+ 0.0|(13424,[7,11,31,6...| 0.0|(13424,[0,24,297,...| 1.0|(13424,[2,13,19,3...| 0.0|(13424,[0,70,80,1...| 0.0|(13424,[36,134,31...| 1.0|(13424,[10,60,139...| 0.0|(13424,[10,53,103...| 0.0|(13424,[125,184,4...| 1.0|(13424,[1,47,118,...| 1.0|(13424,[0,1,13,27...| 0.0|(13424,[18,43,120...| 1.0|(13424,[8,17,37,8...| 1.0|(13424,[13,30,47,...| 0.0|(13424,[39,96,217...| 0.0|(13424,[552,1697,...| 1.0|(13424,[30,109,11...| 0.0|(13424,[82,214,47...| 0.0|(13424,[0,2,49,13...| 0.0|(13424,[0,74,105,...| 1.0|(13424,[4,30,33,5...| +--+--+ only showing top 20 rows (training,testing) = clean_data.randomSplit([0.7,0.3]) . spam_predictor = nb.fit(training) . data.printSchema() . root -- class: string (nullable = true) -- text: string (nullable = true) -- length: integer (nullable = true) test_results = spam_predictor.transform(testing) . test_results.show() . +--+--+--+--+-+ label| features| rawPrediction| probability|prediction| +--+--+--+--+-+ 0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...| 0.0| 0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...| 0.0| 0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...| 0.0| 0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...| 0.0| 0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...| 0.0| 0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...| 0.0| 0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...| 0.0| 0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...| 0.0| 0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...| 0.0| 0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...| 0.0| 0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...| 1.0| 0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...| 0.0| 0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...| 0.0| 0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...| 0.0| 0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...| 0.0| 0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...| 0.0| 0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...| 0.0| 0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...| 0.0| 0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...| 0.0| 0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...| 0.0| +--+--+--+--+-+ only showing top 20 rows ## Evaluating Model Accuracy . from pyspark.ml.evaluation import MulticlassClassificationEvaluator . acc_eval = MulticlassClassificationEvaluator() acc = acc_eval.evaluate(test_results) print(&quot;Accuracy of model at predicting spam was: {}&quot;.format(acc)) . Accuracy of model at predicting spam was: 0.9204435112848836",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/30/NLP-with-Pyspark.html",
            "relUrl": "/2020/08/30/NLP-with-Pyspark.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Clustering with Pyspark",
            "content": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler . feat_cols = [&#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;,&#39;fr&#39;] . feat_cols = [&#39;gdp&#39;] . vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol=&#39;features&#39;) . final_df = vec_assembler.transform(df) . Using the StandardScaler . from pyspark.ml.feature import StandardScaler . scaler = StandardScaler(inputCol=&quot;features&quot;, outputCol=&quot;scaledFeatures&quot;, withStd=True, withMean=False) . Fitting the StandardScaler . # Compute summary statistics by fitting the StandardScaler scalerModel = scaler.fit(final_df) . # Normalize each feature to have unit standard deviation. cluster_final_data = scalerModel.transform(final_df) . kmeans3 = KMeans(featuresCol=&#39;scaledFeatures&#39;,k=3) kmeans2 = KMeans(featuresCol=&#39;scaledFeatures&#39;,k=2) . model_k3 = kmeans3.fit(cluster_final_data) model_k2 = kmeans2.fit(cluster_final_data) . model_k3.transform(cluster_final_data).groupBy(&#39;prediction&#39;).count().show() . +-+--+ prediction|count| +-+--+ 1| 15| 2| 86| 0| 259| +-+--+ model_k2.transform(cluster_final_data).groupBy(&#39;prediction&#39;).count().show() . +-+--+ prediction|count| +-+--+ 1| 308| 0| 52| +-+--+",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/29/Clustering-with-Pyspark.html",
            "relUrl": "/2020/08/29/Clustering-with-Pyspark.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Regression and Classification with Pyspark ML",
            "content": "Linear Regression and Random Forest/GBT Classification with Pyspark . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType from pyspark.sql.functions import * data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows df.groupBy(&#39;province&#39;).count().show() . ++--+ province|count| ++--+ Guangdong| 12| Hunan| 12| Shanxi| 12| Tibet| 12| Hubei| 12| Tianjin| 12| Beijing| 12| Heilongjiang| 12| Liaoning| 12| Henan| 12| Anhui| 12| Xinjiang| 12| Fujian| 12| Jiangxi| 12| Jilin| 12| Chongqing| 12| Shaanxi| 12| Sichuan| 12| Yunnan| 12| Gansu| 12| ++--+ only showing top 20 rows Imputation of mean values to prepare the data . mean_val = df.select(mean(df[&#39;general&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;general&quot;]) . mean_val = df.select(mean(df[&#39;specific&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;specific&quot;]) . mean_val = df.select(mean(df[&#39;rr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;rr&quot;]) . mean_val = df.select(mean(df[&#39;fr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;fr&quot;]) . mean_val = df.select(mean(df[&#39;rnr&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;rnr&quot;]) . mean_val = df.select(mean(df[&#39;i&#39;])).collect() mean_val[0][0] mean_gen = mean_val[0][0] df = df.na.fill(mean_gen,[&quot;i&quot;]) . Creating binary target feature from extant column for classification . from pyspark.sql.functions import * df = df.withColumn(&#39;specific_classification&#39;,when(df.specific &gt;= 583470.7303370787,1).otherwise(0)) . Using StringIndexer for categorical encoding of string type columns . from pyspark.ml.feature import StringIndexer . indexer = StringIndexer(inputCol=&quot;province&quot;, outputCol=&quot;provinceIndex&quot;) df = indexer.fit(df).transform(df) . indexer = StringIndexer(inputCol=&quot;reg&quot;, outputCol=&quot;regionIndex&quot;) df = indexer.fit(df).transform(df) . df.show() . ++--+++-+-+--++-+-+-+--+-+--+-+--+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it|specific_classification|provinceIndex|regionIndex| ++--+++-+-+--++-+-+-+--+-+--+-+--+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 0| 0.0| 0.0| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 0| 0.0| 0.0| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 0| 0.0| 0.0| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131.0|0.0355944252244898|0.05968862|0.08376352|1646891| East China|1227364| 0| 0.0| 0.0| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 0| 0.0| 0.0| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 0| 0.0| 0.0| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 0| 0.0| 0.0| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 1| 0.0| 0.0| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0|2522449| East China|3422176| 1| 0.0| 0.0| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000.0| 0.0| 0.0| 0.3243243|2522449| East China|3874846| 1| 0.0| 0.0| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354.0| 0.0| 0.0| 0.3243243|3434548| East China|5167300| 1| 0.0| 0.0| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892.0| 0.0| 0.0| 0.3243243|4468640| East China|7040099| 1| 0.0| 0.0| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290.0|0.0355944252244898|0.05968862|0.08376352| 634562|North China| 508135| 0| 1.0| 4.0| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 0| 1.0| 4.0| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 0| 1.0| 4.0| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525.0| 0.0| 0.0| 0.53|2522449|North China| 944047| 0| 1.0| 4.0| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 0| 1.0| 4.0| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 0| 1.0| 4.0| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 0| 1.0| 4.0| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126.0| 0.0| 0.7948718| 0.0|2823366|North China|1426600| 1| 1.0| 4.0| ++--+++-+-+--++-+-+-+--+-+--+-+--+ only showing top 20 rows Using VectorAssembler to prepare features for machine learning . from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler . df.columns . Out[375]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;, &#39;specific_classification&#39;, &#39;provinceIndex&#39;, &#39;regionIndex&#39;] assembler = VectorAssembler( inputCols=[ &#39;provinceIndex&#39;, # &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, #&#39;rnr&#39;, #&#39;rr&#39;, #&#39;i&#39;, #&#39;fr&#39;, &#39;regionIndex&#39;, &#39;it&#39; ], outputCol=&quot;features&quot;) . output = assembler.transform(df) . final_data = output.select(&quot;features&quot;, &quot;specific&quot;) . Spliting data into train and test . train_data,test_data = final_data.randomSplit([0.7,0.3]) . Regression with Pyspark ML . from pyspark.ml.regression import LinearRegression lr = LinearRegression(labelCol=&#39;specific&#39;) . Fitting the linear regression model to the training data . lrModel = lr.fit(train_data) . Coefficients and Intercept of the linear regression model . print(&quot;Coefficients: {} Intercept: {}&quot;.format(lrModel.coefficients,lrModel.intercept)) . Coefficients: [-4936.461707001148,0.8007702471080539,-3994.683052325085,-7.5033201950338,0.42095493334994133,50994.51222529955,0.2531915644818595] Intercept: 7695214.561654471 Evaluating trained linear regression model on the test data . test_results = lrModel.evaluate(test_data) . Metrics of trained linear regression model on the test data (RMSE, MSE, R2) . print(&quot;RMSE: {}&quot;.format(test_results.rootMeanSquaredError)) print(&quot;MSE: {}&quot;.format(test_results.meanSquaredError)) print(&quot;R2: {}&quot;.format(test_results.r2)) . RMSE: 292695.0825058327 MSE: 85670411323.0962 R2: 0.7853651103073853 Looking at correlations with corr . from pyspark.sql.functions import corr . df.select(corr(&#39;specific&#39;,&#39;gdp&#39;)).show() . +-+ corr(specific, gdp)| +-+ 0.5141876884991972| +-+ Classification with Pyspark ML . from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier from pyspark.ml import Pipeline . DecisionTreeClassifier, RandomForestClassifier and GBTClassifier . dtc = DecisionTreeClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) rfc = RandomForestClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) gbt = GBTClassifier(labelCol=&#39;specific_classification&#39;,featuresCol=&#39;features&#39;) . Selecting features and binary target . final_data = output.select(&quot;features&quot;, &quot;specific_classification&quot;) train_data,test_data = final_data.randomSplit([0.7,0.3]) . Fitting the Classifiers to the Training Data . rfc_model = rfc.fit(train_data) gbt_model = gbt.fit(train_data) dtc_model = dtc.fit(train_data) . Classifier predictions on test data . dtc_predictions = dtc_model.transform(test_data) rfc_predictions = rfc_model.transform(test_data) gbt_predictions = gbt_model.transform(test_data) . Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator . from pyspark.ml.evaluation import MulticlassClassificationEvaluator . Classifier Accuracy . acc_evaluator = MulticlassClassificationEvaluator(labelCol=&quot;specific_classification&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;) . Classifier Accuracy Metrics . dtc_acc = acc_evaluator.evaluate(dtc_predictions) rfc_acc = acc_evaluator.evaluate(rfc_predictions) gbt_acc = acc_evaluator.evaluate(gbt_predictions) . print(&#39;-&#39;*80) print(&#39;Decision tree accuracy: {0:2.2f}%&#39;.format(dtc_acc*100)) print(&#39;-&#39;*80) print(&#39;Random forest ensemble accuracy: {0:2.2f}%&#39;.format(rfc_acc*100)) print(&#39;-&#39;*80) print(&#39;GBT accuracy: {0:2.2f}%&#39;.format(gbt_acc*100)) print(&#39;-&#39;*80) . -- Decision tree accuracy: 81.98% -- Random forest ensemble accuracy: 88.29% -- GBT accuracy: 81.08% -- Classification Correlation with Corr . df.select(corr(&#39;specific_classification&#39;,&#39;fdi&#39;)).show() . +-+ corr(specific_classification, fdi)| +-+ 0.307429849493392| +-+ df.select(corr(&#39;specific_classification&#39;,&#39;gdp&#39;)).show() . +-+ corr(specific_classification, gdp)| +-+ 0.492176921599151| +-+ This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/25/Linear-Regression-and-Random-Forest_GBT-Classification-with-Pyspark.html",
            "relUrl": "/2020/08/25/Linear-Regression-and-Random-Forest_GBT-Classification-with-Pyspark.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Window functions and Pivot Tables with Pyspark",
            "content": "Resilient Distributed Datasets . Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory. . from pyspark.sql import SparkSession from pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType from pyspark.sql.functions import * data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, DoubleType(), True) ,StructField(&quot;general&quot;, DoubleType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, FloatType(), True) ,StructField(&quot;fdi&quot;, FloatType(), True) ,StructField(&quot;rnr&quot;, DoubleType(), True) ,StructField(&quot;rr&quot;, FloatType(), True) ,StructField(&quot;i&quot;, FloatType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] final_struc = StructType(fields=data_schema) file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).schema(final_struc).option(&quot;header&quot;, True).load(file_location) #df.printSchema() df.show() . ++--++--+-+-+--+-+++-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-+--+-+++-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661.0| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443.0| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673.0| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131.0|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672.0| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000.0| 0.0| 0.0|0.3243243| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0| 0.0|0.3243243|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0| 0.0|0.3243243|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290.0|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286.0| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800.0| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525.0| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818.0| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718| 0.0|2823366|North China|1426600| ++--++--+-+-+--+-+++-+--+-+ only showing top 20 rows Using toPandas to look at the data . df.limit(10).toPandas() . _c0 province specific general year gdp fdi rnr rr i fr reg it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.300049 | 50661.0 | 0.0 | 0.0 | 0.000000 | 1128873.0 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.320068 | 43443.0 | 0.0 | 0.0 | 0.000000 | 1356287.0 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.959961 | 27673.0 | 0.0 | 0.0 | 0.000000 | 1518236.0 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.340088 | 26131.0 | NaN | NaN | NaN | 1646891.0 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.090088 | 31847.0 | 0.0 | 0.0 | 0.000000 | 1601508.0 | East China | 1499110 | . 5 5 | Anhui | 250898.0 | NaN | 2001 | 3246.709961 | 33672.0 | 0.0 | 0.0 | 0.000000 | 1672445.0 | East China | 2165189 | . 6 6 | Anhui | 434149.0 | 66529.0 | 2002 | 3519.719971 | 38375.0 | 0.0 | 0.0 | 0.000000 | 1677840.0 | East China | 2404936 | . 7 7 | Anhui | 619201.0 | 52108.0 | 2003 | 3923.110107 | 36720.0 | 0.0 | 0.0 | 0.000000 | 1896479.0 | East China | 2815820 | . 8 8 | Anhui | 898441.0 | 349699.0 | 2004 | 4759.299805 | 54669.0 | 0.0 | 0.0 | 0.000000 | NaN | East China | 3422176 | . 9 9 | Anhui | 898441.0 | NaN | 2005 | 5350.169922 | 69000.0 | 0.0 | 0.0 | 0.324324 | NaN | East China | 3874846 | . Renaming Columns . df = df.withColumnRenamed(&quot;reg&quot;,&quot;region&quot;) . df.limit(10).toPandas() . _c0 province specific general year gdp fdi rnr rr i fr region it . 0 0 | Anhui | 147002.0 | NaN | 1996 | 2093.300049 | 50661.0 | 0.0 | 0.0 | 0.000000 | 1128873.0 | East China | 631930 | . 1 1 | Anhui | 151981.0 | NaN | 1997 | 2347.320068 | 43443.0 | 0.0 | 0.0 | 0.000000 | 1356287.0 | East China | 657860 | . 2 2 | Anhui | 174930.0 | NaN | 1998 | 2542.959961 | 27673.0 | 0.0 | 0.0 | 0.000000 | 1518236.0 | East China | 889463 | . 3 3 | Anhui | 285324.0 | NaN | 1999 | 2712.340088 | 26131.0 | NaN | NaN | NaN | 1646891.0 | East China | 1227364 | . 4 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.090088 | 31847.0 | 0.0 | 0.0 | 0.000000 | 1601508.0 | East China | 1499110 | . 5 5 | Anhui | 250898.0 | NaN | 2001 | 3246.709961 | 33672.0 | 0.0 | 0.0 | 0.000000 | 1672445.0 | East China | 2165189 | . 6 6 | Anhui | 434149.0 | 66529.0 | 2002 | 3519.719971 | 38375.0 | 0.0 | 0.0 | 0.000000 | 1677840.0 | East China | 2404936 | . 7 7 | Anhui | 619201.0 | 52108.0 | 2003 | 3923.110107 | 36720.0 | 0.0 | 0.0 | 0.000000 | 1896479.0 | East China | 2815820 | . 8 8 | Anhui | 898441.0 | 349699.0 | 2004 | 4759.299805 | 54669.0 | 0.0 | 0.0 | 0.000000 | NaN | East China | 3422176 | . 9 9 | Anhui | 898441.0 | NaN | 2005 | 5350.169922 | 69000.0 | 0.0 | 0.0 | 0.324324 | NaN | East China | 3874846 | . # df = df.toDF(*[&#39;year&#39;, &#39;region&#39;, &#39;province&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;it&#39;, &#39;fr&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;_c0&#39;, &#39;specific_classification&#39;, &#39;provinceIndex&#39;, &#39;regionIndex&#39;]) . Selecting Columns of Interest . df = df.select(&#39;year&#39;,&#39;region&#39;,&#39;province&#39;,&#39;gdp&#39;, &#39;fdi&#39;) . df.sort(&quot;gdp&quot;).show() . +-++--++-+ year| region|province| gdp| fdi| +-++--++-+ 1996|Southwest China| Tibet| 64.98| 679.0| 1997|Southwest China| Tibet| 77.24| 63.0| 1998|Southwest China| Tibet| 91.5| 481.0| 1999|Southwest China| Tibet|105.98| 196.0| 2000|Southwest China| Tibet| 117.8| 2.0| 2001|Southwest China| Tibet|139.16| 106.0| 2002|Southwest China| Tibet|162.04| 293.0| 1996|Northwest China| Qinghai|184.17| 576.0| 2003|Southwest China| Tibet|185.09| 467.0| 1997|Northwest China| Qinghai|202.79| 247.0| 1996|Northwest China| Ningxia| 202.9| 2826.0| 2004|Southwest China| Tibet|220.34| 2699.0| 1998|Northwest China| Qinghai|220.92| 1010.0| 1997|Northwest China| Ningxia|224.59| 671.0| 1999|Northwest China| Qinghai|239.38| 459.0| 1998|Northwest China| Ningxia|245.44| 1856.0| 2005|Southwest China| Tibet| 248.8| 1151.0| 2000|Northwest China| Qinghai|263.68|11020.0| 1999|Northwest China| Ningxia|264.58| 5134.0| 2006|Southwest China| Tibet|290.76| 1522.0| +-++--++-+ only showing top 20 rows Sorting RDDs by Columns . from pyspark.sql import functions as F df.sort(F.desc(&quot;gdp&quot;)).show() . +-+-++--++ year| region| province| gdp| fdi| +-+-++--++ 2007|South Central China|Guangdong|31777.01|1712603.0| 2006|South Central China|Guangdong|26587.76|1451065.0| 2007| East China| Shandong|25776.91|1101159.0| 2005|South Central China|Guangdong|22557.37|1236400.0| 2006| East China| Shandong|21900.19|1000069.0| 2007| East China| Jiangsu|21742.05|1743140.0| 2004|South Central China|Guangdong|18864.62|1001158.0| 2007| East China| Zhejiang|18753.73|1036576.0| 2006| East China| Jiangsu|18598.69|1318339.0| 2005| East China| Shandong|18366.87| 897000.0| 2003|South Central China|Guangdong|15844.64| 782294.0| 2006| East China| Zhejiang|15718.47| 888935.0| 2004| East China| Shandong|15021.84| 870064.0| 2007|South Central China| Henan|15012.46| 306162.0| 2005| East China| Jiangsu| 15003.6|1213800.0| 2007| North China| Hebei|13607.32| 241621.0| 2002|South Central China|Guangdong|13502.42|1133400.0| 2005| East China| Zhejiang|13417.68| 772000.0| 2007| East China| Shanghai|12494.01| 792000.0| 2004| East China| Jiangsu|12442.87|1056365.0| +-+-++--++ only showing top 20 rows Casting Data Types . from pyspark.sql.types import IntegerType, StringType, DoubleType df = df.withColumn(&#39;gdp&#39;, F.col(&#39;gdp&#39;).cast(DoubleType())) . df = df.withColumn(&#39;province&#39;, F.col(&#39;province&#39;).cast(StringType())) . df.filter((df.gdp&gt;10000) &amp; (df.region==&#39;East China&#39;)).show() . +-+-+--+-++ year| region|province| gdp| fdi| +-+-+--+-++ 2003|East China| Jiangsu| 10606.849609375|1018960.0| 2004|East China| Jiangsu|12442.8701171875|1056365.0| 2005|East China| Jiangsu| 15003.599609375|1213800.0| 2006|East China| Jiangsu| 18598.689453125|1318339.0| 2007|East China| Jiangsu| 21742.05078125|1743140.0| 2002|East China|Shandong| 10275.5| 473404.0| 2003|East China|Shandong| 12078.150390625| 601617.0| 2004|East China|Shandong| 15021.83984375| 870064.0| 2005|East China|Shandong| 18366.869140625| 897000.0| 2006|East China|Shandong| 21900.189453125|1000069.0| 2007|East China|Shandong| 25776.91015625|1101159.0| 2006|East China|Shanghai| 10572.240234375| 710700.0| 2007|East China|Shanghai| 12494.009765625| 792000.0| 2004|East China|Zhejiang|11648.7001953125| 668128.0| 2005|East China|Zhejiang| 13417.6796875| 772000.0| 2006|East China|Zhejiang|15718.4697265625| 888935.0| 2007|East China|Zhejiang| 18753.73046875|1036576.0| +-+-+--+-++ Aggregating using groupBy, .agg and sum/max . from pyspark.sql import functions as F df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg(F.sum(&quot;gdp&quot;) ,F.max(&quot;gdp&quot;)).show() . +-++++ region| province| sum(gdp)| max(gdp)| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg(F.sum(&quot;gdp&quot;).alias(&quot;SumGDP&quot;),F.max(&quot;gdp&quot;).alias(&quot;MaxGDP&quot;)).show() . +-++++ region| province| GDP| MaxGDP| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.groupBy([&quot;region&quot;,&quot;province&quot;]).agg( F.sum(&quot;gdp&quot;).alias(&quot;SumGDP&quot;), F.max(&quot;gdp&quot;).alias(&quot;MaxGDP&quot;) ).show() . +-++++ region| province| SumGDP| MaxGDP| +-++++ South Central China| Hunan| 57190.69970703125| 9439.599609375| North China| Tianjin|30343.979858398438| 5252.759765625| Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375| North China| Beijing|56081.439208984375| 9846.8095703125| South Central China|Guangdong| 184305.376953125| 31777.009765625| South Central China| Henan| 86507.60034179688| 15012.4599609375| East China| Jiangsu|129142.15966796875| 21742.05078125| Northwest China| Gansu| 16773.99005126953| 2703.97998046875| Southwest China| Guizhou|17064.130249023438| 2884.110107421875| Southwest China| Sichuan|64533.479736328125| 10562.3896484375| South Central China| Hainan| 8240.570068359375|1254.1700439453125| East China| Shandong| 147888.0283203125| 25776.91015625| Southwest China|Chongqing|29732.549926757812| 4676.1298828125| Northwest China| Shaanxi|31896.409790039062| 5757.2900390625| East China| Shanghai| 77189.4501953125| 12494.009765625| Southwest China| Tibet| 2045.120002746582|341.42999267578125| North China| Hebei| 83241.8994140625| 13607.3203125| Northeast China| Jilin|27298.250366210938| 4275.1201171875| East China| Zhejiang|109657.81884765625| 18753.73046875| North China| Shanxi| 33806.52990722656| 6024.4501953125| +-++++ only showing top 20 rows df.limit(10).toPandas() . year region province gdp fdi . 0 1996 | East China | Anhui | 2093.300049 | 50661.0 | . 1 1997 | East China | Anhui | 2347.320068 | 43443.0 | . 2 1998 | East China | Anhui | 2542.959961 | 27673.0 | . 3 1999 | East China | Anhui | 2712.340088 | 26131.0 | . 4 2000 | East China | Anhui | 2902.090088 | 31847.0 | . 5 2001 | East China | Anhui | 3246.709961 | 33672.0 | . 6 2002 | East China | Anhui | 3519.719971 | 38375.0 | . 7 2003 | East China | Anhui | 3923.110107 | 36720.0 | . 8 2004 | East China | Anhui | 4759.299805 | 54669.0 | . 9 2005 | East China | Anhui | 5350.169922 | 69000.0 | . Exponentials using exp . df = df.withColumn(&quot;Exp_GDP&quot;, F.exp(&quot;gdp&quot;)) df.show() . +-+--+--+--+--+--+ year| region|province| gdp| fdi| Exp_GDP| +-+--+--+--+--+--+ 1996| East China| Anhui|2093.300048828125| 50661.0|Infinity| 1997| East China| Anhui|2347.320068359375| 43443.0|Infinity| 1998| East China| Anhui| 2542.9599609375| 27673.0|Infinity| 1999| East China| Anhui|2712.340087890625| 26131.0|Infinity| 2000| East China| Anhui|2902.090087890625| 31847.0|Infinity| 2001| East China| Anhui| 3246.7099609375| 33672.0|Infinity| 2002| East China| Anhui|3519.719970703125| 38375.0|Infinity| 2003| East China| Anhui|3923.110107421875| 36720.0|Infinity| 2004| East China| Anhui| 4759.2998046875| 54669.0|Infinity| 2005| East China| Anhui| 5350.169921875| 69000.0|Infinity| 2006| East China| Anhui| 6112.5|139354.0|Infinity| 2007| East China| Anhui| 7360.919921875|299892.0|Infinity| 1996|North China| Beijing|1789.199951171875|155290.0|Infinity| 1997|North China| Beijing|2077.090087890625|159286.0|Infinity| 1998|North China| Beijing|2377.179931640625|216800.0|Infinity| 1999|North China| Beijing|2678.820068359375|197525.0|Infinity| 2000|North China| Beijing|3161.659912109375|168368.0|Infinity| 2001|North China| Beijing| 3707.9599609375|176818.0|Infinity| 2002|North China| Beijing| 4315.0|172464.0|Infinity| 2003|North China| Beijing| 5007.2099609375|219126.0|Infinity| +-+--+--+--+--+--+ only showing top 20 rows Window functions . . Note: Window functions . # Window functions from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(F.desc(&#39;gdp&#39;)) df.withColumn(&quot;rank&quot;,F.rank().over(windowSpec)).show() . +-+-++--++--+-+ year| region| province| gdp| fdi| Exp_GDP|rank| +-+-++--++--+-+ 2007|South Central China|Guangdong| 31777.009765625|1712603.0|Infinity| 1| 2006|South Central China|Guangdong| 26587.759765625|1451065.0|Infinity| 2| 2005|South Central China|Guangdong| 22557.369140625|1236400.0|Infinity| 3| 2004|South Central China|Guangdong| 18864.619140625|1001158.0|Infinity| 4| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity| 5| 2002|South Central China|Guangdong| 13502.419921875|1133400.0|Infinity| 6| 2001|South Central China|Guangdong| 12039.25|1193203.0|Infinity| 7| 2000|South Central China|Guangdong| 10741.25|1128091.0|Infinity| 8| 1999|South Central China|Guangdong| 9250.6796875|1165750.0|Infinity| 9| 1998|South Central China|Guangdong| 8530.8798828125|1201994.0|Infinity| 10| 1997|South Central China|Guangdong| 7774.52978515625|1171083.0|Infinity| 11| 1996|South Central China|Guangdong| 6834.97021484375|1162362.0|Infinity| 12| 2007|South Central China| Hunan| 9439.599609375| 327051.0|Infinity| 1| 2006|South Central China| Hunan| 7688.669921875| 259335.0|Infinity| 2| 2005|South Central China| Hunan| 6596.10009765625| 207200.0|Infinity| 3| 2004|South Central China| Hunan| 5641.93994140625| 141800.0|Infinity| 4| 2003|South Central China| Hunan| 4659.990234375| 101835.0|Infinity| 5| 2002|South Central China| Hunan| 4151.5400390625| 90022.0|Infinity| 6| 2001|South Central China| Hunan| 3831.89990234375| 81011.0|Infinity| 7| 2000|South Central China| Hunan|3551.489990234375| 67833.0|Infinity| 8| +-+-++--++--+-+ only showing top 20 rows from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;) . Lagging Variables . dfWithLag = df.withColumn(&quot;lag_7&quot;,F.lag(&quot;gdp&quot;, 7).over(windowSpec)) . df.filter(df.year&gt;&#39;2000&#39;).show() . +-++++--+--+ year| region| province| gdp| fdi| Exp_GDP| +-++++--+--+ 2001| East China| Anhui| 3246.7099609375| 33672.0|Infinity| 2002| East China| Anhui| 3519.719970703125| 38375.0|Infinity| 2003| East China| Anhui| 3923.110107421875| 36720.0|Infinity| 2004| East China| Anhui| 4759.2998046875| 54669.0|Infinity| 2005| East China| Anhui| 5350.169921875| 69000.0|Infinity| 2006| East China| Anhui| 6112.5|139354.0|Infinity| 2007| East China| Anhui| 7360.919921875|299892.0|Infinity| 2001| North China| Beijing| 3707.9599609375|176818.0|Infinity| 2002| North China| Beijing| 4315.0|172464.0|Infinity| 2003| North China| Beijing| 5007.2099609375|219126.0|Infinity| 2004| North China| Beijing| 6033.2099609375|308354.0|Infinity| 2005| North China| Beijing| 6969.52001953125|352638.0|Infinity| 2006| North China| Beijing| 8117.77978515625|455191.0|Infinity| 2007| North China| Beijing| 9846.8095703125|506572.0|Infinity| 2001|Southwest China|Chongqing|1976.8599853515625| 25649.0|Infinity| 2002|Southwest China|Chongqing| 2232.860107421875| 19576.0|Infinity| 2003|Southwest China|Chongqing| 2555.719970703125| 26083.0|Infinity| 2004|Southwest China|Chongqing| 3034.580078125| 40508.0|Infinity| 2005|Southwest China|Chongqing| 3467.719970703125| 51600.0|Infinity| 2006|Southwest China|Chongqing| 3907.22998046875| 69595.0|Infinity| +-++++--+--+ only showing top 20 rows Looking at windows within the data . from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;).rowsBetween(-6,0) . dfWithRoll = df.withColumn(&quot;roll_7_confirmed&quot;,F.mean(&quot;gdp&quot;).over(windowSpec)) . dfWithRoll.filter(dfWithLag.year&gt;&#39;2001&#39;).show() . +-+-++++--++ year| region| province| gdp| fdi| Exp_GDP| roll_7_confirmed| +-+-++++--++ 2002|South Central China|Guangdong| 13502.419921875|1133400.0| Infinity| 9810.56849888393| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0| Infinity|11097.664132254464| 2004|South Central China|Guangdong| 18864.619140625|1001158.0| Infinity|12681.962611607143| 2005|South Central China|Guangdong| 22557.369140625|1236400.0| Infinity|14685.746791294643| 2006|South Central China|Guangdong| 26587.759765625|1451065.0| Infinity|17162.472516741072| 2007|South Central China|Guangdong| 31777.009765625|1712603.0| Infinity| 20167.5810546875| 2002|South Central China| Hunan| 4151.5400390625| 90022.0| Infinity|3309.1999860491073| 2003|South Central China| Hunan| 4659.990234375| 101835.0| Infinity| 3612.037179129464| 2004|South Central China| Hunan| 5641.93994140625| 141800.0| Infinity|4010.9900251116073| 2005|South Central China| Hunan| 6596.10009765625| 207200.0| Infinity| 4521.07146344866| 2006|South Central China| Hunan| 7688.669921875| 259335.0| Infinity| 5160.232875279018| 2007|South Central China| Hunan| 9439.599609375| 327051.0| Infinity| 6001.391392299107| 2002| North China| Shanxi| 2324.800048828125| 21164.0| Infinity|1749.4771379743304| 2003| North China| Shanxi| 2855.22998046875| 21361.0| Infinity| 1972.779994419643| 2004| North China| Shanxi| 3571.3701171875| 62184.0| Infinity| 2272.118582589286| 2005| North China| Shanxi| 4230.52978515625| 27516.0| Infinity| 2646.325701032366| 2006| North China| Shanxi| 4878.60986328125| 47199.0| Infinity|3105.1128278459823| 2007| North China| Shanxi| 6024.4501953125| 134283.0| Infinity| 3702.074288504464| 2002| Southwest China| Tibet| 162.0399932861328| 293.0|2.360885537826244E70|108.38571493966239| 2003| Southwest China| Tibet|185.08999633789062| 467.0|2.418600091901801E80|125.54428536551339| +-+-++++--++ only showing top 20 rows from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;year&#39;).rowsBetween(Window.unboundedPreceding,Window.currentRow) . dfWithRoll = df.withColumn(&quot;cumulative_gdp&quot;,F.sum(&quot;gdp&quot;).over(windowSpec)) . dfWithRoll.filter(dfWithLag.year&gt;&#39;1999&#39;).show() . +-+-++--++--++ year| region| province| gdp| fdi| Exp_GDP| cumulative_gdp| +-+-++--++--++ 2000|South Central China|Guangdong| 10741.25|1128091.0|Infinity| 43132.3095703125| 2001|South Central China|Guangdong| 12039.25|1193203.0|Infinity| 55171.5595703125| 2002|South Central China|Guangdong| 13502.419921875|1133400.0|Infinity| 68673.9794921875| 2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity| 84518.619140625| 2004|South Central China|Guangdong| 18864.619140625|1001158.0|Infinity| 103383.23828125| 2005|South Central China|Guangdong| 22557.369140625|1236400.0|Infinity| 125940.607421875| 2006|South Central China|Guangdong| 26587.759765625|1451065.0|Infinity| 152528.3671875| 2007|South Central China|Guangdong| 31777.009765625|1712603.0|Infinity| 184305.376953125| 2000|South Central China| Hunan|3551.489990234375| 67833.0|Infinity| 15180.9599609375| 2001|South Central China| Hunan| 3831.89990234375| 81011.0|Infinity| 19012.85986328125| 2002|South Central China| Hunan| 4151.5400390625| 90022.0|Infinity| 23164.39990234375| 2003|South Central China| Hunan| 4659.990234375| 101835.0|Infinity| 27824.39013671875| 2004|South Central China| Hunan| 5641.93994140625| 141800.0|Infinity| 33466.330078125| 2005|South Central China| Hunan| 6596.10009765625| 207200.0|Infinity| 40062.43017578125| 2006|South Central China| Hunan| 7688.669921875| 259335.0|Infinity| 47751.10009765625| 2007|South Central China| Hunan| 9439.599609375| 327051.0|Infinity| 57190.69970703125| 2000| North China| Shanxi|1845.719970703125| 22472.0|Infinity|7892.0098876953125| 2001| North China| Shanxi|2029.530029296875| 23393.0|Infinity| 9921.539916992188| 2002| North China| Shanxi|2324.800048828125| 21164.0|Infinity|12246.339965820312| 2003| North China| Shanxi| 2855.22998046875| 21361.0|Infinity|15101.569946289062| +-+-++--++--++ only showing top 20 rows Pivot Dataframes . . Note: Pivot Dataframes . pivoted_df = df.groupBy(&#39;year&#39;).pivot(&#39;province&#39;) .agg(F.sum(&#39;gdp&#39;).alias(&#39;gdp&#39;) , F.sum(&#39;fdi&#39;).alias(&#39;fdi&#39;)) pivoted_df.limit(10).toPandas() . year Anhui_gdp Anhui_fdi Beijing_gdp Beijing_fdi Chongqing_gdp Chongqing_fdi Fujian_gdp Fujian_fdi Gansu_gdp Gansu_fdi Guangdong_gdp Guangdong_fdi Guangxi_gdp Guangxi_fdi Guizhou_gdp Guizhou_fdi Hainan_gdp Hainan_fdi Hebei_gdp Hebei_fdi Heilongjiang_gdp Heilongjiang_fdi Henan_gdp Henan_fdi Hubei_gdp Hubei_fdi Hunan_gdp Hunan_fdi Jiangsu_gdp Jiangsu_fdi Jiangxi_gdp Jiangxi_fdi Jilin_gdp Jilin_fdi Liaoning_gdp Liaoning_fdi Ningxia_gdp Ningxia_fdi Qinghai_gdp Qinghai_fdi Shaanxi_gdp Shaanxi_fdi Shandong_gdp Shandong_fdi Shanghai_gdp Shanghai_fdi Shanxi_gdp Shanxi_fdi Sichuan_gdp Sichuan_fdi Tianjin_gdp Tianjin_fdi Tibet_gdp Tibet_fdi Xinjiang_gdp Xinjiang_fdi Yunnan_gdp Yunnan_fdi Zhejiang_gdp Zhejiang_fdi . 0 2003 | 3923.110107 | 36720.0 | 5007.209961 | 219126.0 | 2555.719971 | 26083.0 | 4983.669922 | 259903.0 | 1399.829956 | 2342.0 | 15844.639648 | 782294.0 | 2821.110107 | 41856.0 | 1426.339966 | 4521.0 | 713.960022 | 42125.0 | 6921.290039 | 96405.0 | 4057.399902 | 32180.0 | 6867.700195 | 53903.0 | 4757.450195 | 156886.0 | 4659.990234 | 101835.0 | 10606.849609 | 1018960.0 | 2450.479980 | 108197.0 | 2348.540039 | 24468.0 | 5458.220215 | 341168.0 | 445.359985 | 1743.0 | 390.200012 | 2522.0 | 2587.719971 | 33190.0 | 12078.150391 | 601617.0 | 6694.229980 | 546849.0 | 2855.229980 | 21361.0 | 5333.089844 | 41231.0 | 2578.030029 | 153473.0 | 185.089996 | 467.0 | 1886.349976 | 1534.0 | 2556.020020 | 8384.0 | 9705.019531 | 498055.0 | . 1 2007 | 7360.919922 | 299892.0 | 9846.809570 | 506572.0 | 4676.129883 | 108534.0 | 9248.530273 | 406058.0 | 2703.979980 | 11802.0 | 31777.009766 | 1712603.0 | 5823.410156 | 68396.0 | 2884.110107 | 12651.0 | 1254.170044 | 112001.0 | 13607.320312 | 241621.0 | 7104.000000 | 208508.0 | 15012.459961 | 306162.0 | 9333.400391 | 276622.0 | 9439.599609 | 327051.0 | 21742.050781 | 1743140.0 | 4820.529785 | 280657.0 | 4275.120117 | 76064.0 | 9304.519531 | 598554.0 | 919.109985 | 5047.0 | 797.349976 | 31000.0 | 5757.290039 | 119516.0 | 25776.910156 | 1101159.0 | 12494.009766 | 792000.0 | 6024.450195 | 134283.0 | 10562.389648 | 149322.0 | 5252.759766 | 527776.0 | 341.429993 | 2418.0 | 3523.159912 | 12484.0 | 4772.520020 | 39453.0 | 18753.730469 | 1036576.0 | . 2 2006 | 6112.500000 | 139354.0 | 8117.779785 | 455191.0 | 3907.229980 | 69595.0 | 7583.850098 | 322047.0 | 2277.350098 | 2954.0 | 26587.759766 | 1451065.0 | 4746.160156 | 44740.0 | 2338.979980 | 9384.0 | 1065.670044 | 74878.0 | 11467.599609 | 201434.0 | 6211.799805 | 170801.0 | 12362.790039 | 184526.0 | 7617.470215 | 244853.0 | 7688.669922 | 259335.0 | 18598.689453 | 1318339.0 | 4056.760010 | 242000.0 | 3620.270020 | 66100.0 | 8047.259766 | 359000.0 | 725.900024 | 3718.0 | 648.500000 | 27500.0 | 4743.609863 | 92489.0 | 21900.189453 | 1000069.0 | 10572.240234 | 710700.0 | 4878.609863 | 47199.0 | 8690.240234 | 120819.0 | 4462.740234 | 413077.0 | 290.760010 | 1522.0 | 3045.260010 | 10366.0 | 3988.139893 | 30234.0 | 15718.469727 | 888935.0 | . 3 1997 | 2347.320068 | 43443.0 | 2077.090088 | 159286.0 | 1509.750000 | 38675.0 | 2870.899902 | 419666.0 | 793.570007 | 4144.0 | 7774.529785 | 1171083.0 | 1817.250000 | 87986.0 | 805.789978 | 4977.0 | 411.160004 | 70554.0 | 3953.780029 | 110064.0 | 2667.500000 | 73485.0 | 4041.090088 | 69204.0 | 2856.469971 | 79019.0 | 2849.270020 | 91702.0 | 6004.209961 | 507208.0 | 1409.739990 | 30068.0 | 1346.790039 | 45155.0 | 3157.689941 | 167142.0 | 224.589996 | 671.0 | 202.789993 | 247.0 | 1363.599976 | 62816.0 | 6537.069824 | 249294.0 | 3438.790039 | 422536.0 | 1476.000000 | 26592.0 | 3241.469971 | 24846.0 | 1264.630005 | 251135.0 | 77.239998 | 63.0 | 1039.849976 | 2472.0 | 1676.170044 | 16566.0 | 4686.109863 | 150345.0 | . 4 2004 | 4759.299805 | 54669.0 | 6033.209961 | 308354.0 | 3034.580078 | 40508.0 | 5763.350098 | 474801.0 | 1688.489990 | 3539.0 | 18864.619141 | 1001158.0 | 3433.500000 | 29579.0 | 1677.800049 | 6533.0 | 819.659973 | 64343.0 | 8477.629883 | 162341.0 | 4750.600098 | 123639.0 | 8553.790039 | 87367.0 | 5633.240234 | 207126.0 | 5641.939941 | 141800.0 | 12442.870117 | 1056365.0 | 2807.409912 | 161202.0 | 2662.080078 | 19059.0 | 6002.540039 | 282410.0 | 537.109985 | 6689.0 | 466.100006 | 22500.0 | 3175.580078 | 52664.0 | 15021.839844 | 870064.0 | 8072.830078 | 654100.0 | 3571.370117 | 62184.0 | 6379.629883 | 70129.0 | 3110.969971 | 247243.0 | 220.339996 | 2699.0 | 2209.090088 | 4586.0 | 3081.909912 | 14200.0 | 11648.700195 | 668128.0 | . 5 1996 | 2093.300049 | 50661.0 | 1789.199951 | 155290.0 | 1315.119995 | 21878.0 | 2484.250000 | 407876.0 | 722.520020 | 9002.0 | 6834.970215 | 1162362.0 | 1697.900024 | 66618.0 | 723.179993 | 3138.0 | 389.679993 | 78960.0 | 3452.969971 | 123652.0 | 2370.500000 | 54841.0 | 3634.689941 | 52566.0 | 2499.770020 | 68878.0 | 2540.129883 | 70344.0 | 5155.250000 | 478058.0 | 1169.729980 | 28818.0 | 1137.229980 | 39876.0 | 2793.370117 | 140405.0 | 202.899994 | 2826.0 | 184.169998 | 576.0 | 1215.839966 | 33008.0 | 5883.799805 | 259041.0 | 2957.550049 | 471578.0 | 1292.109985 | 13802.0 | 2871.649902 | 22519.0 | 1121.930054 | 200587.0 | 64.980003 | 679.0 | 900.929993 | 6639.0 | 1517.689941 | 18000.0 | 4188.529785 | 152021.0 | . 6 1998 | 2542.959961 | 27673.0 | 2377.179932 | 216800.0 | 1602.380005 | 43107.0 | 3159.909912 | 421211.0 | 887.669983 | 3864.0 | 8530.879883 | 1201994.0 | 1911.300049 | 88613.0 | 858.390015 | 4535.0 | 442.130005 | 71715.0 | 4256.009766 | 142868.0 | 2774.399902 | 52639.0 | 4308.240234 | 61654.0 | 3114.020020 | 97294.0 | 3025.530029 | 81816.0 | 6680.339844 | 543511.0 | 1605.770020 | 47768.0 | 1464.339966 | 40227.0 | 3582.459961 | 220470.0 | 245.440002 | 1856.0 | 220.919998 | 1010.0 | 1458.400024 | 30010.0 | 7021.350098 | 220274.0 | 3801.090088 | 360150.0 | 1611.079956 | 24451.0 | 3474.090088 | 37248.0 | 1374.599976 | 211361.0 | 91.500000 | 481.0 | 1106.949951 | 2167.0 | 1831.329956 | 14568.0 | 5052.620117 | 131802.0 | . 7 2001 | 3246.709961 | 33672.0 | 3707.959961 | 176818.0 | 1976.859985 | 25649.0 | 4072.850098 | 391804.0 | 1125.369995 | 7439.0 | 12039.250000 | 1193203.0 | 2279.340088 | 38416.0 | 1133.270020 | 2829.0 | 579.169983 | 46691.0 | 5516.759766 | 66989.0 | 3390.100098 | 34114.0 | 5533.009766 | 45729.0 | 3880.530029 | 118860.0 | 3831.899902 | 81011.0 | 8553.690430 | 642550.0 | 2003.069946 | 22724.0 | 1951.510010 | 33701.0 | 4669.060059 | 204446.0 | 337.440002 | 1680.0 | 300.130005 | 3649.0 | 2010.619995 | 35174.0 | 9195.040039 | 352093.0 | 5210.120117 | 429159.0 | 2029.530029 | 23393.0 | 4293.490234 | 58188.0 | 1919.089966 | 213348.0 | 139.160004 | 106.0 | 1491.599976 | 2035.0 | 2138.310059 | 6457.0 | 6898.339844 | 221162.0 | . 8 2005 | 5350.169922 | 69000.0 | 6969.520020 | 352638.0 | 3467.719971 | 51600.0 | 6554.689941 | 260800.0 | 1933.979980 | 2000.0 | 22557.369141 | 1236400.0 | 3984.100098 | 37866.0 | 2005.420044 | 10768.0 | 918.750000 | 68400.0 | 10012.110352 | 191000.0 | 5513.700195 | 145000.0 | 10587.419922 | 123000.0 | 6590.189941 | 218500.0 | 6596.100098 | 207200.0 | 15003.599609 | 1213800.0 | 3456.699951 | 205238.0 | 3122.010010 | 45266.0 | 6672.000000 | 540679.0 | 612.609985 | 14100.0 | 543.320007 | 26600.0 | 3933.719971 | 62800.0 | 18366.869141 | 897000.0 | 9247.660156 | 685000.0 | 4230.529785 | 27516.0 | 7385.100098 | 88686.0 | 3905.639893 | 332885.0 | 248.800003 | 1151.0 | 2604.189941 | 4700.0 | 3462.729980 | 17352.0 | 13417.679688 | 772000.0 | . 9 2000 | 2902.090088 | 31847.0 | 3161.659912 | 168368.0 | 1791.000000 | 24436.0 | 3764.540039 | 343191.0 | 1052.880005 | 6235.0 | 10741.250000 | 1128091.0 | 2080.040039 | 52466.0 | 1029.920044 | 2501.0 | 526.820007 | 43080.0 | 5043.959961 | 67923.0 | 3151.399902 | 30086.0 | 5052.990234 | 56403.0 | 3545.389893 | 94368.0 | 3551.489990 | 67833.0 | 7697.819824 | 607756.0 | 1853.650024 | 32080.0 | 1672.959961 | 30120.0 | 4171.689941 | 106173.0 | 295.019989 | 1741.0 | 263.679993 | 11020.0 | 1804.000000 | 28842.0 | 8337.469727 | 297119.0 | 4771.169922 | 316014.0 | 1845.719971 | 22472.0 | 3928.199951 | 43694.0 | 1701.880005 | 116601.0 | 117.800003 | 2.0 | 1363.560059 | 1911.0 | 2011.189941 | 12812.0 | 6141.029785 | 161266.0 | . pivoted_df.columns . Out[55]: [&#39;year&#39;, &#39;Anhui_gdp&#39;, &#39;Anhui_fdi&#39;, &#39;Beijing_gdp&#39;, &#39;Beijing_fdi&#39;, &#39;Chongqing_gdp&#39;, &#39;Chongqing_fdi&#39;, &#39;Fujian_gdp&#39;, &#39;Fujian_fdi&#39;, &#39;Gansu_gdp&#39;, &#39;Gansu_fdi&#39;, &#39;Guangdong_gdp&#39;, &#39;Guangdong_fdi&#39;, &#39;Guangxi_gdp&#39;, &#39;Guangxi_fdi&#39;, &#39;Guizhou_gdp&#39;, &#39;Guizhou_fdi&#39;, &#39;Hainan_gdp&#39;, &#39;Hainan_fdi&#39;, &#39;Hebei_gdp&#39;, &#39;Hebei_fdi&#39;, &#39;Heilongjiang_gdp&#39;, &#39;Heilongjiang_fdi&#39;, &#39;Henan_gdp&#39;, &#39;Henan_fdi&#39;, &#39;Hubei_gdp&#39;, &#39;Hubei_fdi&#39;, &#39;Hunan_gdp&#39;, &#39;Hunan_fdi&#39;, &#39;Jiangsu_gdp&#39;, &#39;Jiangsu_fdi&#39;, &#39;Jiangxi_gdp&#39;, &#39;Jiangxi_fdi&#39;, &#39;Jilin_gdp&#39;, &#39;Jilin_fdi&#39;, &#39;Liaoning_gdp&#39;, &#39;Liaoning_fdi&#39;, &#39;Ningxia_gdp&#39;, &#39;Ningxia_fdi&#39;, &#39;Qinghai_gdp&#39;, &#39;Qinghai_fdi&#39;, &#39;Shaanxi_gdp&#39;, &#39;Shaanxi_fdi&#39;, &#39;Shandong_gdp&#39;, &#39;Shandong_fdi&#39;, &#39;Shanghai_gdp&#39;, &#39;Shanghai_fdi&#39;, &#39;Shanxi_gdp&#39;, &#39;Shanxi_fdi&#39;, &#39;Sichuan_gdp&#39;, &#39;Sichuan_fdi&#39;, &#39;Tianjin_gdp&#39;, &#39;Tianjin_fdi&#39;, &#39;Tibet_gdp&#39;, &#39;Tibet_fdi&#39;, &#39;Xinjiang_gdp&#39;, &#39;Xinjiang_fdi&#39;, &#39;Yunnan_gdp&#39;, &#39;Yunnan_fdi&#39;, &#39;Zhejiang_gdp&#39;, &#39;Zhejiang_fdi&#39;] newColnames = [x.replace(&quot;-&quot;,&quot;_&quot;) for x in pivoted_df.columns] . pivoted_df = pivoted_df.toDF(*newColnames) . expression = &quot;&quot; cnt=0 for column in pivoted_df.columns: if column!=&#39;year&#39;: cnt +=1 expression += f&quot;&#39;{column}&#39; , {column},&quot; expression = f&quot;stack({cnt}, {expression[:-1]}) as (Type,Value)&quot; . Unpivoting RDDs . unpivoted_df = pivoted_df.select(&#39;year&#39;,F.expr(expression)) unpivoted_df.show() . +-+-++ year| Type| Value| +-+-++ 2003| Anhui_gdp| 3923.110107421875| 2003| Anhui_fdi| 36720.0| 2003| Beijing_gdp| 5007.2099609375| 2003| Beijing_fdi| 219126.0| 2003|Chongqing_gdp| 2555.719970703125| 2003|Chongqing_fdi| 26083.0| 2003| Fujian_gdp| 4983.669921875| 2003| Fujian_fdi| 259903.0| 2003| Gansu_gdp|1399.8299560546875| 2003| Gansu_fdi| 2342.0| 2003|Guangdong_gdp| 15844.6396484375| 2003|Guangdong_fdi| 782294.0| 2003| Guangxi_gdp| 2821.110107421875| 2003| Guangxi_fdi| 41856.0| 2003| Guizhou_gdp|1426.3399658203125| 2003| Guizhou_fdi| 4521.0| 2003| Hainan_gdp| 713.9600219726562| 2003| Hainan_fdi| 42125.0| 2003| Hebei_gdp| 6921.2900390625| 2003| Hebei_fdi| 96405.0| +-+-++ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/22/Window-functions-and-Pivot-Tables-with-Pyspark.html",
            "relUrl": "/2020/08/22/Window-functions-and-Pivot-Tables-with-Pyspark.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "RDDs and Schemas and Data Types with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) df.columns . Out[64]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;] df.describe() . Out[65]: DataFrame[summary: string, _c0: string, province: string, specific: string, general: string, year: string, gdp: string, fdi: string, rnr: string, rr: string, i: string, fr: string, reg: string, it: string] Setting Data Schema and Data Types . from pyspark.sql.types import StructField,StringType,IntegerType,StructType . data_schema = [ StructField(&quot;_c0&quot;, IntegerType(), True) ,StructField(&quot;province&quot;, StringType(), True) ,StructField(&quot;specific&quot;, IntegerType(), True) ,StructField(&quot;general&quot;, IntegerType(), True) ,StructField(&quot;year&quot;, IntegerType(), True) ,StructField(&quot;gdp&quot;, IntegerType(), True) ,StructField(&quot;fdi&quot;, IntegerType(), True) ,StructField(&quot;rnr&quot;, IntegerType(), True) ,StructField(&quot;rr&quot;, IntegerType(), True) ,StructField(&quot;i&quot;, IntegerType(), True) ,StructField(&quot;fr&quot;, IntegerType(), True) ,StructField(&quot;reg&quot;, StringType(), True) ,StructField(&quot;it&quot;, IntegerType(), True) ] . final_struc = StructType(fields=data_schema) . Applying the Data Schema/Data Types while reading in a CSV . df = spark.read.format(&quot;CSV&quot;).schema(final_struc).load(file_location) . df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) df.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows df[&#39;fr&#39;] . Out[72]: Column&lt;b&#39;fr&#39;&gt; type(df[&#39;fr&#39;]) . Out[73]: pyspark.sql.column.Column df.select(&#39;fr&#39;) . Out[74]: DataFrame[fr: int] type(df.select(&#39;fr&#39;)) . Out[75]: pyspark.sql.dataframe.DataFrame df.select(&#39;fr&#39;).show() . +-+ fr| +-+ null| 1128873| 1356287| 1518236| 1646891| 1601508| 1672445| 1677840| 1896479| null| null| 3434548| 4468640| 634562| 634562| 938788| null| 1667114| 2093925| 2511249| +-+ only showing top 20 rows df.head(2) . Out[77]: [Row(_c0=None, province=&#39;province&#39;, specific=None, general=None, year=None, gdp=None, fdi=None, rnr=None, rr=None, i=None, fr=None, reg=&#39;reg&#39;, it=None), Row(_c0=0, province=&#39;Anhui&#39;, specific=None, general=None, year=1996, gdp=None, fdi=50661, rnr=None, rr=None, i=None, fr=1128873, reg=&#39;East China&#39;, it=631930)] df.select([&#39;reg&#39;,&#39;fr&#39;]) . Out[78]: DataFrame[reg: string, fr: int] Using select with RDDs . df.select([&#39;reg&#39;,&#39;fr&#39;]).show() . +--+-+ reg| fr| +--+-+ reg| null| East China|1128873| East China|1356287| East China|1518236| East China|1646891| East China|1601508| East China|1672445| East China|1677840| East China|1896479| East China| null| East China| null| East China|3434548| East China|4468640| North China| 634562| North China| 634562| North China| 938788| North China| null| North China|1667114| North China|2093925| North China|2511249| +--+-+ only showing top 20 rows df.withColumn(&#39;fiscal_revenue&#39;,df[&#39;fr&#39;]).show() . +-+--+--+-+-+-++-+-+-+-+--+-+--+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-+--+ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1128873| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 1356287| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 1518236| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 1646891| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 1601508| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 1672445| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 1677840| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 1896479| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 3434548| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 4468640| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 634562| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 634562| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 938788| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 1667114| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 2093925| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 2511249| +-+--+--+-+-+-++-+-+-+-+--+-+--+ only showing top 20 rows df.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows Renaming Columns using withColumnRenamed . df.withColumnRenamed(&#39;fr&#39;,&#39;new_fiscal_revenue&#39;).show() . +-+--+--+-+-+-++-+-+-++--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i|new_fiscal_revenue| reg| it| +-+--+--+-+-+-++-+-+-++--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null| 1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null| 1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null| 1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null| 1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null| 1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null| 1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null| 1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null| 1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null| 3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null| 4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null| 1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null| 2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null| 2511249|North China|1078754| +-+--+--+-+-+-++-+-+-++--+-+ only showing top 20 rows New Columns by Transforming extant Columns using withColumn . df.withColumn(&#39;double_fiscal_revenue&#39;,df[&#39;fr&#39;]*2).show() . +-+--+--+-+-+-++-+-+-+-+--+-++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|double_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-++ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 2257746| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2712574| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3036472| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 3293782| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 3203016| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 3344890| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 3355680| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 3792958| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 6869096| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 8937280| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 1269124| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 1269124| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 1877576| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 3334228| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 4187850| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 5022498| +-+--+--+-+-+-++-+-+-+-+--+-++ only showing top 20 rows df.withColumn(&#39;add_fiscal_revenue&#39;,df[&#39;fr&#39;]+1).show() . +-+--+--+-+-+-++-+-+-+-+--+-++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|add_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-++ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1128874| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 1356288| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 1518237| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 1646892| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 1601509| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 1672446| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 1677841| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 1896480| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 3434549| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 4468641| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 634563| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 634563| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 938789| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 1667115| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 2093926| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 2511250| +-+--+--+-+-+-++-+-+-+-+--+-++ only showing top 20 rows df.withColumn(&#39;half_fiscal_revenue&#39;,df[&#39;fr&#39;]/2).show() . +-+--+--+-+-+-++-+-+-+-+--+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it|half_fiscal_revenue| +-+--+--+-+-+-++-+-+-+-+--+-+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 564436.5| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 678143.5| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 759118.0| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 823445.5| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 800754.0| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 836222.5| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 838920.0| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 948239.5| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| null| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| null| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 1717274.0| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 2234320.0| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 317281.0| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 317281.0| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 469394.0| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| null| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 833557.0| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 1046962.5| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| 1255624.5| +-+--+--+-+-+-++-+-+-+-+--+-+-+ only showing top 20 rows df.withColumn(&#39;half_fr&#39;,df[&#39;fr&#39;]/2) . Out[86]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int, half_fr: double] Spark SQL for SQL functionality using createOrReplaceTempView . df.createOrReplaceTempView(&quot;economic_data&quot;) . sql_results = spark.sql(&quot;SELECT * FROM economic_data&quot;) . sql_results . Out[89]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int] sql_results.show() . +-+--+--+-+-+-++-+-+-+-+--+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +-+--+--+-+-+-++-+-+-+-+--+-+ null|province| null| null|null|null| null|null|null|null| null| reg| null| 0| Anhui| null| null|1996|null| 50661|null|null|null|1128873| East China| 631930| 1| Anhui| null| null|1997|null| 43443|null|null|null|1356287| East China| 657860| 2| Anhui| null| null|1998|null| 27673|null|null|null|1518236| East China| 889463| 3| Anhui| null| null|1999|null| 26131|null|null|null|1646891| East China|1227364| 4| Anhui| null| null|2000|null| 31847|null|null|null|1601508| East China|1499110| 5| Anhui| null| null|2001|null| 33672|null|null|null|1672445| East China|2165189| 6| Anhui| null| null|2002|null| 38375|null|null|null|1677840| East China|2404936| 7| Anhui| null| null|2003|null| 36720|null|null|null|1896479| East China|2815820| 8| Anhui| null| null|2004|null| 54669|null|null|null| null| East China|3422176| 9| Anhui| null| null|2005|null| 69000|null|null|null| null| East China|3874846| 10| Anhui| null| null|2006|null|139354|null|null|null|3434548| East China|5167300| 11| Anhui| null| null|2007|null|299892|null|null|null|4468640| East China|7040099| 12| Beijing| null| null|1996|null|155290|null|null|null| 634562|North China| 508135| 13| Beijing| null| null|1997|null|159286|null|null|null| 634562|North China| 569283| 14| Beijing| null| null|1998|null|216800|null|null|null| 938788|North China| 695528| 15| Beijing| null| null|1999|null|197525|null|null|null| null|North China| 944047| 16| Beijing| null| null|2000|null|168368|null|null|null|1667114|North China| 757990| 17| Beijing| null| null|2001|null|176818|null|null|null|2093925|North China|1194728| 18| Beijing| null| null|2002|null|172464|null|null|null|2511249|North China|1078754| +-+--+--+-+-+-++-+-+-+-+--+-+ only showing top 20 rows spark.sql(&quot;SELECT * FROM economic_data WHERE fr=634562&quot;).show() . ++--+--+-+-+-++-+-+-++--++ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+-++-+-+-++--++ 12| Beijing| null| null|1996|null|155290|null|null|null|634562|North China|508135| 13| Beijing| null| null|1997|null|159286|null|null|null|634562|North China|569283| ++--+--+-+-+-++-+-+-++--++ This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/21/RDDs-and-Schemas-and-Data-Types-with-Pyspark.html",
            "relUrl": "/2020/08/21/RDDs-and-Schemas-and-Data-Types-with-Pyspark.html",
            "date": " • Aug 21, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.columns . Out[85]: [&#39;_c0&#39;, &#39;province&#39;, &#39;specific&#39;, &#39;general&#39;, &#39;year&#39;, &#39;gdp&#39;, &#39;fdi&#39;, &#39;rnr&#39;, &#39;rr&#39;, &#39;i&#39;, &#39;fr&#39;, &#39;reg&#39;, &#39;it&#39;] df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) # for row in df.head(5): # print(row) # print(&#39; n&#39;) . df.describe().show() . +-++--+--+++--++-+--+-++++ summary| _c0|province| specific| general| year| gdp| fdi| rnr| rr| i| fr| reg| it| +-++--+--+++--++-+--+-++++ count| 360| 360| 356| 169| 360| 360| 360| 294| 296| 287| 295| 360| 360| mean| 179.5| null|583470.7303370787|309127.53846153844| 2001.5|4428.653416666667|196139.38333333333| 0.0355944252244898|0.059688621057432424|0.08376351662369343|2522449.0034013605| null|2165819.2583333333| stddev|104.06728592598157| null|654055.3290782663| 355423.5760674793|3.4568570586927794|4484.668659976412|303043.97011891654|0.16061503029299648| 0.15673351824073453| 0.1838933104683607|3491329.8613106664| null|1769294.2935487411| min| 0| Anhui| 8964.0| 0.0| 1996| 64.98| 2| 0.0| 0.0| 0.0| #REF!| East China| 147897| max| 359|Zhejiang| 3937966.0| 1737800.0| 2007| 31777.01| 1743140| 1.214285714| 0.84| 1.05| 9898522|Southwest China| 10533312| +-++--+--+++--++-+--+-++++ df.describe().printSchema() . root -- summary: string (nullable = true) -- _c0: string (nullable = true) -- province: string (nullable = true) -- specific: string (nullable = true) -- general: string (nullable = true) -- year: string (nullable = true) -- gdp: string (nullable = true) -- fdi: string (nullable = true) -- rnr: string (nullable = true) -- rr: string (nullable = true) -- i: string (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: string (nullable = true) Casting Data Types and Formatting Significant Digits . from pyspark.sql.functions import format_number . result = df.describe() result.select(result[&#39;province&#39;] ,format_number(result[&#39;specific&#39;].cast(&#39;float&#39;),2).alias(&#39;specific&#39;) ,format_number(result[&#39;general&#39;].cast(&#39;float&#39;),2).alias(&#39;general&#39;) ,format_number(result[&#39;year&#39;].cast(&#39;int&#39;),2).alias(&#39;year&#39;),format_number(result[&#39;gdp&#39;].cast(&#39;float&#39;),2).alias(&#39;gdp&#39;) ,format_number(result[&#39;rnr&#39;].cast(&#39;int&#39;),2).alias(&#39;rnr&#39;),format_number(result[&#39;rr&#39;].cast(&#39;float&#39;),2).alias(&#39;rr&#39;) ,format_number(result[&#39;fdi&#39;].cast(&#39;int&#39;),2).alias(&#39;fdi&#39;),format_number(result[&#39;it&#39;].cast(&#39;float&#39;),2).alias(&#39;it&#39;) ,result[&#39;reg&#39;].cast(&#39;string&#39;).alias(&#39;reg&#39;) ).show() . +--+++--+++++-++ province| specific| general| year| gdp| rnr| rr| fdi| it| reg| +--+++--+++++-++ 360| 356.00| 169.00| 360.00| 360.00|294.00|296.00| 360.00| 360.00| 360| null| 583,470.75| 309,127.53|2,001.00| 4,428.65| 0.00| 0.06| 196,139.00| 2,165,819.25| null| null| 654,055.31| 355,423.56| 3.00| 4,484.67| 0.00| 0.16| 303,043.00| 1,769,294.25| null| Anhui| 8,964.00| 0.00|1,996.00| 64.98| 0.00| 0.00| 2.00| 147,897.00| East China| Zhejiang|3,937,966.00|1,737,800.00|2,007.00|31,777.01| 1.00| 0.84|1,743,140.00|10,533,312.00|Southwest China| +--+++--+++++-++ New Columns generated from extant columns using withColumn . df2 = df.withColumn(&quot;specific_gdp_ratio&quot;,df[&quot;specific&quot;]/(df[&quot;gdp&quot;]*100))#.show() . df2.select(&#39;specific_gdp_ratio&#39;).show() . ++ specific_gdp_ratio| ++ 0.7022500358285959| 0.6474660463848132| 0.6878991411583352| 1.0519477646607727| 0.673928100093381| 0.7727761333780966| 1.233475958314866| 1.5783421826051272| 1.8877587040110941| 1.6792756118029895| 2.3850666666666664| 3.0077639751552794| 0.9275486250838364| 0.7989880072601573| 1.0314658544998698| 1.448708759827088| 0.8912058855158366| 1.1918224576316896| 1.2944820393974508| 1.283311464867661| ++ only showing top 20 rows df.orderBy(df[&quot;specific&quot;].asc()).head(1)[0][0] . Out[94]: 24 Finding the Mean, Max, and Min . from pyspark.sql.functions import mean df.select(mean(&quot;specific&quot;)).show() . +--+ avg(specific)| +--+ 583470.7303370787| +--+ from pyspark.sql.functions import max,min . df.select(max(&quot;specific&quot;),min(&quot;specific&quot;)).show() . +-+-+ max(specific)|min(specific)| +-+-+ 3937966.0| 8964.0| +-+-+ df.filter(&quot;specific &lt; 60000&quot;).count() . Out[98]: 23 df.filter(df[&#39;specific&#39;] &lt; 60000).count() . Out[99]: 23 from pyspark.sql.functions import count result = df.filter(df[&#39;specific&#39;] &lt; 60000) result.select(count(&#39;specific&#39;)).show() . ++ count(specific)| ++ 23| ++ (df.filter(df[&quot;gdp&quot;]&gt;8000).count()*1.0/df.count())*100 . Out[101]: 14.444444444444443 from pyspark.sql.functions import corr df.select(corr(&quot;gdp&quot;,&quot;fdi&quot;)).show() . ++ corr(gdp, fdi)| ++ 0.8366328478935896| ++ Finding the max value by Year . from pyspark.sql.functions import year #yeardf = df.withColumn(&quot;Year&quot;,year(df[&quot;year&quot;])) . max_df = df.groupBy(&#39;year&#39;).max() . max_df.select(&#39;year&#39;,&#39;max(gdp)&#39;).show() . +-+--+ year|max(gdp)| +-+--+ 2003|15844.64| 2007|31777.01| 2006|26587.76| 1997| 7774.53| 2004|18864.62| 1996| 6834.97| 1998| 8530.88| 2001|12039.25| 2005|22557.37| 2000|10741.25| 1999| 9250.68| 2002|13502.42| +-+--+ from pyspark.sql.functions import month . #df.select(&quot;year&quot;,&quot;avg(gdp)&quot;).orderBy(&#39;year&#39;).show() . This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/20/Pyspark-Dataframes-Data-Types.html",
            "relUrl": "/2020/08/20/Pyspark-Dataframes-Data-Types.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Dataframe Filitering and Operations with Pyspark",
            "content": "from pyspark.sql import SparkSession # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . Filtering on values in a column . df.filter(&quot;specific&lt;10000&quot;).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| 269|Shanghai| 9834.0| null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917| ++--+--+-+-+-++++-+-+-+-+ df.filter(&quot;specific&lt;10000&quot;).select(&#39;province&#39;).show() . +--+ province| +--+ Shanghai| Shanghai| +--+ df.filter(&quot;specific&lt;10000&quot;).select([&#39;province&#39;,&#39;year&#39;]).show() . +--+-+ province|year| +--+-+ Shanghai|2000| Shanghai|2001| +--+-+ df.filter(df[&quot;specific&quot;] &lt; 10000).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| 269|Shanghai| 9834.0| null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917| ++--+--+-+-+-++++-+-+-+-+ Filtering on values in 2+ columns . df.filter((df[&quot;specific&quot;] &lt; 55000) &amp; (df[&#39;gdp&#39;] &gt; 200) ).show() . ++--+--+-+-+--++-+-+-+-+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+--++-+-+-+-+-+-+ 98| Hainan| 54462.0| null|1998| 442.13| 71715|null|null|null| 236461|South Central China| 177748| 216| Ningxia| 32088.0| null|1996| 202.9| 2826|null|null|null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671|null|null|null| 102083| Northwest China| 195295| 268|Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269|Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 270|Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271|Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 272|Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 273|Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 274|Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6|211361|null|null|null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464| North China| 547120| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| ++--+--+-+-+--++-+-+-+-+-+-+ df.filter((df[&quot;specific&quot;] &lt; 55000) | (df[&#39;gdp&#39;] &gt; 20000) ).show() . ++++--+-+--+-+--+-+--+--+-+-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++++--+-+--+-+--+-+--+--+-+-+ 69|Guangdong|1491588.0| null|2005|22557.37|1236400|0.027027027000000002| 0.0| 0.0| null|South Central China|4327217| 70|Guangdong|1897575.0|498913.0|2006|26587.76|1451065|0.027027027000000002| 0.0| 0.0|16804703|South Central China|4559252| 71|Guangdong| 859482.0| 0.0|2007|31777.01|1712603|0.027027027000000002| 0.0| 0.0|27858007|South Central China|4947824| 98| Hainan| 54462.0| null|1998| 442.13| 71715| null|null| null| 236461|South Central China| 177748| 179| Jiangsu|1188989.0| 0.0|2007|21742.05|1743140| 0.0| 0.0|0.275862069|22377276| East China|3557071| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null| null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null| null| 102083| Northwest China| 195295| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null| null| 73260| Northwest China| 218361| 262| Shandong|1204547.0|112137.0|2006|21900.19|1000069| 0.0| 0.0| 0.0|11673659| East China|5304833| 263| Shandong|2121243.0|581800.0|2007|25776.91|1101159| 0.0| 0.0| 0.0|16753980| East China|6357869| 268| Shanghai| 8964.0| null|2000| 4771.17| 316014| 0.0| 0.0| 0.44| 2224124| East China|1212473| 269| Shanghai| 9834.0| null|2001| 5210.12| 429159| 0.0| 0.0| 0.44| 2947285| East China|1053917| 270| Shanghai| 19985.0| null|2002| 5741.03| 427229| 0.0| 0.0| 0.44| 3380397| East China|1572208| 271| Shanghai| 23547.0| null|2003| 6694.23| 546849| 0.0|0.53| 0.0| 4461153| East China|2031496| 272| Shanghai| 29943.0| null|2004| 8072.83| 654100| 0.0|0.53| 0.0| null| East China|2703643| 273| Shanghai| 29943.0| null|2005| 9247.66| 685000| 0.0|0.53| 0.0| null| East China|2140461| 274| Shanghai| 42928.0| null|2006|10572.24| 710700| 0.0|0.53| 0.0| 8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6| 211361| null|null| null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95| 176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88| 116601| 0.0| 0.0| 0.0| 757464| North China| 547120| ++++--+-+--+-+--+-+--+--+-+-+ only showing top 20 rows df.filter((df[&quot;specific&quot;] &lt; 55000) &amp; ~(df[&#39;gdp&#39;] &gt; 20000) ).show() . ++--+--+-+-+--++--+-+-+-+-+-+ _c0|province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+--+-+-+--++--+-+-+-+-+-+ 98| Hainan| 54462.0| null|1998| 442.13| 71715| null|null|null| 236461|South Central China| 177748| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null|null| 90805| Northwest China| 178668| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null|null| 102083| Northwest China| 195295| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null|null| 73260| Northwest China| 218361| 268|Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269|Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 270|Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271|Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 272|Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 273|Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 274|Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 302| Tianjin| 39364.0| null|1998| 1374.6|211361| null|null|null| 540178| North China| 361723| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 304| Tianjin| 51821.0| null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464| North China| 547120| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| 312| Tibet| 18829.0| null|1996| 64.98| 679|0.181818182| 0.0| 0.0| 27801| Southwest China| 306114| 313| Tibet| 25185.0| null|1997| 77.24| 63|0.181818182| 0.0| 0.0| 33787| Southwest China| 346368| 314| Tibet| 48197.0| null|1998| 91.5| 481| 0.0|0.24| 0.0| 3810| Southwest China| 415547| ++--+--+-+-+--++--+-+-+-+-+-+ df.filter(df[&quot;specific&quot;] == 8964.0).show() . ++--+--+-+-+-++++-+-+-+-+ _c0|province|specific|general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--+--+-+-+-++++-+-+-+-+ 268|Shanghai| 8964.0| null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473| ++--+--+-+-+-++++-+-+-+-+ df.filter(df[&quot;province&quot;] == &quot;Zhejiang&quot;).show() . ++--++--+-+--+-+--+--+--+--+-+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+--+-+--+--+--+--+-+-+ 348|Zhejiang| 273253.0| null|1996| 4188.53| 152021| 0.0| 0.0| 0.0| 1291252|East China| 740327| 349|Zhejiang| 330558.0| null|1997| 4686.11| 150345| 0.0| 0.0| 0.0| 1432453|East China| 814253| 350|Zhejiang| 426756.0| null|1998| 5052.62| 131802| 0.0| 0.0| 0.0| 1761084|East China| 923455| 351|Zhejiang| 586457.0| null|1999| 5443.92| 123262| 0.0| 0.0| 0.0| 2146200|East China|1001703| 352|Zhejiang| 408151.0| null|2000| 6141.03| 161266| 0.0| 0.0| 0.0| 2955508|East China|1135215| 353|Zhejiang| 358714.0| null|2001| 6898.34| 221162| 0.0| 0.0| 0.0| 4436868|East China|1203372| 354|Zhejiang| 365437.0|321686.0|2002| 8003.67| 307610| 0.0| 0.0| 0.0| 4958329|East China|1962633| 355|Zhejiang| 391292.0|260313.0|2003| 9705.02| 498055|1.214285714|0.035714286|0.035714286| 6217715|East China|2261631| 356|Zhejiang| 656175.0|276652.0|2004| 11648.7| 668128|1.214285714|0.035714286|0.035714286| null|East China|3162299| 357|Zhejiang| 656175.0| null|2005|13417.68| 772000|1.214285714|0.035714286|0.035714286| null|East China|2370200| 358|Zhejiang|1017303.0|394795.0|2006|15718.47| 888935|1.214285714|0.035714286|0.035714286|11537149|East China|2553268| 359|Zhejiang| 844647.0| 0.0|2007|18753.73|1036576|0.047619048| 0.0| 0.0|16494981|East China|2939778| ++--++--+-+--+-+--+--+--+--+-+-+ df.filter(df[&quot;specific&quot;] == 8964.0).collect() . Out[15]: [Row(_c0=268, province=&#39;Shanghai&#39;, specific=8964.0, general=None, year=2000, gdp=4771.17, fdi=316014, rnr=0.0, rr=0.0, i=0.44, fr=&#39;2224124&#39;, reg=&#39;East China&#39;, it=1212473)] result = df.filter(df[&quot;specific&quot;] == 8964.0).collect() . type(result[0]) . Out[17]: pyspark.sql.types.Row row = result[0] . row.asDict() . Out[19]: {&#39;_c0&#39;: 268, &#39;province&#39;: &#39;Shanghai&#39;, &#39;specific&#39;: 8964.0, &#39;general&#39;: None, &#39;year&#39;: 2000, &#39;gdp&#39;: 4771.17, &#39;fdi&#39;: 316014, &#39;rnr&#39;: 0.0, &#39;rr&#39;: 0.0, &#39;i&#39;: 0.44, &#39;fr&#39;: &#39;2224124&#39;, &#39;reg&#39;: &#39;East China&#39;, &#39;it&#39;: 1212473} for item in result[0]: print(item) . 268 Shanghai 8964.0 None 2000 4771.17 316014 0.0 0.0 0.44 2224124 East China 1212473 This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/19/Pyspark-Filtering.html",
            "relUrl": "/2020/08/19/Pyspark-Filtering.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Handling Missing Data with Pyspark",
            "content": "from pyspark.sql import SparkSession from pyspark.sql.functions import countDistinct, avg,stddev # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Dropping Columns without non-null values . # Has to have at least 2 NON-null values df.na.drop(thresh=2).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Dropping any row that contains missing data . df.na.drop().show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| 46| Fujian| 397517.0| 149549.0|2006|7583.85|322047| 0.4| 0.0| 0.0| 4830320| East China|2135224| 47| Fujian| 753552.0| 317700.0|2007|9248.53|406058| 0.4| 0.0| 0.0| 6994577| East China|2649011| 52| Gansu| 223984.0| 58533.0|2000|1052.88| 6235| 0.0|0.153846154| 0.0| 505196|Northwest China|1258100| 54| Gansu| 337894.0| 129791.0|2002|1232.03| 6121| 0.0| 0.13| 0.0| 597159|Northwest China|1898911| 58| Gansu| 833430.0| 516342.0|2006|2277.35| 2954| 0.0| 0.0|0.128205128| 924080|Northwest China|3847158| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(subset=[&quot;general&quot;]).show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 20| Beijing|1009936.0| 309025.0|2004|6033.21|308354| 0.0|0.794871795| 0.0| null| North China|1644601| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 30|Chongqing| 311770.0| 41907.0|2002|2232.86| 19576| null| null| null| 762806|Southwest China|1906968| 31|Chongqing| 335715.0| 18700.0|2003|2555.72| 26083| null| null| null| 929935|Southwest China|1778125| 32|Chongqing| 568835.0| 97500.0|2004|3034.58| 40508| null| null| null| null|Southwest China|2197948| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(how=&#39;any&#39;).show() . +++++-+-++-+--+--+--++-+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+-++-+--+--+--++-+ 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0| 1601508| East China|1499110| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0| 1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0| 1896479| East China|2815820| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324| 3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324| 4468640| East China|7040099| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53| 1667114| North China| 757990| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53| 2511249| North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0| 2823366| North China|1426600| 22| Beijing|1315102.0| 405966.0|2006|8117.78|455191| 0.0|0.794871795| 0.0| 4830392| North China|1782317| 23| Beijing| 752279.0|1023453.0|2007|9846.81|506572| 0.0|0.794871795| 0.0|14926380| North China|1962192| 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001| 0.0| 0.0| 1762409|Southwest China|3124234| 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001| 0.0| 0.0| 4427000|Southwest China|3923569| 40| Fujian| 142650.0| 71807.0|2000|3764.54|343191| 0.0| 0.0| 0.0| 2110577| East China| 819028| 42| Fujian| 137190.0| 59263.0|2002|4467.55|383837| 0.22| 0.3| 0.0| 2373047| East China|1184990| 43| Fujian| 148812.0| 68142.0|2003|4983.67|259903| 0.0| 0.0| 0.3| 2648861| East China|1364980| 46| Fujian| 397517.0| 149549.0|2006|7583.85|322047| 0.4| 0.0| 0.0| 4830320| East China|2135224| 47| Fujian| 753552.0| 317700.0|2007|9248.53|406058| 0.4| 0.0| 0.0| 6994577| East China|2649011| 52| Gansu| 223984.0| 58533.0|2000|1052.88| 6235| 0.0|0.153846154| 0.0| 505196|Northwest China|1258100| 54| Gansu| 337894.0| 129791.0|2002|1232.03| 6121| 0.0| 0.13| 0.0| 597159|Northwest China|1898911| 58| Gansu| 833430.0| 516342.0|2006|2277.35| 2954| 0.0| 0.0|0.128205128| 924080|Northwest China|3847158| +++++-+-++-+--+--+--++-+ only showing top 20 rows df.na.drop(how=&#39;all&#39;).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of Null Values . df.na.fill(&#39;example&#39;).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0|example| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324|example| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53|example|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of 0 . df.na.fill(0).show() . ++--++--+-+-+++--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi|rnr| rr| i| fr| reg| it| ++--++--+-+-+++--+--+-+--+-+ 0| Anhui| 147002.0| 0.0|1996| 2093.3| 50661|0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| 0.0|1997|2347.32| 43443|0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| 0.0|1998|2542.96| 27673|0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| 0.0|1999|2712.34| 26131|0.0| 0.0| 0.0|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847|0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| 0.0|2001|3246.71| 33672|0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375|0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720|0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669|0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| 0.0|2005|5350.17| 69000|0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354|0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892|0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| 0.0|1996| 1789.2|155290|0.0| 0.0| 0.0| 634562|North China| 508135| 13| Beijing| 165957.0| 0.0|1997|2077.09|159286|0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| 0.0|1998|2377.18|216800|0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| 0.0|1999|2678.82|197525|0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368|0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| 0.0|2001|3707.96|176818|0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464|0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126|0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-+++--+--+-+--+-+ only showing top 20 rows df.na.fill(&#39;example&#39;,subset=[&#39;fr&#39;]).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| null|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| null|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| null|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| null|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| null|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0|example| East China|3422176| 9| Anhui| 898441.0| null|2005|5350.17| 69000| 0.0| 0.0|0.324324324|example| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| null|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| null|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| null|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| null|1999|2678.82|197525| 0.0| 0.0| 0.53|example|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| null|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows df.na.fill(0,subset=[&#39;general&#39;]).show() . ++--++--+-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--++--+-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0| 0.0|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0| 0.0|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0| 0.0|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0| 0.0|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0| 0.0|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0| 0.0|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0| 0.0|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0| 0.0|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0| 0.0|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0| 0.0|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0| 0.0|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--++--+-+-++-+--+--+-+--+-+ only showing top 20 rows Imputation of the Mean . from pyspark.sql.functions import mean mean_val = df.select(mean(df[&#39;general&#39;])).collect() . mean_val[0][0] . Out[19]: 309127.53846153844 mean_gen = mean_val[0][0] . df.na.fill(mean_gen,[&quot;general&quot;]).show() . ++--+++-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+++-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--+++-+-++-+--+--+-+--+-+ only showing top 20 rows df.na.fill(df.select(mean(df[&#39;general&#39;])).collect()[0][0],[&#39;general&#39;]).show() . ++--+++-+-++-+--+--+-+--+-+ _c0|province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| ++--+++-+-++-+--+--+-+--+-+ 0| Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0| 0.0| 0.0|1128873| East China| 631930| 1| Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0| 0.0| 0.0|1356287| East China| 657860| 2| Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0| 0.0| 0.0|1518236| East China| 889463| 3| Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null| null| null|1646891| East China|1227364| 4| Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0| 0.0| 0.0|1601508| East China|1499110| 5| Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0| 0.0| 0.0|1672445| East China|2165189| 6| Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0| 0.0| 0.0|1677840| East China|2404936| 7| Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0| 0.0| 0.0|1896479| East China|2815820| 8| Anhui| 898441.0| 349699.0|2004| 4759.3| 54669| 0.0| 0.0| 0.0| null| East China|3422176| 9| Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0| 0.0|0.324324324| null| East China|3874846| 10| Anhui|1457872.0| 279052.0|2006| 6112.5|139354| 0.0| 0.0|0.324324324|3434548| East China|5167300| 11| Anhui|2213991.0| 178705.0|2007|7360.92|299892| 0.0| 0.0|0.324324324|4468640| East China|7040099| 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null| null| null| 634562|North China| 508135| 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0| 0.0| 0.6| 634562|North China| 569283| 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0| 0.0| 0.53| 938788|North China| 695528| 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0| 0.0| 0.53| null|North China| 944047| 16| Beijing| 281769.0| 188633.0|2000|3161.66|168368| 0.0| 0.0| 0.53|1667114|North China| 757990| 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0| 0.0| 0.53|2093925|North China|1194728| 18| Beijing| 558569.0| 280277.0|2002| 4315.0|172464| 0.0| 0.0| 0.53|2511249|North China|1078754| 19| Beijing| 642581.0| 269596.0|2003|5007.21|219126| 0.0|0.794871795| 0.0|2823366|North China|1426600| ++--+++-+-++-+--+--+-+--+-+ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/18/Pyspark-NAs.html",
            "relUrl": "/2020/08/18/Pyspark-NAs.html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Group By and Aggregation with Pyspark",
            "content": "&quot;Group By and Aggregation with Pyspark&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [pyspark, jupyter] | description: Group By and Aggregation with Pyspark | title: Group By and Aggregation with Pyspark | . Read CSV and inferSchema . from pyspark.sql import SparkSession from pyspark.sql.functions import countDistinct, avg,stddev # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: double (nullable = true) -- general: double (nullable = true) -- year: integer (nullable = true) -- gdp: double (nullable = true) -- fdi: integer (nullable = true) -- rnr: double (nullable = true) -- rr: double (nullable = true) -- i: double (nullable = true) -- fr: string (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) Using groupBy for Averages and Counts . df.groupBy(&quot;province&quot;) . Out[8]: &lt;pyspark.sql.group.GroupedData at 0x7f939a0aada0&gt; df.groupBy(&quot;province&quot;).mean().show() . ++--++++++--+--+--++ province|avg(_c0)| avg(specific)| avg(general)|avg(year)| avg(gdp)| avg(fdi)| avg(rnr)| avg(rr)| avg(i)| avg(it)| ++--++++++--+--+--++ Guangdong| 65.5|1123328.0833333333| 312308.0| 2001.5|15358.781666666668| 1194950.25|0.011261261250000001| 0.0| 0.0| 3099014.25| Hunan| 161.5| 824676.9166666666| 480788.3333333333| 2001.5| 4765.891666666666| 132110.25| 0.0| 0.07291666666666667| 0.0| 3215128.5| Shanxi| 281.5| 577540.4166666666| 351680.0| 2001.5| 2817.210833333333|38628.833333333336| 0.0| 0.0| 0.0|1983718.3333333333| Tibet| 317.5|189219.91666666666|165365.33333333334| 2001.5|170.42666666666665| 839.75| 0.03030303033333333| 0.15583333333333335| 0.20278090583333333|1174175.5833333333| Hubei| 149.5| 595463.25| 391326.5| 2001.5| 4772.503333333333| 149713.25| 0.045045045| 0.11386386375000002| 0.06230392158333333| 2904659.75| Tianjin| 305.5| 76884.16666666667| 126636.0| 2001.5|2528.6650000000004|250173.33333333334| 0.0| 0.0| 0.0| 831028.4166666666| Beijing| 17.5| 581440.8333333334| 412825.0| 2001.5| 4673.453333333333|257369.33333333334| 0.0| 0.3613053613636364| 0.29545454545454547|1175965.4166666667| Heilongjiang| 125.5|1037878.1666666666| 315925.3333333333| 2001.5| 4041.241666666667| 82719.33333333333| 0.0| 0.0| 0.03931203927272728|3230451.1666666665| Liaoning| 209.5| 1111002.75|185280.83333333334| 2001.5| 5231.135000000001| 285925.3333333333| 0.11469534044444446| 0.0| null|2628358.4166666665| Henan| 137.5| 955407.4166666666| 673392.5| 2001.5| 7208.966666666667| 94426.0| 0.0| 0.04| 0.08602150533333335|3671970.6666666665| Anhui| 5.5| 643984.1666666666|159698.83333333334| 2001.5|3905.8700000000003| 70953.08333333333| 0.0| 0.0| 0.08845208836363637|2649674.4166666665| Xinjiang| 329.5| 345334.3333333333| 412906.0| 2001.5|1828.8966666666665| 4433.083333333333| 0.0| 0.0| 0.0| 2251012.0| Fujian| 41.5|246144.16666666666|140619.33333333334| 2001.5|4864.0233333333335| 374466.4166666667| 0.1366666666666667|0.049999999999999996| 0.09999999999999999| 1274116.75| Jiangxi| 185.5| 592906.3333333334| 458268.6666666667| 2001.5| 2460.7825| 103735.25| 0.0| 0.1491841490909091|0.042727272727272725| 1760613.25| Jilin| 197.5| 711132.25| 348186.0| 2001.5|2274.8541666666665|41226.583333333336| 0.0| 0.0| 0.0|2136634.9166666665| Chongqing| 29.5| 561854.1111111111| 151201.4| 2001.5| 2477.7125|41127.833333333336| 0.09677419400000001| 0.0| 0.0|1636146.4166666667| Shaanxi| 245.5| 387167.1666666667| 386760.5| 2001.5| 2658.034166666667|50892.583333333336|0.002840909090909091| 0.0| 0.07386363636363637|2474031.4166666665| Sichuan| 293.5| 1194640.5| 707032.8333333334| 2001.5| 5377.79|62197.166666666664| 0.00818181818181818| 0.00818181818181818| 0.2|4016479.5833333335| Yunnan| 341.5| 802151.1666666666| 200426.0| 2001.5| 2604.054166666667|17048.333333333332| 0.0| 0.0| 0.0|3165418.9166666665| Gansu| 53.5| 498930.9166666667| 382092.6666666667| 2001.5|1397.8325000000002| 5295.5| 0.11111111120000002| 0.088974359| 0.13038461533333334| 2045347.0| ++--++++++--+--+--++ only showing top 20 rows df.groupBy(&quot;reg&quot;).mean().show() . +-+++++++--+--+--++ reg| avg(_c0)| avg(specific)| avg(general)|avg(year)| avg(gdp)| avg(fdi)| avg(rnr)| avg(rr)| avg(i)| avg(it)| +-+++++++--+--+--++ Southwest China| 214.3| 648086.8070175438| 327627.0| 2001.5|2410.3988333333336|25405.083333333332| 0.01764440930612245|0.053185448081632655| 0.13679739081632653| 2424971.4| East China|183.78571428571428|517524.90476190473|230217.37142857144| 2001.5| 7126.732976190476|414659.03571428574| 0.08284508739240506| 0.05701117448101268| 0.09036240282278483|1949130.4761904762| Northeast China| 177.5| 953337.7222222222|283130.72222222225| 2001.5| 3849.076944444444| 136623.75| 0.03686635942857143| 0.0| 0.02275960168421053|2665148.1666666665| North China| 179.5|506433.57446808513|334689.14285714284| 2001.5| 4239.038541666667|169600.58333333334| 0.0| 0.15428824051724138| 0.11206896551724138|1733718.7291666667| Northwest China| 216.7|324849.06666666665|293066.73333333334| 2001.5|1340.0261666666668|15111.133333333333|0.022847222240000003|0.033887245249999996|0.048179240615384616| 1703537.75| South Central China| 115.5| 690125.8333333334| 382414.8888888889| 2001.5| 5952.826944444445|281785.59722222225|0.014928879322033899| 0.07324349771186443| 0.06797753142372882| 2626299.875| +-+++++++--+--+--++ # Count df.groupBy(&quot;reg&quot;).count().show() . +-+--+ reg|count| +-+--+ Southwest China| 60| East China| 84| Northeast China| 36| North China| 48| Northwest China| 60| South Central China| 72| +-+--+ # Max df.groupBy(&quot;reg&quot;).max().show() . +-+--+-+++--+--++--+-+--+ reg|max(_c0)|max(specific)|max(general)|max(year)|max(gdp)|max(fdi)| max(rnr)| max(rr)| max(i)| max(it)| +-+--+-+++--+--++--+-+--+ Southwest China| 347| 3937966.0| 1725100.0| 2007|10562.39| 149322| 0.181818182| 0.84| 0.75|10384846| East China| 359| 2213991.0| 1272600.0| 2007|25776.91| 1743140| 1.214285714| 0.53| 0.6| 7040099| Northeast China| 215| 3847672.0| 1046700.0| 2007| 9304.52| 598554| 0.516129032| 0.0|0.21621621600000002| 7968319| North China| 311| 2981235.0| 1023453.0| 2007|13607.32| 527776| 0.0|0.794871795| 0.6| 7537692| Northwest China| 335| 2669238.0| 1197400.0| 2007| 5757.29| 119516|0.5555555560000001| 0.5| 1.05| 6308151| South Central China| 167| 3860764.0| 1737800.0| 2007|31777.01| 1712603| 0.27027027| 0.4375| 0.6176470589999999|10533312| +-+--+-+++--+--++--+-+--+ # Min df.groupBy(&quot;reg&quot;).min().show() . +-+--+-+++--+--+--+-++-+ reg|min(_c0)|min(specific)|min(general)|min(year)|min(gdp)|min(fdi)|min(rnr)|min(rr)|min(i)|min(it)| +-+--+-+++--+--+--+-++-+ Southwest China| 24| 18829.0| 18700.0| 1996| 64.98| 2| 0.0| 0.0| 0.0| 176802| East China| 0| 8964.0| 0.0| 1996| 1169.73| 22724| 0.0| 0.0| 0.0| 489132| Northeast China| 120| 80595.0| 19360.0| 1996| 1137.23| 19059| 0.0| 0.0| 0.0| 625471| North China| 12| 35084.0| 32119.0| 1996| 1121.93| 13802| 0.0| 0.0| 0.0| 303992| Northwest China| 48| 32088.0| 2990.0| 1996| 184.17| 247| 0.0| 0.0| 0.0| 178668| South Central China| 60| 54462.0| 0.0| 1996| 389.68| 29579| 0.0| 0.0| 0.0| 147897| +-+--+-+++--+--+--+-++-+ # Sum df.groupBy(&quot;reg&quot;).sum().show() . +-+--+-++++--+++-++ reg|sum(_c0)|sum(specific)|sum(general)|sum(year)| sum(gdp)|sum(fdi)| sum(rnr)| sum(rr)| sum(i)| sum(it)| +-+--+-++++--+++-++ Southwest China| 12858| 3.6940948E7| 9501183.0| 120090|144623.93000000002| 1524305| 0.864576056| 2.606086956| 6.70307215|145498284| East China| 15438| 4.3472092E7| 8057608.0| 168126| 598645.57|34831359| 6.544761904| 4.503882784000002| 7.138629823000002|163726960| Northeast China| 6390| 3.4320158E7| 5096353.0| 72054| 138566.77| 4918455| 1.032258064| 0.0|0.43243243200000003| 95945334| North China| 8616| 2.3802378E7| 7028472.0| 96072| 203473.85| 8140828| 0.0| 4.474358975| 3.25| 83218499| Northwest China| 13002| 1.9490944E7| 8792002.0| 120090| 80401.57| 906668|1.1423611120000001|1.7621367529999998| 2.505320512|102212265| South Central China| 8316| 4.968906E7| 1.3766936E7| 144108|428603.54000000004|20288563| 0.88080388| 4.321366365000001| 4.010674354000001|189093591| +-+--+-++++--+++-++ # Max it across everything df.agg({&#39;specific&#39;:&#39;max&#39;}).show() . +-+ max(specific)| +-+ 3937966.0| +-+ grouped = df.groupBy(&quot;reg&quot;) grouped.agg({&quot;it&quot;:&#39;max&#39;}).show() . +-+--+ reg| max(it)| +-+--+ Southwest China|10384846| East China| 7040099| Northeast China| 7968319| North China| 7537692| Northwest China| 6308151| South Central China|10533312| +-+--+ df.select(countDistinct(&quot;reg&quot;)).show() . +-+ count(DISTINCT reg)| +-+ 6| +-+ df.select(countDistinct(&quot;reg&quot;).alias(&quot;Distinct Region&quot;)).show() . ++ Distinct Region| ++ 6| ++ df.select(avg(&#39;specific&#39;)).show() . +--+ avg(specific)| +--+ 583470.7303370787| +--+ df.select(stddev(&quot;specific&quot;)).show() . ++ stddev_samp(specific)| ++ 654055.3290782663| ++ Choosing Significant Digits with format_number . from pyspark.sql.functions import format_number . specific_std = df.select(stddev(&quot;specific&quot;).alias(&#39;std&#39;)) specific_std.show() . +--+ std| +--+ 654055.3290782663| +--+ specific_std.select(format_number(&#39;std&#39;,0)).show() . ++ format_number(std, 0)| ++ 654,055| ++ Using orderBy . df.orderBy(&quot;specific&quot;).show() . +++--+-+-+--++--+-+-+-++-+ _c0| province|specific|general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++--+-+-+--++--+-+-+-++-+ 28|Chongqing| null| null|2000| 1791.0| 24436| null|null|null| null|Southwest China|1022148| 109| Hebei| null| null|1997| 3953.78|110064| null|null|null| null| North China| 826734| 24|Chongqing| null| null|1996| 1315.12| 21878| null|null|null| null|Southwest China| 176802| 25|Chongqing| null| null|1997| 1509.75| 38675| null|null|null| null|Southwest China| 383402| 268| Shanghai| 8964.0| null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124| East China|1212473| 269| Shanghai| 9834.0| null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285| East China|1053917| 312| Tibet| 18829.0| null|1996| 64.98| 679|0.181818182| 0.0| 0.0| 27801|Southwest China| 306114| 270| Shanghai| 19985.0| null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397| East China|1572208| 271| Shanghai| 23547.0| null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153| East China|2031496| 313| Tibet| 25185.0| null|1997| 77.24| 63|0.181818182| 0.0| 0.0| 33787|Southwest China| 346368| 273| Shanghai| 29943.0| null|2005| 9247.66|685000| 0.0|0.53| 0.0| null| East China|2140461| 272| Shanghai| 29943.0| null|2004| 8072.83|654100| 0.0|0.53| 0.0| null| East China|2703643| 216| Ningxia| 32088.0| null|1996| 202.9| 2826| null|null|null| 90805|Northwest China| 178668| 305| Tianjin| 35084.0| null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763| North China| 688810| 228| Qinghai| 37976.0| null|1996| 184.17| 576| null|null|null| 73260|Northwest China| 218361| 302| Tianjin| 39364.0| null|1998| 1374.6|211361| null|null|null| 540178| North China| 361723| 274| Shanghai| 42928.0| null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966| East China|2239987| 217| Ningxia| 44267.0| null|1997| 224.59| 671| null|null|null| 102083|Northwest China| 195295| 303| Tianjin| 45463.0| null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662| North China| 422522| 314| Tibet| 48197.0| null|1998| 91.5| 481| 0.0|0.24| 0.0| 3810|Southwest China| 415547| +++--+-+-+--++--+-+-+-++-+ only showing top 20 rows df.orderBy(df[&quot;specific&quot;].desc()).show() . +++++-+--+-+--+--+-+--+-+--+ _c0| province| specific| general|year| gdp| fdi| rnr| rr| i| fr| reg| it| +++++-+--+-+--+--+-+--+-+--+ 299| Sichuan|3937966.0|1725100.0|2007|10562.39| 149322| null| null| null| 8508606| Southwest China|10384846| 143| Henan|3860764.0|1737800.0|2007|15012.46| 306162| 0.0| 0.0| 0.0| 8620804|South Central China|10533312| 131|Heilongjiang|3847672.0|1046700.0|2007| 7104.0| 208508| 0.0| 0.0|0.21621621600000002| 4404689| Northeast China| 7968319| 215| Liaoning|3396397.0| 599600.0|2007| 9304.52| 598554|0.516129032| 0.0| null|10826948| Northeast China| 5502192| 167| Hunan|3156087.0|1329200.0|2007| 9439.6| 327051| 0.0| 0.4375| 0.0| 6065508|South Central China| 8340692| 119| Hebei|2981235.0| 694400.0|2007|13607.32| 241621| 0.0| 0.5| 0.0| 7891198| North China| 7537692| 155| Hubei|2922784.0|1263500.0|2007| 9333.4| 276622| 0.0|0.111111111| 0.0| 5903552|South Central China| 7666512| 251| Shaanxi|2669238.0|1081000.0|2007| 5757.29| 119516| 0.03125| 0.0| 0.8125| 4752398| Northwest China| 6308151| 203| Jilin|2663667.0|1016400.0|2007| 4275.12| 76064| 0.0| 0.0| 0.0| 3206892| Northeast China| 4607955| 347| Yunnan|2482173.0| 564400.0|2007| 4772.52| 39453| 0.0| 0.0| 0.0| 4867146| Southwest China| 6832541| 298| Sichuan|2225220.0|1187958.0|2006| 8690.24| 120819| 0.0| 0.0| 0.55| 4247403| Southwest China| 7646885| 11| Anhui|2213991.0| 178705.0|2007| 7360.92| 299892| 0.0| 0.0| 0.324324324| 4468640| East China| 7040099| 287| Shanxi|2189020.0| 661200.0|2007| 6024.45| 134283| null| null| null| 5978870| North China| 5070166| 263| Shandong|2121243.0| 581800.0|2007|25776.91|1101159| 0.0| 0.0| 0.0|16753980| East China| 6357869| 191| Jiangxi|2045869.0|1272600.0|2007| 4820.53| 280657| 0.0| 0.41025641| 0.0| 3898510| East China| 4229821| 83| Guangxi|2022957.0|1214100.0|2007| 5823.41| 68396|0.205128205| 0.0|0.23076923100000002| 4188265|South Central China| 6185600| 142| Henan|2018158.0|1131615.0|2006|12362.79| 184526| 0.0| 0.0| 0.0| 6212824|South Central China| 7601825| 59| Gansu|2010553.0|1039400.0|2007| 2703.98| 11802| null| 0.0| 1.05| 1909107| Northwest China| 5111059| 95| Guizhou|1956261.0|1239200.0|2007| 2884.11| 12651| 0.0| 0.0| 0.7105263159999999| 2851375| Southwest China| 5639838| 214| Liaoning|1947031.0| 179893.0|2006| 8047.26| 359000|0.516129032| 0.0| null| 6530236| Northeast China| 4605917| +++++-+--+-+--+--+-+--+-+--+ only showing top 20 rows This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/17/Pyspark-Group-By.html",
            "relUrl": "/2020/08/17/Pyspark-Group-By.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
            "content": "# Import required packages import numpy as np import pandas as pd %matplotlib inline import seaborn as sns import statsmodels.formula.api as smf from matplotlib import pyplot as plt from matplotlib.lines import Line2D %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . np.random.seed(42) import matplotlib as mpl mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) . Read required datasets . df = pd.read_csv(&#39;ttb_county_clean.csv&#39;) df1 = pd.read_csv(&#39;df_panel_fix.csv&#39;) . Figure 1 . df.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=df[&quot;specific&quot;]/100, label=&quot;Specific Purpose Transfers&quot;, figsize=(12,8), c=&quot;nightlights&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) #save_fig(&quot;cn-spt-county-heat&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f0a4e16b4e0&gt; . Panel regression framework with year and province fixed effects . lin_reg = smf.ols(&#39;np.log(specific) ~ np.log(gdp) + np.log(fdi) + i + rnr + rr + C(province) + C(year)&#39;, data=df1).fit() . #lin_reg.summary() . Figure 2 . coef_df = pd.read_csv(&#39;coef.csv&#39;) fig, ax = plt.subplots(figsize=(16, 10)) coef_df.plot(x=&#39;varname&#39;, y=&#39;coef&#39;, kind=&#39;bar&#39;, ax=ax, color=&#39;none&#39;, yerr=&#39;err&#39;, legend=False) ax.set_ylabel(&#39;Specific Purpose Transfers (ln)&#39;) ax.set_xlabel(&#39;Independant Variables&#39;) ax.scatter(x=pd.np.arange(coef_df.shape[0]), marker=&#39;s&#39;, s=120, y=coef_df[&#39;coef&#39;], color=&#39;black&#39;) ax.axhline(y=0, linestyle=&#39;--&#39;, color=&#39;blue&#39;, linewidth=4) ax.xaxis.set_ticks_position(&#39;none&#39;) _ = ax.set_xticklabels([&#39;GDP&#39;, &#39;FDI&#39;, &#39;Incumbent&#39;, &#39;Non Relevant Rival&#39;, &#39;Relevant Rival&#39;], rotation=0, fontsize=20) fs = 16 ax.annotate(&#39;Controls&#39;, xy=(0.2, -0.2), xytext=(0.2, -0.3), xycoords=&#39;axes fraction&#39;, textcoords=&#39;axes fraction&#39;, fontsize=fs, ha=&#39;center&#39;, va=&#39;bottom&#39;, bbox=dict(boxstyle=&#39;square&#39;, fc=&#39;white&#39;, ec=&#39;blue&#39;), arrowprops=dict(arrowstyle=&#39;-[, widthB=5.5, lengthB=1.2&#39;, lw=2.0, color=&#39;blue&#39;)) _ = ax.annotate(&#39;Connections&#39;, xy=(0.7, -0.2), xytext=(0.7, -0.3), xycoords=&#39;axes fraction&#39;, textcoords=&#39;axes fraction&#39;, fontsize=fs, ha=&#39;center&#39;, va=&#39;bottom&#39;, bbox=dict(boxstyle=&#39;square&#39;, fc=&#39;white&#39;, ec=&#39;red&#39;), arrowprops=dict(arrowstyle=&#39;-[, widthB=10.5, lengthB=1.2&#39;, lw=2.0, color=&#39;red&#39;)) #save_fig(&quot;i-coef-plot&quot;) . Figure 3 . import numpy as np from plotly import __version__ from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot print (__version__) # requires version &gt;= 1.9.0 #Always run this the command before at the start of notebook init_notebook_mode(connected=True) import plotly.graph_objs as go trace1 = go.Bar( x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007], y=[188870900000.0, 185182900000.0, 237697500000.0, 347187900000.0, 296716700000.0, 397833100000.0, 440204800000.0, 514254300000.0, 686016600000.0, 677746300000.0, 940057900000.0, 1881304000000], name=&#39;All Other Province Leaders&#39;, marker=dict( color=&#39;rgb(55, 83, 109)&#39; ) ) trace2 = go.Bar( x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007], y=[260376000000.0, 264934700000.0, 367350200000.0, 463861200000.0, 199068500000.0, 216582600000.0, 298631800000.0, 409759300000.0, 830363200000.0, 878158000000.0, 1143745000000.0, 2125891000000.0], name=&#39;Incumbent Connected Province Leaders&#39;, marker=dict( color=&#39;rgb(26, 118, 255)&#39; ) ) data = [trace1, trace2] layout = go.Layout( title=&#39;Specific Purpose Transfers&#39;, xaxis=dict( tickfont=dict( size=14, color=&#39;rgb(107, 107, 107)&#39; ) ), yaxis=dict( title=&#39;RMB&#39;, titlefont=dict( size=16, color=&#39;rgb(107, 107, 107)&#39; ), tickfont=dict( size=14, color=&#39;rgb(107, 107, 107)&#39; ) ), legend=dict( x=0, y=1.0, bgcolor=&#39;rgba(255, 255, 255, 0)&#39;, bordercolor=&#39;rgba(255, 255, 255, 0)&#39; ), barmode=&#39;group&#39;, bargap=0.15, bargroupgap=0.1 ) fig = go.Figure(data=data, layout=layout) #iplot(fig, filename=&#39;style-bar&#39;) iplot(fig, image=&#39;png&#39;,filename=&#39;spt-i-bar&#39;) . 4.1.1 .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/16/pandas-stats-fiscal.html",
            "relUrl": "/2020/08/16/pandas-stats-fiscal.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Pyspark Regression with Fiscal Data",
            "content": "&quot;A minimal example of using Pyspark for Linear Regression&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [pyspark, jupyter] | description: A minimal example of using Pyspark for Linear Regression | title: Pyspark Regression with Fiscal Data | . Bring in needed imports . from pyspark.sql.functions import col from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType from pyspark.sql.functions import * . Load data from CSV . #collapse-hide # Load data from a CSV file_location = &quot;/FileStore/tables/df_panel_fix.csv&quot; df = spark.read.format(&quot;CSV&quot;).option(&quot;inferSchema&quot;, True).option(&quot;header&quot;, True).load(file_location) display(df.take(5)) . . _c0provincespecificgeneralyeargdpfdirnrrrifrregit . 0 | Anhui | 147002.0 | null | 1996 | 2093.3 | 50661 | 0.0 | 0.0 | 0.0 | 1128873 | East China | 631930 | . 1 | Anhui | 151981.0 | null | 1997 | 2347.32 | 43443 | 0.0 | 0.0 | 0.0 | 1356287 | East China | 657860 | . 2 | Anhui | 174930.0 | null | 1998 | 2542.96 | 27673 | 0.0 | 0.0 | 0.0 | 1518236 | East China | 889463 | . 3 | Anhui | 285324.0 | null | 1999 | 2712.34 | 26131 | null | null | null | 1646891 | East China | 1227364 | . 4 | Anhui | 195580.0 | 32100.0 | 2000 | 2902.09 | 31847 | 0.0 | 0.0 | 0.0 | 1601508 | East China | 1499110 | . df.createOrReplaceTempView(&quot;fiscal_stats&quot;) sums = spark.sql(&quot;&quot;&quot; select year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr from fiscal_stats group by 1 order by year asc &quot;&quot;&quot;) sums.show() . +-+++ year|total_yearly_it|total_yearly_fr| +-+++ 1996| 19825341| 2.9579215E7| 1997| 21391321| 2.9110765E7| 1998| 25511453| 3.8154711E7| 1999| 31922107| 4.2128627E7| 2000| 38721293| 4.8288092E7| 2001| 50754944| 5.8910649E7| 2002| 62375881| 6.2071474E7| 2003| 69316709| 7.2479293E7| 2004| 88626786| null| 2005| 98263665| null| 2006| 119517822| 1.3349148E8| 2007| 153467611| 2.27385701E8| +-+++ Describing the Data . df.describe().toPandas().transpose() . 0 1 2 3 4 . summary count | mean | stddev | min | max | . _c0 360 | 179.5 | 104.06728592598157 | 0 | 359 | . province 360 | None | None | Anhui | Zhejiang | . specific 356 | 583470.7303370787 | 654055.3290782663 | 8964.0 | 3937966.0 | . general 169 | 309127.53846153844 | 355423.5760674793 | 0.0 | 1737800.0 | . year 360 | 2001.5 | 3.4568570586927794 | 1996 | 2007 | . gdp 360 | 4428.653416666667 | 4484.668659976412 | 64.98 | 31777.01 | . fdi 360 | 196139.38333333333 | 303043.97011891654 | 2 | 1743140 | . rnr 294 | 0.0355944252244898 | 0.16061503029299648 | 0.0 | 1.214285714 | . rr 296 | 0.059688621057432424 | 0.15673351824073453 | 0.0 | 0.84 | . i 287 | 0.08376351662369343 | 0.1838933104683607 | 0.0 | 1.05 | . fr 295 | 2522449.0034013605 | 3491329.8613106664 | #REF! | 9898522 | . reg 360 | None | None | East China | Southwest China | . it 360 | 2165819.2583333333 | 1769294.2935487411 | 147897 | 10533312 | . Cast Data Type . df2 = df.withColumn(&quot;gdp&quot;,col(&quot;gdp&quot;).cast(IntegerType())) .withColumn(&quot;specific&quot;,col(&quot;specific&quot;).cast(IntegerType())) .withColumn(&quot;general&quot;,col(&quot;general&quot;).cast(IntegerType())) .withColumn(&quot;year&quot;,col(&quot;year&quot;).cast(IntegerType())) .withColumn(&quot;fdi&quot;,col(&quot;fdi&quot;).cast(IntegerType())) .withColumn(&quot;rnr&quot;,col(&quot;rnr&quot;).cast(IntegerType())) .withColumn(&quot;rr&quot;,col(&quot;rr&quot;).cast(IntegerType())) .withColumn(&quot;i&quot;,col(&quot;i&quot;).cast(IntegerType())) .withColumn(&quot;fr&quot;,col(&quot;fr&quot;).cast(IntegerType())) . printSchema . df2.printSchema() . root -- _c0: integer (nullable = true) -- province: string (nullable = true) -- specific: integer (nullable = true) -- general: integer (nullable = true) -- year: integer (nullable = true) -- gdp: integer (nullable = true) -- fdi: integer (nullable = true) -- rnr: integer (nullable = true) -- rr: integer (nullable = true) -- i: integer (nullable = true) -- fr: integer (nullable = true) -- reg: string (nullable = true) -- it: integer (nullable = true) from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import LinearRegression assembler = VectorAssembler(inputCols=[&#39;gdp&#39;, &#39;fdi&#39;], outputCol=&quot;features&quot;) train_df = assembler.transform(df2) . train_df.select(&quot;specific&quot;, &quot;year&quot;).show() . +--+-+ specific|year| +--+-+ 147002|1996| 151981|1997| 174930|1998| 285324|1999| 195580|2000| 250898|2001| 434149|2002| 619201|2003| 898441|2004| 898441|2005| 1457872|2006| 2213991|2007| 165957|1996| 165957|1997| 245198|1998| 388083|1999| 281769|2000| 441923|2001| 558569|2002| 642581|2003| +--+-+ only showing top 20 rows Linear Regression in Pyspark . lr = LinearRegression(featuresCol = &#39;features&#39;, labelCol=&#39;it&#39;) lr_model = lr.fit(train_df) trainingSummary = lr_model.summary print(&quot;Coefficients: &quot; + str(lr_model.coefficients)) print(&quot;RMSE: %f&quot; % trainingSummary.rootMeanSquaredError) print(&quot;R2: %f&quot; % trainingSummary.r2) . Coefficients: [495.05888709337756,-4.968141828763066] RMSE: 1234228.673087 R2: 0.512023 lr_predictions = lr_model.transform(train_df) lr_predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show(5) from pyspark.ml.evaluation import RegressionEvaluator lr_evaluator = RegressionEvaluator(predictionCol=&quot;prediction&quot;, labelCol=&quot;it&quot;,metricName=&quot;r2&quot;) . ++-+-+ prediction| it| features| ++-+-+ 1732528.7382477913| 631930|[2093.0,50661.0]| 1894133.7432895212| 657860|[2347.0,43443.0]| 2069017.8229123235| 889463|[2542.0,27673.0]| 2160838.7084181504|1227364|[2712.0,26131.0]| 2226501.9982726825|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows print(&quot;R Squared (R2) on test data = %g&quot; % lr_evaluator.evaluate(lr_predictions)) . R Squared (R2) on test data = 0.512023 print(&quot;numIterations: %d&quot; % trainingSummary.totalIterations) print(&quot;objectiveHistory: %s&quot; % str(trainingSummary.objectiveHistory)) trainingSummary.residuals.show() . numIterations: 1 objectiveHistory: [0.0] +-+ residuals| +-+ -1100598.7382477913| -1236273.7432895212| -1179554.8229123235| -933474.7084181504| -727391.9982726825| -222546.39659531135| -94585.30175113119| 108072.63313654158| 389732.58121094666| 621021.2194867637| 1885768.997742407| 3938310.059555837| -554084.125169754| -615660.3899049093| -352195.3468934437| -348450.00565795833| -918476.5594253046| -710059.9133252408| -1148661.0062004486| -911572.322055324| +-+ only showing top 20 rows predictions = lr_model.transform(test_df) predictions.select(&quot;prediction&quot;,&quot;it&quot;,&quot;features&quot;).show() . ++-++ prediction| it| features| ++-++ 976371.9212205639| 306114| [64.0,679.0]| 990722.2032541803| 415547| [91.0,481.0]| 1016348.0830204486| 983251| [139.0,106.0]| 1036290.7062801318| 218361| [184.0,576.0]| 1034023.4471330958| 178668| [202.0,2826.0]| 1060130.0768520113| 274994| [245.0,1856.0]| 1023513.0851009073| 546541|[263.0,11020.0]| 1053250.6267921| 361358| [264.0,5134.0]| 1123768.8091592425| 866691| [377.0,2200.0]| 1128604.8330225947| 948521| [390.0,2522.0]| 810587.2575938476| 177748|[442.0,71715.0]| 1159703.254297337| 736165| [445.0,1743.0]| 1066975.770986663|1260633|[466.0,22500.0]| 1288507.6625716756|1423771| [725.0,3718.0]| 1320055.238474972| 573905| [793.0,4144.0]| 1188611.0570700848|2347862|[797.0,31000.0]| 1321857.482976733| 582711| [805.0,4977.0]| 1033849.5995896922| 746784|[819.0,64343.0]| 1445051.792853667|1216605|[1029.0,2501.0]| 1437887.1056682135|1258100|[1052.0,6235.0]| ++-++ only showing top 20 rows from pyspark.ml.regression import DecisionTreeRegressor dt = DecisionTreeRegressor(featuresCol =&#39;features&#39;, labelCol = &#39;it&#39;) dt_model = dt.fit(train_df) dt_predictions = dt_model.transform(train_df) dt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = dt_evaluator.evaluate(dt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . Root Mean Squared Error (RMSE) on test data = 1.01114e+06 from pyspark.ml.regression import GBTRegressor gbt = GBTRegressor(featuresCol = &#39;features&#39;, labelCol = &#39;it&#39;, maxIter=10) gbt_model = gbt.fit(train_df) gbt_predictions = gbt_model.transform(train_df) gbt_predictions.select(&#39;prediction&#39;, &#39;it&#39;, &#39;features&#39;).show(5) gbt_evaluator = RegressionEvaluator( labelCol=&quot;it&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;) rmse = gbt_evaluator.evaluate(gbt_predictions) print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) . ++-+-+ prediction| it| features| ++-+-+ 1388898.308543053| 631930|[2093.0,50661.0]| 1388898.308543053| 657860|[2347.0,43443.0]| 1649083.6277172007| 889463|[2542.0,27673.0]| 1649083.6277172007|1227364|[2712.0,26131.0]| 1649083.6277172007|1499110|[2902.0,31847.0]| ++-+-+ only showing top 5 rows Root Mean Squared Error (RMSE) on test data = 778728 This post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks. .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "relUrl": "/2020/08/15/Pyspark-Fiscal-Data-Regression.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Pyspark for Data Science",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. . # Prints &#39;2&#39; print(1+1) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/markdown/2020/06/28/2st-markdown-post.html",
            "relUrl": "/markdown/2020/06/28/2st-markdown-post.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Data Science, Big Data and Healthcare Research",
            "content": "Data Science, Big Data and Healthcare Research . This post will consider the use of hive, spark, and for machine learning pipelines and workflows. . # Prints &#39;2&#39; print(1+1) .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/markdown/2020/06/08/1st-markdown-post.html",
            "relUrl": "/markdown/2020/06/08/1st-markdown-post.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "A timer for ML functions",
            "content": "&quot;A timer for ML functions&quot; . toc:true- branch: master- badges: true- comments: true | author: David Kearney | categories: [timer, jupyter] | description: A timer for ML functions | title: A timer for ML functions | . #collapse-hide from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper . . @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . from collections import Counter import math, random # # data splitting # def split_data(data, prob): &quot;&quot;&quot;split data into fractions [prob, 1 - prob]&quot;&quot;&quot; results = [], [] for row in data: results[0 if random.random() &lt; prob else 1].append(row) return results def train_test_split(x, y, test_pct): data = list(zip(x, y)) # pair corresponding values train, test = split_data(data, 1 - test_pct) # split the dataset of pairs x_train, y_train = list(zip(*train)) # magical un-zip trick x_test, y_test = list(zip(*test)) return x_train, x_test, y_train, y_test # # correctness # def accuracy(tp, fp, fn, tn): correct = tp + tn total = tp + fp + fn + tn return correct / total def precision(tp, fp, fn, tn): return tp / (tp + fp) def recall(tp, fp, fn, tn): return tp / (tp + fn) def f1_score(tp, fp, fn, tn): p = precision(tp, fp, fn, tn) r = recall(tp, fp, fn, tn) return 2 * p * r / (p + r) if __name__ == &quot;__main__&quot;: print(&quot;accuracy(70, 4930, 13930, 981070)&quot;, accuracy(70, 4930, 13930, 981070)) print(&quot;precision(70, 4930, 13930, 981070)&quot;, precision(70, 4930, 13930, 981070)) print(&quot;recall(70, 4930, 13930, 981070)&quot;, recall(70, 4930, 13930, 981070)) print(&quot;f1_score(70, 4930, 13930, 981070)&quot;, f1_score(70, 4930, 13930, 981070)) . favorite_number = 7 def add(a, b): return a + b def sub(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): return a / b def count_vowels(word): count = 0 for letter in word.lower(): count += letter in &#39;aeiou&#39; return count . # import example_module as sm # print(sm.favorite_number) # # add two numbers together # print(sm.add(3, 8)) # # count the number of vowels in a string # print(sm.count_vowels(&#39;Testing&#39;)) . import pandas as pd from alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler config_handler.set_global(theme=&#39;ascii&#39;, spinner=&#39;notes&#39;, bar=&#39;solid&#39;) with alive_bar(3) as bar: df = pd.read_csv(&#39;https://gist.githubusercontent.com/davidrkearney/bb461ba351da484336a19bd00a2612e2/raw/18dd90b57fec46a247248d161ffd8085de2a00db/china_province_economicdata_1996_2007.csv&#39;) bar(&#39;file read, printing file&#39;) print(df.head) bar(&#39;data printed ok, printing methods of data&#39;) print(dir(df)) bar(&#39;process complete&#39;) . from functools import wraps import time def timer(func): &quot;&quot;&quot;[This decorator is a timer for functions] Args: func ([function]): [This decorator takes a function as argument] Returns: [string]: [states the duration of time between the function begining and ending] &quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): print(f&quot;{func.__name__!r} begins&quot;) start_time = time.time() result = func(*args, **kwargs) print(f&quot;{func.__name__!r} ends in {time.time()-start_time} secs&quot;) return result return wrapper @timer def model_metrics(*args, **kwargs): &quot;&quot;&quot;[This is a function to print model metrics of interest] &quot;&quot;&quot; print(&quot;Model ID Number:&quot;, args) print(&quot;Metric of Interest:&quot;, kwargs) model_metrics(1, 2, 10, key=&quot;word&quot;, key2=&quot;word2&quot;, numtrees=&quot;200&quot;) . This post includes code adapted from Data Science from Scratch .",
            "url": "https://davidrkearney.github.io/Kearney_Data_Science/2020/06/07/kwargs-decorators.html",
            "relUrl": "/2020/06/07/kwargs-decorators.html",
            "date": " • Jun 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Senior Data Scientist at CVS Health. | Ph.D. from Duke University | .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://davidrkearney.github.io/Kearney_Data_Science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}